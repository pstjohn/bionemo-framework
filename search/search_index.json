{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>Documentation revamp is in progress.</p> <p>These docs are currently being refactored as we consolidate 5D Parallelism Training code with bionemo-recipes. The best source of documentation today is the module READMEs + your favorite genai assistant.</p> <p>5D Parallel Models Moved to bionemo-recipes</p> <p>The 5D parallel training implementations for ESM-2, Amplify, and Geneformer have been migrated to simplified TransformerEngine + FSDP implementations in bionemo-recipes. For training these models, please refer to the recipes in <code>bionemo-recipes/recipes/</code> (e.g., <code>esm2_native_te</code>, <code>geneformer_native_te_mfsdp_fp8</code>). Model cards and checkpoint information remain available in these docs.</p> <p> NVIDIA BioNeMo Framework is a collection of programming tools, libraries, and models for computational drug discovery.   It accelerates the most time-consuming and costly stages of building and adapting biomolecular AI models by providing domain-specific, optimized models and tooling that are easily integrated into GPU-based computational resources for the fastest performance on the market.   You can access BioNeMo Framework as a free community resource or learn more about getting an enterprise license for improved expert-level support at the   BioNeMo homepage. </p> User Guide <p>Install BioNeMo and set up your environment to start accelerating your bioinformatics workflows.</p> <p> Get Started </p> API Reference <p>Access comprehensive documentation on BioNeMo's sub-packages, functions, and classes.</p> <p> API Reference </p> Models <p>Explore detailed instructions and best practices for using BioNeMo models in your research.</p> <p> Explore Models </p> Datasets <p>Explore biomolecular datasets that come pre-packaged with the BioNeMo Framework.</p> <p> Explore Datasets </p>"},{"location":"SUMMARY/","title":"SUMMARY","text":"<ul> <li>Home</li> <li>BioNeMo Framework</li> <li>Models</li> </ul>"},{"location":"main/","title":"What is BioNeMo?","text":"<p>BioNeMo is a software ecosystem produced by NVIDIA for the development and deployment of life sciences-oriented artificial intelligence models. BioNeMo provides a set of tools to help researchers build, train, and deploy AI models for various biological applications. The main components of BioNeMo are:</p> <ul> <li> <p>BioNeMo Framework: a free-to-use collection of programming tools and packages offering access to optimized, pre-trained biomolecular models and workflows. The framework enables building and customizing models, including training and fine-tuning. Capabilities span various workloads and therapeutic modalities, such as molecular generation, protein structure prediction, protein-ligand, and representation learning.</p> </li> <li> <p>BioNeMo NIMs: easy-to-use, enterprise-ready inference microservices with built-in API endpoints. NIMs are engineered for scalable, self- or cloud-hosted deployment of optimized, production-grade biomolecular foundation models. Check out the growing list of BioNeMo NIMs here.</p> </li> </ul> <p>When choosing between the BioNeMo Framework and BioNeMo NIMs, consider your project's specific requirements. The Framework is ideal for scenarios that require model training, fine-tuning, or customization, offering a comprehensive suite of tools and packages. In contrast, NIMs are optimized for inference-only workflows, providing easy-to-use, enterprise-ready microservices with built-in API endpoints. As a rule, use the Framework for custom model development or high-control modeling, and NIMs for inference against existing models.</p> <p>Get notified of new releases, bug fixes, critical security updates, and more for biopharma. Subscribe.</p>"},{"location":"main/#bionemo-user-success-stories","title":"BioNeMo User Success Stories","text":"<p>Enhancing Biologics Discovery and Development With Generative AI - Amgen leverages BioNeMo and DGX Cloud to train large language models (LLMs) on proprietary protein sequence data, predicting protein properties and designing biologics with enhanced capabilities. By using BioNeMo, Amgen achieved faster training and up to 100X faster post-training analysis, accelerating the drug discovery process.</p> <p>Cognizant to apply generative AI to enhance drug discovery for pharmaceutical clients with NVIDIA BioNeMo - Cognizant leverages BioNeMo to enhance drug discovery for pharmaceutical clients using generative AI technology. This collaboration enables researchers to rapidly analyze vast datasets, predict interactions between drug compounds, and create new development pathways, aiming to improve productivity, reduce costs, and accelerate the development of life-saving treatments.</p> <p>Cadence and NVIDIA Unveil Groundbreaking Generative AI and Accelerated Compute-Driven Innovations - Cadence's Orion molecular design platform will integrate with BioNeMo generative AI tool to accelerate therapeutic design and shorten time to trusted results in drug discovery. The combined platform will enable pharmaceutical companies to quickly generate and assess design hypotheses across various therapeutic modalities using on-demand GPU access.</p> <p>Find more user stories on NVIDIA's Customer Stories and Technical Blog sites.</p>"},{"location":"main/SUMMARY/","title":"SUMMARY","text":"<ul> <li>About</li> <li>Get Started</li> <li>Developer Guide</li> <li>Recipes</li> <li>Tutorials</li> <li>Data Sets</li> <li>Contributing</li> <li>References</li> </ul>"},{"location":"main/about/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Overview</li> <li>Background</li> <li>Release Notes</li> </ul>"},{"location":"main/about/overview/","title":"Overview of BioNeMo","text":"<p>BioNeMo is a software ecosystem produced by NVIDIA for the development and deployment of life sciences-oriented artificial intelligence models. BioNeMo provides a set of tools to help researchers build, train, and deploy AI models for various biological applications. The main components of BioNeMo are:</p> <ul> <li> <p>BioNeMo Framework: A free-to-use collection of programming tools and packages offering access to optimized, pre-trained biomolecular models and workflows. The framework enables building and customizing models, including training and fine-tuning. Capabilities span various workloads and therapeutic modalities, such as molecular generation, protein structure prediction, protein-ligand, and representation learning.</p> </li> <li> <p>BioNeMo NIMs: Easy-to-use, enterprise-ready inference microservices with built-in API endpoints. NIMs are engineered for scalable, self- or cloud-hosted deployment of optimized, production-grade biomolecular foundation models. Check out the growing list of BioNeMo NIMs here.</p> </li> </ul> <p>When choosing between the BioNeMo Framework and BioNeMo NIMs, consider your project's specific requirements. The Framework is ideal for scenarios that require model training, fine-tuning, or customization, offering a comprehensive suite of tools and packages. In contrast, NIMs are optimized for inference-only workflows, providing easy-to-use, enterprise-ready microservices with built-in API endpoints. As a rule, use the Framework for custom model development or high-control modeling, and NIMs for inference against existing models.</p> <p>Get notified of new releases, bug fixes, critical security updates, and more for biopharma. Subscribe.</p>"},{"location":"main/about/overview/#bionemo-user-success-stories","title":"BioNeMo User Success Stories","text":"<p>Enhancing Biologics Discovery and Development With Generative AI - Amgen leverages BioNeMo and DGX Cloud to train large language models (LLMs) on proprietary protein sequence data, predicting protein properties and designing biologics with enhanced capabilities. By using BioNeMo, Amgen achieved faster training and up to 100X faster post-training analysis, accelerating the drug discovery process.</p> <p>Cognizant to apply generative AI to enhance drug discovery for pharmaceutical clients with NVIDIA BioNeMo - Cognizant leverages BioNeMo to enhance drug discovery for pharmaceutical clients using generative AI technology. This collaboration enables researchers to rapidly analyze vast datasets, predict interactions between drug compounds, and create new development pathways, aiming to improve productivity, reduce costs, and accelerate the development of life-saving treatments.</p> <p>Cadence and NVIDIA Unveil Groundbreaking Generative AI and Accelerated Compute-Driven Innovations - Cadence's Orion molecular design platform will integrate with BioNeMo generative AI tool to accelerate therapeutic design and shorten time to trusted results in drug discovery. The combined platform will enable pharmaceutical companies to quickly generate and assess design hypotheses across various therapeutic modalities using on-demand GPU access.</p> <p>Find more user stories on NVIDIA's Customer Stories and Technical Blog sites.</p>"},{"location":"main/about/releasenotes-fw/","title":"Release Notes","text":""},{"location":"main/about/releasenotes-fw/#bionemo-framework-v27","title":"BioNeMo Framework v2.7","text":""},{"location":"main/about/releasenotes-fw/#updates-improvements","title":"Updates &amp; Improvements","text":"<ul> <li> <p>Evo2 model improvements:</p> </li> <li> <p>Context, tensor and data parallelism support in the prediction endpoint as well as support for context lengths over 8192 https://github.com/NVIDIA/bionemo-framework/pull/1123. Fixes https://github.com/NVIDIA/bionemo-framework/issues/910 and https://github.com/NVIDIA/bionemo-framework/issues/1048.</p> </li> <li> <p>LoRA fine-tuning by @gabenavarro: https://github.com/NVIDIA/bionemo-framework/pull/980. Note: internal CI coverage of LoRA convergence is still a work in progress; therefore, we cannot guarantee convergence.</p> </li> <li> <p>Fix a 2x memory-usage issue during Evo2 generation: https://github.com/NVIDIA/NeMo/pull/14515</p> </li> <li> <p>Add flash-decode support in inference: https://github.com/NVIDIA/bionemo-framework/pull/1000</p> </li> <li> <p>Update Rotary Embedding and sequence-length defaults to address incorrect checkpoint conversion: https://github.com/NVIDIA/NeMo/pull/14514</p> </li> <li> <p>Improvements to tag masking in the Evo2 loss: https://github.com/NVIDIA/bionemo-framework/pull/1008</p> </li> <li> <p>Support for Spike-no-more to improve training stability: https://github.com/NVIDIA/bionemo-framework/pull/1011</p> </li> <li> <p>Added a header to SCDL archives, providing improved provenance tracking and supporting future releases. It also adds tracking of AnnData API coverage in SCDL tests.   This header stores metadata about the archive and its composite arrays, including a version; the array lengths and data types; and information about the RowFeatureIndexes. This adds the features necessary to fix https://github.com/NVIDIA/bionemo-framework/issues/999 as well as to implement simple bit-packing of the rowptr, colptr, and data arrays. It should also make SCDL more secure, enable strict compatibility checking, and open the door to further performance improvements: https://github.com/NVIDIA/bionemo-framework/pull/1030</p> </li> <li> <p><code>bionemo-geometric</code> has been deprecated and removed. The molecular-featurization tooling in this package has moved to cuik-molmaker.</p> </li> </ul>"},{"location":"main/about/releasenotes-fw/#known-issues","title":"Known Issues","text":"<ul> <li>We have removed <code>libtiff</code> from the container due to a known vulnerability, CVE-2025-9900. <code>libtiff</code> isn't directly used in any BioNeMo code; however, users might face issues with e.g. Pillow or other common image-manipulation libraries inside this container.</li> </ul>"},{"location":"main/about/releasenotes-fw/#bionemo-framework-v263","title":"BioNeMo Framework v2.6.3","text":""},{"location":"main/about/releasenotes-fw/#updates-improvements_1","title":"Updates &amp; Improvements","text":"<ul> <li>Fixes numerous issues with Evo2 model:</li> <li>Inference/Generation issues resolved. https://github.com/NVIDIA/bionemo-framework/issues/890</li> <li>FP8 training resumption issues resolved. https://github.com/NVIDIA/bionemo-framework/issues/973</li> <li>Bug in inference script that concerns checkpoint loading is fixed. https://github.com/NVIDIA/bionemo-framework/pull/950</li> <li>ESM2 LoRA model inference issue resolved. https://github.com/NVIDIA/bionemo-framework/pull/996</li> <li>Added experimental evo2-mamba model. https://github.com/NVIDIA/bionemo-framework/pull/888</li> <li>Updated base Docker image to nvidia-pytorch 25.06-py3</li> <li>NCCL issue in ESM2 pretraining resolved. https://github.com/NVIDIA/bionemo-framework/issues/970</li> </ul>"},{"location":"main/about/releasenotes-fw/#whats-changed","title":"What's Changed","text":"<ul> <li>Fix test_train_evo2_stops test by @balvisio in https://github.com/NVIDIA/bionemo-framework/pull/965</li> <li>Enable test_train_evo2_stop_at_max_steps_and_continue. by @balvisio in https://github.com/NVIDIA/bionemo-framework/pull/966</li> <li>automated benchmarks: esm2 650M training analogous to bionemo-recipes by @dorotat-nv in https://github.com/NVIDIA/bionemo-framework/pull/975</li> <li>Fix database path in esm2_pretrain_recipes by @pstjohn in https://github.com/NVIDIA/bionemo-framework/pull/978</li> <li>Add fp8 stop and go test for evo2 by @jwilber in https://github.com/NVIDIA/bionemo-framework/pull/974</li> <li>Update Docs Banner for GitHub Pages-hosted Docs by @tshimko-nv in https://github.com/NVIDIA/bionemo-framework/pull/981</li> <li>Add release notes for v2.6.2 (25.06) by @trvachov in https://github.com/NVIDIA/bionemo-framework/pull/971</li> <li>Evo2 Generation fixes and necessary base dependency and container updates. Large change. by @jwilber in https://github.com/NVIDIA/bionemo-framework/pull/949</li> <li>Point NeMo submodule back to main repo by @trvachov in https://github.com/NVIDIA/bionemo-framework/pull/984</li> <li>Use new b2b kernels in evo2 jet tests by @jwilber in https://github.com/NVIDIA/bionemo-framework/pull/985</li> <li>change where dtype is found in checkpoint export by @pstjohn in https://github.com/NVIDIA/bionemo-framework/pull/989</li> <li>Evo2 Mamba by @jstjohn in https://github.com/NVIDIA/bionemo-framework/pull/888</li> <li>Adding inference CDS length tests by @jstjohn in https://github.com/NVIDIA/bionemo-framework/pull/991</li> <li>Fix PIL CVE by @trvachov in https://github.com/NVIDIA/bionemo-framework/pull/992</li> <li>(BIONEMO-2334) Patch TE to fix Evo2 stop and go training by @balvisio in https://github.com/NVIDIA/bionemo-framework/pull/987</li> <li>Fix bug in evo2-mamba train and add test by @jstjohn in https://github.com/NVIDIA/bionemo-framework/pull/994</li> <li>Fix esm2 lora inference by @yzhang123 in https://github.com/NVIDIA/bionemo-framework/pull/996</li> <li>Reset parameters for the ESM-2 contact head on HF export by @pstjohn in https://github.com/NVIDIA/bionemo-framework/pull/983</li> </ul>"},{"location":"main/about/releasenotes-fw/#bionemo-framework-v262","title":"BioNeMo Framework v2.6.2","text":""},{"location":"main/about/releasenotes-fw/#updates-improvements_2","title":"Updates &amp; Improvements","text":"<ul> <li>Fixes numerous ESM2 model issues:</li> <li>Finetuning metric for token classification is fixed. https://github.com/NVIDIA/bionemo-framework/pull/946</li> <li>Losses for finetuning were fixed for data and model parallelism. https://github.com/NVIDIA/bionemo-framework/pull/959</li> <li>Bug in inference script that concerns checkpoint loading is fixed. https://github.com/NVIDIA/bionemo-framework/pull/950</li> <li>Updated base Docker image to nvidia-pytorch 25.04-py3</li> </ul>"},{"location":"main/about/releasenotes-fw/#known-issues_1","title":"Known Issues","text":"<ul> <li>Evo2 generation is broken (i.e. <code>bionemo-evo2/src/bionemo/evo2/run/infer.py</code>). See issue https://github.com/NVIDIA/bionemo-framework/issues/890. A workaround exists on branch https://github.com/NVIDIA/bionemo-framework/pull/949 and we are working to fix this issue for the July release.</li> <li>There is a NCCL communication issue on certain A100 multi-node environments. In our internal testing, we were not able to reproduce the issue reliably across environments. If end users see the following error, please report in issue https://github.com/NVIDIA/bionemo-framework/issues/970 :</li> </ul> <pre><code>[rank9]: torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:3356, internal error - please report this issue to the NCCL developers, NCCL version 2.26.3\n</code></pre>"},{"location":"main/about/releasenotes-fw/#bionemo-framework-v261","title":"BioNeMo Framework v2.6.1","text":""},{"location":"main/about/releasenotes-fw/#updates-improvements_3","title":"Updates &amp; Improvements","text":"<ul> <li>Fixes around ESM2 pretraining and finetuning checkpoints.</li> <li>Added sanity dataset for AMPLIFY testing.</li> <li>Tested against A100 brev instances.</li> <li>Update <code>tornado</code> package to <code>&gt;6.5.0</code> to fix container CVEs.</li> </ul>"},{"location":"main/about/releasenotes-fw/#bionemo-framework-v26","title":"BioNeMo Framework v2.6","text":""},{"location":"main/about/releasenotes-fw/#new-features","title":"New Features","text":"<ul> <li>Adds support for AMPLIFY doi:10.1101/2024.09.23.614603 pre-training and inference, offering a 70% speedup over the xformers-based attention backend with similar final perplexity values at 1M pre-training steps. (4.23 for 120M, 3.05 for 350M). The model is fully compatible with existing weights on HuggingFace.</li> <li>Adds alpha support for LoRA fine-tuning to for ESM2 models. Inference and fine-tuning are enabled along with resumption from a checkpoint.</li> </ul>"},{"location":"main/about/releasenotes-fw/#updates-improvements_4","title":"Updates &amp; Improvements","text":"<ul> <li>Blackwell support, tested on B200 systems.</li> <li>Fixed Grace CPU support, released ARM compatible container.</li> </ul>"},{"location":"main/about/releasenotes-fw/#bionemo-framework-v25","title":"BioNeMo Framework v2.5","text":""},{"location":"main/about/releasenotes-fw/#new-features_1","title":"New Features","text":"<ul> <li>Adding the Evo2 model training workflow, including data preprocessing, pre-training, fine-tuning and inference with bf16 and fp8 support.</li> </ul>"},{"location":"main/about/releasenotes-fw/#updates-improvements_5","title":"Updates &amp; Improvements","text":"<ul> <li>Supporting/upgrading federated learning examples of BioNeMo in NVFlare</li> <li>Upgrade bionemo-moco to v0.0.2</li> <li>Brev.dev launchable tutorials</li> </ul>"},{"location":"main/about/releasenotes-fw/#known-issues_2","title":"Known Issues","text":"<ul> <li>Partial test failures on ARM CPUs.</li> </ul>"},{"location":"main/about/releasenotes-fw/#bionemo-framework-v241","title":"BioNeMo Framework v2.4.1","text":""},{"location":"main/about/releasenotes-fw/#updates-improvements_6","title":"Updates &amp; Improvements","text":"<ul> <li>Applies fixes to ESM2 metric logging that result in NotImplementedError while using Model Parallelism.</li> </ul>"},{"location":"main/about/releasenotes-fw/#bionemo-framework-v24","title":"BioNeMo Framework v2.4","text":""},{"location":"main/about/releasenotes-fw/#new-features_2","title":"New Features","text":"<ul> <li>Draft implementation of Evo2 with support for Hyena operators</li> <li>bionemo-moco v0.0.1 released for building diffusion-like generative models.</li> </ul>"},{"location":"main/about/releasenotes-fw/#known-issues_3","title":"Known Issues","text":"<ul> <li>Partial test failures on ARM CPUs.</li> </ul>"},{"location":"main/about/releasenotes-fw/#updates-improvements_7","title":"Updates &amp; Improvements","text":"<ul> <li>ESM2 fine-tuning script with CLI (finetune_esm2) that supports sequence-level/token-level classification/regression using a CSV dataset.</li> <li>Brev.dev launchable fine-tuning tutorial for ESM2</li> </ul>"},{"location":"main/about/releasenotes-fw/#bionemo-framework-v23","title":"BioNeMo Framework v2.3","text":""},{"location":"main/about/releasenotes-fw/#new-features_3","title":"New Features","text":"<ul> <li>Distributed Inference Support for ESM2 and Geneformer</li> <li>Enables linear inference throughput as GPU number is increased</li> <li>See ESM2 inference notebook and use <code>--num-gpus</code> parameter.</li> </ul>"},{"location":"main/about/releasenotes-fw/#updates-improvements_8","title":"Updates &amp; Improvements","text":"<ul> <li>Prior Geneformer inference on H100 accuracy regression fixed.</li> <li>Base image updated to <code>nvcr.io/nvidia/pytorch:24.12-py3</code>; python updated to 3.12 among other core dependency upgrades (base container release notes here).</li> </ul>"},{"location":"main/about/releasenotes-fw/#bionemo-framework-v22","title":"BioNeMo Framework v2.2","text":""},{"location":"main/about/releasenotes-fw/#new-features_4","title":"New Features","text":"<ul> <li>Small Molecule Featurization</li> <li>Implemented elementary and advanced atom, bond, and full molecule featurizers.</li> <li>GH200 Support for BioNeMo</li> <li>Added a <code>Dockerfile.arm</code> that builds a BioNeMo container that runs on GH200 machines.</li> <li>Publish a version of the BioNeMo container that supports multiple architectures to NGC.</li> </ul>"},{"location":"main/about/releasenotes-fw/#updates-improvements_9","title":"Updates &amp; Improvements","text":"<ul> <li>Single-Cell Dataloader (SCDL)</li> <li>Changed metadata storage to <code>parquet</code> files, which creates a 30x speed up when iterating over a large dataset.</li> <li>Added functionality to concatenate several <code>anndata</code> files without doubling disk memory usage.</li> <li>ESM2</li> <li>Added support for <code>SIGTERM</code> preemption checkpoint saving.</li> <li>Moved ESM-2 and Geneformer training scripts to new executables, <code>train_esm2</code> and <code>train_geneformer</code>, respectively.</li> <li>Moved inference script to a new executable <code>infer_esm2</code>, and deprecated the inference example in the fine-tuning tutorial.</li> <li>Added new Jupyter notebook tutorials for inference and zero-shot protein design. These notebooks can be deployed on the cloud resources as a brev.dev launchable.</li> </ul>"},{"location":"main/about/releasenotes-fw/#known-issues_4","title":"Known Issues:","text":"<ul> <li>Loading a checkpoint for Geneformer inference on H100 has a known regression in accuracy. Work is in progress to resolve by next release.</li> </ul>"},{"location":"main/about/releasenotes-fw/#bionemo-framework-v21","title":"BioNeMo Framework v2.1","text":""},{"location":"main/about/releasenotes-fw/#new-features_5","title":"New Features:","text":"<ul> <li>ESM2 Implementation</li> <li>Updated the ESM-2 Model Card with detailed performance benchmarks comparing BioNeMo2 training against vanilla pytorch.</li> <li>Added ESM-2 inference endpoint for evaluating pre-trained models</li> <li>Size-Aware Batching</li> <li>Added SizeAwareBatchSampler, a pytorch data sampler that batches elements of varying sizes while ensuring that the total size of each batch does not exceed a specified maximum.</li> <li>Added BucketBatchSampler, another pytorch data sampler that groups elements of varying sizes based on predefined bucket ranges, and create batches with elements from each bucket to ensure that each batch has elements with homogeneous sizes.</li> <li>CLI Support</li> <li>Added pydantic interface for pretraining jobs via parsing JSON configuration files that enables passing customized Model and DataModules classes.</li> <li>Implemented pydantic configuration for Geneformer and ESM2 pretraining and finetuning.</li> <li>Added 'recipes' for generating validated JSON files to be used with pydantic interface.</li> <li>Added installable scripts for 2/3 respectively, bionemo-esm2-recipe, bionemo-esm2-train, bionemo-geneformer-recipe, bionemo-geneformer-train.</li> <li>Geneformer support in BioNeMo2:</li> <li>Tested pre-training scripts and fine-tuning example scripts that can be used as a starting point for users to create custom derivative models.</li> <li>Geneformer 10M and 106M checkpoints ported from BioNeMo v1 into BioNeMo v2 available and included in documentation.</li> <li>Added inference scripts</li> <li>Documentation</li> <li>Cell type classification example notebook which covers the process of converting anndata into our internal format, and running inference on that data with a geneformer checkpoint, as well as making use of the inference results.</li> <li>Updated Getting Started guide, ESM-2 tutorials</li> <li>Added Frequently Asked Questions (FAQ) page</li> </ul>"},{"location":"main/about/releasenotes-fw/#bionemo-framework-v20","title":"BioNeMo Framework v2.0","text":""},{"location":"main/about/releasenotes-fw/#new-features_6","title":"New Features:","text":"<ul> <li>ESM-2 implementation</li> <li>State of the art training performance and equivalent accuracy to the reference implementation</li> <li>650M, and 3B scale checkpoints available which mirror the reference model</li> <li>Flexible fine-tuning examples that can be copied and modified to accomplish a wide variety of downstream tasks</li> <li>First version of our NeMo v2 based reference implementation which re-imagines bionemo as a repository of megatron models, dataloaders, and training recipes which make use of NeMo v2 for training loops.</li> <li>Modular design and permissible Apache 2 OSS licenses enables the import and use of our framework in proprietary applications.</li> <li>NeMo2 training abstractions allows the user to focus on the model implementation while the training strategy handles distribution and model parallelism.</li> <li>Documentation and documentation build system for BioNeMo 2.</li> </ul>"},{"location":"main/about/releasenotes-fw/#known-issues_5","title":"Known Issues:","text":"<ul> <li>PEFT support is not yet fully functional.</li> <li>Partial implementation of Geneformer is present, use at your own risk. It will be optimized and officially released in the future.</li> <li>Command line interface is currently based on one-off training recipes and scripts. We are working on a configuration based approach that will be released in the future.</li> <li>Fine-tuning workflow is implemented for BERT based architectures and could be adapted for others, but it requires you to inherit from the biobert base model config. You can follow similar patterns in the short term to load weights from an old checkpoint partially into a new model, however in the future we will have a more direct API which is easier to follow.</li> <li>Slow memory leak occurs during ESM-2 pretraining, which can cause OOM during long pretraining runs. Training with a   microbatch size of 48 on 40 A100s raised an out-of-memory error after 5,800 training steps.</li> <li>Possible workarounds include calling <code>gc.collect(); torch.cuda.empty_cache()</code> at every ~1,000 steps, which appears     to reclaim the consumed memory; or training with a lower microbatch size and re-starting training from a saved     checkpoint periodically.</li> </ul>"},{"location":"main/about/releasenotes-fw/#bionemo-framework-v19","title":"BioNeMo Framework v1.9","text":""},{"location":"main/about/releasenotes-fw/#new-features_7","title":"New Features","text":"<ul> <li>[Documentation] Updated, executable ESM-2nv notebooks demonstrating: Data preprocessing and model training with custom datasets, Fine-tuning on FLIP data, Inference on OAS sequences, Pre-training from scratch and continuing training</li> <li>[Documentation] New notebook demonstrating Zero-Shot Protein Design Using ESM-2nv. Thank you to @awlange from A-Alpha Bio for contributing the original version of this recipe!</li> </ul>"},{"location":"main/about/releasenotes-fw/#bug-fixes-and-improvements","title":"Bug fixes and Improvements","text":"<ul> <li>[Geneformer] Fixed bug in preprocessing due to a relocation of dependent artifacts.</li> <li>[Geneformer] Fixes bug in finetuning to use the newer preprocessing constructor.</li> </ul>"},{"location":"main/about/releasenotes-fw/#bionemo-framework-v18","title":"BioNeMo Framework v1.8","text":""},{"location":"main/about/releasenotes-fw/#new-features_8","title":"New Features","text":"<ul> <li>[Documentation] Updated, executable MolMIM notebooks demonstrating: Training on custom data, Inference and downstream prediction, ZINC15 dataset preprocesing, and CMA-ES optimization</li> <li>[Dependencies] Upgraded the framework to NeMo v1.23, which updates PyTorch to version 2.2.0a0+81ea7a4 and CUDA to version 12.3.</li> </ul>"},{"location":"main/about/releasenotes-fw/#bug-fixes-and-improvements_1","title":"Bug fixes and Improvements","text":"<ul> <li>[ESM2] Fixed a bug in gradient accumulation in encoder fine-tuning</li> <li>[MegaMolBART] Make MegaMolBART encoder finetuning respect random seed set by user</li> <li>[MegaMolBART] Finetuning with val_check_interval=1 bug fix</li> </ul>"},{"location":"main/about/releasenotes-fw/#known-issues_6","title":"Known Issues","text":"<ul> <li>Minor training speed regression observed for models DNABERT, Geneformer, MolMIM</li> <li>Two known critical CVEs GHSA-cgwc-qvrx-rf7f, GHSA-mr7h-w2qc-ffc2. The vulnerabilities arise within a package that's installed by lightning by default. We do not use that package in bionemo framework container. we are also unable to remove the package in question as it's installed as a side-effect of installing lightning.</li> <li>Two known High CVEs from pytorch : GHSA-pg7h-5qx3-wjr3, GHSA-5pcm-hx3q-hm94.</li> </ul>"},{"location":"main/about/releasenotes-fw/#bionemo-framework-v17","title":"BioNeMo Framework v1.7","text":""},{"location":"main/about/releasenotes-fw/#new-models","title":"New Models","text":"<ul> <li>DSMBind, developed under the BioNeMo framework, is a model which can produce comparative values for ranking protein-ligand binding affinities. This release features the capability to perform inference using a newly trained checkpoint.</li> </ul>"},{"location":"main/about/releasenotes-fw/#new-features_9","title":"New Features","text":"<ul> <li>[EquiDock] Remove steric clashes as a post-processing step after equidock inference.</li> <li>[Documentation] Updated Getting Started section which sequentially describes prerequisites, BioNeMo Framework access, startup instructions, and next steps.</li> </ul>"},{"location":"main/about/releasenotes-fw/#known-issues_7","title":"Known Issues","text":"<ul> <li>There is a known security vulnerability with NLTK that can allow for arbitrary code execution via pickle files that are external assets downloaded via nltk.download() (https://github.com/nltk/nltk/issues/3266). BioNeMo itself does not use this dependency in any way, however parts of NeMo text-to-speech (nemo.collections.tts) does use this vulnerable codepath. Since NeMo is installed in the BioNeMo release containers, users are urged to exercise caution when using nemo.collections.tts or nltk.</li> </ul>"},{"location":"main/about/releasenotes-fw/#bionemo-framework-v16","title":"BioNeMo Framework v1.6","text":""},{"location":"main/about/releasenotes-fw/#new-features_10","title":"New Features","text":"<ul> <li>[Model Fine-tuning] <code>model.freeze_layers</code> fine-tuning config parameter added to freeze a specified number of layers. Thank you to github user @nehap25!</li> <li>[ESM2] Loading pre-trained ESM-2 weights and continue pre-training on the MLM objective on a custom FASTA dataset is now supported.</li> <li>[OpenFold] MLPerf feature 3.2 bug (mha_fused_gemm) fix has merged.</li> <li>[OpenFold] MLPerf feature 3.10 integrated into bionemo framework.</li> <li>[DiffDock] Updated data loading module for DiffDock model training, changing from sqlite3 backend to webdataset.</li> </ul>"},{"location":"main/about/releasenotes-fw/#bionemo-framework-v15","title":"BioNeMo Framework v1.5","text":""},{"location":"main/about/releasenotes-fw/#new-models_1","title":"New Models","text":"<ul> <li>Geneformer is out of Beta status. This release includes newly trained checkpoints and benchmarks, including a variant based on the publication with 10M parameters, and the largest variant of geneformer publically available to date with 106M parameters.</li> </ul>"},{"location":"main/about/releasenotes-fw/#bionemo-framework-v14","title":"BioNeMo Framework v1.4","text":""},{"location":"main/about/releasenotes-fw/#new-models_2","title":"New Models","text":"<ul> <li>Beta Geneformer a foundation model for single-cell data that encodes each cell as represented by an ordered list of differentially expressed genes for that cell.</li> </ul>"},{"location":"main/about/releasenotes-fw/#new-features_11","title":"New Features","text":"<ul> <li>Beta Geneformer pretraining with custom datasets</li> <li>Low-Rank Adaptation (LoRA) finetuning for ESM2</li> </ul>"},{"location":"main/about/releasenotes-fw/#bug-fixes-and-improvements_2","title":"Bug fixes and Improvements","text":"<ul> <li>OpenFold training improved benchmarks and validation of optimizations</li> </ul>"},{"location":"main/about/releasenotes-fw/#known-issues_8","title":"Known Issues","text":"<ul> <li>BioNeMo Framework v24.04 container is vulnerable to GHSA-whh8-fjgc-qp73 in onnx 1.14.0. Users are advised not to open untrusted onnx files with this image. Restrict your mount point to minimize directory traversal impact. A fix for this is scheduled in the 24.05 (May) release.</li> </ul>"},{"location":"main/about/releasenotes-fw/#bionemo-framework-v13","title":"BioNeMo Framework v1.3","text":""},{"location":"main/about/releasenotes-fw/#new-models_3","title":"New Models","text":"<ul> <li>MolMIM implementation under BioNeMo framework, a small molecule model developed at NVIDIA which can be used to produce embeddings and novel molecules.</li> </ul>"},{"location":"main/about/releasenotes-fw/#new-features_12","title":"New Features","text":"<ul> <li>MolMIM re-trained on more data is now available in the framework, and achieves state of the art performance.</li> <li>MolMIM property guided tutorial notebook covering property guided optimization using our new framework model.</li> <li>MolMIM training tutorial available walking users through either training from scratch or from an existing checkpoint on your own data.</li> <li>MolMIM tutorial notebook covering molecular sampling and property prediction is also now available.</li> <li>Numerous optimizations from NVIDIA's entry to the MLPerf competition have been added to OpenFold. Documentation and detailed benchmarks are works in progress and will be published in upcoming releases. This release contains the following performance optimizations:</li> <li>Fused GEMMs in multi-head attention (MHA)</li> <li>Non-blocking data pipeline</li> <li>BF16 precision training</li> <li>Fused MHA gating</li> <li>Inductor Compiled LayerNorm</li> <li>OpenAI Triton LayerNorm kernels</li> <li>OpenAI Triton MHA</li> </ul>"},{"location":"main/about/releasenotes-fw/#bug-fixes-and-improvements_3","title":"Bug fixes and Improvements","text":"<ul> <li>NeMo upgraded to v1.22 (see NeMo release notes),</li> <li>PyTorch Lightning upgraded to 2.0.7</li> <li>NGC CLI has been removed from the release container. If users   download models from inside the container (e.g. using <code>bionemo_data_download</code> or via running specific unit tests),   the NGC CLI will be auto-installed to pull the models from NGC.</li> </ul>"},{"location":"main/about/releasenotes-fw/#known-issues_9","title":"Known Issues","text":"<ul> <li>BioNeMo Framework v24.03 container is vulnerable to GHSA-whh8-fjgc-qp73 in onnx 1.14.0. Users are advised not to open untrusted onnx files with this image. Restrict your mount point to minimize directory traversal impact.</li> </ul>"},{"location":"main/about/releasenotes-fw/#bionemo-framework-v12","title":"BioNeMo Framework v1.2","text":""},{"location":"main/about/releasenotes-fw/#new-models_4","title":"New Models","text":"<ul> <li>OpenFold implementation under BioNeMo framework, derived from public OpenFold and DeepMind AlphaFold-2.</li> <li>DNABERT implementation for computing embeddings for each nucleotide in the input DNA sequence.</li> </ul>"},{"location":"main/about/releasenotes-fw/#new-features_13","title":"New Features","text":"<ul> <li>Training recipes for DNABERT and OpenFold, including automated data processing and full configuration for training.</li> <li>Example tutorials for running inference using OpenFold.</li> <li>Splice Prediction downstream task example for DNABERT.</li> <li>Wrapper scripts for DNABERT and OpenFold to launch jobs on BCP.</li> </ul>"},{"location":"main/about/releasenotes-fw/#bug-fixes-and-improvements_4","title":"Bug fixes and Improvements","text":"<ul> <li>Interface improvements for ESM-2 data ingestion and pre-processing. The interface allows for explicit specification of training, validation, and test sets. The user may set <code>config.model.data.default_dataset_path</code> to maintain prior behavior, or set <code>config.model.data.train.dataset_path</code>, <code>config.model.data.val.dataset_path</code>, <code>config.model.data.test.dataset_path</code> which may all be unique.</li> </ul>"},{"location":"main/about/releasenotes-fw/#known-issues_10","title":"Known Issues","text":"<ul> <li>OpenFold training speed does not yet include MLPerf optimizations, and these will be released in the subsequent release.</li> </ul>"},{"location":"main/about/releasenotes-fw/#bionemo-framework-v11","title":"BioNeMo Framework v1.1","text":""},{"location":"main/about/releasenotes-fw/#new-models_5","title":"New Models","text":"<ul> <li>EquiDock for protein-protein docking pose prediction</li> <li>DiffDock for protein-ligand blind docking pose generation</li> </ul>"},{"location":"main/about/releasenotes-fw/#new-features_14","title":"New Features","text":"<ul> <li>Training recipes for EquiDock and DiffDock, including automated data processing and full configuration for training.</li> <li>Accelerated inference and training for DiffDock via fast tensor-product kernels.</li> <li>Example tutorials for running inference using EquiDock and DiffDock.</li> <li>Recipes for running EquiDock and DiffDock on BCP and Slurm.</li> <li>Pipeline parallel supported for ESM-2nv.</li> <li>Migration of inference notebooks to using pytriton.</li> </ul>"},{"location":"main/about/releasenotes-fw/#bug-fixes-and-improvements_5","title":"Bug fixes and Improvements","text":"<ul> <li>Faster pre-processing of data on BCP.</li> <li>Refactor of download_models.sh to download_models.py for easier CLI use.</li> <li>Refactor of install structure to move from /opt/nvidia to /workspace/bionemo. The environment variable $BIONEMO_HOME now points to the repo base and is required to be set for tests to pass.</li> </ul>"},{"location":"main/about/releasenotes-fw/#security-notice","title":"Security Notice","text":"<p>SchedMD Slurm in the release container is shipped with a security vulnerability, CVE-2022-29501, and therefore this version of Slurm should not be used to run a Slurm cluster (specifically, the processes <code>slurmdbd</code>, <code>slurmctld</code>, and <code>slurmd</code>.</p> <p>In general, the BioNeMo Framework release is designed to ship code and an environment that would be executed on local workstations, or deployed on clusters for large scale training jobs. This container is not designed to run as a service with public facing APIs. A full summary of security vulnerabilities can be found here.</p>"},{"location":"main/about/releasenotes-fw/#bionemo-framework-v10","title":"BioNeMo Framework v1.0","text":""},{"location":"main/about/releasenotes-fw/#new-models_6","title":"New Models","text":"<ul> <li>ESM-2nv for protein sequence representations, pretrained weights of ESM-2 650M and ESM-2 3B converted from HF checkpoint available.</li> </ul>"},{"location":"main/about/releasenotes-fw/#new-features_15","title":"New Features","text":"<ul> <li>Pre-training recipes for ESM-2nv, including automated data processing and full configuration for training</li> <li>Fine-tuning of ESM-2nv with encoder frozen or trainable</li> <li>Downstream task finetuning support for single-value classification (e.g. subcellular localization), single-value regression (e.g. meltome) and per-token classification (e.g. secondary structure)</li> <li>Validation in loop to evaluate performance on downstream tasks during training</li> <li>Example tutorials for pre-training, fine tuning, and downstream tasks</li> </ul>"},{"location":"main/about/releasenotes-fw/#bionemo-framework-v040","title":"BioNeMo Framework v0.4.0","text":""},{"location":"main/about/releasenotes-fw/#new-models_7","title":"New Models","text":"<ul> <li>ESM-1nv for protein sequence representations, pretrained weights available</li> <li>ProtT5nv for protein sequence representation and sequence-to-sequence tasks, pretrained weights available</li> </ul>"},{"location":"main/about/releasenotes-fw/#new-features_16","title":"New Features","text":"<ul> <li>Pre-training for all models, including automated data processing and full configuration for training</li> <li>Fine-tuning of MegaMolBART, ESM-1nv, and ProtT5nv with encoder frozen or trainable</li> <li>Downstream task example applications \u2013 secondary structure prediction for ESM-1nv and ProtT5nv, physchem prediction (lipophilicity, FreeSolv, ESOL) and retrosynthesis prediction for MegaMolBART</li> <li>Validation in loop to evaluate performance on downstream tasks during training: physchem prediction (MegaMolBART) and secondary structure prediction (ESM-1nv and ProtT5nv).</li> <li>Pipeline parallelism supported as a beta feature. Not fully tested.</li> <li>Example notebooks for pre-training, fine tuning, and downstream tasks</li> </ul>"},{"location":"main/about/releasenotes-fw/#known-issues_11","title":"Known Issues","text":"<ul> <li>Data preprocessing on DGX Cloud is slow. Faster to do it on a local machine.</li> </ul>"},{"location":"main/about/releasenotes-fw/#new-apis","title":"New APIs","text":"<ul> <li>BioNeMoDataModule - Encapsulates dataset instantiation in bionemo models so that many different datasets can be used with the same model</li> <li>EncoderFineTuning - Base class to facilitate implementation of downstream tasks built on embeddings from other models</li> </ul>"},{"location":"main/about/background/SUMMARY/","title":"SUMMARY","text":"<ul> <li>NeMo2</li> <li>Megatron Dataset Considerations</li> </ul>"},{"location":"main/about/background/megatron_datasets/","title":"Writing Megatron-LM Compatible Datamodules","text":"<p>Megatron-LM relies on determinism in the training dataset classes to ensure that input tensors are initialized correctly across model-parallel ranks (see NeMo2 Parallelism). As a consequence, ensure that the new dataset classes preserve the required determinism. Common operations such as data augmentation and masking can cause <code>dataset[i]</code> to return random results for a given index, breaking this megatron contract.</p>"},{"location":"main/about/background/megatron_datasets/#multi-epoch-training","title":"Multi-Epoch Training","text":"<p>One training regime where this limitation is most apparent is multi-epoch training, where standard training recipes would apply different random masks or different data augmentation strategies each time the data is encountered. BioNeMo provides some utilities that make multi-epoch training easier, while obeying the determinism requirements of megatron.</p> <p>The MultiEpochDatasetResampler class simplifies the process of multi-epoch training, where the data should both be re-shuffled each epoch with different random effects applied each time the data is seen. To be compatible with this resampler, the provided dataset class's <code>__getitem__</code> method should accept a EpochIndex tuple that contains both an epoch and index value. Random effects can then be performed by setting the torch random seed based on the epoch value:</p> <pre><code>class MyDataset:\n    def __getitem__(self, idx: EpochIndex):\n        rng = torch.Generator()\n        rng.manual_seed(idx.epoch)\n        ...\n</code></pre> <p>Avoid <code>torch.manual_seed</code></p> <pre><code>Megatron-LM handles torch seeding internally. Calling `torch.cuda.manual_seed` inside the user-provided dataset\ncan cause issues with model parallelism. See [megatron/core/tensor_parallel/random.py#L198-L199](\nhttps://github.com/NVIDIA/Megatron-LM/blob/dddecd19/megatron/core/tensor_parallel/random.py#L198-L199) for more\ndetails.\n</code></pre> <p>For deterministic datasets that still want to train for multiple epochs with epoch-level shuffling, the IdentityMultiEpochDatasetWrapper class can simplify this process by wrapping a dataset that accepts integer indices and passes along the EpochIndex index values from the resampled dataset.</p> <pre><code>class MyDeterministicDataset:\n    def __getitem__(self, index: int): ...\n\n\ndataset = IdentityMultiEpochDatasetWrapper(MyDeterministicDataset())\nfor sample in MultiEpochDatasetResampler(dataset, num_epochs=3, shuffle=True):\n    ...\n</code></pre>"},{"location":"main/about/background/megatron_datasets/#training-resumption","title":"Training Resumption","text":"<p>To ensure identical behavior with and without job interruption, BioNeMo provides MegatronDataModule to save and load state dict for training resumption, and provides [WrappedDataLoader][nemo.lightning.data.WrappedDataLoader] to add a <code>mode</code> attribute to [DataLoader][torch.utils.data.DataLoader].</p> <pre><code>class MyDataModule(MegatronDataModule):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        ...\n\n    def train_dataloader(self):\n        self.update_init_global_step()  # required to set the correct `global_step` for resumption\n        return WrappedDataLoader(\n            ...,  # any other arguments for DataLoader\n            mode=\"train\",\n        )\n\n    def val_dataloader(self):\n        self.update_init_global_step()  # required to set the correct `global_step` for resumption\n        return WrappedDataLoader(\n            ...,  # any other arguments for DataLoader\n            mode=\"validation\",\n        )\n\n    def test_dataloader(self):\n        self.update_init_global_step()  # required to set the correct `global_step` for resumption\n        return WrappedDataLoader(\n            ...,  # any other arguments for DataLoader\n            mode=\"test\",\n        )\n</code></pre> <p>MegatronDataModule</p> <pre><code>Users will see non-overlapping training curve if their datamodule is not inheritting from `MegatronDataModule`, unless similar logics are handled by the users. In `MegatronDataModule`, `self.update_init_global_step()` must be called right before the dataloaders are returned to ensure that training resumes with the correct sample index instead of restarting from 0 everytime. We recommend users to inherit from `MegatronDataModule` similar to the pattern above.\n</code></pre> <p>WrappedDataLoader</p> <pre><code>The `WrappedDataLoader` class is a wrapper around the PyTorch DataLoader class that adds the `mode` attribute to the dataloader. The dataloader will resume from the last sample index only when mode is 'train'. `val_dataloader` and `test_dataloader` are unaffected.\n\nWARNING: 'train' is the default value of `mode` in `WrappedDataLoader`. If not set, users might find their validation/test dataloader changes behavior by resuming from a non-zero sample index.\n</code></pre>"},{"location":"main/about/background/megatron_datasets/#testing-datasets-for-megatron-compatibility","title":"Testing Datasets for Megatron Compatibility","text":"<p>BioNeMo also provides utility functions for test suites to validate that datasets conform to the megatron data model. The [assert_dataset_compatible_with_megatron][bionemo.testing.data_utils.assert_dataset_compatible_with_megatron] function calls the dataset with identical indices and ensures the outputs are identical, while also checking to see if <code>torch.manual_seed</code> was used.</p> <p>Example datasets in BioNeMo</p> <pre><code>The [ESMMaskedResidueDataset][bionemo.esm2.data.dataset.ESMMaskedResidueDataset] demonstrates one approach for\nleveraging [EpochIndex][bionemo.core.data.multi_epoch_dataset.EpochIndex] indices to perform epoch-level\nrandomization within the confines of megatron's data model.\n</code></pre>"},{"location":"main/about/background/nemo2/","title":"NeMo2","text":""},{"location":"main/about/background/nemo2/#checkpointing","title":"Checkpointing","text":"<p>In NeMo, there are two distinct mechanisms for continuing training from a checkpoint: resuming from a training directory and restoring from a checkpoint.</p> <p>Note: If both <code>--result-dir</code> and <code>--ckpt-dir</code> are provided, checkpoints in <code>--result-dir</code> take precedence. The <code>--ckpt-dir</code> is only used if <code>--result-dir</code> contains no checkpoints.</p> <p>1. Resuming Training from a Directory</p> <p>When a training job runs, NeMo saves checkpoints in a designated results directory specified with the <code>--result-dir</code> flag. If the same job is restarted and a checkpoint exists in that directory, the most recent checkpoint is automatically loaded and training continues from the exact step and optimizer state stored there.</p> <p>If no checkpoint is found in the results directory:</p> <ul> <li>No checkpoint directory specified \u2192 training starts from scratch.</li> <li>Checkpoint dir is specified by <code>--ckpt-dir</code> \u2192 NeMo attempts to restore from that checkpoint (see \"Restoring from a   Checkpoint\" below).</li> </ul> <p>2. Restoring from a Checkpoint</p> <p>To start a new training run initialized from a checkpoint in a different directory, the restore configuration can be set to point to that checkpoint via the <code>--ckpt-dir</code> flag. NeMo will begin training from that checkpoint\u2019s weights and optimizer state. After the initial restoration, subsequent runs of the same job follow the standard resuming flow - loading from the results directory \u2014 without repeating the restore step.</p> <pre><code>               +-------------------------+\n               | Start Training Job      |\n               +-----------+-------------+\n                         |\n     +-------------------+-------------------+\n     |                                       |\nResults dir has                         Results dir empty\ncheckpoint \u2192 Resume                           |\n                                             v\n                                   +----------------------------+\n                                   | Checkpoint dir specified?  |\n                                   +-------------+--------------+\n                                             |\n                              +----------------+----------------+\n                              |                                 |\n                         No \u2192 Start from scratch        Yes \u2192 Restore\n                                                       from checkpoint\n                                                                 \u2193\n                                                            Resume flow\n</code></pre>"},{"location":"main/about/background/nemo2/#parallelism","title":"Parallelism","text":"<p>NeMo2 represents tools and utilities to extend the capabilities of <code>pytorch-lightning</code> to support training and inference with megatron models. While pytorch-lightning supports parallel abstractions sufficient for LLMs that fit on single GPUs (distributed data parallel, aka DDP) and even somewhat larger architectures that need to be sharded across small clusters of GPUs (Fully Sharded Data Parallel, aka FSDP), when you get to very large architectures and want the most efficient pretraining and inference possible, megatron-supported parallelism is a great option.</p> <p>So in other words, NeMo2 adds the Megatron strategy in addition to the standard DDP and FSDP strategies.</p> <p>Many downstream constraints and conventions are driven by the underlying constraints of megatron.</p>"},{"location":"main/about/background/nemo2/#deeper-background-on-megatron","title":"Deeper Background on Megatron","text":""},{"location":"main/about/background/nemo2/#other-options-for-parallelizing-smaller-models","title":"Other Options for Parallelizing Smaller Models","text":"<p>Megatron is a system for supporting advanced varieties of model parallelism. While vanilla models can be executed in parallel with systems, such as distributed data parallel (DDP), or moderately large models can be trained with Meta's Fully Sharded Data Parallel (FSDP/FSDP2), when you work with larger models and want to train them with maximal efficiency, it is ideal to use some variant of megatron.</p>"},{"location":"main/about/background/nemo2/#ddp-background","title":"DDP Background","text":"<p>DDP is the best option when you can fit the entire model on every GPU in your cluster. With DDP, you can parallelize your <code>global batch</code> across multiple GPUs by splitting it into smaller <code>mini-batches</code>, one for each GPU. Each GPU computes the forward and backward pass independently for its subset of data, allowing for maximal utilization. Synchronization of gradients occurs after the backward pass is complete for each batch, followed by a weight update that ensures all GPUs have synchronized parameters for the next iteration. Here is an example of how this might appear on your cluster with a small model:</p> <p></p>"},{"location":"main/about/background/nemo2/#fsdp-background","title":"FSDP Background","text":"<p>FSDP extends DDP by sharding (splitting) model weights across GPUs in your cluster to optimize memory usage. While data is still split across GPUs in the same way as DDP, FSDP strategically synchronizes and broadcasts the necessary shards of model weights to all GPUs just-in-time for computation during the forward pass.</p> <p>For example, when a layer is needed for computation, the owning GPU sends that shard of weights to the other GPUs, which then perform the forward computation on that layer. After the computation is complete, FSDP frees the memory for that layer on all GPUs except the one that owns the shard. This process continues iteratively for each layer until the entire model has been executed on the data.</p> <p>Note: This process parallelizes the storage in a way that enables too large models to be executed (assuming a single layer is not too large to fit on a GPU). Megatron (next) co-locates both storage and compute.</p> <p>The following two figures show two steps through the forward pass of a model that has been sharded with FSDP.  </p>"},{"location":"main/about/background/nemo2/#model-parallelism","title":"Model Parallelism","text":"<p>Model parallelism is the catch-all term for the variety of different parallelism strategies that could be applied to parallelizing your model across a cluster. Below we explain several varieties of model parallelism that are implemented in megatron. As mentioned in the previous section, one key advantage to the megatron-specific parallelism types described next are that they co-locate storage and compute of the layers. Inefficiencies caused by naive scheduler implementations are also addressed (discussed in the section on schedulers).</p>"},{"location":"main/about/background/nemo2/#pipeline-parallelism","title":"Pipeline Parallelism","text":"<p>Pipeline parallelism is similar to FSDP, but the model blocks that are sharded are also computed in parallel on the nodes that own the model weight in question. You can think of this as a larger simulated GPU that happens to be spread across several child GPUs. Examples of this include <code>parallel_state.is_pipeline_last_stage()</code>, which is commonly used to tell if a particular node is on last pipeline stage, where you compute the final head outputs or loss. </p> <p>Similarly, there are convenience environmental lookups for the first pipeline stage (where you compute the embedding for example) <code>parallel_state.is_pipeline_first_stage()</code>.</p>"},{"location":"main/about/background/nemo2/#tensor-parallelism","title":"Tensor Parallelism","text":"<p>Tensor parallelism represents splitting single layers across GPUs. This can also solve the problem where some individual layers could in theory be too large to fit on a single GPU, where FSDP would not be possible. This would still work since individual layer weights (and computations) are distributed. Examples of this in megatron include <code>RowParallelLinear</code> and <code>ColumnParallelLinear</code> layers. </p>"},{"location":"main/about/background/nemo2/#sequence-parallelism","title":"Sequence Parallelism","text":"<p>In megatron, \"sequence parallelism\" refers to the parallelization of the dropout, and layernorm blocks of a transformer. The idea is roughly as follows. First, remember that in a typical transformer architecture, the <code>embedding_dimension</code> is the only dimension that <code>LayerNorm</code> is applied over. Similarly, Dropout (outside of the attention block) is an operation that is applied on the last embedding dimension. These two layers are independent over the sequence dimension, so they can be processed in blocks on separate GPUs. As can be seen in the following figure, the initial <code>LayerNorm</code> in a multi-headed transformer block is executed in parallel. Next the results are gathered for the self attention and linear layers (which are typically set up for tensor parallelism). Next the result from those layers is scattered back to sequence parallel nodes which execute dropout, do a residual connection from the previous sequence parallel output, and a layernorm. Next those results are again gathered for the final FFN and activation layers prior to a final scattering across sequence parallel GPUs for the output of that transformer block. </p> <p>As a user, if you know that your transformer is executed in parallel and you have custom losses or downstream layers, you need to make sure that the appropriate gather operations are occurring for your loss computation etc.</p>"},{"location":"main/about/background/nemo2/#context-parallelism","title":"Context Parallelism","text":"<p>Context parallelism extends sequence parallelism by also parallelizing the attention mechanism itself, similar to Ring Attention. In general, if you are using a transformer, context parallelism is going to perform better than sequence parallelism for very long input sequences. That said, due to the necessity of all-gather and reduce scatter operations throughout the architecture, the general advice that you should avoid these kinds of parallelism if a micro-batch fits on a single device still holds. Splitting across elements in a global batch represent the fewest necessary communications between GPUs on your cluster, so standard DDP should run the fastest if you can get your training loop for a micro batch to fit on one GPU.</p>"},{"location":"main/about/background/nemo2/#mixing-parallelism-strategies","title":"Mixing Parallelism Strategies","text":"<p>You can mix different kinds of parallelism together to achieve a better result. In general, experimentation should be done to identify the optimal mix of parallelism. See this YouTube tutorial from Jared Casper for more background on megatron parallelism strategies.</p> <p>The figure below demonstrates how mixing strategies results in larger \"virtual GPUs\", which similarly means you have fewer distinct micro-batches in flight across your cluster. Note that the number of virtual GPUs is multiplicative so if you have <code>TP=2</code> and <code>PP=2</code>, then you are creating a larger virtual GPU out of <code>2*2=4</code> GPUs, so your cluster size needs to be a multiple of 4 in this case. </p>"},{"location":"main/about/background/nemo2/#scheduling-model-parallelism","title":"Scheduling Model Parallelism","text":"<p>You can improve on naive schedules by splitting up micro-batches into smaller pieces, executing many stages of the model on single GPUs, and starting computing the backwards pass of one micro-batch while another is going through forward. These optimizations allow for better cluster GPU utilization to be achieved. For example, the following figure shows how more advanced splitting techniques in megatron (for example, the interleaved scheduler) offer better utilization when model parallelism is used. As best possible, we don't recommend using model parallelism (DDP). </p>"},{"location":"main/contributing/code-review/","title":"Code Review","text":"<p>This document describes the process and etiquette for code review in the BioNeMo repo. You should read this document if you are a developer working in the BioNeMo repo.</p> <p>The purpose of these guidelines is to help reduce the friction between engineers writing code and those reviewing code. As with many rules, there are exceptions. These exceptions are not comprehensive, so if you find exceptions that should be listed, please raise them so they can be evaluated for their inclusion.</p>"},{"location":"main/contributing/code-review/#code-review-process","title":"Code Review Process","text":"<p>The code review process is progressive:</p> <ol> <li>Review by your team</li> <li>Review by domain experts (CODEOWNERS)</li> <li>Approval by Approval-list users.</li> <li>(Optional) Coverage Check approval by Approval-list users.</li> </ol>"},{"location":"main/contributing/code-review/#1-team-review","title":"1. Team Review","text":"<p>You should first ask contributors to review your change. The contributing team can provide the most contextualized feedback, and this review step is where most issues with the change should be caught and addressed.</p> <p>Proceed to the next step after you've addressed your team's comments and have received an approval. There is no actual requirement in gitlab to receive your team-based approval - it is simply best practice.</p>"},{"location":"main/contributing/code-review/#2-owner-review","title":"2. Owner Review","text":"<p>Code owners are domain experts for a particular part of the repository. They are typically the original authors of a set of source files. Code ownership structures tend to mirror team structures; a single team is often responsible for a functional component.</p> <p>You must receive approval from at least one owner of every file you change. Unless the file(s) do not have any owners specified in the <code>CODEOWNERS</code> file.</p> <p>If your change only modifies files owned by your team, owner review happens implicitly in the team review step (i.e. in many cases you may already have this requirement satisfied after step 1 above).</p>"},{"location":"main/contributing/code-review/#3-approval","title":"3. Approval","text":"<p>You must also receive approval for your change. Approval indicates that the change is ready to be merged, and is the final step in the review process. Self approval is strictly forbidden.</p> <p>Approval is granted by an approver, in the form of an approval stamp in gitlab. Approvers review an entire change, in contrast to owners, who focus on reviewing the files that they own.</p> <p>To find an approver, first ask your team if there is a preferred approver. You can also check in #clara-discovery-bionemo-dev.</p> <p>*If your team has an approver, approval will usually be granted shortly after your team has had a chance to review your change.</p> <p>*Often, a single review from one person will be sufficient to complete the entire review process. If the reviewer is on your team, is an approver, and is an owner of the files you modified, then the process may collapse to a single review.</p>"},{"location":"main/contributing/code-review/#4-coverage-check","title":"4. Coverage Check","text":"<p>Our repository has automated checks to ensure test coverage has not regressed. The coverage check approvers will be the same as the Approval-list users. If codeline test coverage regresses, the Approvers must make a judgement call whether it is acceptable or not to merge the code. Occasionally the coverage check algorithm has a false positive (i.e. code coverage doesn't regress, yet coverage check approval is flagged by gitlab), and in this case Approvers are free to simply approve the false coverage regression.</p>"},{"location":"main/contributing/code-review/#responsibilities","title":"Responsibilities","text":""},{"location":"main/contributing/code-review/#all-commenters-reviewers-owners-approvers-etc","title":"All Commenters (Reviewers, Owners, Approvers, etc.)","text":"<p>If a comment thread is started by anyone, it is expected that the thread starter resolves the comment. Resolving a thread by the original thread starter indicates that the person who started the discussion is happy with the outcome.</p>"},{"location":"main/contributing/code-review/#reviewers","title":"Reviewers","text":"<p>All developers can and should review changes. These reviews are fundamental to maintaining the quality of the codebase. Reviews are also an important way to stay aware of what people are working on and increase the bus factor for areas of the code.</p> <p>Reviewers should always do the following when reviewing code:</p> <ul> <li> <p>Be respectful.</p> </li> <li> <p>Assume best intentions.</p> </li> <li> <p>Review the code, not the person.</p> </li> <li> <p>Leave clear, actionable feedback. Use comments in gitlab to signal that you expect   changes to be made, and explicitly enumerate them. If you don't, it can leave the author of the patch wondering if   your comments are optional. Requesting code changes should not be interpreted as a   judgment of the change, but rather as an indicator that the   reviewer wants to engage with the author to turn it into an   approval.</p> </li> </ul> <p>A more detailed etiquette guide can be found later in this document.</p>"},{"location":"main/contributing/code-review/#owners","title":"Owners","text":"<p>As an owner, you have the ability to delay code from being merged by withholding your signoff. This ability comes with important responsibilities:</p> <ul> <li> <p>A duty to respond to reviews in a timely manner. If you can't stay   on top of review requests, you should relinquish ownership.</p> </li> <li> <p>A bias to \"yes\". \"No\", by itself, is never an acceptable answer.   When you reject a change you must work with the change author to   arrive at a mutually agreeable solution.</p> </li> </ul>"},{"location":"main/contributing/code-review/#approvers","title":"Approvers","text":"<p>As an approver, you have additional responsibilities beyond that of an owner:</p> <ul> <li> <p>You are responsible for the change and the effect it has on the   codebase.</p> </li> <li> <p>You must ensure that reviews have been performed by the appropriate   reviewers, that these reviews were not rushed.</p> </li> <li> <p>If a change breaks something, you are expected to be actively   involved in the cleanup.</p> </li> </ul> <p>You should always decline to approve a change if you're unfamiliar with the areas of code it touches.</p>"},{"location":"main/contributing/code-review/#becoming-an-owner","title":"Becoming an Owner","text":"<p>Code ownership information is stored in CODEOWNERS file. Since these CODEOWNERS file are stored in the repository, changing them follows the same process as changing code.</p> <p>To become an owner, add your github username to the CODEOWNERS file that you want to be a part of, and submit the change to Gitlab. The change to the CODEOWNERS file will follow the same review process as any other code change. The existing owners will decide whether to delegate ownership to you or not.</p>"},{"location":"main/contributing/code-review/#becoming-an-approver","title":"Becoming an Approver","text":"<p>As an approver, you are responsible for ensuring the consistency and integrity of our code base. Before becoming an approver, study this document so that you are completely familiar with the responsibilities of reviewers and approvers. Additionally, make sure that you are intimately familiar with our coding style guides and best practices:</p> <ul> <li>Contributing</li> <li>In addition, make sure that you understand and can apply all elements of the     Google Python style guide, which we adhere     to for all Python code</li> </ul>"},{"location":"main/contributing/code-review/#details-on-etiquette-for-good-citizens","title":"Details on Etiquette for Good Citizens","text":"<p>In the following sections, we provide deeper thoughts and recommendations for everyone contributing to the repo, in order to have a fruitful interaction across the team members.</p>"},{"location":"main/contributing/code-review/#patch-changes-sizes-and-policies","title":"Patch changes, sizes, and policies","text":"<ul> <li> <p>It takes 30-45 minutes to review every 100-200 lines of code. Be   mindful of the size of changes you are introducing and try to keep   your patch under 500 lines of code whenever possible. As a change   grows in size (lines of codes) the reviewer's ability to find   issues diminishes.</p> </li> <li> <p>As a corollary, for a refactor-type change, consider separating     \"no-logic-change\" and \"logic-change\" into distinct reviews     (perhaps in a dependent review change) to ease the pattern     matching burden on your reviewers.</p> </li> <li> <p>All reviewers may request an PR is too large if it is larger than   500 lines of net code addition. The only exception are MRs into   the <code>bionemo2/contrib</code> directory, where larger MRs are permissible.   This includes lines of code, but not something such as dummy data or   a fake dataset that may contain thousands of lines of stuff that is not   actually functional code.</p> </li> <li> <p>Each patch should be kept to one logical change, which should be   described in the title of the patch. Unrelated changes should be   split out into separate patches. Fixing whitespace on a line   you're editing is reasonable. Fixing whitespace around the code   you're working on should be a separate 'cleanup' patch.</p> </li> <li> <p>Where possible, larger patches (&gt;500 LOC) should be split into   multiple smaller patches that are consistent individually. Test   your patches before submitting them to Gitlab via gitlab pipelines or locally.   The more you test before submitting your patch for review, the better. It's also   appreciated if you add a line to the commit message describing how   the patch was tested. This prevents people from having to ask   whether and how the patch was tested. Stating that the patch was   not tested is also fine, although you might be asked to do some   testing in cases where that would be reasonable.</p> </li> <li> <p>Abandon patches that are no longer useful or that you don't intend   to keep working on.</p> </li> <li> <p>Follow code styling and rules stated in the project's documents   (for example, contributing.md, of which the Google Python   Style Guide is a subset) as these define the   look and feel of the code which defines the most fundamentals of how the code should be   developed and allows reviewers to focus on the most important aspects of a new piece of code.   For bash scripting please follow the Google Shell Style Guide here</p> </li> <li> <p>We follow a revert + fix policy in the codebase for any showstopper   bug that might appear as a result of an PR introducing errors not   caught by sanity. In exceptional circumstances when an MR cannot be   reverted and there is a hotfix ready, leadership can consider   merging without revert. However, this should be an exception.   Failures policies are described in more depth here.</p> </li> </ul>"},{"location":"main/contributing/code-review/#review-timelines","title":"Review timelines","text":"<ul> <li> <p>In general, patches should remain open for review for at least 24   hours since the last significant modification to the change. The   purpose is to let developers around the world have a chance to   review. Complex reworks, even if they don't change the purpose of   the patch but the way it's implemented, should restart the waiting   period.</p> </li> <li> <p>Speedy changes: A change can go in without the waiting period if its   purpose is to fix a recently-introduced issue that has not been   possible to revert. In that case, the commit message has to   explain what change introduced the problem and the nature of the   problem so that the emergency need becomes apparent. The change   itself should be as limited in scope and impact as possible to   make it simple to assess the impact.</p> </li> <li> <p>Trivial changes that deal with minor issues like inconsistencies in   whitespace or spelling fixes that don't impact the final binary   output also don't need to wait for the round of the world reviews.   Such changes should point out in their commit messages how the   author verified that the binary output is identical. Note that   trivial fixes shouldn't necessarily be expedited: Just like   they're not critical enough for things to go wrong because of   them, they're not critical enough to require quick handling.</p> </li> </ul>"},{"location":"main/contributing/code-review/#reviewers_1","title":"Reviewers","text":"<ul> <li> <p>Do not approve if you have not reviewed the code you were asked to   review. Spend time reviewing or re-assign to someone else. Someone   that approves the change must review the entire change holistically   If you are a code owner of a particular file, it is appropriate to only reviews the files you own.</p> </li> <li> <p>If you request a PR to change their code, you are responsible for giving concrete   recommendations for what could be changed to resolve the issue the   patch addresses. If you feel strongly that a patch should NEVER be   merged, you are responsible for defending your position and   listening to other points of view. Asking for changes and walking away is   not acceptable, and may cause your approval status to be removed by the   leadership team.</p> </li> <li> <p>Include justification for critique: When you review code, you should   always try to include justification for your critique, unless that   critique is a nit, a style guide violation, or an obvious bug.   Nits and style guide violations tend to overlap, and you shouldn't   have to justify the use of a shared style guide or things like   proper spelling. Likewise, you shouldn't have to justify bug-free   code. However, when it comes to design choices you should always   include justification for when you might want to change certain   things</p> </li> <li> <p>If there have been comments or discussion on a patch, verify that   the comments have been addressed before giving an approval. If you feel   that a comment is invalid, please respond to that comment instead   of just ignoring it.</p> </li> <li> <p>Be conscientious when approving patches. As the arbiter, you need to make sure the MR   is complete, that proper reviews are done, that all the required   tests have passed, that API changes have been reviewed by the API   owners. Please make sure that the necessary convergence tests and unit tests   have passed and have not regressed. If KPI regression (convergence tests) is found, you may need to   consult with other stakeholders before approving the MR. In some   cases a Cl will require convergence testing. Owners/ Approvers   should make sure convergence testing is performed when necessary and   that no new issues are identified. And that overall, the code   changes are sound and integrate well with the rest of the modules   and systems. In the event that the patch breaks things, you are   expected to be actively involved in the cleanup effort and support   the authors by reverting and speeding the fixes. This means you   shouldn't approve a patch just because you trust the author of a   patch - Make sure you understand what the implications of a patch   might be, or leave the review to others. Partial reviews,   reviewing code style, for example, can be given a positive review or a LGTM.   This also applies if you think the patch looks good, but may   not have the experience to know if there may be unintended   consequences.</p> </li> <li> <p>Please make sure that the code changes are covered by the existing   unit tests and if necessary ask the contributor to add or update   tests.</p> </li> </ul>"},{"location":"main/contributing/code-review/#contributors","title":"Contributors","text":"<ul> <li> <p>Before providing a patch for review ask yourself these questions:</p> </li> <li> <p>Is this PR the right size? If it's too long break it up.</p> </li> <li> <p>Is this MR and all the changes included necessary? (all code has     to be maintained)</p> </li> <li> <p>Does this MR duplicate existing functionality? If yes, can I     extend what is there?</p> </li> <li> <p>Is the code readable? Am I using esoteric language constructs     that affect readability? Does the code follow the conventions     of the codebase?</p> </li> <li> <p>Is the MR production ready? In other words - does it have tests,     documentation, error handling, etc, etc.</p> </li> <li> <p>Bring attention to patches that you would like reviewed. Add   reviewers, ask for reviewers on slack or even just rebase it   against the current codebase to bring it to the top of the Gitlab   list. If you're not sure who would be a good reviewer, gitlab will suggest OWNERS based   on the CODEOWNERS file in the UI or look at the   git history of the files that you've changed, and add those   people. For NIM-API based changes there is a small team managing these   therefore seek for those people to review APIs.</p> </li> <li> <p>Try to coordinate with other significant contributors to the code   when making changes to areas you do not own. Before you type a   single line of code!. These people made the most significant   changes to that part of the code and therefore are knowledgeable   of any tradeoffs to be made. Coming with new code already written   will cause painful back and forth and will be less efficient for   all. Learn the design, propose changes and get an agreement before   making changes.</p> </li> <li> <p>Don't modify other people's branches unless you have coordinated this   with the owner of that branch. Not only is this considered rude,   but your changes could be unintentionally lost. An exception to   this would be for branches that have not been updated for more than   90 days and therefore can be considered orphaned.</p> </li> <li> <p>Respond to anyone who has taken the time to review your branches,   even if it's just to say that you disagree. While it may seem   annoying to address a request to fix spelling or 'trivial' issues,   it's generally easy to handle in Gitlab's built-in editor. If you   do use the built-in editor, remember to get that change to your   local copy before re-pushing. It's also acceptable to add fixes   for these sorts of comments to another branch, but it's recommended   that that branch be pushed to Gitlab before the initial branch gets   submitted.</p> </li> <li> <p>Check if there's documentation that needs to be updated to remain   current after your change. If there's no documentation for the   part you're working on, consider adding some.</p> </li> <li> <p>When contributing a significant change to core parts of the code   base, or when introducing a new way of doing something that you   think is worthwhile to apply across the tree, please bring up your   design doc to the commit, so reviewers can read it.</p> </li> <li> <p>Don't expect that people will review your patch unless you ask them   to. Adding other people as reviewers is the easiest way.   But also you can use Slack to actively ping the reviewers, which   is especially useful for urgent MRs.</p> </li> <li> <p>Don't expect people to drop all of what they are doing to review   your patch. Everyone has a day-time job, and while code reviews   are part of that job, they usually do not extend the full working   day.</p> </li> <li> <p>Do not resolve (ack/Done) any comments open by others so they can   find their comments easily and resolve them. Unless being told to   self-resolve.</p> </li> <li> <p>As a contributor you are responsible for the code you bring in and   any failures being caught during unit or convergence testing. Etc.   You should monitor that your code gets merged successfully and   that no errors have appeared after the code has been merged   (nightly testing / convergence testing) and respond promptly to address   any failures.</p> </li> </ul>"},{"location":"main/contributing/code-review/#general-etiquette","title":"General Etiquette","text":"<ul> <li> <p>We try to assume the best of each other in this community. It's okay   to discuss mistakes (e.g., isolated instances of non-trivial and   non-critical changes submitted early) but try to keep such   inquiries blameless. If a change leads to problems with the code,   the focus should be on fixing the issue, not on assigning blame.</p> </li> <li> <p>Be respectful to others when commenting on branches. Comments should   be kept to the code and should be kept in a polite tone. Assume   your colleagues are intelligent and do not intend any malice or   disrespect. Resist the urge to retaliate against perceived verbal   misconduct, such behavior is not conducive to getting branches   merged. Also, avoid absolute and aggressive language as this can   tend to escalate emotions. Comments are meant to be collaborative.   Very often, the commenter might also be incorrect since they may not have   the full story of the patch in mind since they were not the author. A comment   is not a demand, it's a suggestion towards a mutually acceptable solution   between the author and the reviewer.</p> </li> <li> <p>Don't submit code that you know will break other   platforms/dependencies. If your patch affects code that is used by   others, it should be compatible with those. While it would be nice   to update any other dependents, you must at least provide a path   that will allow other platforms to continue working.</p> </li> <li> <p>Don't write commit messages that are vague or wouldn't make sense to   partners that read the logs. For example, do not write \"[topic]   Bugfix\" as your header in the commit message. Keep links to videos   out of the commit message. Again, partners are going to see these   logs and it does not make sense to link to something they will not   have access to view. Keep your commit messages under 72 chars in   length per line. Good commits have an appropriate topic with a   nice one-liner that explains the change briefly. This is then   followed by a few sentences or more on a description of the   change. Be professional in your language and do not put things in   the message that a partner would not understand. Avoid the use of   acronyms, abbreviations, or codenames, especially those meaningful   only at nvidia. Also, use correct English (check your spelling   please). This pertains to the final commit message after the branch   is squashed. Intermediary commit messages do not matter at all. Commits   must be squashed on merge.</p> </li> <li> <p>Consider breaking up large individual patches into smaller patches   grouped by areas. This makes the patches easier to review but   increases the number of patches. The way this is handled is a   personal decision, as long as each patch is still one logical   change.</p> </li> <li> <p>Contributions made to the <code>bionemo/contrib</code> folder have more flexible requirements. All requirements not mentioned below still   apply to the contrib folder. These exceptions are</p> </li> <li> <p>Code line numbers limit is increased to 2200 lines.</p> </li> <li>Unit test coverage requirements are decreased to 65% coverage.</li> <li>Code line length of \\&lt;=120 character spaces is acceptable.</li> <li> <p>Commit messages do not need have as verbose of an explanation.     All other requirements pertaining to approver, contributor, and reviewer responsibility still apply.</p> </li> <li> <p>Reviews are about the code. It's easy to take it personally when   someone is criticizing your code, but the whole idea is to get   better code into our codebase. Again, this also applies in the   other direction: review code, critically think about and respond   to the code, but don't make it personal.</p> </li> <li> <p>Don't develop on a personal branch for a long time and then dump a   large number of files and lines of code into a review as a new   \"feature.\" Features should be developed off of the main trunk just   like any other change. It is even more important to follow   processes as the code becomes more critical. Features can   be broken into small working changes and don't need to be   developed all at once. Creating large changes like this leads to   less efficiency in the review process and can lead to more   mistakes.</p> </li> <li> <p>In BioNeMo, for features under development that are not ready for   production, we put them inside the <code>bionemo2/contrib</code> folder. This allows for teams to develop   faster (less strict reviews) while testing code. When a feature is   complete and well tested, we move it to <code>bionemo2/src</code> or   <code>bionemo2/core</code> and we complete all the requirements for production.</p> </li> </ul>"},{"location":"main/contributing/code-review/#references","title":"References","text":"<ul> <li>Coreboot gerrit guidelines (heavily - inspired the etiquette portion of this document)</li> <li>Code Review Culture</li> <li>Proven practices for peer review</li> <li>https://confluence.nvidia.com/display/DS/SDK+Code+reviews</li> <li>https://confluence.nvidia.com/display/DS/SDK+Best+Practices (Review and source control sections)</li> </ul>"},{"location":"main/contributing/contributing/","title":"Contributing Guidelines","text":"<p>Note</p> <p>For code review standards please see the Code Review page.</p> <pre><code>For all PRs, an approved NVIDIA staff member must sign off and trigger the continuous integration (CI) tests.\nThese are initiated by the member commenting `/build-ci` directly on the PR. All PRs must have successful CI runs and\nsufficient code review before being merged.\n</code></pre>"},{"location":"main/contributing/contributing/#developer-certificate-of-origin-dco","title":"Developer Certificate of Origin (DCO)","text":"<p>We require that all contributors \"sign-off\" on their commits (not GPG signing, just adding the <code>-s | --signoff</code> argument, or follow the instructions below for auto-signing). This sign-off certifies that you adhere to the Developer Certificate of Origin (DCO) (full text); in short that the contribution is your original work, or you have rights to submit it under the same license or a compatible license.</p> <p>Any contribution which contains commits that are not signed-off will not be accepted.</p> <p>To sign off on a commit, simply use the <code>--signoff</code> (or <code>-s</code>) option when committing your changes:</p> <pre><code>git commit -s -m \"Add cool feature.\"\n</code></pre> <p>This will append the following to your commit message:</p> <pre><code>Signed-off-by: Your Name &lt;your@email.com&gt;\n</code></pre> <p>If you would like this to happen automatically to all of your commits, you can modify your local <code>~/.git-commit-template.txt</code> file. You can do this with a command like the following:</p> <pre><code>echo \"Signed-off-by: Your Name &lt;your@email.com&gt;\" &gt; ~/.git-commit-template.txt\ngit config --local commit.template ~/.git-commit-template.txt\n</code></pre> <p>If you have a commit that you want to retroactively sign, you can do that with:</p> <pre><code>git commit --amend --no-edit --signoff\n</code></pre>"},{"location":"main/contributing/contributing/#python-coding-standards","title":"Python Coding Standards","text":"<p>This page contains the Python coding standards for the BioNeMo repository. They apply to all Python code in the repository (unless external constraints prevent it).</p>"},{"location":"main/contributing/contributing/#coding-style","title":"Coding Style","text":"<ul> <li>We follow the Google Python Style Guide with a few tweaks.</li> <li>The most important parts of this style guide that our code must adhere to are:</li> <li>Docstring</li> <li>Mutable global state</li> <li>Do not use mutable values as default arguments</li> <li>Default iterators</li> <li>Bad naming / abbreviation</li> <li>The exceptions to this style guide are:</li> <li>Module imports. If a module is uniquely named, import     the module. Otherwise, import the value, type, or function directly.</li> <li>Linting and formatting of all code is required by using <code>ruff</code> with BioNeMo's configured options.</li> <li>Unit testing with <code>pytest</code>. See Unit Tests for more details.</li> <li>Add type annotations everywhere. In particular, new code should all be type-annotated as thoroughly as possible. This   also obviates the need for including type hints in the function docstring. It is ok to omit annotations for private   helper functions, but use your best judgement.</li> <li>Include docstrings for every class, function, and method exposed to the user.</li> <li>Docstrings should answer (a) what is the code doing and (b) why would someone use it.</li> <li>Never use wildcard imports.</li> <li>Define <code>__all__ = (,)</code> in modules: make explicit the API of each module, auto-documenting the most important definitions.</li> <li>Minimize the use of <code>**kwargs</code>.</li> <li><code>raise</code> an <code>Exception</code> instead of using an <code>assert</code> statement.</li> <li>F-strings are preferred to format strings.</li> <li>Loggers are preferred to print. In BioNeMo, you can use logger from <code>import logging</code>.</li> <li>Private functions (functions starting with <code>_</code>) shouldn't be called outside its host file.</li> </ul>"},{"location":"main/contributing/contributing/#general-guidelines","title":"General Guidelines","text":"<ul> <li>User-oriented: make it easy for end users, even at the cost of writing more code in the background</li> <li>Robust: make it hard for users to make mistakes.</li> <li>Well-tested: please add simple, fast unit tests. See Unit Tests.</li> <li>Reusable: for every piece of code, think about how it can be reused in the future and make it easy to reuse.</li> <li>Readable: code should be easy to read and well documented (with comments and docstrings).</li> <li>Legal: if you copy even one line of code from the Internet, make sure that the code allows the license that   BioNeMo supports. Give credit and link back to the code.</li> <li>Sensible: code should make sense. If you think a piece of code might be confusing, write comments.</li> <li>Consistent: we work in a team. It is important to integrate changes with existing code.</li> <li>Readable: your code should be easy to read and understand by any other engineer, including outside NVIDIA. Some   tips:</li> <li>Document your code. Make all comments complete sentences, starting with a capitalized letter and ending with a     period.</li> <li>Avoid abbreviations: 'bn' is harder to understand than 'batch_norm'.</li> <li>Avoid baked-in constants throughout the code. Instead, specify them as parameters to your function. If you must have     a constant, follow the naming guideline (e.g., <code>GLOBAL_CONSTANT</code>).</li> <li>Avoid functions that span hundreds of lines. Large functions are more difficult to read and more difficult to test.     If &gt;120 lines, consider re-factoring it into smaller logical functions, each unit-tested and well-documented.</li> <li>Re-use code by importing. Do not copy and paste code.</li> <li>Usage of third-party code should be legally compatible and attributed.</li> </ul>"},{"location":"main/contributing/contributing/#pull-request-pr-guidelines","title":"Pull Request (PR) Guidelines","text":""},{"location":"main/contributing/contributing/#labeling-your-pr-as-external-contributor","title":"Labeling Your PR as External Contributor","text":"<p>If you are an external contributor (not an NVIDIA employee), please add the <code>contribution</code> label to your PR before submitting. Labels can be accessed in the right sidebar of the GitHub user interface when creating or editing a PR.</p>"},{"location":"main/contributing/contributing/#ci-pipeline-configuration-controls","title":"CI Pipeline Configuration Controls","text":"<p>CI pipeline behavior can be controlled by labels to optimize test execution:</p> <p>Key behaviors:</p> <ul> <li>Labels are processed automatically on PR submit/update</li> <li>Invalid combinations default to most restrictive option</li> </ul> <p>By default, CI pipeline is enabled for all PRs and only unit tests are run. To skip CI pipeline, add the <code>ciflow:skip</code> label</p>"},{"location":"main/contributing/contributing/#ciflowskip","title":"ciflow:skip","text":"<ul> <li>Skips entire CI pipeline</li> <li>Use for documentation typos, README updates</li> </ul>"},{"location":"main/contributing/contributing/#ciflowslow","title":"ciflow:slow","text":"<ul> <li>Run slow single GPU integration tests marked as @pytest.mark.slow for bionemo2</li> <li>Use when modifying core functionalities and require extensive moderate complexity testing on a single GPU</li> <li>Disabled by default</li> </ul>"},{"location":"main/contributing/contributing/#ciflownotebooks","title":"ciflow:notebooks","text":"<ul> <li>Enables Jupyter notebooks validation tests under <code>./docs</code> subfolder and <code>./sub-packages/*</code></li> <li>Use when modifying notebooks or notebook-related code</li> <li>Disabled by default</li> </ul>"},{"location":"main/contributing/contributing/#ciflowall","title":"ciflow:all","text":"<ul> <li>Run all tests (unit tests, slow tests, and notebooks) for bionemo2.</li> <li>Without this label, unit tests for bionemo2 in PR CI are run only on when the bionemo2 codebase has been modified</li> <li>Use when introducing significant codebase changes for comprehensive testing</li> <li>Disabled by default</li> </ul>"},{"location":"main/contributing/contributing/#ciflowall-recipes","title":"ciflow:all-recipes","text":"<ul> <li>Run tests for all recipes (under bionemo-recipes). This label can be used to enforce running tests for all recipes.</li> <li>Without this label, unit tests for recipes in PR CI are run only on folders whose codebases have been modified</li> </ul>"},{"location":"main/contributing/contributing/#developer-workflows","title":"Developer Workflows","text":"<p>You should always carefully test your changes. Run <code>pytest ...</code> in your container locally. All tests are done via <code>pytest</code>.</p> <p>Changes that affect model training accuracy or compute performance should be tested on SLURM.</p> <p>Developer workflow for external code contributions is as follows:</p> <ol> <li> <p>External developers must first fork the    upstream BioNeMo OSS repository and for BioNeMo2 (this branch)    use the <code>main</code> branch as base.</p> </li> <li> <p>Clone the forked repository and push changes to the personal fork.</p> </li> </ol> <pre><code>git clone https://github.com/YOUR_USERNAME/YOUR_FORK.git bionemo-framework\n# Checkout the targeted branch and commit changes\n# Push the commits to a branch on the fork (remote).\ngit push -u origin &lt;local-branch&gt;:&lt;remote-branch&gt;\n</code></pre> <p>Developer workflow for internal or those developers that have been granted push access to our repository is as follows:</p> <ol> <li>Clone this repository locally</li> <li>Create a branch which ideally should be of the form <code>username/branch_description</code></li> <li>Push branch up to our repository <code>git push -u origin HEAD</code></li> </ol> <p>For both internal and external developers, the next step is opening a PR:</p> <ol> <li>Once the code changes are staged on the fork and ready for review, a    Pull Request (PR) can be    requested to merge the changes from a branch of the    fork or branch into <code>main</code>.</li> <li>Exercise caution when selecting the source and target branches for the PR.      Note that versioned releases of BioNeMo are posted to <code>release/</code> branches of the upstream repo.</li> <li>Creation of a PR kicks off the code review process.</li> <li>At least one BioNeMo engineer will be assigned for the review.</li> <li>While under review, mark your PRs as work-in-progress by prefixing the PR title with [WIP].</li> <li>Once ready, CI can be started by a developer with permissions when they add a <code>/build-ci</code> comment. This must pass    prior to merging.</li> </ol>"},{"location":"main/contributing/contributing/#general-guidelines_1","title":"General Guidelines","text":"<p>Send your PRs to the <code>main</code> branch. Branch off from <code>main</code> when making your changes. Prefix your branches with your name or initials (for example, <code>your_name/branch_description</code>) if you have push access to our repository otherwise please create a fork with your branch and submit a PR with <code>main</code> as the target.</p> <ul> <li>Make sure your PR does one thing. Have a clear answer to \"What does this PR do?\"</li> <li>Make sure you have the linters enabled via pre-commit hooks (<code>pre-commit install</code>) (See also Pre-commit   validation)</li> <li>Follow the default PR template</li> <li>Make sure all unit tests finish successfully before running PR pipeline by invoking <code>pytest scripts sub-packages</code>.</li> <li>Make sure you added necessary tests and documentation changes (could be just comments in the config files) for the   feature in your PR</li> <li>Rebase your feature branch with the latest <code>main</code> to include any new changes that have been added. Resolve merge   conflicts, if any</li> <li>Send your PR and request a review</li> <li>If your PR is still a work in progress, mark it as \"Draft\"</li> <li>Your merge request must pass all pipelines and be peer-reviewed before it can be merged.</li> <li>Make sure to merge your PR when it is ready and pipeline is successful</li> </ul>"},{"location":"main/contributing/contributing/#unit-tests","title":"Unit Tests","text":"<p>Contributors to BioNeMo FW are expected to unit test their introduced changes.</p> <p>After testing your code locally, trigger tests in the PR's CI. Let a code-owner know that you are ready for the build to run and they will leave a <code>/build-ci</code> comment on your PR which will run the CI test suite.</p>"},{"location":"main/contributing/contributing/#adding-unit-tests","title":"Adding Unit Tests","text":"<p>Add unit tests under <code>tests</code> to examine use cases of new classes or methods that are being added to the codebase. Each test file must be for a particular file or module. For example if you have a file that is under <code>src/path/to/module/my_file_name.py</code> then your test should match the path at <code>tests/path/to/module/test_my_file_name.py</code>. Check the tests folders in the sub-modules of this repository for examples. If you are testing a module, such as integrating multiple examples of different files, then you can use the following pattern to test the module, say in the above example, if you wanted to test functions from several files together that all exist in the same <code>src/path/to/module</code> then you could create a <code>tests/path/to/test_module.py</code> file. The same is true for parents of that module and so on. Generally unit tests should exist at the level of the individual file however.</p>"},{"location":"main/contributing/contributing/#pre-commit-validation","title":"Pre-Commit Validation","text":"<p>We use pre-commit for essential static checks. These checks are enforced on new PRs through the CI process, but should also be run locally. After following the installation instructions for pre-commit, run <code>pre-commit install</code> in the bionemo-framework repository to initialize the checks.</p> <p>To run pre-commit checks (and fix errors where possible), run <code>pre-commit run --all-files</code>. To ignore a pre-commit error locally, use <code>git commit -n ...</code> to allow the commit to proceed with some failing pre-commit checks.</p>"},{"location":"main/contributing/contributing/#updating-license-header-on-python-files","title":"Updating License Header on Python Files","text":"<p>If you add new Python (<code>.py</code>) files, be sure to run our license-check. If you have not already done sone, please install the dev-requirements.txt. If you are working directly inside a release container, you may need to manually install these. We recommend using the developer container for contributions.</p> <pre><code>pip install -r dev-requirements.txt --user\npython ci/scripts/license_check.py --modify --replace --license-header ./license_header -c sub-packages/ -c docs/ -c scripts/ -c ci/ -c internal/\n</code></pre>"},{"location":"main/contributing/contributing/#updating-the-secrets-baseline-file","title":"Updating the secrets baseline file","text":"<p>If false-positives are raised by the detect-secrets pre-commit hook, they can be added to the baseline files by running the following commands:</p> <pre><code>detect-secrets scan --baseline .secrets.baseline --exclude-files '(.*\\.ipynb|.*\\.baseline)$'\ndetect-secrets scan --baseline .secrets-nb.baseline --exclude-files '^.(?!.*\\.ipynb)' --exclude-lines '\"(hash|id|image/\\w+)\":.*'\n</code></pre> <p>The resulting altered baseline files should then be committed.</p>"},{"location":"main/contributing/contributing/#contributing-python-sub-packages-to-bionemo-framework","title":"Contributing Python Sub-Packages to BioNeMo Framework","text":""},{"location":"main/contributing/contributing/#requirements","title":"Requirements","text":"<ul> <li>The sub-package should be located in <code>bionemo-framework/sub-packages</code>.</li> <li>The sub-package should have a <code>pyproject.toml</code> or equivalent package, a <code>VERSION</code> file dynamically linked to the <code>pyproject.toml</code>, a <code>README.md</code> that documents the functionality, usage, and structure of the modules in the package, and a <code>LICENSE</code>.</li> <li>Source code should be placed into <code>src/bionemo/&lt;package-name-suffix&gt;</code> and testing code should be placed into <code>tests/...</code> following the exact same directory structure as the source code that is tested, i.e. <code>src/bionemo/evo2/run/train.py</code> should have unit tests located in <code>tests/bionemo/evo2/run/test_train.py</code> for organizational purposes.</li> <li>Unit tests not associated with source code in BioNeMo can be placed anywhere reasonable under <code>tests/bionemo/&lt;package-name-suffix&gt;</code>.</li> <li>Verify that the <code>pyproject.toml</code> is <code>pip install</code>-able (and <code>python -m build</code>-able).</li> <li>If the sub-package is publishable, follow the instructions in Publishing to PyPI to register or link your package to the sub-package workflow in BioNeMo Framework.</li> <li>Add test dependencies to a <code>test</code> field under <code>[project.optional-dependencies]</code> for test-only dependencies.</li> </ul>"},{"location":"main/contributing/contributing/#publishing-to-pypi","title":"Publishing to PyPI","text":"<p>To publish your sub-package via \"Trusted Publishing\" to PyPI, you can follow the directions specified here: Publishing PyPI Package Distribution Releases using GitHub Actions CI/CD Workflows</p> <ul> <li>Log in to PyPI and Test PyPI.</li> <li>Go to <code>Account Settings &gt; Publishing</code>, and either...</li> <li>If your package already exists on PyPI: <code>Manage [Package Name] &gt; Add a New Publisher</code>.</li> <li>If your package does not yet exist on PyPI: <code>Add a New Pending Publisher</code>.</li> <li>Under the <code>GitHub</code> tab of the publisher management page, input the following information:</li> <li>Owner: <code>NVIDIA</code></li> <li>Repository Name: <code>bionemo-framework</code></li> <li>Workflow: <code>bionemo-subpackage-ci.yml</code></li> <li>Environment Name:<ul> <li><code>pypi</code> for PyPI</li> <li><code>testpypi</code> for Test PyPI</li> </ul> </li> <li>NVIDIA-Only: Run the workflow! For more information, refer to: Sub-Package GitHub Actions Workflow</li> <li>Optional: Add <code>bionemo</code> as an owner or maintainer of the PyPI package if you want help maintaining it.</li> <li>Disclaimer: If this is not done, and the package becomes dysfunctional, then NVIDIA / BioNeMo are not responsible for the health of the package or the sub-package source code, because we will not have the ability to deprecate versions, etc.</li> </ul>"},{"location":"main/contributing/contributing/#sub-package-github-actions-workflow","title":"Sub-Package GitHub Actions Workflow","text":"<ul> <li>Dispatch the <code>bionemo-subpackage-ci.yml</code> workflow from GitHub Actions to test, build, and publish your sub-packages to PyPI!</li> <li>Required: Input a comma-separated list of sub-packages you want to test and/or publish into <code>subpackages</code>.<ul> <li>For example, <code>bionemo-moco,bionemo-llm,bionemo-webdatamodule</code>. The sub-packages will be tested and published in separate parallel environments.</li> </ul> </li> <li>Optional: Set <code>test</code> to <code>true</code> if you want to test your sub-package. (Default: <code>true</code>)<ul> <li>Sub-packages that require pre- or post- installation steps may require modification of the <code>install-and-test</code> job in <code>bionemo-framework/.github/workflows/bionemo-subpackage-ci.yml</code>.</li> <li>Supported <code>pyproject.toml</code> Optional Dependencies: [ <code>te</code> ]</li> </ul> </li> <li>Optional: Set <code>publish</code> to <code>true</code> if you want to publish to Test PyPI or PyPI. (Default: <code>false</code>)<ul> <li>Pre-Requisite: BioNeMo Publishing to PyPI</li> <li>Publishing requires package building, but does not require testing for flexibility of package management.</li> </ul> </li> <li>Optional: Publishes to Test PyPI by default. To publish to PyPI, check <code>Publish to PyPI instead of TestPyPI</code>.</li> <li>Optional: Overwrite the published version of the sub-package on PyPI.<ul> <li>Not recommended, because overwriting a published version will break the <code>pip cache</code> for all users. They will need to re-install the updated package.</li> </ul> </li> <li>Optional: Python-wrapped (PyO3) Rust-based sub-packages are supported with <code>maturin</code> if you set <code>build_framework</code> to <code>rust_pyo3_maturin</code>.</li> <li>Optional: Python, CUDA, Linux, and hardware architecture types can be specified if your sub-package is only supported on a specific ecosystem.</li> </ul>"},{"location":"main/contributing/contributing/#faq","title":"FAQ","text":"<ul> <li>What do I do if I want to test and publish two updated sub-packages that depend on each other?</li> <li>To deal with circular dependencies, publish one package to PyPI first, followed by testing and publishing the other. <code>pip</code> installs dependencies in reverse topological order, and will resolve / break circular dependencies as long as dependency conflicts do not exist. (If dependency conflicts exist, resolve them!)</li> <li>For example, if <code>A</code> depends on <code>B</code>, and <code>B</code> depends on <code>A</code>...<ul> <li>Publish <code>B</code> to PyPI without testing. Untested sub-packages will be published with the version suffix <code>*-dev</code>.</li> <li>Set <code>A</code> to depend on the latest version (i.e. the <code>*-dev</code> version) of <code>B</code>.</li> <li>Test and publish <code>A</code> to PyPI.</li> <li>Test and publish <code>B</code> (which depends on the now-released <code>A</code>) to PyPI.</li> </ul> </li> </ul>"},{"location":"main/contributing/contributing/#todo","title":"TODO","text":"<ul> <li>Support building packages that have installation dependencies, such as <code>bionemo-noodles</code> dependent on <code>maturin</code> or <code>bionemo-&lt;model&gt;</code> dependent on <code>transformer-engine</code>.</li> <li>Automatically cut a release tag for the sub-package via GHA.</li> <li>Support <code>--no-deps</code> installation for BioNeMo sub-packages in TestPyPI, but install OSS dependencies from PyPI, because many OSS dependencies do not have functional versions on TestPyPI.</li> </ul>"},{"location":"main/contributing/sub-package_dependency_graph/","title":"Sub package dependency graph","text":""},{"location":"main/contributing/sub-package_dependency_graph/#sub-package-dependency-graph","title":"Sub-Package Dependency Graph","text":"<p>The script in <code>sub-packages/bionemo/fw/src/dependency_graph.py</code> generates a dependency graph for the BioNeMo sub-packages and verifies that the pyproject.toml and tach.toml files align and capture the dependencies needed for imports in the python files. Additionally, it checks dependencies between BioNeMo sub-packages and creates visual representations of the dependencies in pyproject.toml files, in tach.toml, and in the source files.</p> <p>These are visualizations of the dependency graph from the pyproject.toml files:</p> <p></p> <p>Similarly from the tach.toml file:</p> <p></p> <p>And these are the dependencies from the file imports:</p> <p></p>"},{"location":"main/contributing/Writing%20Documentation/","title":"Writing Good and Thorough Documentation","text":"<p>As a contributor to our codebase, writing high-quality documentation is an essential part of ensuring that others can understand and work with your code effectively. Good documentation helps to reduce confusion, facilitate collaboration, and streamline the development process. In this guide, we will outline the principles and best practices for writing thorough and readable documentation that adheres to the Chicago Manual of Style.</p>"},{"location":"main/contributing/Writing%20Documentation/#chicago-manual-of-style","title":"Chicago Manual of Style","text":"<p>Our documentation follows the Chicago Manual of Style, a widely accepted standard for writing and formatting. This style guide provides a consistent approach to writing, grammar, and punctuation, making it easier for readers to understand and navigate our documentation.</p>"},{"location":"main/contributing/Writing%20Documentation/#key-principles","title":"Key Principles","text":"<p>When writing documentation, keep the following principles in mind:</p> <ol> <li>Clarity: Use clear and concise language to convey your message. Avoid ambiguity and jargon that may confuse readers.</li> <li>Accuracy: Ensure that your documentation is accurate and up-to-date. Verify facts, details, and code snippets    before publishing.</li> <li>Completeness: Provide all necessary information to understand the code, including context, syntax, and examples.</li> <li>Consistency: Use a consistent tone, voice, and style throughout the documentation.</li> <li>Accessibility: Make your documentation easy to read and understand by using headings, bullet points, and short paragraphs.</li> </ol>"},{"location":"main/contributing/Writing%20Documentation/#documentation-structure","title":"Documentation Structure","text":"<p>A well-structured documentation page should include the following elements:</p> <ol> <li>Header: A brief title that summarizes the content of the page.</li> <li>Introduction: A short overview of the topic, including its purpose and relevance.</li> <li>Syntax and Parameters: A detailed explanation of the code syntax, including parameters, data types, and return values.</li> <li>Examples: Concrete examples that illustrate how to use the code, including input and output.</li> <li>Tips and Variations: Additional information, such as best practices, common pitfalls, and alternative approaches.</li> <li>Related Resources: Links to relevant documentation, tutorials, and external resources.</li> </ol>"},{"location":"main/contributing/Writing%20Documentation/#best-practices","title":"Best Practices","text":"<p>To ensure high-quality documentation, follow these best practices:</p> <ol> <li>Use headings and subheadings: Organize your content with clear headings and subheadings to facilitate scanning and navigation.</li> <li>Use bullet points and lists: Break up complex information into easy-to-read lists and bullet points.</li> <li>Provide context: Give readers a clear understanding of the code's purpose, history, and relationships to other components.</li> <li>Review and edit: Carefully review and edit your documentation to ensure accuracy, completeness, and consistency.</li> </ol>"},{"location":"main/contributing/Writing%20Documentation/#resources","title":"Resources","text":"<p>For more information on the Chicago Manual of Style, refer to their online published version.</p> <p>By following these guidelines and principles, you wi ll be able to create high-quality documentation that helps others understand and work with your code effectively. Remember to always prioritize clarity, accuracy, and completeness, and to use the Chicago Style Guide as your reference for writing and formatting.</p>"},{"location":"main/contributing/Writing%20Documentation/jupyter-notebooks/","title":"Jupyter Notebook Support","text":"In\u00a0[1]: Copied! <pre>a = 1\nb = 2\na + b\n</pre> a = 1 b = 2 a + b Out[1]: <pre>3</pre> In\u00a0[2]: Copied! <pre>%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nxs = np.linspace(0, 2 * np.pi, 100)\nplt.plot(xs, np.sin(xs))\n</pre> %matplotlib inline import matplotlib.pyplot as plt import numpy as np   xs = np.linspace(0, 2 * np.pi, 100) plt.plot(xs, np.sin(xs)) Out[2]: <pre>[&lt;matplotlib.lines.Line2D at 0x7672067fb190&gt;]</pre> In\u00a0[3]: Copied! <pre># NBVAL_CHECK_OUTPUT\n\nimport numpy as np\n\n\nprint(np.arange(5))\n</pre> # NBVAL_CHECK_OUTPUT  import numpy as np   print(np.arange(5)) <pre>[0 1 2 3 4]\n</pre>"},{"location":"main/contributing/Writing%20Documentation/jupyter-notebooks/#jupyter-notebook-support","title":"Jupyter Notebook Support\u00b6","text":"<p>Jupyter notebooks can be rendered as part of the documentation build system as an alternative to markdown files. The docs site uses mkdocs-jupyter to build and render jupyter notebooks as markdown files.</p> <p>Note: There are some limitations to jupyter rendering.</p> <ol> <li>Notebooks are not executed as part of the docs publishing pipeline. CI tests to ensure notebook consistency are run separately (see Testing Jupyter Notebooks).</li> <li>Notebook markdown cells don't support the full range of mkdocs-material configuration, including things like admonitions, referencing automatically-generated API documentation via mkdocstrings etc. (more here).</li> </ol>"},{"location":"main/contributing/Writing%20Documentation/jupyter-notebooks/#example-code-block","title":"Example code block\u00b6","text":"<p>Markdown headings can be used to create a TOC similarly to traditional mkdocs pages.</p>"},{"location":"main/contributing/Writing%20Documentation/jupyter-notebooks/#embedded-visualizations","title":"Embedded visualizations\u00b6","text":"<p>We can also embed images using standard approaches to embedding graphics in notebooks.</p>"},{"location":"main/contributing/Writing%20Documentation/jupyter-notebooks/#testing-jupyter-notebooks","title":"Testing Jupyter Notebooks\u00b6","text":"<p>Jupyter notebooks are run as part of the CI build suite using <code>nbval</code>. To run these tests locally, run</p> <pre>pytest --nbval-lax docs/\n</pre> <p>from the repository root. By default, <code>nbval</code> will only check that the notebook executes successfully. To add additional checks to ensure the consistency of the output, add a <code>#NBVAL_CHECK_OUTPUT</code> marker comment, which will ensure that the output of the saved jupyter notebook matches the output when the notebook is executed in CI.</p> <p>For example:</p>"},{"location":"main/contributing/Writing%20Documentation/mkdocs/","title":"MkDocs","text":""},{"location":"main/contributing/Writing%20Documentation/mkdocs/#build-system","title":"Build system","text":"<p>BioNeMo 2 uses Material for MkDocs to build it's documentation. Docstrings are converted to automatically-generated API reference pages using <code>mkdocstrings</code>, and can be linked from markdown pages using paths.</p>"},{"location":"main/datasets/","title":"BioNeMo Framework: Available Datasets","text":"<p>The BioNeMo Framework provides access to a variety of high-quality datasets for bioinformatics and cheminformatics research. These datasets cover a range of biological and chemical modalities, supporting various research applications. The following table lists the currently available datasets:</p> Dataset Modality Uses CELLxGENE Single Cell Single-Cell Gene Expression UniProt Protein Protein Sequence and Function Analysis <p>For more information about the datasets included in the BioNeMo Framework, refer to the Dataset Cards linked in the table above or the original sources referenced in the respective dataset descriptions.</p>"},{"location":"main/datasets/CELLxGENE/","title":"CELLxGENE","text":""},{"location":"main/datasets/CELLxGENE/#description","title":"Description","text":"<p>CELLxGENE is an aggregation of publicly available single-cell datasets collected by CZI.</p>"},{"location":"main/datasets/CELLxGENE/#dataset-attributes-of-version-2023-12-15","title":"Dataset attributes of version 2023-12-15","text":"<p>Data was downloaded using the CELLxGENE Discover Census version <code>2023-12-15</code>. We first downloaded CELLxGENE census version 2023-12-15 using the <code>cellxgene_census</code> python API. We limited cell data to <code>organism=\"Homo sapiens\"</code>, with a non \"na\" <code>suspension_type</code>, <code>is_primary_data=True</code>, and <code>disease=\"normal\"</code> to limit to non-diseased tissues that are also the primary data source per cell to make sure that cells are only included once in the download. We tracked metadata including \"assay\", \"sex\", \"development_stage\", \"tissue_general\", \"dataset_id\" and \"self_reported_ethnicity\". The metadata \"assay\", \"tissue_general\", and \"dataset_id\" were used to construct dataset splits into train, validation, and test sets. The training set represented 99% of the downloaded cells. We partitioned the data by dataset_id into a train set (99%) and a hold-out set (1%), to make sure that the hold-out datasets were independently collected single cell experiments, which helps evaluate generalizability to new future datasets. In this training split, we made sure that all \"assay\" and \"tissue_general\" labels were present in the training set so that our model would have maximal visibility into different tissues and assay biases. Finally the 1% hold-out set was split further into a validation and test set. This final split was mostly done randomly by cell, however we set aside a full dataset into the test split so that we could evaluate performance after training on a completely unseen dataset, including when monitoring the validation loss during training.</p> <p>These parameters resulted in 23.87 Million single cells collected from a variety of public datasets, all hosted by CZI CELLxGENE census. After the splitting procedure we had:</p> <ul> <li>23.64 Million cells in the training split</li> <li>0.13 Million cells in the validation split</li> <li>0.11 Million cells in the test split</li> </ul>"},{"location":"main/datasets/CELLxGENE/#distributions-of-donor-covariates","title":"Distributions of donor covariates","text":"<p>There are various biases apparent in this dataset.</p>"},{"location":"main/datasets/CELLxGENE/#tissue-distribution","title":"Tissue distribution","text":"<p>At a high level tissues were heavily biased toward the nervous system, which made up nearly 40 percent of the data.</p> <p></p>"},{"location":"main/datasets/CELLxGENE/#assay-distribution","title":"Assay distribution","text":"<p>Assays were also imbalanced in this dataset. As the 10x machine is fairly high throughput and currently popular, it makes sense that the majority of cells present would be from this instrument. Various versions of the 10x instrument made up 18M of the 24M cells while the next largest category was <code>sci-RNA-seq</code>. </p>"},{"location":"main/datasets/CELLxGENE/#sex-distribution","title":"Sex distribution","text":"<p>A bias exists in this dataset for sex. Most of the donor's cells were male-derived at 52%, while female donor's cell contribution made up 42%, and the remaining 6% were not annotated. .</p>"},{"location":"main/datasets/CELLxGENE/#reported-ethnicity-distribution","title":"Reported ethnicity distribution","text":"<p>The dataset has a heavy bias toward cells derived from donors with european ethnicity at 40%, while the next largest category, asian, made up 8%. When considering that nearly 50% were unknown, we might expect that as much as 75% of this dataset is made up of cells extracted from donors of self reported european ethnicity. </p>"},{"location":"main/datasets/CELLxGENE/#age-distribution","title":"Age distribution","text":"<p>This dataset is very heavily balanced toward younger donors. Many of the cells are derived from donors that are under a year of age (over 25%). After that the remaining 75% of cells are dispersed roughly under a normal distribution with a mode of 51-60 other than an additional peak in the 21-30 range. Donors over 61 years old make up approximately 15% of the data.</p> <p></p>"},{"location":"main/datasets/CELLxGENE/#assay-size-distribution","title":"Assay size distribution","text":"<p>Different assays have different ranges of reported gene measurements. On the low end <code>BD Rapsody Targetted mRNA</code> has only a few genes reported, while 10x instruments tend to report on 30,000 genes.</p> <p></p>"},{"location":"main/datasets/CELLxGENE/#dataset-distribution","title":"Dataset distribution","text":"<p>Dataset (for example, a publication that produces data and uploads to CELLxGENE) leads to known batch effects due to different handling procedures, collection procedures, and more. Hence, we stratify our training rather than hold out split by this covariate. Exploring the breakdown of datasets, we see that the top 10 datasets represent approximately 10 million cells of the full CELLxGENE dataset. The largest dataset alone has 4 million cells.</p> <p></p> <p>Looking at the makeup of these top datasets, we see that they represent single tissue categories predominately. Most of these tend to be nervous system datasets, with the exception of one that is balanced between many cell types. </p>"},{"location":"main/datasets/CELLxGENE/#references","title":"References","text":"<ul> <li>CZ CELLxGENE Discover: A single-cell data platform for scalable exploration, analysis and modeling of aggregated data CZI Single-Cell Biology, et al. bioRxiv 2023.10.30; doi: https://doi.org/10.1101/2023.10.30.563174</li> </ul>"},{"location":"main/datasets/CELLxGENE/#data-license","title":"Data License","text":"<p>The data in CELLxGENE are made available by the study authors and Chan Zuckerberg Initiative under the creative commons CC BY 4.0 license. Study authors agree prior to submission that their data is not identifiable, lacking any direct personal identifiers in the metadata. More information may be found in the CELLxGENE Data Submission Policy. Our training, validation and test data, including subsets made available for testing and demonstration purposes, was contributed to CELLxGENE through one or more of the following sources:</p> <ul> <li>Publication Reference: ; Dataset Version: https://datasets.cellxgene.cziscience.com/01fee550-877c-4a13-97b2-96bb43e5a2a5.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/6b701826-37bb-4356-9792-ff41fc4c3161</li> <li>Publication Reference: ; Dataset Version: https://datasets.cellxgene.cziscience.com/9959402b-2e69-4aeb-ba39-efdfa5e0de1a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/10bf5c50-8d85-4c5f-94b4-22c1363d9f31</li> <li>Publication Reference: ; Dataset Version: https://datasets.cellxgene.cziscience.com/bc484ee8-b3cc-47a3-8f4f-f95aa1fec803.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/a98b828a-622a-483a-80e0-15703678befd</li> <li>Publication Reference: Ahern et al. (2022) Cell; Publication: https://doi.org/10.1016/j.cell.2022.01.012 Dataset Version: https://datasets.cellxgene.cziscience.com/e72d0170-3399-4aa0-8c56-da7b4f0ced6b.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/8f126edf-5405-4731-8374-b5ce11f53e82</li> <li>Publication Reference: Andrews et al. (2022) Hepatology Communications; Publication: https://doi.org/10.1002/hep4.1854 Dataset Version: https://datasets.cellxgene.cziscience.com/19b364f7-db0c-430b-bc16-9b31cbd45a58.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/44531dd9-1388-4416-a117-af0a99de2294</li> <li>Publication Reference: Arutyunyan et al. (2023) Nature; Publication: https://doi.org/10.1038/s41586-023-05869-0 Dataset Version: https://datasets.cellxgene.cziscience.com/5720f13d-fc15-4859-90df-447637fb37c4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/e2c257e7-6f79-487c-b81c-39451cd4ab3c</li> <li>Publication Reference: Bhaduri et al. (2021) Nature; Publication: https://doi.org/10.1038/s41586-021-03910-8 Dataset Version: https://datasets.cellxgene.cziscience.com/677082ca-48d3-44b8-b5c4-84dffafbba23.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/c8565c6a-01a1-435b-a549-f11b452a83a8</li> <li>Publication Reference: Bhat-Nakshatri et al. (2021) Cell Reports Medicine; Publication: https://doi.org/10.1016/j.xcrm.2021.100219 Dataset Version: https://datasets.cellxgene.cziscience.com/f72aae6e-c997-484c-bffd-6d09e41ef9a4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/c9706a92-0e5f-46c1-96d8-20e42467f287</li> <li>Publication Reference: Bondoc et al. (2021) Commun Biol; Publication: https://doi.org/10.1038/s42003-021-02562-8 Dataset Version: https://datasets.cellxgene.cziscience.com/20d54624-2098-4ed5-89f8-6da2bb460c3c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/a261413d-835b-4f1e-ab0c-dada55ea6afd</li> <li>Publication Reference: Burclaff et al. (2022) Cellular and Molecular Gastroenterology and Hepatology; Publication: https://doi.org/10.1016/j.jcmgh.2022.02.007 Dataset Version: https://datasets.cellxgene.cziscience.com/e00e3a74-038a-46fd-8931-f6dc8c90fd13.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/64b24fda-6591-4ce1-89e7-33eb6c43ad7b</li> <li>Publication Reference: Calandrelli et al. (2020) Nat Commun; Publication: https://doi.org/10.1038/s41467-020-18957-w Dataset Version: https://datasets.cellxgene.cziscience.com/c3189372-fceb-493d-98be-23abe1947253.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/db468083-041c-41ca-8f6f-bf991a070adf</li> <li>Publication Reference: Cao et al. (2020) Science; Publication: https://doi.org/10.1126/science.aba7721 Dataset Version: https://datasets.cellxgene.cziscience.com/8f6296d0-5b29-4dca-8061-b97147df5fcc.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/c114c20f-1ef4-49a5-9c2e-d965787fb90c</li> <li>Publication Reference: Chan Zuckerberg Initiative Single-Cell COVID-19 Consortia et al. (2020) medRxiv; Publication: https://doi.org/10.1101/2020.11.20.20227355 Dataset Version: https://datasets.cellxgene.cziscience.com/6c779b98-f437-4cbf-9b81-a3e6be637419.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/0434a9d4-85fd-4554-b8e3-cf6c582bb2fa</li> <li>Publication Reference: Chan et al. (2021) Cancer Cell; Publication: https://doi.org/10.1016/j.ccell.2021.09.008 Dataset Version: https://datasets.cellxgene.cziscience.com/1ba7d495-c1a8-4809-b56d-548fbea77c8a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/62e8f058-9c37-48bc-9200-e767f318a8ec</li> <li>Publication Reference: Chan et al. (2021) Cancer Cell; Publication: https://doi.org/10.1016/j.ccell.2021.09.008 Dataset Version: https://datasets.cellxgene.cziscience.com/c40911a4-47de-460e-be86-52e39800654c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/62e8f058-9c37-48bc-9200-e767f318a8ec</li> <li>Publication Reference: Cheng et al. (2018) Cell Reports; Publication: https://doi.org/10.1016/j.celrep.2018.09.006 Dataset Version: https://datasets.cellxgene.cziscience.com/912d943b-9060-4fd3-a12c-ad641a89f0e4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/43d4bb39-21af-4d05-b973-4c1fed7b916c</li> <li>Publication Reference: Cowan et al. (2020) Cell; Publication: https://doi.org/10.1016/j.cell.2020.08.013 Dataset Version: https://datasets.cellxgene.cziscience.com/b1989183-5808-46ab-87f5-978febb2d26e.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/2f4c738f-e2f3-4553-9db2-0582a38ea4dc</li> <li>Publication Reference: Cowan et al. (2020) Cell; Publication: https://doi.org/10.1016/j.cell.2020.08.013 Dataset Version: https://datasets.cellxgene.cziscience.com/c0d3867e-1a7b-4e57-af62-c563f1934226.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/2f4c738f-e2f3-4553-9db2-0582a38ea4dc</li> <li>Publication Reference: Dom\u00ednguez Conde et al. (2022) Science; Publication: https://doi.org/10.1126/science.abl5197 Dataset Version: https://datasets.cellxgene.cziscience.com/08f58b32-a01b-4300-8ebc-2b93c18f26f7.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/62ef75e4-cbea-454e-a0ce-998ec40223d3</li> <li>Publication Reference: Easter et al. (2024) Nat Commun; Publication: https://doi.org/10.1038/s41467-024-49037-y Dataset Version: https://datasets.cellxgene.cziscience.com/221dff56-a47d-4563-90ed-51b60e2f16d5.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/71f4bccf-53d4-4c12-9e80-e73bfb89e398</li> <li>Publication Reference: Egozi et al. (2021) Nat Med; Publication: https://doi.org/10.1038/s41591-021-01586-1 Dataset Version: https://datasets.cellxgene.cziscience.com/e3a84fef-b6df-49b2-b0ca-ecaf444773ec.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/7651ac1a-f947-463a-9223-a9e408a41989</li> <li>Publication Reference: Elmentaite et al. (2020) Developmental Cell; Publication: https://doi.org/10.1016/j.devcel.2020.11.010 Dataset Version: https://datasets.cellxgene.cziscience.com/3aedefc0-401a-4ee8-a1b5-a0ffc20e1ff2.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/17481d16-ee44-49e5-bcf0-28c0780d8c4a</li> <li>Publication Reference: Elmentaite et al. (2020) Developmental Cell; Publication: https://doi.org/10.1016/j.devcel.2020.11.010 Dataset Version: https://datasets.cellxgene.cziscience.com/5d27ffd6-1769-4564-961f-9bb32d9ca3a4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/17481d16-ee44-49e5-bcf0-28c0780d8c4a</li> <li>Publication Reference: Elmentaite et al. (2021) Nature; Publication: https://doi.org/10.1038/s41586-021-03852-1 Dataset Version: https://datasets.cellxgene.cziscience.com/c463b937-dbdc-48ff-8037-dd191ea4e41e.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/e33ffcd3-7cbf-4b8c-b0f4-85587ad5019a</li> <li>Publication Reference: Eraslan et al. (2022) Science; Publication: https://doi.org/10.1126/science.abl4290 Dataset Version: https://datasets.cellxgene.cziscience.com/355ed159-f7d7-45e9-bc55-95639f0ab8b0.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/a3ffde6c-7ad2-498a-903c-d58e732f7470</li> <li>Publication Reference: Fan et al. (2019) Nat Commun; Publication: https://doi.org/10.1038/s41467-019-11036-9 Dataset Version: https://datasets.cellxgene.cziscience.com/9b2536db-4576-4906-ae9b-a01a623462f9.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/2902f08c-f83c-470e-a541-e463e25e5058</li> <li>Publication Reference: Fasolino et al. (2022) Nat Metab; Publication: https://doi.org/10.1038/s42255-022-00531-x Dataset Version: https://datasets.cellxgene.cziscience.com/d39144df-fa59-4b63-b07b-9b34613b5c84.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/51544e44-293b-4c2b-8c26-560678423380</li> <li>Publication Reference: Fawkner-Corbett et al. (2021) Cell; Publication: https://doi.org/10.1016/j.cell.2020.12.016 Dataset Version: https://datasets.cellxgene.cziscience.com/e8473c46-eada-43b7-bd1c-e0fed1c4c913.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/60358420-6055-411d-ba4f-e8ac80682a2e</li> <li>Publication Reference: Gabitto et al. (2023) bioRxiv; Publication: https://doi.org/10.1101/2023.05.08.539485 Dataset Version: https://datasets.cellxgene.cziscience.com/291ce735-8d18-4a2f-a6bc-98f75f8d6bc0.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1ca90a2d-2943-483d-b678-b809bf464c30</li> <li>Publication Reference: Gabitto et al. (2023) bioRxiv; Publication: https://doi.org/10.1101/2023.05.08.539485 Dataset Version: https://datasets.cellxgene.cziscience.com/e9bffe1d-9f07-4467-9230-c080b362e737.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1ca90a2d-2943-483d-b678-b809bf464c30</li> <li>Publication Reference: Garcia-Alonso et al. (2021) Nat Genet; Publication: https://doi.org/10.1038/s41588-021-00972-2 Dataset Version: https://datasets.cellxgene.cziscience.com/15f77a91-aadc-4e63-81c7-a8614e9ad33d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/32f2fd23-ec74-486f-9544-e5b2f41725f5</li> <li>Publication Reference: Garcia-Alonso et al. (2022) Nature; Publication: https://doi.org/10.1038/s41586-022-04918-4 Dataset Version: https://datasets.cellxgene.cziscience.com/366847dc-8fc4-42c3-9c27-9704929c6792.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/661a402a-2a5a-4c71-9b05-b346c57bc451</li> <li>Publication Reference: Garcia-Alonso et al. (2022) Nature; Publication: https://doi.org/10.1038/s41586-022-04918-4 Dataset Version: https://datasets.cellxgene.cziscience.com/4b894b0d-6b27-4ab4-a48a-0205e4aaf348.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/661a402a-2a5a-4c71-9b05-b346c57bc451</li> <li>Publication Reference: Garcia-Alonso et al. (2022) Nature; Publication: https://doi.org/10.1038/s41586-022-04918-4 Dataset Version: https://datasets.cellxgene.cziscience.com/c74c3be0-1a80-4af9-8241-d560afc67886.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/661a402a-2a5a-4c71-9b05-b346c57bc451</li> <li>Publication Reference: Gray et al. (2022) Developmental Cell; Publication: https://doi.org/10.1016/j.devcel.2022.05.003 Dataset Version: https://datasets.cellxgene.cziscience.com/9fecd056-d8c8-4ec6-8522-8d40f19c90a8.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/99f1515b-46a2-4bc4-94c3-f62659dc1eb4</li> <li>Publication Reference: Guilliams et al. (2022) Cell; Publication: https://doi.org/10.1016/j.cell.2021.12.018 Dataset Version: https://datasets.cellxgene.cziscience.com/5f2d618d-2a5f-4c31-8750-982342d7dd04.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/74e10dc4-cbb2-4605-a189-8a1cd8e44d8c</li> <li>Publication Reference: Han et al. (2020) Nature; Publication: https://doi.org/10.1038/s41586-020-2157-4 Dataset Version: https://datasets.cellxgene.cziscience.com/7a455e3b-dd79-499b-95c9-8b1b2dde5339.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/38833785-fac5-48fd-944a-0f62a4c23ed1</li> <li>Publication Reference: Han et al. (2022) Blood Cancer Discovery; Publication: https://doi.org/10.1158/2643-3230.BCD-21-0075 Dataset Version: https://datasets.cellxgene.cziscience.com/f905e484-1162-467e-b8c5-835dcfe9bd5c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/968834a0-1895-40df-8720-666029b3bbac</li> <li>Publication Reference: Hao et al. (2021) Cell; Publication: https://doi.org/10.1016/j.cell.2021.04.048 Dataset Version: https://datasets.cellxgene.cziscience.com/55c120dc-6a20-4caf-9513-f5970b24b1be.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b0cf0afa-ec40-4d65-b570-ed4ceacc6813</li> <li>Publication Reference: He et al. (2022) Cell; Publication: https://doi.org/10.1016/j.cell.2022.11.005 Dataset Version: https://datasets.cellxgene.cziscience.com/256d6049-b499-47a9-9c9a-20b92f9c6ba6.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/2d2e2acd-dade-489f-a2da-6c11aa654028</li> <li>Publication Reference: He et al. (2022) Cell; Publication: https://doi.org/10.1016/j.cell.2022.11.005 Dataset Version: https://datasets.cellxgene.cziscience.com/25dbd3da-8cb0-4b9d-814c-85ae0a710353.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/2d2e2acd-dade-489f-a2da-6c11aa654028</li> <li>Publication Reference: He et al. (2022) Cell; Publication: https://doi.org/10.1016/j.cell.2022.11.005 Dataset Version: https://datasets.cellxgene.cziscience.com/2b55f5c0-aa82-41e8-9d84-915b1d5a797b.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/2d2e2acd-dade-489f-a2da-6c11aa654028</li> <li>Publication Reference: He et al. (2022) Cell; Publication: https://doi.org/10.1016/j.cell.2022.11.005 Dataset Version: https://datasets.cellxgene.cziscience.com/c4122ff1-79d9-405b-92f1-c1c27234a125.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/2d2e2acd-dade-489f-a2da-6c11aa654028</li> <li>Publication Reference: James et al. (2020) Nat Immunol; Publication: https://doi.org/10.1038/s41590-020-0602-z Dataset Version: https://datasets.cellxgene.cziscience.com/6e2ab5f9-bb51-459a-9fd7-605d29661823.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/7681c7d7-0168-4892-a547-6f02a6430ace</li> <li>Publication Reference: Jardine et al. (2021) Nature; Publication: https://doi.org/10.1038/s41586-021-03929-x Dataset Version: https://datasets.cellxgene.cziscience.com/7c452616-bc00-4499-b511-bfd5de1b7cd6.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/793fdaec-5067-428a-a9db-ecefe135c945</li> <li>Publication Reference: Jardine et al. (2021) Nature; Publication: https://doi.org/10.1038/s41586-021-03929-x Dataset Version: https://datasets.cellxgene.cziscience.com/8ef64f32-461d-4d50-81b6-5718b506a7a8.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/793fdaec-5067-428a-a9db-ecefe135c945</li> <li>Publication Reference: Jardine et al. (2021) Nature; Publication: https://doi.org/10.1038/s41586-021-03929-x Dataset Version: https://datasets.cellxgene.cziscience.com/f402b857-7871-45f4-9ad1-ff943552285a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/793fdaec-5067-428a-a9db-ecefe135c945</li> <li>Publication Reference: Jin et al. (2021) iScience; Publication: https://doi.org/10.1016/j.isci.2021.103115 Dataset Version: https://datasets.cellxgene.cziscience.com/753caa81-61c5-4126-a61c-df2c546b16d1.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b9fc3d70-5a72-4479-a046-c2cc1ab19efc</li> <li>Publication Reference: Jorstad et al. (2023) Science; Publication: https://doi.org/10.1126/science.adf6812 Dataset Version: https://datasets.cellxgene.cziscience.com/97a035e0-b9d3-4e7b-adc2-b318316da7f9.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/d17249d2-0e6e-4500-abb8-e6c93fa1ac6f</li> <li>Publication Reference: Joseph et al. (2021) J. Pathol.; Publication: https://doi.org/10.1002/path.5751 Dataset Version: https://datasets.cellxgene.cziscience.com/fb812806-ae3a-45de-9102-0b2f801424e2.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/4b54248f-2165-477c-a027-dd55082e8818</li> <li>Publication Reference: Kamath et al. (2022) Nat Neurosci; Publication: https://doi.org/10.1038/s41593-022-01061-1 Dataset Version: https://datasets.cellxgene.cziscience.com/4936be1a-c766-46e2-815f-1c994aed7a4f.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b0f0b447-ac37-45b0-b1bf-5c0b7d871120</li> <li>Publication Reference: Kamath et al. (2022) Nat Neurosci; Publication: https://doi.org/10.1038/s41593-022-01061-1 Dataset Version: https://datasets.cellxgene.cziscience.com/dd206caf-13ca-4598-86af-339189adff0d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b0f0b447-ac37-45b0-b1bf-5c0b7d871120</li> <li>Publication Reference: Kanemaru et al. (2023) Nature; Publication: https://doi.org/10.1038/s41586-023-06311-1 Dataset Version: https://datasets.cellxgene.cziscience.com/1a7a9fb0-aee1-437f-8a7c-9d132253a4db.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/3116d060-0a8e-4767-99bb-e866badea1ed</li> <li>Publication Reference: King et al. (2021) Sci. Immunol.; Publication: https://doi.org/10.1126/sciimmunol.abe6291 Dataset Version: https://datasets.cellxgene.cziscience.com/02675fa7-5f13-4d89-a07d-9d0ff7996f0d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/3a2af25b-2338-4266-aad3-aa8d07473f50</li> <li>Publication Reference: King et al. (2021) Sci. Immunol.; Publication: https://doi.org/10.1126/sciimmunol.abe6291 Dataset Version: https://datasets.cellxgene.cziscience.com/6c8a5c10-2617-4cb1-8ed6-20a5e25065ef.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/3a2af25b-2338-4266-aad3-aa8d07473f50</li> <li>Publication Reference: Knight-Schrijver et al. (2022) Nat Cardiovasc Res; Publication: https://doi.org/10.1038/s44161-022-00183-w Dataset Version: https://datasets.cellxgene.cziscience.com/c1e3c998-4961-46c9-929d-d011900964e8.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/43b45a20-a969-49ac-a8e8-8c84b211bd01</li> <li>Publication Reference: Kock et al. (2024) bioRxiv; Publication: https://doi.org/10.1101/2024.06.30.601119 Dataset Version: https://datasets.cellxgene.cziscience.com/a3850101-1e15-4ae2-82f7-bb9289e911d4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/ced320a1-29f3-47c1-a735-513c7084d508</li> <li>Publication Reference: Kong et al. (2023) Immunity; Publication: https://doi.org/10.1016/j.immuni.2023.01.002 Dataset Version: https://datasets.cellxgene.cziscience.com/30d6b4f5-1475-4b86-a47c-48609d6706c2.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/5c868b6f-62c5-4532-9d7f-a346ad4b50a7</li> <li>Publication Reference: Kong et al. (2023) Immunity; Publication: https://doi.org/10.1016/j.immuni.2023.01.002 Dataset Version: https://datasets.cellxgene.cziscience.com/41bdc5da-b436-485a-a753-9ae297057ee6.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/5c868b6f-62c5-4532-9d7f-a346ad4b50a7</li> <li>Publication Reference: Kong et al. (2023) Immunity; Publication: https://doi.org/10.1016/j.immuni.2023.01.002 Dataset Version: https://datasets.cellxgene.cziscience.com/485d1aee-db62-4373-854c-12a34237e97b.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/5c868b6f-62c5-4532-9d7f-a346ad4b50a7</li> <li>Publication Reference: Kong et al. (2023) Immunity; Publication: https://doi.org/10.1016/j.immuni.2023.01.002 Dataset Version: https://datasets.cellxgene.cziscience.com/8db9570a-3f80-41c9-b927-ae3eb115ba1d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/5c868b6f-62c5-4532-9d7f-a346ad4b50a7</li> <li>Publication Reference: Kong et al. (2023) Immunity; Publication: https://doi.org/10.1016/j.immuni.2023.01.002 Dataset Version: https://datasets.cellxgene.cziscience.com/dedfeef9-bfb8-4c2f-a196-1afea9a846d8.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/5c868b6f-62c5-4532-9d7f-a346ad4b50a7</li> <li>Publication Reference: Kong et al. (2023) Immunity; Publication: https://doi.org/10.1016/j.immuni.2023.01.002 Dataset Version: https://datasets.cellxgene.cziscience.com/ef7d48a5-c56b-4f13-9903-fb3327924445.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/5c868b6f-62c5-4532-9d7f-a346ad4b50a7</li> <li>Publication Reference: Kumar et al. (2023) Nature; Publication: https://doi.org/10.1038/s41586-023-06252-9 Dataset Version: https://datasets.cellxgene.cziscience.com/24ee53d1-e5ed-47ae-8b8e-7a0d62d91513.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/4195ab4c-20bd-4cd3-8b3d-65601277e731</li> <li>Publication Reference: Kumar et al. (2023) Nature; Publication: https://doi.org/10.1038/s41586-023-06252-9 Dataset Version: https://datasets.cellxgene.cziscience.com/b8b5be07-061b-4390-af0a-f9ced877a068.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/4195ab4c-20bd-4cd3-8b3d-65601277e731</li> <li>Publication Reference: Kuppe et al. (2022) Nature; Publication: https://doi.org/10.1038/s41586-022-05060-x Dataset Version: https://datasets.cellxgene.cziscience.com/c1f6034b-7973-45e1-85e7-16933d0550bc.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/8191c283-0816-424b-9b61-c3e1d6258a77</li> <li>Publication Reference: Lake et al. (2023) Nature; Publication: https://doi.org/10.1038/s41586-023-05769-3 Dataset Version: https://datasets.cellxgene.cziscience.com/1cbe52c1-0567-4188-9c18-9d7271c56055.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/bcb61471-2a44-4d00-a0af-ff085512674c</li> <li>Publication Reference: Lake et al. (2023) Nature; Publication: https://doi.org/10.1038/s41586-023-05769-3 Dataset Version: https://datasets.cellxgene.cziscience.com/d0ddf40e-dc4b-4134-8439-bfb8bf7a81f4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/bcb61471-2a44-4d00-a0af-ff085512674c</li> <li>Publication Reference: Lavaert et al. (2020) Immunity; Publication: https://doi.org/10.1016/j.immuni.2020.03.019 Dataset Version: https://datasets.cellxgene.cziscience.com/22e3ee6e-e47b-4502-9b2c-21b4c30a455f.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/83ed3be8-4cb9-43e6-9aaa-3fbbf5d1bd3a</li> <li>Publication Reference: Ledergor et al. (2018) Nat Med; Publication: https://doi.org/10.1038/s41591-018-0269-2 Dataset Version: https://datasets.cellxgene.cziscience.com/680c8801-ccdd-4018-a14a-cefda2da3848.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/2a0b02c0-fea6-47bd-92b9-9b03f5d2580c</li> <li>Publication Reference: Lee et al. (2020) Sci. Immunol.; Publication: https://doi.org/10.1126/sciimmunol.abd1554 Dataset Version: https://datasets.cellxgene.cziscience.com/b0a99b60-7480-4b5c-93c5-11020c36adb2.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/4f889ffc-d4bc-4748-905b-8eb9db47a2ed</li> <li>Publication Reference: Lengyel et al. (2022) Cell Reports; Publication: https://doi.org/10.1016/j.celrep.2022.111838 Dataset Version: https://datasets.cellxgene.cziscience.com/da638059-73e0-4a3b-a6fc-5fb8e47d4bff.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/d36ca85c-3e8b-444c-ba3e-a645040c6185</li> <li>Publication Reference: Lengyel et al. (2022) Cell Reports; Publication: https://doi.org/10.1016/j.celrep.2022.111838 Dataset Version: https://datasets.cellxgene.cziscience.com/de2a800c-249f-4072-8454-cde3d6bfb5b4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/d36ca85c-3e8b-444c-ba3e-a645040c6185</li> <li>Publication Reference: Li et al. (2022) Cancer Cell; Publication: https://doi.org/10.1016/j.ccell.2022.11.001 Dataset Version: https://datasets.cellxgene.cziscience.com/39196e03-e248-4724-a618-b3bef017d6b2.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/f7cecffa-00b4-4560-a29a-8ad626b8ee08</li> <li>Publication Reference: Liang et al. (2023) Cell Genomics; Publication: https://doi.org/10.1016/j.xgen.2023.100298 Dataset Version: https://datasets.cellxgene.cziscience.com/0da7c2ec-246f-4ffb-9c25-1aa059870a0a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/af893e86-8e9f-41f1-a474-ef05359b1fb7</li> <li>Publication Reference: Liang et al. (2023) Cell Genomics; Publication: https://doi.org/10.1016/j.xgen.2023.100298 Dataset Version: https://datasets.cellxgene.cziscience.com/9a199269-5a65-4ade-aa85-07ab4cfb4c26.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/af893e86-8e9f-41f1-a474-ef05359b1fb7</li> <li>Publication Reference: Litvi\\u0148ukov\\u00e1 et al. (2020) Nature; Publication: https://doi.org/10.1038/s41586-020-2797-4 Dataset Version: https://datasets.cellxgene.cziscience.com/a5618935-5bbd-494d-b300-9ecc2402d5b0.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b52eb423-5d0d-4645-b217-e1c6d38b2e72</li> <li>Publication Reference: Liu et al. (2021) Cell; Publication: https://doi.org/10.1016/j.cell.2021.02.018 Dataset Version: https://datasets.cellxgene.cziscience.com/504b4b53-f0fb-43a6-8b8d-6254e8d81e85.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/ed9185e3-5b82-40c7-9824-b2141590c7f0</li> <li>Publication Reference: Liu et al. (2021) Cell; Publication: https://doi.org/10.1016/j.cell.2021.02.018 Dataset Version: https://datasets.cellxgene.cziscience.com/6d14e6f5-9b9f-4e8e-8b31-cee9020e18a5.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/ed9185e3-5b82-40c7-9824-b2141590c7f0</li> <li>Publication Reference: Lukassen et al. (2020) EMBO J; Publication: https://doi.org/10.15252/embj.20105114 Dataset Version: https://datasets.cellxgene.cziscience.com/a6c47325-eae6-4111-9e47-89a550ef99af.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/6ff3401b-d72c-4940-a00c-3f0792397082</li> <li>Publication Reference: Lukassen et al. (2020) EMBO J; Publication: https://doi.org/10.15252/embj.20105114 Dataset Version: https://datasets.cellxgene.cziscience.com/b0c0edd1-a8ba-436c-bef4-667bdf3fc8f1.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/6ff3401b-d72c-4940-a00c-3f0792397082</li> <li>Publication Reference: Lukowski et al. (2019) EMBO J; Publication: https://doi.org/10.15252/embj.2018100811 Dataset Version: https://datasets.cellxgene.cziscience.com/9b910901-bcf8-4d51-a85b-268ccbd54544.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/3472f32d-4a33-48e2-aad5-666d4631bf4c</li> <li>Publication Reference: MacParland et al. (2018) Nat Commun; Publication: https://doi.org/10.1038/s41467-018-06318-7 Dataset Version: https://datasets.cellxgene.cziscience.com/f07d2b1d-2e04-4f30-92d2-7d55e22da909.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/bd5230f4-cd76-4d35-9ee5-89b3e7475659</li> <li>Publication Reference: Madissoon et al. (2020) Genome Biol; Publication: https://doi.org/10.1186/s13059-019-1906-x Dataset Version: https://datasets.cellxgene.cziscience.com/2c6ab5e2-09c9-410b-a472-7559e45e553a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/4d74781b-8186-4c9a-b659-ff4dc4601d91</li> <li>Publication Reference: Madissoon et al. (2020) Genome Biol; Publication: https://doi.org/10.1186/s13059-019-1906-x Dataset Version: https://datasets.cellxgene.cziscience.com/55abbbab-bb53-4f92-92cd-483d8f74a881.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/4d74781b-8186-4c9a-b659-ff4dc4601d91</li> <li>Publication Reference: Madissoon et al. (2020) Genome Biol; Publication: https://doi.org/10.1186/s13059-019-1906-x Dataset Version: https://datasets.cellxgene.cziscience.com/ee76e44a-8b7a-4ddc-8e29-c8a355254033.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/4d74781b-8186-4c9a-b659-ff4dc4601d91</li> <li>Publication Reference: Melms et al. (2021) Nature; Publication: https://doi.org/10.1038/s41586-021-03569-1 Dataset Version: https://datasets.cellxgene.cziscience.com/739bae5d-2d6f-4664-a79f-d065610da5ae.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/e4c9ed14-e560-4900-a3bf-b0f8d2ce6a10</li> <li>Publication Reference: Menon et al. (2019) Nat Commun; Publication: https://doi.org/10.1038/s41467-019-12780-8 Dataset Version: https://datasets.cellxgene.cziscience.com/f34734b0-5de7-4b48-a3ba-bd9bb8c48e73.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1a486c4c-c115-4721-8c9f-f9f096e10857</li> <li>Publication Reference: Muto et al. (2021) Nat Commun; Publication: https://doi.org/10.1038/s41467-021-22368-w Dataset Version: https://datasets.cellxgene.cziscience.com/3b84b0b5-3d0a-41a1-860a-bbadad3717bb.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/9b02383a-9358-4f0f-9795-a891ec523bcc</li> <li>Publication Reference: Nowicki-Osuch et al. (2023) Cancer Discovery; Publication: https://doi.org/10.1158/2159-8290.cd-22-0824 Dataset Version: https://datasets.cellxgene.cziscience.com/074caa96-9205-4bc6-b5ba-eef787f131dc.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/a18474f4-ff1e-4864-af69-270b956cee5b</li> <li>Publication Reference: Orozco et al. (2020) Cell Reports; Publication: https://doi.org/10.1016/j.celrep.2019.12.082 Dataset Version: https://datasets.cellxgene.cziscience.com/56334044-9f3a-4541-be24-28bed94c2d4a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/939769a8-d8d2-4d01-abfc-55699893fd49</li> <li>Publication Reference: Otero-Garcia et al. (2022) Neuron; Publication: https://doi.org/10.1016/j.neuron.2022.06.021 Dataset Version: https://datasets.cellxgene.cziscience.com/70170717-45c4-4891-9b14-fb795ecc3d94.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b953c942-f5d8-434f-9da7-e726ba7c1481</li> <li>Publication Reference: Otero-Garcia et al. (2022) Neuron; Publication: https://doi.org/10.1016/j.neuron.2022.06.021 Dataset Version: https://datasets.cellxgene.cziscience.com/b6ddb1cd-f058-450e-8e66-e81f6e8a3c4a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b953c942-f5d8-434f-9da7-e726ba7c1481</li> <li>Publication Reference: Park et al. (2020) Science; Publication: https://doi.org/10.1126/science.aay3224 Dataset Version: https://datasets.cellxgene.cziscience.com/59d5b3c5-9a55-44ae-a7fa-c14567e02755.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/de13e3e2-23b6-40ed-a413-e9e12d7d3910</li> <li>Publication Reference: Park et al. (2020) Science; Publication: https://doi.org/10.1126/science.aay3224 Dataset Version: https://datasets.cellxgene.cziscience.com/c6e08ab6-ab3b-41dc-8058-8e6442e081ec.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/de13e3e2-23b6-40ed-a413-e9e12d7d3910</li> <li>Publication Reference: Perez et al. (2022) Science; Publication: https://doi.org/10.1126/science.abf1970 Dataset Version: https://datasets.cellxgene.cziscience.com/8f2e4a29-e397-4f33-bc9a-3181e06b64b0.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/436154da-bcf1-4130-9c8b-120ff9a888f2</li> <li>Publication Reference: Reed et al. (2024) Nat Genet; Publication: https://doi.org/10.1038/s41588-024-01688-9 Dataset Version: https://datasets.cellxgene.cziscience.com/5a611776-aae0-41b9-9f2b-aaf5f83771a3.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/48259aa8-f168-4bf5-b797-af8e88da6637</li> <li>Publication Reference: Reichart et al. (2022) Science; Publication: https://doi.org/10.1126/science.abo1984 Dataset Version: https://datasets.cellxgene.cziscience.com/e0251a80-0058-4686-9ab8-b2e751313e18.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/e75342a8-0f3b-4ec5-8ee1-245a23e0f7cb</li> <li>Publication Reference: Ren et al. (2021) Cell; Publication: https://doi.org/10.1016/j.cell.2021.01.053 Dataset Version: https://datasets.cellxgene.cziscience.com/ae18e694-6f45-4635-affa-63ac0e29323d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/0a839c4b-10d0-4d64-9272-684c49a2c8ba</li> <li>Publication Reference: Rodr\\u00edguez-Ubreva et al. (2022) Nat Commun; Publication: https://doi.org/10.1038/s41467-022-29450-x Dataset Version: https://datasets.cellxgene.cziscience.com/0ac22902-dfaa-4ceb-9cad-03afbd96f6d4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/bf325905-5e8e-42e3-933d-9a9053e9af80</li> <li>Publication Reference: Rodr\\u00edguez-Ubreva et al. (2022) Nat Commun; Publication: https://doi.org/10.1038/s41467-022-29450-x Dataset Version: https://datasets.cellxgene.cziscience.com/5b4b134c-42f6-415b-a4fd-7f2a60128ff1.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/bf325905-5e8e-42e3-933d-9a9053e9af80</li> <li>Publication Reference: Rodr\\u00edguez-Ubreva et al. (2022) Nat Commun; Publication: https://doi.org/10.1038/s41467-022-29450-x Dataset Version: https://datasets.cellxgene.cziscience.com/d6e742c5-f6e5-42f4-8064-622783542f6b.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/bf325905-5e8e-42e3-933d-9a9053e9af80</li> <li>Publication Reference: Salcher et al. (2022) Cancer Cell; Publication: https://doi.org/10.1016/j.ccell.2022.10.008 Dataset Version: https://datasets.cellxgene.cziscience.com/99040b08-7e1a-4d81-911c-4d2fd2335757.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/edb893ee-4066-4128-9aec-5eb2b03f8287</li> <li>Publication Reference: Seeker et al. (2023) acta neuropathol commun; Publication: https://doi.org/10.1186/s40478-023-01568-z Dataset Version: https://datasets.cellxgene.cziscience.com/32c319ef-10e2-4948-8b40-093d2f9d7cb5.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/9d63fcf1-5ca0-4006-8d8f-872f3327dbe9</li> <li>Publication Reference: Sikkema et al. (2023) Nat Med; Publication: https://doi.org/10.1038/s41591-023-02327-2 Dataset Version: https://datasets.cellxgene.cziscience.com/8d84ba15-d367-4dce-979c-85da70b868a2.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/6f6d381a-7701-4781-935c-db10d30de293</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/005aba12-a5af-4fcd-9b80-e28d845885c8.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/05c60502-d980-4b5f-b093-d27cdf1360cd.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/09df20c1-3c87-465d-b67c-73dc5f3d1bc8.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/0eb52059-c5a4-4b6a-aa95-e9bec179e13c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/0f509022-ab14-4151-a435-37bcf582e20d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/137f5623-e19a-4cb4-9466-05ba393c3552.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/1a5d4cf1-e1a7-4081-918a-8d9cb441cd54.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/1eef8184-4629-406d-b04e-13f1c34b1071.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/1faafbca-0062-4fc0-8b50-a3d32a111d39.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/214485af-4c67-4fda-b430-995162804fbf.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/21fd9d75-dfec-443c-b634-9803792a081c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/229ddec2-a177-4325-bb2c-0ec11092982a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/25a92b24-76c5-4bdb-a431-3478a8b38a69.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/25cc85a9-8645-43cf-a552-d7487c04159e.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/2a8c8c02-e64b-4b39-9b1a-e46247031c1a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/2baa0697-1757-4675-b1ae-51672b2d9bce.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/34c82f0d-04a7-4b13-8d4f-ae06065a2b1c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/364a5dc5-2d78-4931-9eeb-477c56ba63e7.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/366254d0-6aef-4c49-a814-bc195a5bb6c6.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/3a64f908-b132-4d0c-aac6-28012cb6fd11.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/3f0713f8-2fd6-4b77-8ea2-010a0d9b51ae.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/3f1a2741-7b58-4eef-91ad-becc1646b1a4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/3ff56875-32be-4008-be41-c9c6526fc730.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/443f8b59-a15d-4d22-9b18-9ec721560c19.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/45d8a451-d313-465d-b66d-bdea50800ce8.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/47a0e562-2745-4619-9623-d6fa431b4026.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/4a65c6dd-cb77-4cc3-8d95-06957a668c1f.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/4df9fbc5-871b-43be-b796-92cd3f592e32.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/54095aef-f5c6-4dfa-b2e3-00fa8bb8d86a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/544c6998-8d55-4c59-bd18-18c81a3a8b37.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/560a125a-9dd4-4fae-b58a-f24c932c5dd9.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/587acfff-2fb3-46eb-8511-8a1cd2e65ad4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/58f6d3e6-aedd-479f-b2f5-9026fac2011c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/595f2363-dd4a-4184-a75a-c8b0fb62e3d1.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/5ae914f7-a02e-4f08-8bf5-45023b1a23d9.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/5dac55f7-6650-4d24-b8c0-bbb9672ac819.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/5e07340f-a505-4272-a688-de9e104de2d6.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/60034ae4-27a7-46f3-a5aa-668eba808b6f.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/609942ad-2a18-4567-a9b7-8bf126cd1732.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/61ee7a9e-216b-4119-93fa-740fe41a8a0a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/63319798-c980-4fd6-866c-46d9fc1f6d28.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/64f05ce7-291c-487e-9b00-498a2279291d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/6918480f-892a-42bf-a5cd-5e988f6e46b9.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/6a65d707-2c28-4d47-8b77-12f8dd3d66e2.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/6be7d5fc-9024-46a9-84aa-26a230ac6733.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/6cc6a9b9-d53a-4b25-9b3f-d1f00e092d85.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/702d58ff-08f2-48ef-85b7-6132e77efdc4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/705b76b8-68a5-44fb-89ef-fb5273d2ec88.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/717b249c-46e3-43b5-9b3e-93bbdf879874.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/73ec94b8-e1c8-4de8-966a-8dea2842b068.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/744be8ae-ddd2-4822-9537-50030b340bef.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/74903ae9-7470-4795-9508-057a058b9447.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/777952cd-2f82-408b-8890-f084f5825d90.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/7d5e92ed-87fa-4e85-831c-0b98479f3ec6.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/80869ae5-15a7-4ddc-9c2e-4f40dffb2aeb.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/81d55237-e3df-4212-84f8-5ed7a1b5b62e.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/81eb9dd8-2b33-4ff5-aa3f-a8842b68ddca.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/821fa877-c2b1-488f-bc12-606e804f6f4b.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/8268fa76-4996-4a21-b687-ad67c5dc383d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/82a043bf-b6c0-4746-9d69-39437796282e.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/82faf671-658a-4c88-8a7d-618fc7f68fad.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/83b8522d-f362-4aca-8c64-2dfb125b87e1.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/88bacb21-c9d7-4d59-a1d9-ac71d75628b3.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/8faf8f5a-284a-4b2f-b8fe-4a71cb38a79a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/9993afca-cfb9-4ae9-851b-07528edd8b20.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/9b42ebb8-5ebe-4707-bb75-7d64a5fdcafb.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/9f0e0ca6-43ed-4e52-9d85-93cbe24346a5.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/9f90a216-3fac-44ff-b0ca-7b77cf53ef07.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/a1446790-c439-4a23-904d-488028e4d34c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/a4581a49-fe69-4039-97dd-a2c54e676159.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/a497b47b-c4a2-42fe-a7a4-8abc8aed4ec5.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/a561b8a5-9c26-4b52-bb0b-dca989c432cd.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/a737721b-e311-40da-9835-fc60d991be9a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/a7f64bb1-6198-406f-ae78-ddb884633937.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/ab10decf-4e42-44ac-bb45-660771526f96.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/ad40f328-75a3-4b5e-8212-308b7d734d45.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/ae98b1bf-fd73-49a0-9732-41f74da1832b.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/aeb99282-b851-469d-bb77-4c550ae71b37.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/b18522ea-2a30-4f9d-b5c1-aa93a9f5677d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/b38e94df-9801-4cf1-a8e2-8b39f25f56b4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/b47045e2-d8ad-4c72-9412-aa4824f532d3.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/b893efbd-c660-404d-9055-6717b7eba280.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/b9ecd819-1a5b-4e8e-9f2d-833b1891af37.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/bc0c001e-5c7f-43ef-9003-40f2e3015ff7.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/bfbe13d3-d6b8-4fd9-aeb7-c35e80219e1d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/c2b0f135-3723-4351-83e5-c4eef19401f4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/c821776d-a9fd-4a9b-b0ef-65bee87c7dbc.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/cb5f81ff-4676-4ead-be89-e7da582ddd94.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/cc1379b7-5d6e-4944-85ab-6b3ddb602730.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/ccef089c-a215-4607-9617-8e00f40ece67.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/cf701447-cfdd-40fd-b8fe-76d181dad7f9.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/d693ce29-f45e-4871-83e5-c60c001c190d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/d8b0c448-6d8c-4286-aa8b-55372af3ad1a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/dcf1e7b7-6e47-4052-a096-2b2ef8deafce.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/ddf86877-360c-44d0-af09-e6645bd0432a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/de350331-df0e-4b8d-88dc-0bc12c4ca845.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/df9fb19c-3e91-484a-bee6-6db14d649e05.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/e0763860-958c-4f2c-928e-9625f7a37a39.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/e1669dcb-672e-4bca-bf68-6ff242618aa0.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/e683f70e-10b0-4189-ae3f-7007d32ca57a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/e9f575c0-bf54-4252-9551-1d4c69eda1bf.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/f143d458-e4d0-4b44-9e25-d13e8578c9a6.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/f146e689-f045-4b3a-999d-ab0b287391bb.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/f867c94f-c15b-4e40-9ffe-b0c598b90881.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/fdda2f51-9ed9-4959-9897-9581595c1bfb.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Smillie et al. (2019) Cell; Publication: https://doi.org/10.1016/j.cell.2019.06.029 Dataset Version: https://datasets.cellxgene.cziscience.com/6c483976-30de-4835-97f0-2b9bc93614e7.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/33d19f34-87f5-455b-8ca5-9023a2e5453d</li> <li>Publication Reference: Smith et al. (2021) Proc. Natl. Acad. Sci. U.S.A.; Publication: https://doi.org/10.1073/pnas.2023333118 Dataset Version: https://datasets.cellxgene.cziscience.com/bf50dbfb-9ca0-4f0d-8deb-a1a810a0e313.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/e02201d7-f49f-401f-baf0-1eb1406546c0</li> <li>Publication Reference: Smith et al. (2021) Proc. Natl. Acad. Sci. U.S.A.; Publication: https://doi.org/10.1073/pnas.2023333118 Dataset Version: https://datasets.cellxgene.cziscience.com/ff7778bf-7a65-4d23-a9f4-b26c47926c28.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/e02201d7-f49f-401f-baf0-1eb1406546c0</li> <li>Publication Reference: Sol\u00e9-Boldo et al. (2020) Commun Biol; Publication: https://doi.org/10.1038/s42003-020-0922-4 Dataset Version: https://datasets.cellxgene.cziscience.com/bc8d7152-3b69-4153-9314-7342ae58fbde.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/c353707f-09a4-4f12-92a0-cb741e57e5f0</li> <li>Publication Reference: Stephenson et al. (2021) Nat Med; Publication: https://doi.org/10.1038/s41591-021-01329-2 Dataset Version: https://datasets.cellxgene.cziscience.com/46586a98-b75d-4557-9cc4-839fc28e67d5.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/ddfad306-714d-4cc0-9985-d9072820c530</li> <li>Publication Reference: Stewart et al. (2019) Science; Publication: https://doi.org/10.1126/science.aat5031 Dataset Version: https://datasets.cellxgene.cziscience.com/40ebb8e4-1a25-4a33-b8ff-02d1156e4e9b.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/120e86b4-1195-48c5-845b-b98054105eec</li> <li>Publication Reference: Stewart et al. (2019) Science; Publication: https://doi.org/10.1126/science.aat5031 Dataset Version: https://datasets.cellxgene.cziscience.com/fe7e4408-7390-4f93-95aa-ffe472843421.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/120e86b4-1195-48c5-845b-b98054105eec</li> <li>Publication Reference: Strati et al. (2023) Cell Reports Medicine; Publication: https://doi.org/10.1016/j.xcrm.2023.101158 Dataset Version: https://datasets.cellxgene.cziscience.com/21d11474-fe9a-420d-9661-5b88ba407bc1.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/26b5b4f6-828c-4791-b4a3-abb19e3b1952</li> <li>Publication Reference: Suo et al. (2022) Science; Publication: https://doi.org/10.1126/science.abo0510 Dataset Version: https://datasets.cellxgene.cziscience.com/fe340fe0-f2e8-4e7b-8879-0161248129d3.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b1a879f6-5638-48d3-8f64-f6592c1b1561</li> <li>Publication Reference: Szabo et al. (2019) Nat Commun; Publication: https://doi.org/10.1038/s41467-019-12464-3 Dataset Version: https://datasets.cellxgene.cziscience.com/71c5026d-9567-4d0f-a808-edf29440df43.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/24d42e5e-ce6d-45ff-a66b-a3b3b715deaf</li> <li>Publication Reference: The Tabula Sapiens Consortium* et al. (2022) Science; Publication: https://doi.org/10.1126/science.abl4896 Dataset Version: https://datasets.cellxgene.cziscience.com/981bcf57-30cb-4a85-b905-e04373432fef.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/e5f58829-1a66-40b5-a624-9046778e74f5</li> <li>Publication Reference: Travaglini et al. (2020) Nature; Publication: https://doi.org/10.1038/s41586-020-2922-4 Dataset Version: https://datasets.cellxgene.cziscience.com/0fd9c007-8ba2-4d87-8568-c938d2631fba.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/5d445965-6f1a-4b68-ba3a-b8f765155d3a</li> <li>Publication Reference: Travaglini et al. (2020) Nature; Publication: https://doi.org/10.1038/s41586-020-2922-4 Dataset Version: https://datasets.cellxgene.cziscience.com/6dde7580-6b89-4cf9-83b4-c778f06eda7c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/5d445965-6f1a-4b68-ba3a-b8f765155d3a</li> <li>Publication Reference: Triana et al. (2021) Nat Immunol; Publication: https://doi.org/10.1038/s41590-021-01059-0 Dataset Version: https://datasets.cellxgene.cziscience.com/3967ab0f-a63c-4318-809e-73329341ba5e.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/93eebe82-d8c3-41bc-a906-63b5b5f24a9d</li> <li>Publication Reference: Triana et al. (2021) Nat Immunol; Publication: https://doi.org/10.1038/s41590-021-01059-0 Dataset Version: https://datasets.cellxgene.cziscience.com/61f15353-e598-43b5-bb5a-80ac44a0cf0b.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/93eebe82-d8c3-41bc-a906-63b5b5f24a9d</li> <li>Publication Reference: Triana et al. (2021) Nat Immunol; Publication: https://doi.org/10.1038/s41590-021-01059-0 Dataset Version: https://datasets.cellxgene.cziscience.com/9397b7fa-a9b5-4b56-8570-c30cb09d9df8.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/93eebe82-d8c3-41bc-a906-63b5b5f24a9d</li> <li>Publication Reference: Tritschler et al. (2022) Molecular Metabolism; Publication: https://doi.org/10.1016/j.molmet.2022.101595 Dataset Version: https://datasets.cellxgene.cziscience.com/9abfdde4-1d63-4c9a-8ec5-2fff1bd6f387.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/0a77d4c0-d5d0-40f0-aa1a-5e1429bcbd7e</li> <li>Publication Reference: Ulrich et al. (2021) bioRxiv; Publication: https://doi.org/10.1101/2021.09.16.460628 Dataset Version: https://datasets.cellxgene.cziscience.com/7fa27624-7eda-454a-a066-4de49d5788bd.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/fc77d2ae-247d-44d7-aa24-3f4859254c2c</li> <li>Publication Reference: Velmeshev et al. (2023) Science; Publication: https://doi.org/10.1126/science.adf0834 Dataset Version: https://datasets.cellxgene.cziscience.com/b126d3e9-a1a2-419a-a049-b4a27d3ce3ab.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/bacccb91-066d-4453-b70e-59de0b4598cd</li> <li>Publication Reference: Vento-Tormo et al. (2018) Nature; Publication: https://doi.org/10.1038/s41586-018-0698-6 Dataset Version: https://datasets.cellxgene.cziscience.com/7a3bdf11-fbb1-4966-8742-38b574c47317.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/a9254216-6cd8-4186-b32c-349363777584</li> <li>Publication Reference: Wang et al. (2020) eLife; Publication: https://doi.org/10.7554/eLife.62522 Dataset Version: https://datasets.cellxgene.cziscience.com/b0afc679-2ec0-4c8d-9e3e-19cd693f8462.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/625f6bf4-2f33-4942-962e-35243d284837</li> <li>Publication Reference: Wang et al. (2023) Immunity; Publication: https://doi.org/10.1016/j.immuni.2023.01.032 Dataset Version: https://datasets.cellxgene.cziscience.com/07d4feab-33de-4bb2-8a5d-452044bc066d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/03cdc7f4-bd08-49d0-a395-4487c0e5a168</li> <li>Publication Reference: Wang et al. (2023) Immunity; Publication: https://doi.org/10.1016/j.immuni.2023.01.032 Dataset Version: https://datasets.cellxgene.cziscience.com/20f29b78-5b74-4e6d-bc88-679116733988.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/03cdc7f4-bd08-49d0-a395-4487c0e5a168</li> <li>Publication Reference: Wiedemann et al. (2023) Cell Reports; Publication: https://doi.org/10.1016/j.celrep.2023.111994 Dataset Version: https://datasets.cellxgene.cziscience.com/1ffe8f40-d258-4c05-885e-46375c483fc7.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/6d203948-a779-4b69-9b3f-1ee1dadc3980</li> <li>Publication Reference: Wilk et al. (2020) Nat Med; Publication: https://doi.org/10.1038/s41591-020-0944-y Dataset Version: https://datasets.cellxgene.cziscience.com/419da3c2-9141-4654-817f-ee6472df4be3.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/a72afd53-ab92-4511-88da-252fb0e26b9a</li> <li>Publication Reference: Wilson et al. (2022) Nat Commun; Publication: https://doi.org/10.1038/s41467-022-32972-z Dataset Version: https://datasets.cellxgene.cziscience.com/14e77ad6-38ce-4aad-8385-68b21aff0737.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b3e2c6e3-9b05-4da9-8f42-da38a664b45b</li> <li>Publication Reference: Xiang et al. (2020) Front. Cardiovasc. Med.; Publication: https://doi.org/10.3389/fcvm.2020.00052 Dataset Version: https://datasets.cellxgene.cziscience.com/4dc06a70-6d39-4da6-aa8d-2f3fcdbbc1ff.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/9c8808ce-1138-4dbe-818c-171cff10e650</li> <li>Publication Reference: Yan et al. (2020) Sci Rep; Publication: https://doi.org/10.1038/s41598-020-66092-9 Dataset Version: https://datasets.cellxgene.cziscience.com/60eec096-34b9-45c2-b433-3caffb84f955.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1d1c7275-476a-49e2-9022-ad1b1c793594</li> <li>Publication Reference: Yan et al. (2020) Sci Rep; Publication: https://doi.org/10.1038/s41598-020-66092-9 Dataset Version: https://datasets.cellxgene.cziscience.com/6d75c1db-1fe2-4a0e-b55f-680c6df9c99e.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1d1c7275-476a-49e2-9022-ad1b1c793594</li> <li>Publication Reference: Yan et al. (2020) Sci Rep; Publication: https://doi.org/10.1038/s41598-020-66092-9 Dataset Version: https://datasets.cellxgene.cziscience.com/8de7d30b-caeb-4def-aad2-592c39cb3f3c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1d1c7275-476a-49e2-9022-ad1b1c793594</li> <li>Publication Reference: Yan et al. (2020) Sci Rep; Publication: https://doi.org/10.1038/s41598-020-66092-9 Dataset Version: https://datasets.cellxgene.cziscience.com/96c7866c-7111-4687-b2c4-fbd445842a30.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1d1c7275-476a-49e2-9022-ad1b1c793594</li> <li>Publication Reference: Yan et al. (2020) Sci Rep; Publication: https://doi.org/10.1038/s41598-020-66092-9 Dataset Version: https://datasets.cellxgene.cziscience.com/a9d71d53-66a1-4681-ba97-9671c05dff6d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1d1c7275-476a-49e2-9022-ad1b1c793594</li> <li>Publication Reference: Yan et al. (2020) Sci Rep; Publication: https://doi.org/10.1038/s41598-020-66092-9 Dataset Version: https://datasets.cellxgene.cziscience.com/b67dcfb1-ccae-404c-b0d4-d681ac227858.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1d1c7275-476a-49e2-9022-ad1b1c793594</li> <li>Publication Reference: Yazar et al. (2022) Science; Publication: https://doi.org/10.1126/science.abf3041 Dataset Version: https://datasets.cellxgene.cziscience.com/b3e8792e-a31b-404b-a866-250c43bc06d5.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/dde06e0f-ab3b-46be-96a2-a8082383c4a1</li> <li>Publication Reference: Yoshida et al. (2022) Nature; Publication: https://doi.org/10.1038/s41586-021-04345-x Dataset Version: https://datasets.cellxgene.cziscience.com/69be948c-03f4-46e8-896e-530c79080808.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/03f821b4-87be-4ff4-b65a-b5fc00061da7</li> <li>Publication Reference: Yoshida et al. (2022) Nature; Publication: https://doi.org/10.1038/s41586-021-04345-x Dataset Version: https://datasets.cellxgene.cziscience.com/d911082e-b5e2-40e6-8f08-fb53c7894622.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/03f821b4-87be-4ff4-b65a-b5fc00061da7</li> <li>Publication Reference: Yu et al. (2021) Cell; Publication: https://doi.org/10.1016/j.cell.2021.04.028 Dataset Version: https://datasets.cellxgene.cziscience.com/c7bec699-47c3-44ee-a311-ae8507adf6bb.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/dfc09a93-bce0-4c77-893d-e153d1b7f9fa</li> <li>Publication Reference: Yu et al. (2021) Cell; Publication: https://doi.org/10.1016/j.cell.2021.04.028 Dataset Version: https://datasets.cellxgene.cziscience.com/da7ca6a3-079c-428f-8453-9b21ecce87a7.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/dfc09a93-bce0-4c77-893d-e153d1b7f9fa</li> <li>Publication Reference: Zhang et al. (2021) Proc. Natl. Acad. Sci. U.S.A.; Publication: https://doi.org/10.1073/pnas.2103240118 Dataset Version: https://datasets.cellxgene.cziscience.com/b6a3566e-a7bf-4fb3-b20c-858c6330e380.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1df8c90d-d299-4b2e-a54d-a5a80f36e780</li> <li>Publication Reference: van Zyl et al. (2022) Proc. Natl. Acad. Sci. U.S.A.; Publication: https://doi.org/10.1073/pnas.2200914119 Dataset Version: https://datasets.cellxgene.cziscience.com/47573dc2-1dfc-4ca8-8c6e-b705a6656159.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/63d03351-06be-478e-a0db-f7a653b6b19b</li> <li>Publication Reference: van Zyl et al. (2022) Proc. Natl. Acad. Sci. U.S.A.; Publication: https://doi.org/10.1073/pnas.2200914119 Dataset Version: https://datasets.cellxgene.cziscience.com/9fa63ab8-6316-4460-8283-af290fec1654.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/63d03351-06be-478e-a0db-f7a653b6b19b</li> <li>Publication Reference: van Zyl et al. (2022) Proc. Natl. Acad. Sci. U.S.A.; Publication: https://doi.org/10.1073/pnas.2200914119 Dataset Version: https://datasets.cellxgene.cziscience.com/a76ab167-d254-4128-b5ca-86960560074a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/63d03351-06be-478e-a0db-f7a653b6b19b</li> <li>Publication Reference: van Zyl et al. (2022) Proc. Natl. Acad. Sci. U.S.A.; Publication: https://doi.org/10.1073/pnas.2200914119 Dataset Version: https://datasets.cellxgene.cziscience.com/e999edf8-f5e6-4af7-975c-1d8f0a5e8d3e.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/63d03351-06be-478e-a0db-f7a653b6b19b</li> <li>Publication Reference: van Zyl et al. (2022) Proc. Natl. Acad. Sci. U.S.A.; Publication: https://doi.org/10.1073/pnas.2200914119 Dataset Version: https://datasets.cellxgene.cziscience.com/f5f99f11-5a99-4a72-bd0d-0a87c5b6b406.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/63d03351-06be-478e-a0db-f7a653b6b19b</li> <li>Publication Reference: van der Wijst et al. (2021) Sci. Transl. Med.; Publication: https://doi.org/10.1126/scitranslmed.abh2624 Dataset Version: https://datasets.cellxgene.cziscience.com/02a1eee1-e290-47d1-8d9d-bc7f51c13670.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/7d7cabfd-1d1f-40af-96b7-26a0825a306d</li> </ul>"},{"location":"main/datasets/uniprot/","title":"UniProt Dataset","text":"<p>The UniProt Knowledgebase (UniProtKB) is an open database of protein sequences curated from translated genomic data [1]. The UniProt Reference Cluster (UniRef) databases provide clustered sets of sequences from UniProtKB [2], which have been used in previous large language model training studies to improve diversity in protein training data. UniRef clusters proteins hierarchically. At the highest level, UniRef100 groups proteins with identical primary sequences from the UniProt Archive (UniParc). UniRef90 clusters these unique sequences into buckets with 90% sequence similarity, selecting a single sequence from within each cluster as the representative sequence. UniRef50 is then built by clustering these UniRef90 representative sequences into groups with 50% sequence similarity.</p>"},{"location":"main/datasets/uniprot/#data-used-for-esm-2-pre-training","title":"Data Used for ESM-2 Pre-training","text":"<p>Since the original train/test splits from ESM-2 were not available [3], we replicated the ESM-2 pre-training experiments with UniProt's 2024_03 release. Following the approach described by the ESM-2 authors, we removed artificial sequences and reserved 0.5% of UniRef50 clusters for validation. From the 65,672,139 UniRef50 clusters, this resulted in 328,360 validation sequences. We then ran MMSeqs to further ensure no contamination of the training set with sequences similar to the validation set. This resulted in 65,182,365 training UniRef50 clusters comprising 187,382,018 UniRef90 sequences.</p> <p>Pretraining batches were formed by uniformly sampling each UniRef50 cluster from the training database, taking a randomly chosen UniRef90 sequence from each.</p>"},{"location":"main/datasets/uniprot/#data-availability","title":"Data Availability","text":"<p>Two versions of the dataset are distributed, a full training dataset (~80GB) and a 10,000 UniRef50 cluster random slice (~150MB). To load and use the sanity dataset, use the bionemo.core.data.load function to materialize the sanity dataset in the BioNeMo2 cache directory:</p> <pre><code>from bionemo.core.data.load import load\n\nsanity_data_dir = load(\"esm2/testdata_esm2_pretrain:2.0\")\n</code></pre>"},{"location":"main/datasets/uniprot/#ngc-resource-links","title":"NGC Resource Links","text":"<ul> <li>Sanity Dataset</li> <li>[Full Dataset]</li> </ul>"},{"location":"main/datasets/uniprot/#references","title":"References","text":"<ol> <li> <p>UniProt Consortium. (2023). UniProt: The universal protein knowledgebase in 2023. Nucleic Acids Research, 51(D1),    D523\u2013D531. doi:10.1093/nar/gkac1052</p> </li> <li> <p>Suzek, B. E., Wang, Y., Huang, H., McGarvey, P. B., Wu, C. H., &amp; UniProt Consortium. (2015). UniRef clusters: a    comprehensive and scalable alternative for improving sequence similarity searches. Bioinformatics (Oxford, England),    31(6), 926\u2013932. doi:10.1093/bioinformatics/btu739</p> </li> <li> <p>Lin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., \u2026 Rives, A. (2023). Evolutionary-scale prediction of    atomic-level protein structure with a language model. Science (New York, N.Y.), 379(6637), 1123\u20131130.    doi:10.1126/science.ade2574</p> </li> </ol>"},{"location":"main/developer-guide/SUMMARY/","title":"SUMMARY","text":"<ul> <li>bionemo-core</li> <li>bionemo-evo2</li> <li>bionemo-example-model</li> <li>bionemo-llm</li> <li>bionemo-moco</li> <li>bionemo-noodles</li> <li>bionemo-scdl</li> <li>bionemo-size-aware-batching</li> <li>bionemo-testing</li> <li>bionemo-webdatamodule</li> </ul>"},{"location":"main/developer-guide/bionemo-core/bionemo-core-Overview/","title":"bionemo-core","text":"<p>Common code that all BioNeMo framework packages depend on. Contains highly reusable, battle-tested abstractions and implementations that are valuable across a wide variety of domains and applications.</p> <p>Crucially, the <code>bionemo-core</code> Python package (namespace <code>bionemo.core</code>) depends on PyTorch and PyTorch Lightning. Other key BioNeMo component libraries, such as <code>bionemo-llm</code> obtain their PyTorch dependencies via <code>bionemo-core</code>.</p>"},{"location":"main/developer-guide/bionemo-core/bionemo-core-Overview/#developer-setup","title":"Developer Setup","text":"<p>After following the setup specified in the README, you may install this project's code in your environment via executing:</p> <pre><code>pip install -e .\n</code></pre> <p>To run unit tests with code coverage, execute:</p> <pre><code>pytest -v --cov=bionemo --cov-report=term .\n</code></pre>"},{"location":"main/developer-guide/bionemo-core/bionemo-core-Overview/#package-highlights","title":"Package Highlights","text":"<p>In <code>bionemo.core.model.config</code>:</p> <ul> <li><code>ModelOutput</code>: A Model's forward pass may produce a tensor, multiple tensors, or named tensors.</li> <li><code>LossType</code>: A generic type parameter for a loss function.</li> <li><code>Model</code>: An interface for any ML model that accepts and produces <code>torch.Tensor</code>s.</li> <li><code>ModelType</code>: A generic type parameter that is constrained to the <code>Model</code> interface.</li> <li><code>BionemoModelConfig</code>: An abstract class that enables parameterizable model instantiation that is compatible with Megatron.</li> <li><code>BionemoTrainableModelConfig</code>: An extension that includes the loss function to use with the model during training.</li> </ul> <p>In <code>bionemo.core.utils</code>:</p> <ul> <li>the <code>batching_utils</code> module's <code>pad_token_ids</code>, which pads token ids with padding value &amp; returns a mask.</li> <li>the <code>dtype</code> module's <code>get_autocast_dtype</code>, which converts from nemo/nvidia datatypes to their PyTorch equivalents.</li> <li>the <code>random_utils</code> module, which includes functions for managing random seeds and performing sampling.</li> </ul> <p>In the <code>bionemo.data</code> package, there is:</p> <ul> <li><code>multi_epoch_dataset</code>: contains many dataset implements that are useful for mutli-epoch training.</li> <li><code>resamplers</code>: contains a P-RNG based Dataset implementation.</li> </ul> <p>There's a constant global value, <code>bionemo.core.BIONEMO_CACHE_DIR</code>, which is used as a local on-disk cache for resources.</p>"},{"location":"main/developer-guide/bionemo-evo2/bionemo-evo2-Overview/","title":"bionemo-evo2","text":"<p><code>bionemo-evo2</code> is a <code>pip</code>-installable package that contains data preprocessing, training, and inferencing code for Evo2, a new <code>Hyena</code>-based foundation model for genome generation and understanding. Built upon <code>Megatron-LM</code> parallelism and <code>NeMo2</code> algorithms, <code>bionemo-evo2</code> provides the remaining tools necessary to effectively fine-tune the pre-trained Evo2 model checkpoint on user-provided sequences at scale, and generate state-of-the-art life-like DNA sequences from Evo2 for downstream metagenomic tasks.</p>"},{"location":"main/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#available-models-in-ngc","title":"Available models in NGC","text":"HF Model BioNeMo Resource Name Blackwell FP8 Blackwell BF16 Hopper FP8 Hopper BF16 Ampere Notes arcinstitute/savanna_evo2_1b_base evo2/1b-8k:1.0 \u2705 \u274c \u2705 \u274c \u274c Low accuracy on bf16 (eg ampere) GPUs evo2/1b-8k-bf16:1.0 \u2705 \u2705 \u2705 \u2705 \u2705 Fine-tuned variant of the 1b-8k that supports bf16 as well as fp8, enabling ampere as well as hopper/blackwell. arcinstitute/savanna_evo2_7b_base evo2/7b-8k:1.0 \u2705 \u2705 \u2705 \u2705 \u2705 The original 7b models have good accuracy across the board at bf16 and fp8 across tested hardware. arcinstitute/savanna_evo2_7b evo2/7b-1m:1.0 \u2705 \u2705 \u2705 \u2705 \u2705 The original 7b models have good accuracy across the board at bf16 and fp8 across tested hardware. arcinstitute/savanna_evo2_40b_base ? ? ? ? ? Unknown, likely has the same support pattern as the 40b-1m row below since this is the same model at an earlier step of training. arcinstitute/savanna_evo2_40b \u274c \u274c \u2705 \u274c \u274c The original 40b-1m context trained model only supports hpper fp8 evo2/40b-1m-fp8-bf16:1.0 \u2705 \u2705 \u2705 \u2705 \u2705 A fine-tuned variant of arcinstitute/savanna_evo2_40b with broad hardware support (fp8 or bf16 and ampere, hopper, and blackwell have all been tested). The original model only has good accuracy on hopper fp8. <p>On the CLI you can access the resources in this table (and others) with:</p> <pre><code>CKPT_PATH=$(download_bionemo_data evo2/40b-1m-fp8-bf16:1.0)\n</code></pre> <p>In code these resources can be accessed with:</p> <pre><code>from bionemo.core.data.load import load\n\nckpt_path = load(\"evo2/40b-1m-fp8-bf16:1.0\")\n</code></pre> <p>Or you can follow the links in the table above to the ngc registry and follow the download links from there.</p> <p>Note, in the following two sections, the model described as <code>ft1(step199)</code> is the model that was released above as <code>evo2/40b-1m-fp8-bf16:1.0</code>.</p>"},{"location":"main/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#loss-evaluation","title":"Loss evaluation","text":"device model_size is_finetune fine_tune_desc precision ctx_length average_nll Notes a100 1b FALSE None bf16 8192 1.242033 1b base model works ok on b300, but cannot handle bf16 precision (and by extension ampere) h200 1b FALSE None fp8 8192 1.076465 b300 1b FALSE None fp8 8192 1.084777 h200 1b FALSE None bf16 8192 1.243525 b300 1b FALSE None bf16 8192 1.243527 a100 1b TRUE ft bf16 8192 1.078681 1b base model fine-tuned for bf16 can handle both bf16 and b300. B300 accuracy is also more similar to H200 accuracy after fine-tuning to handle bf16. Ampere appears to work fine as well. h200 1b TRUE ft fp8 8192 1.078623 b300 1b TRUE ft fp8 8192 1.07901 h200 1b TRUE ft bf16 8192 1.078671 b300 1b TRUE ft bf16 8192 1.078694 a100 7b-1m FALSE None bf16 8192 0.995102 7b model got lucky in training and generalizes well to bf16 precision as well as to blackwell and ampere. h200 7b-1m FALSE None fp8 8192 0.995265 b300 7b-1m FALSE None fp8 8192 0.9951 h200 7b-1m FALSE None bf16 8192 0.995109 b300 7b-1m FALSE None bf16 8192 0.99535 a100 40b-1m FALSE None bf16 8192 1.702023 40b model got unlucky in training. It is sensitive to fp8 and within that appears to have memorized the known difference in hopper that leads to lower accuracy when using standard fp8 computations. (see Deepseek V3 paper where they point out the hopper difference in the \u201cIncreasing Accumulation Precision\u201d sub-section where hopper uses 14 bits to accumulate partials rather than the typical 32 bits). It does not work well on bf16 and that seems to carry over to ampere as expected. Note if we set (use_split_accumulator=True) to True by setting https://github.com/NVIDIA/TransformerEngine/blob/bd55e7ba5f0235a80eaa63d49adaa8fb7c6ced50/transformer_engine/pytorch/module/base.py#L56 to True then the fp8 is more accurate which breaks fp8 on hopper, making it seem more like blackwell. h200 40b-1m FALSE None fp8 8192 0.922422 b300 40b-1m FALSE None fp8 8192 1.789 h200 40b-1m FALSE None fp8-delayed(use_split_accumulator=True) 8192 1.791161 h200 40b-1m FALSE None bf16 8192 1.70015 b300 40b-1m FALSE None bf16 8192 1.700162 a100 40b-1m TRUE ft0 bf16 8192 0.962564 The first fine-tuning run used a global batch size of 4 rather than 16. The training loss curve was very unstable which could have lead to a lower quality fine-tune. This was successful in that every hardware and fp8 precision combination works to some degree. The accuracy sits between the 7b and 40b checkpoints. This is also reflected in a 1% AUC drop on the BRCA1 notebook. https://wandb.ai/nvidia/evo2_40b_finetune/runs/Alp3KXuC/overview. Note that the accuracy on hopper or blackwell bf16 seems to closely track with ampere bf16. h200 40b-1m TRUE ft0 fp8 8192 0.963434 b300 40b-1m TRUE ft0 fp8 8192 0.95985 h200 40b-1m TRUE ft0 fp8-delayed(use_split_accumulator=True) 8192 0.959287 h200 40b-1m TRUE ft0 bf16 8192 0.962654 b300 40b-1m TRUE ft0 bf16 8192 0.962621 a100 40b-1m TRUE ft1(step119) bf16 8192 0.955813 The second fine-tuning run has the same accuracy in the BRCA notebook as the original model, and maintains similar accuracy on hopper at fp8 (0.926 vs 0.922). Unfortunately the accuracy drops somewhat on bf16 as well as blackwell, but it is marginally better than the previous fine-tuning run. Accuracy closely tracks between ampere, hopper, and blackwell at bf16. h200 40b-1m TRUE ft1(step119) fp8 8192 0.926986 b300 40b-1m TRUE ft1(step119) fp8 8192 0.954112 h200 40b-1m TRUE ft1(step119) fp8-delayed(use_split_accumulator=True) 8192 0.953928 h200 40b-1m TRUE ft1(step119) bf16 8192 0.955881 b300 40b-1m TRUE ft1(step119) bf16 8192 0.955859 h200 40b-1m TRUE ft1(step279) fp8 8192 1.379552 Interestingly if you keep training the model, the accuracy continues to degrade on validation slightly, but note that the model has now shifted its sensitivity away from the fp8 rounding pecularity on hopper to requring the more accurate FP8 implementation on blackwell. Perhaps fine-tuning at a lower learning rate (I used the final minimal learning rate from the pretraining run), with more dropout (I used 0.1% dropout), or more weight decay (I set a very smalll value to nearly disable it rather than how the model was trained at 0.1). https://wandb.ai/nvidia/evo2_40b_finetune/runs/Ji2IRcrz/overview. Note if we set (use_split_accumulator=True) to True by setting https://github.com/NVIDIA/TransformerEngine/blob/bd55e7ba5f0235a80eaa63d49adaa8fb7c6ced50/transformer_engine/pytorch/module/base.py#L56 to True. b300 40b-1m TRUE ft1(step279) fp8 8192 0.958749 h200 40b-1m TRUE ft1(step279) fp8-delayed(use_split_accumulator=True) 8192 0.957551 h200 40b-1m TRUE ft1(step279) bf16 8192 0.959398 b300 40b-1m TRUE ft1(step279) bf16 8192 0.959373"},{"location":"main/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#auc-evaluation","title":"AUC Evaluation","text":"device model_size is_finetune fine_tune_desc precision BRCA1 SM AUC BRCA1 Bal AUC BRCA1 AUC A100 40b TRUE ft1(step119) BF16 0.86 H200 40b TRUE ft1(step119) BF16 B300 40b TRUE ft1(step119) BF16 B300 40b TRUE ft1(step119) FP8 0.87 H200 40b TRUE ft1(step119) FP8 0.88 A100 40b TRUE ft1(step279) BF16 0.86 B300 40b TRUE ft1(step279) BF16 B300 40b TRUE ft1(step279) FP8 H200 40b TRUE ft1(step279) FP8 0.5 A100 7b-1m FALSE BF16 0.88 B300 7b-1m FALSE FP8 0.88 H200 7b-1m FALSE FP8 0.88 H200 40b TRUE ft0(step2600) FP8 0.47 B300 40b TRUE ft0(step870) BF16 0.86 B300 40b TRUE ft0(step870) FP8 0.86 H200 40b TRUE ft0(step870) FP8 0.86 0.86 H200 40b FALSE FP8 0.85 0.87 A100 40b FALSE BF16 B300 40b FALSE BF16 0.55 H200 40b FALSE BF16 0.53 B300 40b FALSE FP8 0.48"},{"location":"main/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#quickstart-tutorials","title":"Quickstart tutorials","text":"<p>Two Jupyter notebooks are available to help you get started with Evo 2: one demonstrating how to finetune the model on your own sequences, and another showing how to perform zero-shot BRCA1 variant effect prediction.</p> <ul> <li> <p>Finetuning</p> </li> <li> <p>Zeroshot BRCA1 Variant Effect Prediction</p> </li> </ul>"},{"location":"main/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#installation","title":"Installation","text":"<p>To install this package, execute the following command:</p> <pre><code>pip install -e .\n</code></pre> <p>To run unit tests, execute the following command:</p> <pre><code>pytest -v .\n</code></pre>"},{"location":"main/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#preprocessing","title":"Preprocessing","text":"<p>To train or fine-tune Evo2 on a custom dataset, we need to preprocess and index sequence data for training from raw FASTA files into tokenized binaries compliant with <code>NeMo2</code> / <code>Megatron-LM</code>. For more information about how to configure your data for training, refer to data/README.md and utils.config.Evo2PreprocessingConfig.</p> <pre><code>preprocess_evo2 -c &lt;CONFIG_PATH&gt;\n</code></pre>"},{"location":"main/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#training","title":"Training","text":"<p>Given a preprocessed collection of preprocessed datasets, and optionally a pre-trained NeMo2 checkpoint for Evo2, training can be executed using the following command:</p> <pre><code>$ train_evo2 --help\nusage: train_evo2 [-h] (-d DATASET_CONFIG | --mock-data) [--dataset-dir DATASET_DIR] [--num-nodes NUM_NODES] [--devices DEVICES] [--seq-length SEQ_LENGTH] [--tensor-parallel-size TENSOR_PARALLEL_SIZE]\n                  [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE] [--context-parallel-size CONTEXT_PARALLEL_SIZE] [--create-tensorboard-logger]\n                  [--wandb-entity WANDB_ENTITY] [--wandb-project WANDB_PROJECT] [--wandb-tags WANDB_TAGS [WANDB_TAGS ...]] [--wandb-group WANDB_GROUP] [--wandb-job-type WANDB_JOB_TYPE] [--wandb-id WANDB_ID]\n                  [--wandb-anonymous] [--wandb-log-model] [--wandb-offline] [--sequence-parallel] [--fp8] [--micro-batch-size MICRO_BATCH_SIZE] [--global-batch-size GLOBAL_BATCH_SIZE] [--grad-acc-batches GRAD_ACC_BATCHES]\n                  [--max-steps MAX_STEPS] [--early-stop-on-step EARLY_STOP_ON_STEP] [--val-check-interval VAL_CHECK_INTERVAL] [--grad-reduce-in-fp32] [--fp8-wgrad] [--use-megatron-comm-overlap-llama3-8k] [--tp-comm-overlap-backend {nccl,mpi,gloo}]\n                  [--align-param-gather] [--model-size {1b,1b_nv,40b,40b_arc_longcontext,40b_nv,7b,7b_arc_longcontext,7b_nv,test,test_nv}] [--add-bias-output] [--result-dir RESULT_DIR] [--experiment-name EXPERIMENT_NAME]\n                  [--limit-val-batches LIMIT_VAL_BATCHES] [--log-every-n-steps LOG_EVERY_N_STEPS] [--ckpt-dir CKPT_DIR] [--wd WD] [--restore-optimizer-from-ckpt] [--no-average-in-collective] [--seed SEED]\n                  [--workers WORKERS] [--gc-interval GC_INTERVAL] [--enable-preemption] [--ckpt-async-save] [--ckpt-format {torch_dist,zarr}] [--eod-pad-in-loss-mask] [--cross-entropy-loss-fusion] [--no-fp32-residual-connection]\n                  [--debug-ddp-parity-freq DEBUG_DDP_PARITY_FREQ] [--hybrid-override-pattern HYBRID_OVERRIDE_PATTERN] [--num-layers NUM_LAYERS] [--create-tflops-callback] [--log-parameters-and-shapes] [--lr LR] [--min-lr MIN_LR]\n                  [--warmup-steps WARMUP_STEPS] [--nsys-profiling] [--nsys-start-step NSYS_START_STEP] [--nsys-end-step NSYS_END_STEP] [--no-renormalize-loss] [--nsys-ranks NSYS_RANKS [NSYS_RANKS ...]]\n                  [--activation-checkpoint-recompute-num-layers ACTIVATION_CHECKPOINT_RECOMPUTE_NUM_LAYERS] [--disable-checkpointing] [--clip-grad CLIP_GRAD] [--seq-len-interpolation-factor SEQ_LEN_INTERPOLATION_FACTOR]\n                  [--overlap-param-gather] [--overlap-grad-reduce] [--hidden-dropout HIDDEN_DROPOUT] [--attention-dropout ATTENTION_DROPOUT] [--save-top-k SAVE_TOP_K] [--metric-to-monitor-for-checkpoints METRIC_TO_MONITOR_FOR_CHECKPOINTS] [--save-last-checkpoint] [--no-save-last-checkpoint] [--no-activation-checkpointing | --selective-activation-checkpointing]\n\nTrain a Hyena model using NeMo 2.0.\n\noptions:\n  -h, --help            show this help message and exit\n  -d DATASET_CONFIG, --dataset-config DATASET_CONFIG\n                        Path to the blended / weighted training dataset configuration YAML. (default: None)\n  --mock-data           Train with Mock data (for testing/debugging), either set this or provide a dataset config. (default: False)\n  --dataset-dir DATASET_DIR\n                        Absolute path to the dataset directory. Defaults to using the absolute or relative paths (dataset_prefix) specified in the dataset config YAML. (default: None)\n  --num-nodes NUM_NODES\n                        Number of nodes to use for training, defaults to 1. (default: 1)\n  --devices DEVICES     Number of devices to use for training, defaults to 1. (default: 1)\n  --seq-length SEQ_LENGTH\n                        Training sequence length (default: 8192)\n  --tensor-parallel-size TENSOR_PARALLEL_SIZE\n                        Order of tensor parallelism. Defaults to 1. (default: 1)\n  --pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE\n                        Order of pipeline parallelism. Defaults to 1. (default: 1)\n  --context-parallel-size CONTEXT_PARALLEL_SIZE\n                        Order of context parallelism. Defaults to 1. (default: 1)\n  --create-tensorboard-logger\n                        Create a tensorboard logger. (default: False)\n  --wandb-entity WANDB_ENTITY\n                        The team posting this run (default: None)\n  --wandb-project WANDB_PROJECT\n                        Wandb project name (default: None)\n  --wandb-tags WANDB_TAGS [WANDB_TAGS ...]\n                        Tags associated with this run (default: None)\n  --wandb-group WANDB_GROUP\n                        A unique string shared by all runs in a given group (default: None)\n  --wandb-job-type WANDB_JOB_TYPE\n                        A unique string representing a type of run, which is useful when you're grouping runs together into larger experiments using group. (default: None)\n  --wandb-id WANDB_ID   Sets the version, mainly used to resume a previous run (default: None)\n  --wandb-anonymous     Enable or explicitly disable anonymous logging (default: False)\n  --wandb-log-model     Save checkpoints in wandb dir to upload on W&amp;B servers (default: False)\n  --wandb-offline       Use wandb in offline mode (default: False)\n  --sequence-parallel   Set to enable sequence parallelism. (default: False)\n  --fp8                 Set to enable FP8 (default: False)\n  --micro-batch-size MICRO_BATCH_SIZE\n                        Micro-batch size for data-parallel training. (default: 1)\n  --global-batch-size GLOBAL_BATCH_SIZE\n                        Global batch size for training. If set to None, infer it from the TP, CP, and PP parameters. (default: None)\n  --grad-acc-batches GRAD_ACC_BATCHES\n                        Number of batches to accumulate gradients over. (default: 1)\n  --max-steps MAX_STEPS\n                        Number of training optimizer update steps. This controls the total number of steps as well as the shape of the learning rate curve. (default: 500000)\n  --early-stop-on-step EARLY_STOP_ON_STEP\n                        Stop training on this step, if set. This may be useful for testing or debugging purposes. (default: None)\n  --val-check-interval VAL_CHECK_INTERVAL\n                        Number of steps between validation measurements and model checkpoints. (default: None)\n  --grad-reduce-in-fp32\n                        Gradient reduce in FP32. (default: False)\n  --fp8-wgrad           Faster option that is maybe less accurate (TBD) when using fp8. (default: False)\n  --use-megatron-comm-overlap-llama3-8k\n  --tp-comm-overlap-backend {nccl,mpi,gloo}\n                        TP communication backend to use. Defaults to 'nccl'. (default: nccl)\n  --align-param-gather\n  --model-size {1b,1b_nv,40b,40b_arc_longcontext,40b_nv,7b,7b_arc_longcontext,7b_nv,test,test_nv}\n                        Model architecture to use, choose between 7b, 40b, or test (a sub-model of 4 layers, less than 1B parameters). '_arc_1m' models have GLU / FFN dimensions that support 1M context length when trained with TP&lt;=8. (default: 7b)\n  --add-bias-output     Add bias to the output layer to enable learning a simple prior. (default: False)\n  --result-dir RESULT_DIR\n                        Path to the result directory. (default: results)\n  --experiment-name EXPERIMENT_NAME\n                        Name of the experiment. (default: evo2)\n  --limit-val-batches LIMIT_VAL_BATCHES\n                        Number of validation steps (default: 20)\n  --log-every-n-steps LOG_EVERY_N_STEPS\n                        Number of steps between logging. (default: 1)\n  --ckpt-dir CKPT_DIR   Directory to restore an initial checkpoint from. Use this for supervised fine-tuning. (default: None)\n  --wd WD               Weight decay for optimizer. (default: 0.01)\n  --restore-optimizer-from-ckpt\n                        Restore optimizer state from initial checkpoint. Defaults to False. (default: False)\n  --no-average-in-collective\n                        Avaerage optimizer state in collective rather than dividing by dp size and summing. (default: False)\n  --seed SEED           Set random seed for training. (default: 1234)\n  --workers WORKERS     Number of workers to use for data loading. (default: 8)\n  --gc-interval GC_INTERVAL\n                        Set to a value &gt; 0 if you want to synchronize garbage collection, will do gc every gc-interval steps. (default: 0)\n  --enable-preemption   Enable preemption hooks. If enabled this will save a checkpoint whenever slurm exits. (default: False)\n  --ckpt-async-save\n  --ckpt-format {torch_dist,zarr}\n                        Specify checkpoint format to use. Defaults to 'torch_dist', as 'zarr' is deprecated. Only use if resuming training from a zarr checkpoint. (default: torch_dist)\n  --eod-pad-in-loss-mask\n                        Do not predict EOD/Pad tokens (typical default, but not default in original evo2). (default: False)\n  --cross-entropy-loss-fusion\n                        Use the faster, but maybe less accurate fused form of cross entropy, which also has bf16 grads internally. (default: False)\n  --no-fp32-residual-connection\n                        If set, turn off fp32 residual connections which may be faster but may impact accuracy. (default: False)\n  --debug-ddp-parity-freq DEBUG_DDP_PARITY_FREQ\n                        Set to value &gt; 0 to debug DDP weight parity between ranks. (default: 0)\n  --hybrid-override-pattern HYBRID_OVERRIDE_PATTERN\n                        Override the hybrid override pattern in the config (specifies hyena layer ordering and type). (default: None)\n  --num-layers NUM_LAYERS\n                        If set, override the number of layers specified in the requested config. (default: None)\n  --create-tflops-callback\n                        Enable tflops calculation callback for Hyena / Evo2. Defaults to False. (default: False)\n  --log-parameters-and-shapes\n                        Log training parameters shapes and dtypes for debugging. (default: False)\n  --lr LR               Learning rate. (default: 0.0003)\n  --min-lr MIN_LR       Min learning rate in cosine annealing. (default: 3e-05)\n  --warmup-steps WARMUP_STEPS\n                        Number of warmup steps in cosine annealing (default: 2500)\n  --nsys-profiling      Enable targeted `nsys` profiling on the training loop for a defined step range. To actually get profiling output you must run the whole program with `nsys`. For example: `nsys profile -s none -o output_report_name -t cuda,nvtx --force-overwrite true --capture-range=cudaProfilerApi --capture-range-end=stop [regular python command\n                        here]` (default: False)\n  --nsys-start-step NSYS_START_STEP\n                        Start nsys profiling after this step. (default: 0)\n  --nsys-end-step NSYS_END_STEP\n                        End nsys profiling after this step. (default: None)\n  --no-renormalize-loss\n                        Do not renormalize the loss weights. (default: False)\n  --nsys-ranks NSYS_RANKS [NSYS_RANKS ...]\n                        Enable nsys profiling for these ranks. (default: [0])\n  --activation-checkpoint-recompute-num-layers ACTIVATION_CHECKPOINT_RECOMPUTE_NUM_LAYERS\n                        If set, override the default value set in the config. (default: None)\n  --disable-checkpointing\n                        Disable creating a ModelCheckpoint callback. (default: True)\n  --clip-grad CLIP_GRAD\n                        Grad clip value. Note that when using DDP this may need to be inflated. (default: 1.0)\n  --seq-len-interpolation-factor SEQ_LEN_INTERPOLATION_FACTOR\n                        Adjusts the linear scaling of ROPE (Rotary Position Embedding) for context extension. Set this factor relative to your base context length e.g., for an original context length of 8192 and an extended context length of 524288, use 524288/8192 = 64. (default: None)\n  --overlap-param-gather\n                        Overlap the parameter gather with the optimizer step. This is currently disabled due to a NeMo bug when using DDP. Making this an option defaulting to False is a temporary solution until the bug is fixed. (default: False)\n  --overlap-grad-reduce\n                        Overlap the gradient reduce with the optimizer step. (default: False)\n  --hidden-dropout HIDDEN_DROPOUT\n                        Dropout probability for the hyena layers (default: 0.0)\n  --attention-dropout ATTENTION_DROPOUT\n                        Dropout probability for the attention layers. (default: 0.0)\n  --save-top-k SAVE_TOP_K\n                        Number of best checkpoints to keep. Set to -1 to save all checkpoints. (default: 5)\n  --metric-to-monitor-for-checkpoints METRIC_TO_MONITOR_FOR_CHECKPOINTS\n                        Metric to monitor for checkpoints. (default: val_loss)\n  --save-last-checkpoint\n                        Save the last checkpoint. (default: True)\n  --no-save-last-checkpoint\n                        Disable saving the last checkpoint. (default: True)\n  --no-activation-checkpointing\n  --selective-activation-checkpointing\n</code></pre> <p>To supply a pre-trained checkpoint, pass the NeMo2 checkpoint directory to <code>--ckpt-dir</code>, and the script will dump newly trained checkpoints and logs to <code>--experiment-dir</code>. However, if there are existing well-defined checkpoints in the directory specified by <code>--experiment-dir</code>, the script will automatically resume training from the most recent checkpoint in the experiment directory instead of starting from the checkpoint specified by <code>--ckpt-dir</code>, which streamlines long training sessions. (To disable this behavior, supply a new or clean <code>--experiment-dir</code> when restarting from <code>--ckpt-dir</code>.)</p> <p>Training data and sampling weights can be specified using the <code>--dataset-config</code> argument as a YAML file adhering to the following schema: utils.config.Evo2BlendedDatasetConfig. For more information about dataset sampling and blending during training with Megatron-LM, refer to megatron/core/datasets/readme.md. For example:</p> <pre><code>- dataset_prefix: /workspace/bionemo2/data/metagenomics/pretraining_data_metagenomics/data_metagenomics_train_text_CharLevelTokenizer_document\n  dataset_split: train\n  dataset_weight: 0.18\n- dataset_prefix: /workspace/bionemo2/data/gtdb_imgpr/pretraining_data_gtdb_imgpr/data_gtdb_imgpr_train_text_CharLevelTokenizer_document\n  dataset_split: train\n  dataset_weight: 0.24\n- dataset_prefix: /workspace/bionemo2/data/imgvr_untagged/imgvr_untagged_data/data_imgvr_train_text_CharLevelTokenizer_document\n  dataset_split: train\n  dataset_weight: 0.03\n- dataset_prefix: /workspace/bionemo2/data/promoters/pretraining_data_promoters/data_promoters_valid_text_CharLevelTokenizer_document\n  dataset_split: validation\n  dataset_weight: 0.0003\n- dataset_prefix: /workspace/bionemo2/data/organelle/pretraining_data_organelle/data_organelle_valid_text_CharLevelTokenizer_document\n  dataset_split: validation\n  dataset_weight: 0.005\n- dataset_prefix: /workspace/bionemo2/data/metagenomics/pretraining_data_metagenomics/data_metagenomics_test_text_CharLevelTokenizer_document\n  dataset_split: test\n  dataset_weight: 0.18\n- dataset_prefix: /workspace/bionemo2/data/gtdb_v220/gtdb_v220_imgpr_merged_data/data_gtdb_imgpr_test_text_CharLevelTokenizer_document\n  dataset_split: test\n  dataset_weight: 0.24\n</code></pre>"},{"location":"main/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#inference","title":"Inference","text":"<p>Once you have a pre-trained or fine-tuned Evo2 checkpoint, you can also prompt the model to generate DNA sequences using the following command:</p> <pre><code>$ infer_evo2 --help\nusage: infer_evo2 [-h] [--prompt PROMPT] --ckpt-dir CKPT_DIR [--temperature TEMPERATURE] [--top-k TOP_K] [--top-p TOP_P] [--max-new-tokens MAX_NEW_TOKENS] [--tensor-parallel-size TENSOR_PARALLEL_SIZE] [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE] [--context-parallel-size CONTEXT_PARALLEL_SIZE] [--output-file OUTPUT_FILE]\n\noptions:\n  -h, --help            show this help message and exit\n  --prompt PROMPT       Prompt to generate text from Evo2. Defaults to a phylogenetic lineage tag for E coli.\n  --ckpt-dir CKPT_DIR   Path to checkpoint directory containing pre-trained Evo2 model.\n  --temperature TEMPERATURE\n                        Temperature during sampling for generation.\n  --top-k TOP_K         Top K during sampling for generation.\n  --top-p TOP_P         Top P during sampling for generation.\n  --max-new-tokens MAX_NEW_TOKENS\n                        Maximum number of tokens to generate.\n  --tensor-parallel-size TENSOR_PARALLEL_SIZE\n                        Order of tensor parallelism. Defaults to 1.\n  --pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE\n                        Order of pipeline parallelism. Defaults to 1.\n  --context-parallel-size CONTEXT_PARALLEL_SIZE\n                        Order of context parallelism. Defaults to 1.\n  --output-file OUTPUT_FILE\n                        Output file containing the generated text produced by the Evo2 model. If not provided, the output will be logged.\n</code></pre> <p>As in <code>train_evo2</code>, <code>--ckpt-dir</code> points to the NeMo2 checkpoint directory for Evo2 that you want to load for inference. <code>--output-file</code> can be used to dump the output into a <code>.txt</code> file, and if not specified the output will be logged in the terminal.</p> <pre><code>[NeMo I 2025-01-06 17:22:22 infer:102] ['CTCTTCTGGTATTTGG']\n</code></pre>"},{"location":"main/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#prediction","title":"Prediction","text":"<p>To run a forward pass of the Evo2 model, you can call <code>predict_evo2</code>, which processes a batch of sequences and returns either raw token logits or, if <code>--output-log-prob-seqs</code> is set, log-probability scores.</p> <p>For example, to predict the log-probability scores of a batch of sequences saved to <code>fasta_path</code>, you can run the following command:</p> <pre><code>predict_evo2 \\\n  --fasta &lt;fasta_path&gt; \\\n  --ckpt-dir &lt;PATH_TO_CHECKPOINT&gt; \\\n  --output-dir &lt;PATH_TO_OUTPUT_FILE&gt; \\\n  --model-size 1b \\\n  --tensor-parallel-size 1 \\\n  --pipeline-model-parallel-size 1 \\\n  --context-parallel-size 1 \\\n  --output-log-prob-seqs\n</code></pre> <p>An example of using <code>predict_evo2</code> for variant effect prediction can be found in our Evo 2 Zeroshot BRCA1 Notebook. This notebook demonstrates how to use Evo2 to predict whether single nucleotide variants (SNVs) in the BRCA1 gene are likely to be harmful to protein function and potentially increase cancer risk, by comparing the model's log probability scores between the reference and variant sequences.</p>"},{"location":"main/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#context-extension","title":"Context Extension","text":"<p>Evo2 supports continuing training with longer context lengths beyond those used to train a prior checkpoint. For example, when training the original Evo2 model, the first phase of training was performed at 8192 context length while the next phase continued training at 1m context length, but starting from the prior 8192 context length checkpoint. We call this process context extension.</p> <p>To change the sequence length used in training in this way, supply the prior checkpoint as the <code>--ckpt-dir</code> argument, and set your new desired sequence length with <code>--seq-length</code>. Only doing these two things will run, but one issue is that the model's ROPE embeddings may not be scaled properly out of the box for a new context length. The way that Arc institute handled this was by setting the <code>--seq-len-interpolation-factor</code> to linearly scale the ROPE embedding for context extension. For example, if the base context length is 8192 and the extended context length is 65536, the factor would be 65536/8192 = 8. There are other ways of accomplishing this as well that may require some minor code changes, such as the approach used in llama-3, which is also available in megatron and could be added into argparse as an alternative.</p>"},{"location":"main/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#checkpoint-conversion-from-hugging-face-to-nemo2","title":"Checkpoint conversion from hugging face to NeMo2","text":"<p>The following conversion script should work on any savanna formatted arc evo2 checkpoint. Make sure you match up the model size with the checkpoint you are converting. The pyproject.toml makes the conversion script available as a command line tool <code>evo2_convert_to_nemo2</code>, so you can try replacing:</p> <pre><code>evo2_convert_to_nemo2 \\\n  ...\n</code></pre> <p>with the following if you want to run with <code>-m pdb</code> or something:</p> <pre><code>python \\\n  sub-packages/bionemo-evo2/src/bionemo/evo2/utils/checkpoint/convert_to_nemo.py \\\n  ...\n</code></pre>"},{"location":"main/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#1b-8k","title":"1b-8k","text":"<pre><code>evo2_convert_to_nemo2 \\\n  --model-path hf://arcinstitute/savanna_evo2_1b_base \\\n  --model-size 1b --output-dir nemo2_evo2_1b_8k\n</code></pre> <p>This new checkpoint <code>nemo2_evo2_1b_8k</code> is ready to go in nemo2 format in downstream pretraining or prediction workflows.</p>"},{"location":"main/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#optional-steps-if-you-want-to-register-the-model-with-ngc","title":"Optional steps if you want to register the model with NGC","text":"<p>If you want to register the checkpoint with NGC (typically only NVIDIA employees) then you can do the following.</p> <p>To create the checkpoint for distribution in NGC, first cd into the checkpiont directory:</p> <pre><code>cd nemo2_evo2_1b_8k\n</code></pre> <p>Then run the following command to make a tar of the full directory that gets unpacked into the current directory which our NGC loader expects:</p> <pre><code>tar -czvf ../nemo2_evo2_1b_8k.tar.gz .\n</code></pre> <p>Finally <code>sha256sum</code> the tar file to get the checksum:</p> <pre><code>sha256sum nemo2_evo2_1b_8k.tar.gz\n</code></pre> <p>Then register it into the loader for testing purposes by editing <code>sub-packages/bionemo-core/src/bionemo/core/data/resources/evo2.yaml</code>.</p>"},{"location":"main/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#7b-8k","title":"7b-8k","text":"<pre><code>evo2_convert_to_nemo2 \\\n  --model-path hf://arcinstitute/savanna_evo2_7b_base \\\n  --model-size 7b --output-dir nemo2_evo2_7b_8k\n</code></pre>"},{"location":"main/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#7b-1m","title":"7b-1M","text":"<pre><code>evo2_convert_to_nemo2 \\\n  --model-path hf://arcinstitute/savanna_evo2_7b \\\n  --model-size 7b_arc_longcontext --output-dir nemo2_evo2_7b_1m\n</code></pre>"},{"location":"main/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#40b-8k","title":"40b-8k","text":"<pre><code>evo2_convert_to_nemo2 \\\n  --model-path hf://arcinstitute/savanna_evo2_40b_base \\\n  --model-size 40b --output-dir nemo2_evo2_40b_8k\n</code></pre>"},{"location":"main/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#40b-1m","title":"40b-1M","text":"<pre><code>evo2_convert_to_nemo2 \\\n  --model-path hf://arcinstitute/savanna_evo2_40b \\\n  --model-size 40b_arc_longcontext --output-dir nemo2_evo2_40b_1m\n</code></pre>"},{"location":"main/developer-guide/bionemo-example_model/bionemo-example_model-Overview/","title":"bionemo-example_model","text":"<p>This is a minimalist package containing an example model that makes use of bionemo2 and nemo conventions. It contains the necessary models, dataloaders, datasets, and custom loss functions. The referenced classes and functions are in <code>bionemo.example_model.lightning.lightning_basic</code>.</p> <p>This tutorial demonstrates the creation of a simple MNIST model. This should be run in a BioNeMo container. The BioNeMo Framework container can run in a brev.dev launchable: . It takes about 10 minutes to deploy this notebook as a Launchable. As of this writing, we are working on a free tier so a credit card may be required. You can reach out to your NVIDIA rep for credit. Notebooks and a shell interface can be launched by clicking <code>Open Notebook</code>. (Note: This links to the nightly release and may be out of sync with these docs.)</p> <p>For this tutorial, we will reuse elements from the BioNeMo example_model package.</p> <p><code>Megatron</code>/<code>NeMo</code> modules and datasets are special derivatives of PyTorch modules and datasets that extend and accelerate the distributed training and inference capabilities of PyTorch.</p> <p>Some distinctions of Megatron/NeMo are:</p> <ul> <li><code>torch.nn.Module</code>/<code>LightningModule</code> changes into <code>MegatronModule</code>.</li> <li>Loss functions should extend the <code>MegatronLossReduction</code> module and implement a <code>reduce</code> method for aggregating loss across multiple micro-batches.</li> <li>Megatron configuration classes (for example <code>megatron.core.transformer.TransformerConfig</code>) are extended with a <code>configure_model</code> method that defines how model weights are initialized and loaded in a way that is compliant with training via NeMo2.</li> <li>Various modifications and extensions to common PyTorch classes, such as adding a <code>MegatronDataSampler</code> (and re-sampler such as <code>PRNGResampleDataset</code> or <code>MultiEpochDatasetResampler</code>) to your <code>LightningDataModule</code>.</li> </ul>"},{"location":"main/developer-guide/bionemo-example_model/bionemo-example_model-Overview/#loss-functions","title":"Loss Functions","text":"<p>First, we define a simple loss function in <code>bionemo.example_model.lightning.lightning_basic</code>. These should extend the <code>MegatronLossReduction</code> class. The output of forward and backward passes happen in parallel. There should be a forward function that calculates the loss defined. The reduce function is required.</p> <p>Loss functions used here are <code>MSELossReduction</code> and <code>ClassifierLossReduction</code>. These functions return a Tensor, which contain the losses for the microbatches, and a <code>SameSizeLossDict</code> containing the average loss. This is a Typed Dictionary that is the return type for a loss that is computed for the entire batch, where all microbatches are the same size.</p>"},{"location":"main/developer-guide/bionemo-example_model/bionemo-example_model-Overview/#datasets-and-datamodules","title":"Datasets and Datamodules","text":"<p>Datasets used for model training must be compatible with Megatron datasets. To enable this, the output of a given index and epoch must be deterministic. However, we may wish to have a different ordering in every epoch. To enable this, the items in the dataset should be accessible by both the epoch and the index. This can be done by accessing elements of the dataset with <code>EpochIndex</code> from <code>bionemo.core.data.multi_epoch_dataset</code>. A simple way of doing this is to wrap a dataset with <code>IdentityMultiEpochDatasetWrapper</code> imported from <code>bionemo.core.data.multi_epoch_dataset</code>. In this example, in in <code>bionemo.example_model.lightning.lightning_basic</code>, we use a custom dataset <code>MNISTCustomDataset</code> that wraps the <code>__getitem__</code> method of the MNIST dataset such that it returns a dict instead of a Tuple or tensor. The <code>MNISTCustomDataset</code> returns elements of type <code>MnistItem</code>, which is a <code>TypedDict</code>.</p> <p>In the data module/data loader class, it is necessary to have a data_sampler attribute to shuffle the data and that allows the sampler to be used with Megatron. This is a nemo2 peculiarity. A <code>nemo.lightning.pytorch.plugins.MegatronDataSampler</code> is the best choice. It sets up the capability to utilize micro-batching and gradient accumulation. It is also the place where the global batch size is constructed.</p> <p>Also the sampler will not shuffle your data. So you need to wrap your dataset in a dataset shuffler that maps sequential IDs to random IDs in your dataset. This can be done with <code>MultiEpochDatasetResampler</code> from <code>bionemo.core.data.multi_epoch_dataset</code>.</p> <p>This is implemented in the <code>MNISTDataModule</code>. In the setup method of the dataloader, the train, test and validation sets are <code>MNISTCustomDataset</code> are wrapped in the <code>IdentityMultiEpochDatasetWrapper</code>. These are then wrapped in the <code>MultiEpochDatasetResampler</code>. More information about <code>MegatronCompatability</code> and how to set up more complicated datasets can be found in <code>docs.user-guide.background.megatron_datasets.md</code>.</p> <p>We also define a <code>train_dataloader</code>, <code>val_dataloader</code>, and <code>predict_dataloader</code> methods that return the corresponding dataloaders.</p>"},{"location":"main/developer-guide/bionemo-example_model/bionemo-example_model-Overview/#models","title":"Models","text":"<p>Models need to be Megatron modules. At the most basic level this just means:</p> <ol> <li>They extend <code>MegatronModule</code> from megatron.core.transformer.module.</li> <li>They need a config argument of type <code>megatron.core.ModelParallelConfig</code>. An easy way of implementing this is to inherit from <code>bionemo.llm.model.config.MegatronBioNeMoTrainableModelConfig</code>. This is a class for BioNeMo that supports usage with Megatron models, as NeMo2 requires. This class also inherits <code>ModelParallelConfig</code>.</li> <li>They need a self.<code>model_type:megatron.core.transformer.enums.ModelType</code> enum defined (<code>ModelType.encoder_or_decoder</code> is a good option.)</li> <li><code>def set_input_tensor(self, input_tensor)</code> needs to be present. This is used in model parallelism. This function can be a stub/placeholder function.</li> </ol> <p>The following models are implemented in <code>bionemo.example_model.lightning.lightning_basic</code>.</p> <p><code>ExampleModelTrunk</code> is a base model. This returns a tensor. <code>ExampleModel</code> is a model that extends the base model with a few linear layers and it is used for pretraining. This returns the output of the base model and of the full model.</p> <p><code>ExampleFineTuneModel</code> extends the <code>ExampleModelTrunk</code> by adding a classification layer. This returns a tensor of logits over the 10 potential digits.</p>"},{"location":"main/developer-guide/bionemo-example_model/bionemo-example_model-Overview/#model-configs","title":"Model Configs","text":"<p>The model config class is used to instantiate the model. These configs must have:</p> <ol> <li>A <code>configure_model</code> method which allows the Megatron strategy to lazily initialize the model after the parallel computing environment has been setup. These also handle loading starting weights for fine-tuning cases. Additionally these configs tell the trainer which loss you want to use with a matched model.</li> <li>A <code>get_loss_reduction_class</code> method that defines the loss function.</li> </ol> <p>The following configs are implemented in <code>bionemo.example_model.lightning.lightning_basic</code>.</p> <p>Here, a base generic config <code>ExampleGenericConfig</code> is defined. <code>PretrainConfig</code> extends this class. This defines the model class and the loss class in:</p> <pre><code>class PretrainConfig(ExampleGenericConfig[\"PretrainModel\", \"MSELossReduction\"], iom.IOMixinWithGettersSetters):\n\n    model_cls: Type[PretrainModel] = PretrainModel\n    loss_cls: Type[MSELossReduction] = MSELossReduction\n</code></pre> <p>Similarly, <code>ExampleFineTuneConfig</code> extends <code>ExampleGenericConfig</code> for finetuning.</p>"},{"location":"main/developer-guide/bionemo-example_model/bionemo-example_model-Overview/#training-module","title":"Training Module","text":"<p>It is helpful to have a training module that inherits from <code>lightning.pytorch.LightningModule</code> which organizes the model architecture, training, validation, and testing logic while abstracting away boilerplate code, enabling easier and more scalable training. This wrapper can be used for all model and loss combinations specified in the config. In <code>bionemo.example_model.lightning.lightning_basic</code>, we define <code>BionemoLightningModule</code>.</p> <p>In this example, <code>training_step</code>, <code>validation_step</code>, and <code>predict_step</code> define the training, validation, and prediction loops are independent of the forward method. In nemo:</p> <ol> <li>NeMo's Strategy overrides the <code>train_step</code>, <code>validation_step</code> and <code>prediction_step</code> methods.</li> <li>The strategies' training step will call the forward method of the model.</li> <li>That forward method then calls the wrapped forward step of <code>MegatronParallel</code> which wraps the forward method of the model.</li> <li>That wrapped forward step is then executed inside the <code>MegatronCore</code> scheduler, which calls the <code>_forward_step</code> method from the <code>MegatronParallel</code> class.</li> <li>Which then calls the <code>training_step</code>, <code>validation_step</code> and <code>prediction_step</code> function here.</li> </ol> <p>Additionally, during these steps, we log the validation, testing, and training loss. This is done similarly to https://lightning.ai/docs/torchmetrics/stable/pages/lightning.html. These logs can then be exported to wandb, or other metric viewers. For more complicated tracking, it may be necessary to use pytorch callbacks: https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html.</p> <p>Further <code>loss_reduction_class()</code>, <code>training_loss_reduction()</code>, <code>validation_loss_reduction(),</code> and<code>test_loss_reduction()</code> are defined based on what's in the config. Additionally, <code>configure_model()</code> is defined based on the config.</p>"},{"location":"main/developer-guide/bionemo-example_model/bionemo-example_model-Overview/#training-the-models","title":"Training the models","text":"<p>In <code>bionemo.example_model.lightning.lightning_basic</code> a checkpoint_callback variable is defined. This enables .nemo file-like checkpointing.</p> <p>The remaining functions are defined in the training scripts: <code>pretrain_mnist.py</code>, <code>finetune_mnist.py</code>, and <code>predict_mnist.py</code>.</p> <p>We specify a training strategy of type <code>nemo.lightning.MegatronStrategy</code>. This strategy implements model parallelism using NVIDIA's Megatron-LM framework. It supports various forms of parallelism including tensor model parallelism, pipeline model parallelism, sequence parallelism, and expert parallelism for efficient training of large language models.</p> <p>We specify a trainer of type <code>nemo.lightning.Trainer</code>, which is an extension of the pytorch lightning trainer. This is where the devices, validation intervals, maximal steps, maximal number of epochs, and how frequently to log are specified.</p> <p>We specify a nemo-logger. We can set TensorBoard and WandB logging, along with extra loggers. Here, we specify a <code>CSVLogger</code> from lightning.pytorch.loggers.</p> <p>We can now proceed to training. The first pre-training scripts is <code>bionemo/example_model/training_scripts/pretrain_mnist.py</code></p> <p>Then, we train the model with the <code>BionemoLightningModule</code>, <code>MNISTDataModule</code>, trainer and nemo_logger.</p> <p>This script will print out the location of the final model: \\ <p>Then we can run a finetuning-script:</p> <pre><code>python src/bionemo/example_model/training_scripts/finetune_mnist.py ---pretrain_ckpt_dirpath &lt;pretrain_directory&gt;\n</code></pre> <p>A nuance here is that in the config file, we specify the initial checkpoint path, along with which keys to skip. In the previous model checkpoint, we did not have a head labelled \"digit_classifier\", so we specify it as a head to be skipped. This script will print the location of the finetuned directory: \\. <p>Finally, we can run a classification task with</p> <pre><code>python src/bionemo/example_model/training_scripts/predict_mnist.py  --finetune_dir &lt;finetune_dir&gt;.\n</code></pre> <p>The results can be viewed with TensorBoardLogger if that is configured, or as a CSV file created by the <code>CSVLogger</code>.</p>"},{"location":"main/developer-guide/bionemo-llm/bionemo-llm-Overview/","title":"bionemo-llm","text":"<p>The Bionemo Large Language Model (LLM) submodule contains common code used in submodules that train LLMs on biological datasets. This includes data masking and collate functions, the bio-BERT common architecture code, loss functions, and other NeMo / Megatron-LM compatibility functions. Sub-packages should only depend on <code>bionemo-llm</code> if they need access to NeMo and Megatron-LM.</p>"},{"location":"main/developer-guide/bionemo-moco/bionemo-moco-Overview/","title":"Modular Co-Design (MoCo) Interpolants","text":"<p>MoCo enables abstracted interpolants for building and sampling from a variety of popular generative model frameworks. Specifically, MoCo supports interpolants for both continuous and discrete data types. </p>"},{"location":"main/developer-guide/bionemo-moco/bionemo-moco-Overview/#continuous-data-interpolants","title":"Continuous Data Interpolants","text":"<p>MoCo currently supports the following continuous data interpolants:</p> <ul> <li>DDPM (Denoising Diffusion Probabilistic Models)</li> <li>VDM (Variational Diffusion Models)</li> <li>CFM (Conditional Flow Matching)</li> </ul>"},{"location":"main/developer-guide/bionemo-moco/bionemo-moco-Overview/#discrete-data-interpolants","title":"Discrete Data Interpolants","text":"<p>MoCo also supports the following discrete data interpolants:</p> <ul> <li>D3PM (Discrete Denoising Diffusion Probabilistic Models)</li> <li>MDLM (Masked Diffusion Language Models)</li> <li>DFM (Discrete Flow Matching)</li> </ul>"},{"location":"main/developer-guide/bionemo-moco/bionemo-moco-Overview/#useful-abstractions","title":"Useful Abstractions","text":"<p>MoCo also provides useful wrappers for customizable time distributions and inference time schedules.</p>"},{"location":"main/developer-guide/bionemo-moco/bionemo-moco-Overview/#extendible","title":"Extendible","text":"<p>If the desired interpolant or sampling method is not already supported, MoCo was designed to be easily extended.</p>"},{"location":"main/developer-guide/bionemo-moco/bionemo-moco-Overview/#installation","title":"Installation","text":"<p>For Conda environment setup, please refer to the <code>environment</code> directory for specific instructions.</p> <p>Once your environment is set up, you can install this project by running the following command:</p> <pre><code>pip install -e .\n</code></pre> <p>This will install the project in editable mode, allowing you to make changes and see them reflected immediately.</p>"},{"location":"main/developer-guide/bionemo-moco/bionemo-moco-Overview/#examples","title":"Examples","text":"<p>Please see examples of all interpolants in the examples directory.</p>"},{"location":"main/developer-guide/bionemo-noodles/bionemo-noodles-Overview/","title":"bionemo-noodles","text":"<p><code>bionemo-noodles</code> is a Python wrapper of noodles that extends FAIDX to support <code>memmap</code>-based file I/O for FASTA files.</p>"},{"location":"main/developer-guide/bionemo-noodles/bionemo-noodles-Overview/#installation","title":"Installation","text":"<p>To install from PyPI, execute the following command:</p> <pre><code>pip install bionemo-noodles\n</code></pre>"},{"location":"main/developer-guide/bionemo-noodles/bionemo-noodles-Overview/#compatibility","title":"Compatibility","text":"<p><code>bionemo-noodles</code> has pre-built wheels for Python/Cython <code>3.10</code>, <code>3.11</code>, and <code>3.12</code>, and is compatible with <code>manylinux_2_28</code> on <code>x86_64</code>.</p> <p>For a custom build configuration that is not currently supported on PyPI, reach out to: bionemofeedback@nvidia.com</p>"},{"location":"main/developer-guide/bionemo-noodles/bionemo-noodles-Overview/#usage","title":"Usage","text":"<p>An example <code>torch.utils.data.Dataset</code> using <code>NvFaidx</code> / <code>bionemo-noodles</code>:</p> <pre><code>import json\nfrom pathlib import Path\n\nimport torch\n\nfrom bionemo.noodles.nvfaidx import NvFaidx\n\nclass SimpleFastaDataset(torch.utils.data.Dataset):\n\n    def __init__(self, fasta_path: Path, tokenizer):\n        \"\"\"Initialize the dataset.\"\"\"\n        super().__init__()\n        self.fasta = NvFaidx(fasta_path)\n        self.seqids = sorted(self.fasta.keys())\n        self.tokenizer = tokenizer\n\n    def write_idx_map(self, output_dir: Path):\n        \"\"\"Write the index map to the output directory.\"\"\"\n        with open(output_dir / \"seq_idx_map.json\", \"w\") as f:\n            json.dump({seqid: idx for idx, seqid in enumerate(self.seqids)}, f)\n\n    def __len__(self):\n        \"\"\"Get the length of the dataset.\"\"\"\n        return len(self.seqids)\n\n    def __getitem__(self, idx: int) -&gt; dict[str, torch.Tensor]:\n        \"\"\"Get an item from the dataset.\"\"\"\n        sequence = self.fasta[self.seqids[idx]].sequence().upper()\n        tokenized_seq = self.tokenizer.text_to_ids(sequence)\n        loss_mask = torch.ones_like(torch.tensor(tokenized_seq, dtype=torch.long), dtype=torch.long)\n        return {\n            \"tokens\": torch.tensor(tokenized_seq, dtype=torch.long),\n            \"position_ids\": torch.arange(len(tokenized_seq), dtype=torch.long),\n            \"seq_idx\": torch.tensor(idx, dtype=torch.long),\n            \"loss_mask\": loss_mask,\n        }\n</code></pre>"},{"location":"main/developer-guide/bionemo-noodles/bionemo-noodles-Overview/#bionemo-framework-ecosystem-development","title":"BioNeMo Framework Ecosystem Development","text":"<p>To install this sub-package locally (with <code>--editable</code>):</p> <pre><code>pip install -e .\n</code></pre> <p>To run unit tests, execute:</p> <pre><code>pytest -v .\n</code></pre> <p>To build wheels for different Python, Linux, and system architecture configurations, run the BioNeMo Sub-Package GitHub Actions Workflow (bionemo-subpackage-ci.yml)</p>"},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/","title":"BioNeMo-SCDL: Single Cell Data Loading for Scalable Training of Single Cell Foundation Models.","text":""},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#package-overview","title":"Package Overview","text":"<p>BioNeMo-SCDL provides an independent pytorch-compatible dataset class for single cell data with a consistent API. BioNeMo-SCDL is developed and maintained by NVIDIA. This package can be run independently from BioNeMo. It improves upon simple AnnData-based dataset classes in the following ways:</p> <ul> <li>A consistent API across input formats that is promised to be consistent across package versions.</li> <li>Improved performance when loading large datasets. It allows for loading and fast iteration of large datasets.</li> <li>Ability to use datasets that are much, much larger than memory. This is because the datasets are stored in a numpy memory-mapped format.</li> <li>Additionally, conversion of large (significantly larger than memory) AnnData files into the SCDL format.</li> <li>[Future] Full support for ragged arrays (i.e., datasets with different feature counts; currently only a subset of the API functionality is supported for ragged arrays).</li> <li>[Future] Support for improved compression.</li> </ul> <p>BioNeMo-SCDL's API resembles that of AnnData, so code changes are minimal. In most places a simple swap from an attribute to a function is sufficient (i.e., swapping <code>data.n_obs</code> for <code>data.number_of_rows()</code>).</p>"},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#installation","title":"Installation","text":"<p>This package can be installed with</p> <pre><code>pip install bionemo-scdl\n</code></pre>"},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#usage","title":"Usage","text":""},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#getting-example-data","title":"Getting example data","text":"<p>Here is how to process an example dataset from CellxGene with ~25,000 cells:</p> <p>Download \"https://datasets.cellxgene.cziscience.com/97e96fb1-8caf-4f08-9174-27308eabd4ea.h5ad\" to hdf5s/97e96fb1-8caf-4f08-9174-27308eabd4ea.h5ad</p>"},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#loading-a-single-cell-dataset-from-an-h5ad-file","title":"Loading a single cell dataset from an H5AD file","text":"<pre><code>from bionemo.scdl.io.single_cell_memmap_dataset import SingleCellMemMapDataset\n\ndata = SingleCellMemMapDataset(\n    \"97e_scmm\", \"hdf5s/97e96fb1-8caf-4f08-9174-27308eabd4ea.h5ad\"\n)\n</code></pre> <p>This creates a <code>SingleCellMemMapDataset</code> that is stored at 97e_scmm in large, memory-mapped arrays that enables fast access of datasets larger than the available amount of RAM on a system.</p> <p>If the dataset is large, the AnnData file can be lazy-loaded and then read in based on chunks of rows in a paginated manner. This can be done by setting the parameters when instantiating the <code>SingleCellMemMapDataset</code>:</p> <ul> <li><code>paginated_load_cutoff</code>, which sets the minimal file size in megabytes at which an AnnData file will be read in in a paginated manner.</li> <li><code>load_block_row_size</code>, which is the number of rows that are read into memory at a given time.</li> </ul>"},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#loading-rawx-vs-x-from-the-anndata-file","title":"Loading <code>raw.X</code> vs <code>.X</code> from the anndata file","text":"<p>By default, SCDL will load the data from the <code>raw.X</code> in the anndata file. If using the <code>.X</code> is desired, set <code>use_X_not_raw = True</code> during the dataset creation:</p> <pre><code>from bionemo.scdl.io.single_cell_memmap_dataset import SingleCellMemMapDataset\n\ndata = SingleCellMemMapDataset(\n    \"97e_scmm\", \"hdf5s/97e96fb1-8caf-4f08-9174-27308eabd4ea.h5ad\"\n)\n</code></pre>"},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#interrogating-single-cell-datasets-and-exploring-the-api","title":"Interrogating single cell datasets and exploring the API","text":"<pre><code>data.number_of_rows()\n## 25382\n\ndata.number_of_variables()\n## [34455]\n\ndata.number_of_values()\n## 874536810\n\ndata.number_nonzero_values()\n## 26947275\n</code></pre>"},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#saving-scdl-single-cell-dataloader-datasets-to-disk","title":"Saving SCDL (Single Cell DataLoader) datasets to disk","text":"<p>When you open a SCDL dataset, you must choose a path where the backing data structures are stored. However, these structures are not guaranteed to be in a valid serialized state during runtime.</p> <p>Calling the <code>save</code> method guarantees the on-disk object is in a valid serialized state, at which point the current python process can exit, and the object can be loaded by another process later.</p> <pre><code>data.save()\n</code></pre>"},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#loading-scdl-datasets-from-a-scdl-archive","title":"Loading SCDL datasets from a SCDL archive","text":"<p>When you're ready to reload a SCDL dataset, just pass the path to the serialized data:</p> <pre><code>reloaded_data = SingleCellMemMapDataset(\"97e_scmm\")\n</code></pre>"},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#using-scdl-datasets-in-model-training","title":"Using SCDL datasets in model training","text":"<p>SCDL implements the required functions of the PyTorch Dataset abstract class.</p>"},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#tokenization","title":"Tokenization","text":"<p>A common use case for the single-cell dataloader is tokenizing data using a predefined vocabulary with a defined tokenizer function. These features (that in anndata are stored in .var) can be accessed with <code>return_var_features</code> and setting <code>var_feature_names</code> to the desired feature names. Similarly, row-wise features (that are in the .obs,) can be accessed with <code>return_obs_features</code> and setting <code>obs_feature_names</code>.</p> <pre><code>import numpy as np\n\nds = SingleCellMemMapDataset(\"97e_scmm\")\nindex = 0\nvalues, var_feature_ids, obs_feature_ids = ds.get_row(\n    index,\n    return_var_features=True,\n    var_feature_names=[\"feature_id\"],\n    return_obs_features=True,\n    obs_feature_names=[\"cell_line\"],\n)\nassert (\n    len(var_feature_ids) == 1 and len(obs_feature_ids) == 1\n)  # we expect feature_ids to be a list containing one np.array with the row's feature ids\ngene_data, col_idxs = np.array(values[0]), np.array(values[1])\ntokenizer_function = lambda x, y, z, w: x\ntokenizer_function(gene_data, col_idxs, var_feature_ids[0], obs_feature_ids[0])\n</code></pre>"},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#observed-obs-features","title":"Observed (.obs) features","text":"<p>Observed (row-level, per-cell) features can be accessed using the <code>.obs_features()</code> method on your dataset instance. This method allows you to retrieve per-cell metadata stored in the <code>.obs</code> of underlying AnnData files, either as a single dictionary (if your SCDL archive came from a single AnnData file) or as a list of dictionaries (if multiple input files were concatenated and their <code>.obs</code> columns differ).</p> <p>You can use integer indexing to get the features for a single cell, or slicing to get features for a range of cells:</p> <pre><code># Get .obs features for cells 5 through 9\ndf = data.obs_features()[5:10]\n\n# Get .obs features for cell 3\nrow = data.obs_features()[3]\n</code></pre>"},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#loading-directly-with-pytorch-compatible-dataloaders","title":"Loading directly with PyTorch-compatible DataLoaders","text":"<p>You can use PyTorch-compatible DataLoaders to load batches of data from a SCDL class. With a batch size of 1, this can be run without a collation function. With a batch size greater than 1, there is a collation function (<code>collate_sparse_matrix_batch</code>) that will collate several sparse arrays into the CSR (Compressed Sparse Row) PyTorch tensor format.</p> <pre><code>from torch.utils.data import DataLoader\nfrom bionemo.scdl.util.torch_dataloader_utils import collate_sparse_matrix_batch\n\n## Mock model: you can remove this and pass the batch to your own model in actual code.\nmodel = lambda x: x\n\ndataloader = DataLoader(\n    data, batch_size=8, shuffle=True, collate_fn=collate_sparse_matrix_batch\n)\nn_epochs = 2\nfor e in range(n_epochs):\n    for batch in dataloader:\n        model(batch)\n</code></pre>"},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#data-type-casting","title":"Data Type Casting","text":"<p>SCDL lets you control both storage size and numerical precision by specifying the data type for values loaded from AnnData <code>.X</code>. Supported types include \"uint8\", \"uint16\", \"uint32\", \"uint64\", \"float16\", \"float32\", and \"float64\". Choosing a smaller type (like \"uint8\" or \"float16\") results in more compact storage, while selecting a higher-precision type (such as \"float64\") uses more space but preserves maximum accuracy. You set the data type at the time of dataset creation from an AnnData file using:</p> <pre><code>from bionemo.scdl.io.single_cell_memmap_dataset import SingleCellMemMapDataset\n\ndata = SingleCellMemMapDataset(\n    \"97e_scmm\", \"hdf5s/97e96fb1-8caf-4f08-9174-27308eabd4ea.h5ad\", data_dtype=\"uint64\"\n)\n</code></pre> <p>SCDL checks for minimal loss when doing this. The amount of tolerated loss in the data is set through the <code>data_dtype_tolerance</code> parameter.</p>"},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#changing-data-dtype-after-creation-in-place","title":"Changing data dtype after creation (in-place)","text":"<p>If you need to change the on-disk data dtype after a dataset has been created, you can cast it in place:</p> <pre><code>from bionemo.scdl.io.single_cell_memmap_dataset import SingleCellMemMapDataset\n\nds = SingleCellMemMapDataset(\"97e_scmm\")\nds.cast_data_to_dtype(\"float64\")  # or \"uint16\", \"float32\", etc.\n\n# Optionally reopen to verify\nreloaded = SingleCellMemMapDataset(\"97e_scmm\")\nassert reloaded.dtypes[\"data.npy\"] == \"float64\"\n</code></pre> <p>Notes:</p> <ul> <li>Casting is done in place and updates the on-disk arrays and dtype registry.</li> <li>Avoid mixing integer and floating\u2011point families across datasets you plan to concatenate; SCDL raises an error when families differ.</li> </ul>"},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#examples","title":"Examples","text":"<p>The examples directory contains various examples for utilizing SCDL.</p>"},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#converting-existing-cellxgene-data-to-scdl","title":"Converting existing CellxGene data to SCDL","text":"<p>If there are multiple AnnData files, they can be converted into a single <code>SingleCellMemMapDataset</code>. If the hdf5 directory has one or more AnnData files, the <code>SingleCellCollection</code> class crawls the filesystem to recursively find AnnData files (with the h5ad extension).</p> <p>To convert existing AnnData files, you can either write your own script using the SCDL API or utilize the convenience script <code>convert_h5ad_to_scdl</code>.</p> <p>During dataset concatenation, it is assumed that all of the data types are either floats or ints, and all of the entries are upscaled to the largest data size. If there is a combination of floats and ints when concatenating the data, an error is thrown.</p> <p>To convert multiple files with a given data format, the user can run:</p> <pre><code>convert_h5ad_to_scdl --data-path hdf5s --save-path example_dataset [--data-dtype float64 --paginated_load_cutoff 10_000 --load-block-row-size 1_000_000 --use-X-not-raw]\n</code></pre>"},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#runtimes-with-scdl","title":"Runtimes with SCDL","text":"<p>The runtime is examined on the Tahoe 100M dataset, which contains over 100 million rows. On this dataset, there is either a 12\u00d7 or 53\u00d7 speedup depending on the machine used.</p> <p></p> <p>To replicate this on your machine, see: Tahoe 100M Profiling section.</p>"},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#using-neighbor-information-in-single-cell-datasets","title":"Using Neighbor Information in Single Cell Datasets","text":"<p>SCDL now supports loading and utilizing neighbor information from AnnData objects. This is particularly useful for tasks that require knowledge of cell neighborhoods, trajectory analysis, or spatial relationships.</p>"},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#neighbor-data-structure-in-anndata","title":"Neighbor Data Structure in AnnData","text":"<p>The neighbor functionality reads neighbor information from the <code>.obsp</code> (observations pairwise) attribute of the AnnData object and converts it from sparse matrix format into SCDL's memory-mapped format for efficient access:</p> <ul> <li>Input Location: <code>adata.obsp[neighbor_key]</code> (default key is <code>'next_cell_ids'</code>)</li> <li>Input Format: Sparse matrix (scipy.sparse format, typically CSR - Compressed Sparse Row)</li> <li>SCDL Processing: Converts sparse neighbor data into memory-mapped arrays during dataset creation</li> <li>Dimensions: <code>[n_cells \u00d7 n_cells]</code> adjacency matrix</li> <li>Values: Weights/distances (e.g., pseudotime values, spatial distances, similarity scores)</li> <li>Non-zero entries: Indicate neighbor relationships</li> </ul> <p>Example - Generating Neighbor Data from Trajectory Analysis:</p> <pre><code>import scanpy as sc\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\n# After computing pseudotime with your preferred method (e.g., DPT, Monocle, etc.)\n# adata.obs['pseudotime'] contains pseudotime values for each cell\n# Assuming you define a function create_pseudotime_neighbors() to find k nearest neighbors in pseudotime space and store as sparse matrix\n\n# Create and store neighbor matrix\nneighbor_matrix = create_pseudotime_neighbors(adata.obs[\"pseudotime\"])\nadata.obsp[\"next_cell_ids\"] = neighbor_matrix\n</code></pre>"},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#loading-a-dataset-with-neighbor-support","title":"Loading a Dataset with Neighbor Support","text":"<pre><code>from bionemo.scdl.io.single_cell_memmap_dataset import (\n    SingleCellMemMapDataset,\n    NeighborSamplingStrategy,\n)\n\n# Load dataset with neighbor support\ndata = SingleCellMemMapDataset(\n    \"dataset_path\",\n    \"path/to/anndata.h5ad\",\n    load_neighbors=True,  # Enable neighbor functionality\n    neighbor_key=\"next_cell_ids\",  # Key in AnnData.obsp containing neighbor information\n    neighbor_sampling_strategy=NeighborSamplingStrategy.RANDOM,  # Strategy for sampling neighbors\n    fallback_to_identity=True,  # Use cell itself as neighbor when no neighbors exist\n)\n</code></pre>"},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#accessing-neighbor-data","title":"Accessing Neighbor Data","text":"<p>SCDL provides several methods to access and utilize neighbor information:</p> <pre><code># Get neighbor indices for a specific cell\nneighbor_indices = data.get_neighbor_indices_for_cell(cell_index)\n\n# Get neighbor weights (if available)\nneighbor_weights = data.get_neighbor_weights_for_cell(cell_index)\n\n# Sample a neighbor according to the configured strategy\nneighbor_index = data.sample_neighbor_index(cell_index)\n</code></pre> <p>Example Usage in Contrastive Learning:</p> <pre><code># Contrastive Learning - Compare cells with their neighbors\nfor cell_index in range(len(data)):\n    # Get current cell and its neighbor\n    current_cell_data, _ = data.get_row(cell_index)\n    neighbor_index = data.sample_neighbor_index(cell_index)\n    neighbor_cell_data, _ = data.get_row(neighbor_index)\n\n    # Use in contrastive loss\n    current_embedding = model.encode(current_cell_data)\n    neighbor_embedding = model.encode(neighbor_cell_data)\n    contrastive_loss = compute_contrastive_loss(current_embedding, neighbor_embedding)\n</code></pre>"},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#future-work-and-roadmap","title":"Future Work and Roadmap","text":"<p>SCDL is currently in public beta. In the future, expect improvements in data compression and data loading performance.</p>"},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#license","title":"LICENSE","text":"<p>BioNeMo-SCDL has an Apache 2.0 license, as found in the LICENSE file.</p>"},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#contributing","title":"Contributing","text":"<p>Please follow the guidelines for contributions to the BioNeMo Framework.</p> <p>To contribute to SCDL, we recommend installing additional dependencies for development and installing the SCDL package from source.</p> <pre><code>git clone https://github.com/NVIDIA/bionemo-framework.git\ncd bionemo-framework/sub-packages/bionemo-scdl\npip install -e \".[test]\"\n</code></pre>"},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#tests","title":"Tests","text":"<p>SCDL has its own tests. To run these tests, assuming you have pytest installed:</p> <pre><code>python -m pytest\n</code></pre> <p>To run a specific test:</p> <pre><code>python -m pytest tests/test_&lt;test name&gt;.py\n</code></pre>"},{"location":"main/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#troubleshooting","title":"Troubleshooting","text":"<ul> <li> <p>Mixed data types at concat (ValueError: mix of int and float dtypes)</p> </li> <li> <p>Cause: attempting to concatenate datasets whose data arrays are from different dtype families (e.g., one integer, one floating\u2011point).</p> </li> <li> <p>Fixes:</p> <ul> <li>Recast all input archives to a common dtype family. You can do this in place with <code>ds.cast_data_to_dtype(\"float32\")</code>, then rerun concatenation.</li> <li>Alternatively, rebuild inputs using <code>convert_h5ad_to_scdl --data-dtype &lt;dtype&gt;</code> so they share the same family.</li> </ul> </li> <li> <p>OOM during dataset instantiation or concatenation from h5ad files.</p> </li> <li> <p>Cause: Likely due to overly large chunks of the anndata file being read into memory.</p> </li> <li> <p>Fixes: Set a lower paginated_load_cutoff, load_block_row_size, or number of workers during concatenation.</p> </li> <li> <p>Slow DataLoader throughput when returning rich Python structures</p> </li> <li> <p>Cause: returning dicts or strings from <code>Dataset</code>/<code>collate_fn</code> prevents fast vectorized collation.</p> </li> <li> <p>Fixes:</p> <ul> <li>Return tensors only; prefer a tuple <code>(X, idx)</code> and gather <code>.obs</code> inside the model from a pre\u2011encoded tensor aligned to row order.</li> </ul> </li> <li> <p>Downcasting warnings (data precision loss)</p> </li> <li> <p>Cause: requested <code>data_dtype</code> is narrower than the source values allow.</p> </li> <li>Fixes:<ul> <li>Choose a wider dtype (e.g., <code>float32</code>/<code>float64</code>, or a larger unsigned int), or raise <code>data_dtype_tolerance</code> as appropriate.</li> </ul> </li> </ul>"},{"location":"main/developer-guide/bionemo-scspeedtest/bionemo-scspeedtest-Overview/","title":"BioNeMo Single-Cell Benchmarking Framework","text":"<p>A simple, flexible framework for benchmarking any dataloader without requiring inheritance or modifications to your existing code.</p>"},{"location":"main/developer-guide/bionemo-scspeedtest/bionemo-scspeedtest-Overview/#quick-start","title":"Quick Start","text":""},{"location":"main/developer-guide/bionemo-scspeedtest/bionemo-scspeedtest-Overview/#0-use-a-virtual-environment","title":"0. Use a virtual environment","text":"<pre><code>python -m venv bionemo_singlecell_benchmark\n\nsource bionemo_singlecell_benchmark/bin/activate\n</code></pre>"},{"location":"main/developer-guide/bionemo-scspeedtest/bionemo-scspeedtest-Overview/#1-install-package","title":"1. Install Package","text":"<pre><code>pip install -e .\n</code></pre>"},{"location":"main/developer-guide/bionemo-scspeedtest/bionemo-scspeedtest-Overview/#quick-start_1","title":"Quick Start","text":""},{"location":"main/developer-guide/bionemo-scspeedtest/bionemo-scspeedtest-Overview/#download-data","title":"Download Data","text":"<pre><code>wget -O cellxgene_example_25k.h5ad \"https://datasets.cellxgene.cziscience.com/97e96fb1-8caf-4f08-9174-27308eabd4ea.h5ad\"\n</code></pre>"},{"location":"main/developer-guide/bionemo-scspeedtest/bionemo-scspeedtest-Overview/#run-python-code","title":"Run python code","text":"<pre><code>import anndata as ad\nfrom anndata.experimental import AnnCollection, AnnLoader\nfrom bionemo.scspeedtest.benchmark import benchmark_single_dataloader, print_results\nimport numpy as np\n\nfilepath = \"cellxgene_example_25k.h5ad\"\n\n\n# create a dataloader factory. This returns anndata in a dense format.\ndef anndata_factory(input_path, batch_size=64):\n    def factory():\n        dataset = ad.read_h5ad(input_path)\n        return AnnLoader(\n            dataset,\n            num_workers=0,\n            collate_fn=lambda batch: np.vstack([x.X for x in batch]),\n        )\n\n    return factory\n\n\n# benchmark the dataloader\nresult = benchmark_single_dataloader(\n    dataloader_factory=anndata_factory(filepath),\n    data_path=filepath,\n    name=\"AnnLoader\",\n    max_time_seconds=10,\n)\n\nprint_results(result)\n</code></pre>"},{"location":"main/developer-guide/bionemo-scspeedtest/bionemo-scspeedtest-Overview/#output","title":"Output","text":"<pre><code>============================================================\nBenchmark: AnnLoader\nSamples/sec: 5042.91\nTotal samples: 25381\nTotal time: 5.033s\nDataloader instantiation: 1.501s\nPeak memory durint iteration: 473.2 MB\nPeak memory during instantiation: 469.7 MB\nDisk size: 144.6 MB\n</code></pre>"},{"location":"main/developer-guide/bionemo-scspeedtest/bionemo-scspeedtest-Overview/#bring-your-own-dataloader","title":"Bring your own Dataloader","text":"<p>Your dataloader just needs to be iterable (support <code>for batch in dataloader</code>).</p>"},{"location":"main/developer-guide/bionemo-scspeedtest/bionemo-scspeedtest-Overview/#dataloader-vs-dataset-factories","title":"Dataloader vs Dataset Factories","text":"<p>The framework supports two distinct patterns for benchmarking, each optimized for different scenarios:</p> <p>Dataloader Factory: Creates both dataset and dataloader</p> <pre><code>from torch.utils.data import DataLoader\nfrom bionemo.scspeedtest.benchmark import benchmark_dataloaders_with_configs\n\n\ndef dataloader_factory():\n    dataset = load_dataset()  # Load data each time\n    return DataLoader(dataset, batch_size=32)\n\n\nresult = benchmark_single_dataloader(\n    dataloader_factory=dataloader_factory, data_path=\"/path/to/data\", name=\"MyBenchmark\"\n)\n</code></pre> <ul> <li>Use when: Testing different datasets or when dataset loading is fast</li> <li>Measures: Total instantiation time (dataset + dataloader combined)</li> </ul> <p>Dataset Factory: Loads dataset once, reused across multiple dataloader configs</p> <pre><code>from torch.utils.data import DataLoader\nfrom bionemo.scspeedtest.benchmark import (\n    benchmark_dataloaders_with_configs,\n    print_comparison,\n)\n\n\ndef dataset_factory():\n    return load_dataset()  # Load once\n\n\ndef dataloader_factory_32(dataset):  # Receives pre-loaded dataset\n    return DataLoader(dataset, batch_size=32)\n\n\ndef dataloader_factory_64(dataset):  # Receives pre-loaded dataset\n    return DataLoader(dataset, batch_size=64)\n\n\n# Dataset reuse mode - loads dataset once, tests multiple configs\nresults = benchmark_dataloaders_with_configs(\n    shared_dataset_factory=dataset_factory,\n    dataloader_configs=[\n        {\n            \"name\": \"Config1\",\n            \"dataloader_factory\": dataloader_factory_32,\n            \"data_path\": \"/path/to/data\",\n        },\n        {\n            \"name\": \"Config2\",\n            \"dataloader_factory\": dataloader_factory_64,\n            \"data_path\": \"/path/to/data\",\n        },\n    ],\n    output_prefix=\"my_benchmark\",\n)\n\n# Print comparisons\nprint_comparison(results)\n</code></pre> <ul> <li>Use when: Testing multiple configurations on the same large dataset</li> <li>Performance benefit: Avoids expensive dataset reloading (e.g., 10GB+ datasets)</li> <li>Separates metrics: Dataset vs dataloader instantiation times tracked separately</li> <li>Memory consideration: Dataset stays in memory throughout all tests</li> </ul> <p>Benchmark your dataloader!</p> <pre><code>from bionemo.scspeedtest import benchmark_single_dataloader\n\n# Benchmark it with instantiation measurement!\nresult = benchmark_single_dataloader(\n    dataloader_factory=create_my_dataloader,\n    data_path=\"path/to/data\",  # Required: for disk measurement\n    name=\"My Dataloader\",\n    num_epochs=1,\n    max_batches=100,  # Optional: limit number of batches\n    max_time_seconds=30.0,  # Optional: limit runtime to 30 seconds\n    warmup_batches=5,  # Optional: warmup with 5 batches\n    warmup_time_seconds=2.0,  # Optional: warmup for 2 seconds\n    output_prefix=\"my_dataloader_benchmark\",  # CSV filename prefix\n)\n\n# Print results\nprint(f\"Dataset instantiation time: {result.dataset_instantiation_time_seconds:.4f}s\")\nprint(\n    f\"Dataloader instantiation time: {result.dataloader_instantiation_time_seconds:.4f}s\"\n)\nprint(\n    f\"Peak instantiation memory: {result.peak_memory_during_instan\u25catiation_mb:.2f} MB\"\n)\nprint(f\"Samples/second: {result.samples_per_second:.2f}\")\nprint(f\"Peak memory usage: {result.peak_memory_mb:.2f} MB\")\nprint(f\"Average memory usage: {result.avg_memory_mb:.2f} MB\")\nprint(f\"Disk usage (MB): {result.disk_size_mb:.2f}\")\n</code></pre>"},{"location":"main/developer-guide/bionemo-scspeedtest/bionemo-scspeedtest-Overview/#examples","title":"Examples","text":""},{"location":"main/developer-guide/bionemo-scspeedtest/bionemo-scspeedtest-Overview/#comprehensive-examples","title":"Comprehensive Examples","text":"<p>See the examples directory for complete examples:</p> <pre><code># Full feature demonstration with dataset reuse\npython examples/comprehensive_benchmarking.py \\\n    --adata-path /path/to/data.h5ad \\\n    --scdl-path /path/to/scdl/ \\\n    --num-epochs 2 \\\n    --num-runs 3\n</code></pre> <p>This demonstrates SCDL and AnnLoader dataloaders with a variety of sampling schemes, sequential sampling, and multi-worker settings. Shows both dataset reuse and independent loading patterns.</p>"},{"location":"main/developer-guide/bionemo-scspeedtest/bionemo-scspeedtest-Overview/#scdataset-profiling","title":"scDataset profiling","text":"<pre><code>python examples/scdataset_script.py \\\n    --fetch-factors 1 2 4 8 16 32 64 \\\n    --block-sizes 1 4 8 16 32 64 \\\n    --scdl-path /path/to/scdl/ \\\n    --adata-path /path/to/data.h5ad\n</code></pre> <p>This is code for reproducing AnnDataset and SCDL results wrapped in the scDataset sampler.</p>"},{"location":"main/developer-guide/bionemo-scspeedtest/bionemo-scspeedtest-Overview/#a-note-on-page-cache","title":"A note on page cache","text":"<p>SCDL saves pages it has seen to the page cache. This can lead to faster iteration after the first run. To avoid this, run the above commands with sudeo - this will enable a command to drop the caches to be executed between each run.</p>"},{"location":"main/developer-guide/bionemo-scspeedtest/bionemo-scspeedtest-Overview/#key-features","title":"Key Features","text":"<ul> <li>Zero Inheritance Required: Your dataloader doesn't need to inherit from anything</li> <li>Works with Any Iterable: PyTorch DataLoaders, custom iterators, generators, lists, etc.</li> <li>Time-Based Benchmarking: Set maximum runtime or warmup periods</li> <li>Modular Architecture: Core benchmarking logic is reusable and extensible</li> <li>Comprehensive Metrics: Disk space, memory usage, throughput, timing, AND instantiation</li> <li>Fine-Grained Metrics: Provides per-epoch metrics and options to re-run metrics</li> <li>Real-time CSV Output: Results written to CSV files after every individual run</li> <li>Memory Monitoring: Tracks peak and average PSS memory usage of processes and children</li> <li>Flexible Stopping: Stop by time limit, batch count, or epoch completion</li> <li>Multiple Run Support: Run same configuration multiple times for statistical analysis</li> </ul>"},{"location":"main/developer-guide/bionemo-scspeedtest/bionemo-scspeedtest-Overview/#what-gets-measured","title":"What Gets Measured","text":"<p>Throughput &amp; Performance:</p> <ul> <li>Samples per second (throughput)</li> <li>Batches per second</li> <li>Total iteration time per epoch</li> <li>Warmup time and samples processed</li> <li>Instantiation Time</li> </ul> <p>Memory Usage:</p> <ul> <li>Peak memory (MB) during benchmarking and instantiation</li> <li>Average memory (MB) throughout execution</li> <li>Memory baseline tracking for accurate delta measurements</li> </ul> <p>Storage &amp; Resources:</p> <ul> <li>Disk usage of data files (MB)</li> <li>Support for multiple files/directories</li> </ul>"},{"location":"main/developer-guide/bionemo-scspeedtest/bionemo-scspeedtest-Overview/#output-formats","title":"Output Formats","text":"<p>CSV Export:</p> <ul> <li>Detailed per-epoch breakdown with <code>{output_prefix}_detailed_breakdown.csv</code></li> <li>All configurations consolidated into single CSV file</li> <li>Appends results from multiple benchmark runs</li> <li>Perfect for analysis and comparison</li> </ul> <p>Example Output Files:</p> <ul> <li><code>my_benchmark_detailed_breakdown.csv</code> - Main results file</li> <li>Contains all run data, epochs, memory, throughput metrics</li> </ul>"},{"location":"main/developer-guide/bionemo-scspeedtest/bionemo-scspeedtest-Overview/#troubleshooting","title":"Troubleshooting","text":"<p>\"TypeError: 'NoneType' object is not callable\"</p> <ul> <li>Check that your factory functions return the dataloader, not None</li> <li>Verify lambda functions in dataset reuse mode are correctly formed</li> </ul> <p>High memory usage</p> <ul> <li>In dataset reuse mode, dataset stays in memory throughout all tests</li> <li>Consider reloading the dataset for very large datasets if memory is limited</li> </ul> <p>Slow benchmarking</p> <ul> <li>Use <code>max_time_seconds</code> or <code>max_batches</code> to limit test duration</li> <li>Check if your dataloader factory is doing expensive operations repeatedly</li> <li>Clearing the page cache: With lazy loading, data may be stored in the page cache between runs. This is especially an issue with SCDL. Between runs, the page cache can be cleared with   <code>sudo sh -c 'sync; echo 3 &gt; /proc/sys/vm/drop_caches'</code>. If <code>benchmark_dataloader</code> or any of the example scripts are run with sudo, it will perform this between runs.</li> </ul>"},{"location":"main/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/","title":"bionemo-size-aware-batching","text":"<p>To install, execute the following:</p> <pre><code>pip install -e .\n</code></pre> <p>To run unit tests, execute:</p> <pre><code>pytest -v .\n</code></pre>"},{"location":"main/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#summary-of-usage","title":"Summary of Usage","text":"<p>This package provides a simple way to create mini-batches in a memory consumption-aware (or size-aware) manner, making it useful for tasks like training models on datasets with varying memory requirements. The usage typically consists of the following steps:</p> <ol> <li>Use the <code>collect_cuda_peak_alloc</code> function to collect CUDA peak memory    allocation statistics for a user-defined workflow. It's expected that the    user-defined workflow will return a list of features extracted from the data    so that the memory model in the following step can use these features to    predict the memory allocation.</li> <li>User defines and trains a memory model using the features and memory allocation    data from previous step. This memory model can then be used to predict memory    consumption.</li> <li>Use <code>SizeAwareBatchSampler</code> or <code>size_aware_batching</code> with the memory model    prediction (from the previous step) to build batch of data so that the    resulting mini-batches do not exceed a specified maximum total memory size.</li> </ol> <p>In addition, this package provides one solution to create homogeneous mini-batches, which can be useful to reduce the padding when aligning the shape of inputs when training or evaluating the models. This <code>BucketBatchSampler</code> can be used in conjunction with <code>torch.utils.data.BatchSampler</code>, <code>SizeAwareBatchSampler</code> or other user-defined batch samplers. This usage can leverage the <code>create_buckets</code> function and follow the steps below:</p> <ol> <li>Gather the tensor sizes of elements in the dataset, which are the shapes of    tensors in a specific dimension where you want to reduce the padding.</li> <li>Provide your own bucket boundaries based on the tensor sizes, or create bucket    boundaries with <code>create_buckets</code> function with the tensor sizes and bucket    maximal width and the minimal bucket count. The <code>create_buckets</code> function    will try to create buckets with smallest widths and element counts &gt;= minimal    bucket count, unless the maximal width or the boundary is reached.</li> <li>Use <code>BucketBatchSampler</code> with base batch sampler like <code>torch.utils.data.BatchSampler</code> or    <code>SizeAwareBatchSampler</code> for each bucket. The <code>BucketBatchSampler</code> will select a bucket each time    and generate a mini-batch from this bucket using the base batch sampler for this bucket.    As such, the padding necessary for the generated mini-batches will be always smaller    than the width of buckets.</li> </ol> <p>Refer to the later sections for the API documentation and examples on how to achieve each of the steps above.</p>"},{"location":"main/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#utils-module","title":"utils Module","text":"<ul> <li> <p>collect_cuda_peak_alloc: A function that   collects CUDA peak memory allocation statistics and features to be used for   memory usage prediction for a given workflow.</p> </li> <li> <p>create_buckets: A function to create buckets for a   list of integers with pre-defined maximal width of ranges and minimal   bucket sizes.</p> </li> </ul>"},{"location":"main/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#sampler-module","title":"sampler Module","text":"<ul> <li>size_aware_batching: A generator that batches elements from an iterable while   ensuring that the total size of each batch does not exceed a specified maximum.</li> <li>SizeAwareBatchSampler: A class that batches elements of varying sizes while   ensuring that the total size of each batch does not exceed a specified maximum.</li> <li>BucketBatchSampler: A class that groups elements of varying sizes based on predefined   bucket ranges, and create batches with elements from each bucket to ensure that each batch has elements with   homogeneous sizes.</li> </ul>"},{"location":"main/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#api-reference-and-examples","title":"API reference and examples","text":""},{"location":"main/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#utils","title":"utils","text":""},{"location":"main/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#collect_cuda_peak_alloc","title":"collect_cuda_peak_alloc","text":"<pre><code>def collect_cuda_peak_alloc(\n    dataset: Iterable[Data],\n    work: Callable[[Data], Feature],\n    device: torch.device,\n    cleanup: Optional[Callable[[], None]] = None\n) -&gt; Tuple[List[Feature], List[int]]\n</code></pre> <p>Collects CUDA peak memory allocation statistics for a given workflow.</p> <p>This function iterates through the provided dataset, applies the given feature function to each data point, and records the peak CUDA memory allocation during this process. The features extracted from the data points are collected along with their corresponding memory usage statistics.</p> <p>Note that the first few iterations of the workflow might result in smaller memory allocations due to uninitialized data (e.g., internal PyTorch buffers). Therefore, users may want to skip these initial data points when analyzing the results.</p> <p>Arguments:</p> <ul> <li><code>dataset</code> - An iterable containing the input data.</li> <li><code>work</code> - A function that takes a data point and returns its corresponding feature. This is where   the main computation happens and memory allocations are tracked.</li> <li><code>device</code> - The target Torch CUDA device.</li> <li><code>cleanup</code> - A function that is called after each iteration to perform any necessary cleanup.</li> </ul> <p>Returns:</p> <p>A tuple containing the collected features and their corresponding memory usage statistics.</p> <p>Raises:</p> <ul> <li><code>ValueError</code> - If the provided device is not a CUDA device.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.size_aware_batching.utils import collect_cuda_peak_alloc\n\n\n&gt;&gt;&gt; # prepare dataset, model and other components of a workflow\n&gt;&gt;&gt; # for which the user want to collect CUDA peak memory allocation statistics\n&gt;&gt;&gt; dataset, model, optimizer = ...\n&gt;&gt;&gt; # Set the target Torch CUDA device.\n&gt;&gt;&gt; device = torch.device(\"cuda:0\")\n&gt;&gt;&gt; model = model.to(device)\n\n&gt;&gt;&gt; # Define a function that takes an element of the dataset as input and\n&gt;&gt;&gt; # do a training step\n&gt;&gt;&gt; def work(data):\n...     # example body of a training loop\n...     optimizer.zero_grad()\n...     output = model(data.to(device))\n...     loss = compute_loss(output)\n...     loss.backward()\n...     optimizer.step()\n...     # extract the feature for later to be modeled or analyzed\n...     return featurize(data)\n\n&gt;&gt;&gt; # can optionally use a cleanup function to release the references\n&gt;&gt;&gt; # hold during the work(). This cleanup function will be called\n&gt;&gt;&gt; # at the end of each step before garbage collection and memory allocations measurement\n&gt;&gt;&gt; def cleanup():\n...     model.zero_grad(set_to_none=True)\n\n&gt;&gt;&gt; # Collect features (i.e., model outputs) and memory usage statistics for the workflow.\n&gt;&gt;&gt; features, alloc_peaks = collect_cuda_peak_alloc(\n...     dataset=batches,\n...     work=work,\n...     device=device,\n...     cleanup=cleanup,\n... )\n\n\n&gt;&gt;&gt; # use features and alloc_peaks as needed, e.g., fit a model\n&gt;&gt;&gt; # that can use these statistics to predict memory usage\n&gt;&gt;&gt; memory_model = ...\n&gt;&gt;&gt; memory_model.fit(features, alloc_peaks)\n</code></pre> <p></p>"},{"location":"main/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#create_buckets","title":"create_buckets","text":"<pre><code>def create_buckets(sizes: torch.Tensor, max_width: int,\n                   min_bucket_count: int) -&gt; Buckets\n</code></pre> <p>Create buckets for a list of integers with pre-defined maximal width of interval and minimal bucket count.</p> <p>It will return a named tuple containing the bucket boundaries and the actual bucket sizes. e.g. torch.tensor([0, 5, 7]), torch.tensor([3,2]): specifies 2 buckets: one with range 0\\&lt;= sizes &lt; 5, width=5 and 3 elements and the other one with range 5 \\&lt;= sizes &lt; 7, width=2 and 2 elements.</p> <p>Arguments:</p> <ul> <li><code>sizes</code> - An 1D tensor of integers.</li> <li><code>max_width</code> - The maximum width of a bucket, should be a positive integer.</li> <li><code>min_bucket_count</code> - The minimum count of a bucket, should be a positive integer.   Bucket size may be smaller than min_bucket_count if its width reaches max_width.</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code> - If the provided sizes is empty, or not integers.</li> <li><code>ValueError</code> - If max_width is not a positive integer or min_bucket_count is not a positive integer.</li> </ul> <p>Returns:</p> <p>A named tuple containing bucket boundaries in ascending order and the number of elements in each bucket.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.size_aware_batching.utils import create_buckets\n\n&gt;&gt;&gt; sizes = torch.tensor([1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 22, 22, 22, 22])\n&gt;&gt;&gt; buckets = create_buckets(sizes, max_width=5, min_bucket_count=10)\n&gt;&gt;&gt; # 5 buckets: 1 &lt;= sizes &lt; 6, 6 &lt;= sizes &lt; 11, 11 &lt;= sizes &lt; 16, 16 &lt;= sizes &lt; 21, 21 &lt;= sizes &lt; 23\n&gt;&gt;&gt; print(buckets.bucket_boundaries)\ntensor([ 1,  6, 11, 16, 21, 23])\n\n&gt;&gt;&gt; # each with 12, 0, 0, 0, 4 elements respectively.\n&gt;&gt;&gt; print(buckets.bucket_sizes)\ntensor([12,  0,  0,  0,  4])\n\n&gt;&gt;&gt; sizes = torch.arange(20)\n&gt;&gt;&gt; # min_bucket_count is used to control bucket size\n&gt;&gt;&gt; buckets = create_buckets(sizes, max_width=10, min_bucket_count=5)\n&gt;&gt;&gt; print(buckets.bucket_boundaries)\ntensor([ 0,  5, 10, 15, 20])\n\n&gt;&gt;&gt; print(buckets.bucket_sizes)\ntensor([5, 5, 5, 5])\n</code></pre> <p></p>"},{"location":"main/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#sampler","title":"sampler","text":""},{"location":"main/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#size_aware_batching","title":"size_aware_batching","text":"<pre><code>def size_aware_batching(\n    dataset: Iterable[Data],\n    sizeof: Callable[[Data], Real],\n    max_total_size: Real,\n    collate_fn: Optional[Callable[[Iterable[Data]], BatchCollated]] = None,\n    info_logger: Optional[Callable[[str], None]] = None,\n    warn_logger: Optional[Callable[[str], None]] = None\n) -&gt; Iterator[Union[List[Data], BatchCollated]]\n</code></pre> <p>A generator that batches elements from an iterable while ensuring that the total size of each batch does not exceed a specified maximum. Here the size can be a measurement of memory consumption of the elements in the batch. This can be useful for both indexible data or non-indexible but iterable data.</p> <p>Arguments:</p> <ul> <li><code>dataset</code> - The input iterable.</li> <li><code>sizeof</code> - A function or mapping that returns the \"size\" of each element in <code>dataset</code>.   E.g., this can be used to determine how much memory an element consumes. Its return   type must be comparable with <code>max_total_size</code> and it must be addable (operator <code>+</code>).</li> <li><code>max_total_size</code> - The maximum total \"size\" of each batch. The semantics of \"size\"   is defined by the <code>sizeof</code> argument. The type of this value must be comparable   with the return type of sizeof, i.e., the operator <code>&lt;</code> and <code>==</code> must be meaningful.</li> <li><code>collate_fn</code> - An optional function to collate batches. Defaults to None, in which case   each batch is a list of elements from the input dataset</li> <li><code>info_logger</code> - A function to log info. Defaults to None.</li> <li><code>warn_logger</code> - A function to log warnings. Defaults to None.</li> </ul> <p>Yields:</p> <p>A generator that yields batches from <code>dataset</code>.</p> <p>Assumptions</p> <ol> <li>Linear complexity. This function consumes the given Iterable of data (<code>dataset</code>) once,    by going over the data item one by one to build a batch and yield it as soon as the    addition of the next data item to the batch would exceed <code>max_total_size</code> or if the    batch is the last one (end of iteration)</li> <li>Additive size measurement. For the general usage case of building mini-batches with    a threshold of the batch's memory consumption, it assumes that the size of the batch is    the sum of all elements in the batch (additive property).</li> <li>Comparable type of <code>max_total_size</code> and <code>sizeof</code>'s return. <code>sizeof</code>'s return values    must be compared with <code>max_total_size</code> to threshold the size of batches</li> </ol> <p>Caveat</p> <ul> <li> <p><code>1</code> - The generated batch sizes may have large variance</p> </li> <li> <p>how to workaround: filter the output of this generator using a batch size threshold</p> </li> <li> <p><code>2</code> - The number of batches may vary a lot across different epochs.</p> </li> <li> <p>how to workaround: increase the number of steps that compose an epoch,     e.g., in the Lightning training/validation loop, which effectively increases the input     dataset size per epoch</p> </li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch.utils.data import default_collate\n&gt;&gt;&gt; from bionemo.size_aware_batching.sampler import size_aware_batching\n\n&gt;&gt;&gt; # Define a sample dataset with torch.tensor\n&gt;&gt;&gt; dataset = [torch.tensor([1, 2]), torch.tensor([3, 4]), torch.tensor([5, 6]),\n...            torch.tensor([7, 8]), torch.tensor([9, 10])]\n\n&gt;&gt;&gt; # Define a sizeof function that returns the size of each tensor\n&gt;&gt;&gt; def sizeof(x):\n...     return x.numel()\n\n&gt;&gt;&gt; # Create a generator with max_total_size=4 and default_collate_fn\n&gt;&gt;&gt; gen = size_aware_batching(dataset, sizeof, 4, collate_fn=default_collate)\n&gt;&gt;&gt; batches = list(gen)\n&gt;&gt;&gt; print(batches)\n    [tensor([[1, 2], [3, 4]]), tensor([[5, 6], [7, 8]]), tensor([[9, 10]])]\n</code></pre> <p></p>"},{"location":"main/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#sizeawarebatchsampler-objects","title":"SizeAwareBatchSampler Objects","text":"<pre><code>class SizeAwareBatchSampler(Sampler[List[int]])\n</code></pre> <p>A sampler that batches elements of varying sizes while ensuring that the total size of each batch does not exceed a specified maximum.</p> <p>This is useful when dealing with datasets where each element has a different size, such as graphs or sequences of varying lengths. The sampler uses a provided <code>sizeof</code> function to determine the size of each element in the dataset and ensures that the total size of each batch does not exceed the specified <code>max_total_size</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.size_aware_batching.sampler import SizeAwareBatchSampler\n\n\n&gt;&gt;&gt; # Define a sample dataset with torch.tensor\n&gt;&gt;&gt; dataset = [torch.tensor([1, 2]), torch.tensor([3, 4]), torch.tensor([5, 6]),\n...            torch.tensor([7, 8]), torch.tensor([9, 10])]\n\n\n&gt;&gt;&gt; # Define a function that returns the size of each element in the dataset.\n&gt;&gt;&gt; def sizeof(index):\n...     return dataset[index].numel()\n\n\n&gt;&gt;&gt; # Create a SizeAwareBatchSampler with a maximum total batch size of 10.\n&gt;&gt;&gt; batch_sampler = SizeAwareBatchSampler(\n...     sampler=torch.utils.data.SequentialSampler(dataset),\n...     sizeof=sizeof,\n...     max_total_size=4\n... )\n\n\n&gt;&gt;&gt; # Iterate over batches of indices that do not exceed the maximum total size.\n&gt;&gt;&gt; print(list(batch_sampler))\n    [[0, 1], [2, 3], [4]]\n</code></pre> <p></p>"},{"location":"main/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#__init__","title":"__init__","text":"<pre><code>def __init__(sampler: Union[Sampler[List[int]], Iterable[int]],\n             sizeof: Callable[[int], Real],\n             max_total_size: Real,\n             info_logger: Optional[Callable[[str], None]] = None,\n             warn_logger: Optional[Callable[[str], None]] = None) -&gt; None\n</code></pre> <p>Initializes the SizeAwareBatchSampler.</p> <p>Arguments:</p> <ul> <li><code>sampler</code> - The underlying sampler.</li> <li><code>sizeof</code> - A function that returns the size at each index. E.g., this can used to   determine how much memory an element consumes. Its return type must be   comparable with <code>max_total_size</code> and it must be addable (operator <code>+</code>).</li> <li><code>max_total_size</code> - The maximum total size of a mini-batch. The semantics of \"size\"   is defined by the <code>sizeof</code> argument. The type of this value must be comparable   with the return type of sizeof, i.e., the operator <code>&lt;</code> and <code>==</code> must be meaningful.</li> <li><code>info_logger</code> - A function to log info. Defaults to None.</li> <li><code>warn_logger</code> - A function to log warnings. Defaults None.</li> </ul> <p>Raises:</p> <ul> <li><code>TypeError</code> - If sampler is not an instance of Sampler or Iterable, or if sizeof is not a callable, dictionary, or sequence container.</li> <li><code>ValueError</code> - If max_total_size is not a positive number.</li> </ul> <p></p>"},{"location":"main/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#__iter__","title":"__iter__","text":"<pre><code>def __iter__() -&gt; Iterator[List[int]]\n</code></pre> <p>Iterate over batches of indices.</p> <p>This function yields batches of indices that do not exceed the maximum total size.</p> <p>Yields:</p> <p>A batch of indices that do not exceed the maximum total size.</p> <p></p>"},{"location":"main/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#bucketbatchsampler-objects","title":"BucketBatchSampler Objects","text":"<pre><code>class BucketBatchSampler(Sampler[List[int]])\n</code></pre> <p>A batch sampler to create batches with sizes of elements from each pre-defined bucket ranges.</p> <p>Elements of the dataset are first grouped into each bucket based on the bucket ranges and the sizes of elements. Then, a base batch sampler is used for each bucket to create mini-batches.</p> <p>The bucket ranges are specified by <code>bucket_boundaries</code>, which will be first sorted internally and used to create <code>len(bucket_boundaries) - 1</code> left-closed right-open intervals. e.g. if bucket_boundaries tensor is [10, 5, 0, 16], it will be sorted as [0, 5, 10, 16] and 3 buckets will be created with ranges: [0, 5), [5, 10), [10, 16).</p> <p>The base batch sampler will be created by passing the element indices in each bucket as the data source, and <code>base_batch_sampler_shared_kwargs</code> and <code>base_batch_sampler_individual_kwargs</code> to the constructor of the base batch sampler class specified as <code>base_batch_sampler_class</code>. e.g. <code>base_batch_sampler_shared_kwargs = {'drop_last': True}</code> and <code>base_batch_sampler_individual_kwargs = {'batch_size': [8,10,12]}</code> will be used to create 3 batch samplers with drop_last=True and batch_size=8, 10 and 12, and initialized like <code>base_batch_sampler_class(bucket_element_indices[0], batch_size=8, drop_last=True)</code>.</p> <p>In the <code>__iter__</code> method, if <code>shuffle</code> is <code>True</code>, the element indices in each bucket will be shuffled, and a bucket is randomly selected each time to create a mini-batch. If <code>shuffle</code> is <code>False</code>, there is no shuffle on element indices, and the bucket is selected in ascending order of its interval boundaries.</p> <p>This class is used to create homogeneous batches of data for training or evaluation, and reduce the padding necessary to align the shape of elements.</p> <p>Modified from https://github.com/rssrwn/semla-flow/blob/main/semlaflow/data/util.py</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.size_aware_batching.sampler import BucketBatchSampler\n\n&gt;&gt;&gt; # Define the sizes for a dataset\n&gt;&gt;&gt; sizes = torch.arange(25)\n&gt;&gt;&gt; # Define bucket ranges\n&gt;&gt;&gt; bucket_boundaries = torch.tensor([0, 6, 15, 25])\n\n&gt;&gt;&gt; # Create a bucket batch sampler with torch.utils.data.BatchSampler as base batch sampler\n&gt;&gt;&gt; # As there are 3 buckets, there will be 3 base batch samplers with batch sizes 2, 3, and 5.\n&gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n        sizes=sizes,\n        bucket_boundaries=bucket_boundaries,\n        base_batch_sampler_class=torch.utils.data.BatchSampler,\n        base_batch_sampler_shared_kwargs={'drop_last': False},\n        base_batch_sampler_individual_kwargs={'batch_size': [2,3,5]},\n        shuffle=False,\n    )\n\n&gt;&gt;&gt; # Iterate over batches of indices that lies in the same bucket and with different batch sizes.\n&gt;&gt;&gt; print(list(batch_sampler))\n[[0, 1], [2, 3], [4, 5], [6, 7, 8], [9, 10, 11], [12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24]]\n\n&gt;&gt;&gt; # randomize the dataset and buckets\n&gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n        sizes=sizes,\n        bucket_boundaries=bucket_boundaries,\n        base_batch_sampler_class=torch.utils.data.BatchSampler,\n        base_batch_sampler_shared_kwargs={'drop_last': False},\n        base_batch_sampler_individual_kwargs={'batch_size': [2,3,5]},\n        shuffle=True,\n        generator=torch.Generator().manual_seed(0),\n    )\n&gt;&gt;&gt; print(list(batch_sampler))\n[[24, 17, 16, 22, 19], [2, 5], [12, 10, 11], [3, 0], [15, 18, 20, 21, 23], [7, 13, 6], [14, 9, 8], [1, 4]]\n&gt;&gt;&gt; print(list(batch_sampler))\n[[14, 9, 13], [23, 16, 20, 21, 15], [5, 0], [8, 10, 11], [17, 24, 22, 18, 19], [12, 6, 7], [4, 2], [3, 1]]\n\n&gt;&gt;&gt; # Combine with SizeAwareBatchSampler to control the cost of each batch\n&gt;&gt;&gt; from bionemo.size_aware_batching.sampler import SizeAwareBatchSampler\n&gt;&gt;&gt; item_costs = sizes.tolist()\n&gt;&gt;&gt; def cost_of_element(index):\n        return item_costs[index]\n&gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n        sizes=sizes,\n        bucket_boundaries=bucket_boundaries,\n        base_batch_sampler_class=SizeAwareBatchSampler,\n        base_batch_sampler_shared_kwargs={\"sizeof\": cost_of_element, \"max_total_size\": 40},\n        base_batch_sampler_individual_kwargs={},\n        shuffle=True,\n        generator=torch.Generator().manual_seed(0),\n    )\n&gt;&gt;&gt; print(list(iter(batch_sampler)))\n[[24], [2, 5, 3, 0, 1, 4], [12, 10, 11, 7], [13, 6, 14], [17, 16], [22], [19, 15], [9, 8], [18, 20], [21], [23]]\n</code></pre> <p></p>"},{"location":"main/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#__init___1","title":"__init__","text":"<pre><code>def __init__(sizes: torch.Tensor,\n             bucket_boundaries: torch.Tensor,\n             base_batch_sampler_class: Type[Sampler],\n             base_batch_sampler_shared_kwargs: Optional[Dict[str, Any]] = None,\n             base_batch_sampler_individual_kwargs: Optional[Dict[\n                 str, Iterable]] = None,\n             shuffle: Optional[bool] = True,\n             generator: Optional[torch.Generator] = None) -&gt; None\n</code></pre> <p>Initializes the BucketBatchSampler.</p> <p>Arguments:</p> <ul> <li><code>sizes</code> - A 1D tensor of real numbers representing the size of each element in the dataset.</li> <li><code>bucket_boundaries</code> - A 1D tensor of real numbers representing the boundaries of the bucket ranges.   It will be first sorted and used to create <code>len(bucket_boundaries) - 1</code> left-closed right-open intervals as bucket ranges.   It should not contain any duplicate values.</li> <li><code>base_batch_sampler_class</code> - Base batch sampler class type, which will be used for each bucket, and initialized with the bucket element indices,   <code>base_batch_sampler_shared_kwargs</code> and the corresponding <code>base_batch_sampler_individual_kwargs</code>.</li> <li><code>base_batch_sampler_shared_kwargs</code> - Shared keyword argument dictionary used to initialize all base batch samplers for all buckets.   Sufficient and valid arguments should be provided for <code>base_batch_sampler_class</code> with <code>base_batch_sampler_individual_kwargs</code>. Default to {}.</li> <li><code>base_batch_sampler_individual_kwargs</code> - Keyword argument dictionary used to initialize   each bucket batch sampler with the corresponding key value pairs.   Length of each value in this dict must be equal to len(bucket_boundaries) - 1 (the number of buckets).   Sufficient and valid arguments should be provided for <code>base_batch_sampler_class</code> with <code>base_batch_sampler_shared_kwargs</code>.   Default to {}.</li> <li><code>shuffle</code> - A boolean indicating whether to shuffle the dataset and buckets. Defaults to True.</li> <li><code>generator</code> - Generator used in sampling. Defaults to None.</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code> - If <code>sizes</code> is not a 1D tensor of real numbers.</li> <li><code>ValueError</code> - If <code>bucket_boundaries</code> is not a 1D tensor of real numbers.</li> <li><code>ValueError</code> - If <code>base_batch_sampler_individual_kwargs</code> or <code>base_batch_sampler_individual_kwargs</code> is not a keyword argument dictionary.</li> <li><code>ValueError</code> - If the length of values in the dict of <code>base_batch_sampler_individual_kwargs</code> must be equal to len(bucket_boundaries) - 1.</li> <li><code>RuntimeError</code> - If there is no elements with sizes inside the ranges specified by <code>bucket_boundaries</code>.</li> </ul> <p></p>"},{"location":"main/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#__len__","title":"__len__","text":"<pre><code>def __len__() -&gt; int\n</code></pre> <p>Get the number of batches.</p> <p>Can only be called if the <code>base_batch_sampler_class</code> has len() implemented</p> <p>Returns:</p> <ul> <li><code>int</code> - Number of batches</li> </ul> <p></p>"},{"location":"main/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#__iter___1","title":"__iter__","text":"<pre><code>def __iter__() -&gt; Iterator[List[int]]\n</code></pre> <p>Iterate over batches of indices.</p> <p>This function yields batches of indices of elements with sizes from each bucket range.</p> <p>Yields:</p> <ul> <li><code>List[int]</code> - A batch of indices of elements with sizes from each bucket range.</li> </ul>"},{"location":"main/developer-guide/bionemo-testing/bionemo-testing-Overview/","title":"bionemo-testing","text":"<p>A package of test-time requirements and utilities for bionemo sub-packages. In particular, the <code>bionemo-testing</code> package handles downloading and caching data and other assets for running unit tests and example notebooks. For more information on test data handling, see BioNeMo test data management</p>"},{"location":"main/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/","title":"bionemo-webdatamodule","text":"<p>To install, execute the following:</p> <pre><code>pip install -e .\n</code></pre> <p>To run unit tests, execute:</p> <pre><code>pytest -v .\n</code></pre>"},{"location":"main/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#webdatamodule","title":"WebDataModule","text":"<pre><code>class WebDataModule(L.LightningDataModule)\n</code></pre> <p>A LightningDataModule for using webdataset tar files.</p> <p><code>WebDataModule</code> is a <code>LightningDataModule</code> for using webdataset tar files to setup PyTorch datasets and dataloaders. This data module takes as input a dictionary: Split -&gt; tar file directory and vaiours webdataset config settings. In its setup() function, it creates the webdataset object chaining up the input <code>pipeline_wds</code> workflow. In its train/val/test_dataloader(), it creates the WebLoader object chaining up the <code>pipeline_prebatch_wld</code> workflow.</p> <p>Examples:</p> <ol> <li> <p>create the data module with input directory to webdataset tar files.    Depending on which of the downstream Lightning.Trainer methods are called,    e.g., <code>Trainer.fit()</code>, <code>Trainer.validate()</code>, <code>Trainer.test()</code> or    <code>Trainer.predict()</code>, only a subset of the train, val and test splits need to    be specified in the various input options to the data module:</p> </li> <li> <p><code>Trainer.fit()</code> requires the <code>train</code> and <code>val</code> splits</p> </li> <li><code>Trainer.validate()</code> requires the <code>val</code> split</li> <li><code>Trainer.test()</code> requires the <code>test</code> splits</li> <li><code>Trainer.predict()</code> requires the <code>test</code> splits</li> </ol> <p>Here is an example of constructing the data module for <code>Trainer.fit()</code>:</p> <pre><code>&gt;&gt;&gt; from bionemo.webdatamodule.datamodule import Split, WebDataModule\n&gt;&gt;&gt;\n&gt;&gt;&gt; tar_file_prefix = \"shards\"\n&gt;&gt;&gt;\n&gt;&gt;&gt; dirs_of_tar_files = {\n&gt;&gt;&gt;     Split.train: \"/path/to/train/split/tars\",\n&gt;&gt;&gt;     Split.val: \"/path/to/val/split/tars\",\n&gt;&gt;&gt; }\n&gt;&gt;&gt;\n&gt;&gt;&gt; n_samples {\n&gt;&gt;&gt;     Split.train: 1000,\n&gt;&gt;&gt;     Split.val: 100,\n&gt;&gt;&gt; }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # this is the string to retrieve the corresponding data object from the\n&gt;&gt;&gt; # webdataset file (see\n&gt;&gt;&gt; # https://github.com/webdataset/webdataset?tab=readme-ov-file#the-webdataset-format\n&gt;&gt;&gt; # for details)\n&gt;&gt;&gt; suffix_keys_wds = \"tensor.pyd\"\n&gt;&gt;&gt;\n&gt;&gt;&gt; seed = 27193781\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Specify the routines to process the samples in the WebDataset object.\n&gt;&gt;&gt; # The routine is a generator of an Iterable of generators that are chained\n&gt;&gt;&gt; # together by nested function calling. The following is equivalent of\n&gt;&gt;&gt; # defining a overall generator of `shuffle(untuple(...))` which\n&gt;&gt;&gt; # untuples the samples and shuffles them. See webdataset's Documentation\n&gt;&gt;&gt; # for details.\n&gt;&gt;&gt; # NOTE: the `untuple` is almost always necessary due to the webdataset's\n&gt;&gt;&gt; # file parsing rule.\n&gt;&gt;&gt;\n&gt;&gt;&gt; untuple = lambda source : (sample for (sample,) in source)\n&gt;&gt;&gt;\n&gt;&gt;&gt; from webdatast import shuffle\n&gt;&gt;&gt; pipeline_wds = {\n&gt;&gt;&gt;     Split.train : [untuple, shuffle(n_samples[Split.train],\n&gt;&gt;&gt;                                     rng=random.Random(seed_rng_shfl))],\n&gt;&gt;&gt;     Split.val: untuple\n&gt;&gt;&gt; }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Similarly the user can optionally define the processing routine on the\n&gt;&gt;&gt; # WebLoader (the dataloader of webdataset).\n&gt;&gt;&gt; # NOTE: these routines by default take unbatched sample as input so the\n&gt;&gt;&gt; # user can customize their batching routines here\n&gt;&gt;&gt;\n&gt;&gt;&gt; batch = batched(local_batch_size, collation_fn=lambda\n                    list_samples : torch.vstack(list_samples))\n&gt;&gt;&gt; pipeline_prebatch_wld = {\n        Split.train: [shuffle(n_samples[Split.train],\n                              rng=random.Random(seed_rng_shfl)), batch],\n        Split.val : batch,\n        Split.test : batch\n    }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # the user can optionally specify the kwargs for WebDataset and\n&gt;&gt;&gt; # WebLoader\n&gt;&gt;&gt;\n&gt;&gt;&gt; kwargs_wds = {\n&gt;&gt;&gt;     split : {'shardshuffle' : split == Split.train,\n&gt;&gt;&gt;              'nodesplitter' : wds.split_by_node,\n&gt;&gt;&gt;              'seed' : seed_rng_shfl}\n&gt;&gt;&gt;     for split in Split\n&gt;&gt;&gt;     }\n&gt;&gt;&gt;\n&gt;&gt;&gt; kwargs_wld = {\n&gt;&gt;&gt;     split : {\"num_workers\": 2} for split in Split\n&gt;&gt;&gt;     }\n&gt;&gt;&gt;\n&gt;&gt;&gt; invoke_wds = {\n&gt;&gt;&gt;     split: [(\"with_epoch\", {\"nbatches\" : 5})] for split in Split\n&gt;&gt;&gt;     }\n&gt;&gt;&gt;\n&gt;&gt;&gt; invoke_wld = {\n&gt;&gt;&gt;     split: [(\"with_epoch\", {\"nbatches\" : 5}] for split in Split\n&gt;&gt;&gt;     }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # construct the data module\n&gt;&gt;&gt; data_module = WebDataModule(suffix_keys_wds,\n                                dirs_of_tar_files,\n                                prefix_tars_wds=tar_file_prefix,\n                                pipeline_wds=pipeline_wds,\n                                pipeline_prebatch_wld=pipeline_prebatch_wld,\n                                kwargs_wds=kwargs_wds,\n                                kwargs_wld=kwargs_wld,\n                                invoke_wds=invoke_wds,\n                                invoke_wld=invoke_wld,\n                                )\n</code></pre> <p></p>"},{"location":"main/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#__init__","title":"__init__","text":"<pre><code>def __init__(\n    suffix_keys_wds: Union[str, Iterable[str]],\n    dirs_tars_wds: Dict[Split, str],\n    prefix_tars_wds: str = \"wdshards\",\n    pipeline_wds: Optional[Dict[Split, Union[Iterable[Iterable[Any]],\n                                             Iterable[Any]]]] = None,\n    pipeline_prebatch_wld: Optional[Dict[Split, Union[Iterable[Iterable[Any]],\n                                                      Iterable[Any]]]] = None,\n    kwargs_wds: Optional[Dict[Split, Dict[str, Any]]] = None,\n    kwargs_wld: Optional[Dict[Split, Dict[str, Any]]] = None,\n    invoke_wds: Optional[Dict[Split, List[Tuple[str, Dict[str, Any]]]]] = None,\n    invoke_wld: Optional[Dict[Split, List[Tuple[str, Dict[str,\n                                                          Any]]]]] = None)\n</code></pre> <p>Constructor.</p> <p>Arguments:</p> <ul> <li><code>suffix_keys_wds</code> - a set of keys each   corresponding to a data object in the webdataset tar file   dictionary. The data objects of these keys will be extracted and   tupled for each sample in the tar files</li> <li><code>dirs_tars_wds</code> - input dictionary: Split -&gt; tar file   directory that contains the webdataset tar files for each split   Kwargs:</li> <li><code>prefix_tars_wds</code> - name prefix of the input webdataset tar   files. The input tar files are globbed by   \"{dirs_tars_wds[split]}/{prefix_tars_wds}-*.tar\"</li> <li><code>pipeline_wds</code> - a dictionary of webdatast composable, i.e.,   functor that maps a iterator to another iterator that   transforms the data sample yield from the dataset object, for   different splits, or an iterable to such a sequence of such   iterators. For example, this can be used to transform the   sample in the worker before sending it to the main process of   the dataloader</li> <li><code>pipeline_prebatch_wld</code> - a dictionary   of webloader composable, i.e., functor that maps a iterator to   another iterator that transforms the data sample yield from the   WebLoader object, for different splits, or an iterable to a   seuqnence of such iterators. For example, this can be used for   batching the samples. NOTE: this is applied before batching is   yield from the WebLoader</li> <li><code>kwargs_wds</code> - kwargs for the WebDataset.init()   kwargs_wld : kwargs for the WebLoader.init(), e.g., num_workers, of each split</li> <li><code>invoke_wds</code> - a dictionary of WebDataset methods to be called upon WebDataset   construction. These methods must return the WebDataset object itself. Examples   are .with_length() and .with_epoch(). These methods will be applied towards   the end of returning the WebDataset object, i.e., after the pipline_wds   have been applied. The inner list of tuples each has its first element as the   method name and the second element as the corresponding method's kwargs.</li> <li><code>invoke_wld</code> - a dictionary of WebLoader methods to be called upon WebLoader   construction. These methods must return the WebLoader object itself. Examples   are .with_length() and .with_epoch(). These methods will be applied towards   the end of returning the WebLoader object, i.e., after the pipelin_prebatch_wld   have been applied. The inner list of tuples each has its first element as the   method name and the second element as the corresponding method's kwargs.</li> </ul> <p></p>"},{"location":"main/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#prepare_data","title":"prepare_data","text":"<pre><code>def prepare_data() -&gt; None\n</code></pre> <p>This is called only by the main process by the Lightning workflow.</p> <p>Do not rely on this data module object's state update here as there is no way to communicate the state update to other subprocesses. Is a no-op.</p> <p></p>"},{"location":"main/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#setup","title":"setup","text":"<pre><code>def setup(stage: str) -&gt; None\n</code></pre> <p>This is called on all Lightning-managed nodes in a multi-node training session.</p> <p>Arguments:</p> <ul> <li><code>stage</code> - \"fit\", \"test\" or \"predict\"</li> </ul> <p></p>"},{"location":"main/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#train_dataloader","title":"train_dataloader","text":"<pre><code>def train_dataloader() -&gt; wds.WebLoader\n</code></pre> <p>Webdataset for the training data.</p> <p></p>"},{"location":"main/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#val_dataloader","title":"val_dataloader","text":"<pre><code>def val_dataloader() -&gt; wds.WebLoader\n</code></pre> <p>Webdataset for the validation data.</p> <p></p>"},{"location":"main/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#test_dataloader","title":"test_dataloader","text":"<pre><code>def test_dataloader() -&gt; wds.WebLoader\n</code></pre> <p>Webdataset for the test data.</p> <p></p>"},{"location":"main/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#predict_dataloader","title":"predict_dataloader","text":"<pre><code>def predict_dataloader() -&gt; wds.WebLoader\n</code></pre> <p>Alias for :func:<code>test_dataloader</code>.</p> <p></p>"},{"location":"main/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#pickleddatawds-objects","title":"PickledDataWDS Objects","text":"<pre><code>class PickledDataWDS(WebDataModule)\n</code></pre> <p>A LightningDataModule to process pickled data into webdataset tar files.</p> <p><code>PickledDataWDS</code> is a LightningDataModule to process pickled data into webdataset tar files and setup dataset and dataloader. This inherits the webdataset setup from its parent module <code>WebDataModule</code>. This data module takes a directory of pickled data files, data filename prefixes for train/val/test splits, data filename suffixes and prepare webdataset tar files by globbing the specific pickle data files <code>{dir_pickles}/{name_subset[split]}.{suffix_pickles}</code> and outputing to webdataset tar file with the dict structure: NOTE: this assumes only one pickled file is processed for each sample. In its setup() function, it creates the webdataset object chaining up the input <code>pipeline_wds</code> workflow. In its train/val/test_dataloader(), it creates the WebLoader object chaining up the <code>pipeline_prebatch_wld</code> workflow.</p> <pre><code>    {\"__key__\" : name.replace(\".\", \"-\"),\n     suffix_pickles : pickled.dumps(data) }\n</code></pre> <p>Examples:</p> <ol> <li>create the data module with a directory of pickle files and the file name    prefix thereof for different splits to used by <code>Lightning.Trainer.fit()</code></li> </ol> <pre><code>&gt;&gt;&gt; from bionemo.core.data.datamodule import Split, PickledDataWDS\n\n&gt;&gt;&gt; dir_pickles = \"/path/to/my/pickles/dir\"\n\n&gt;&gt;&gt; # the following will use `sample1.mydata.pt` and `sample2.mydata.pt` as the\n&gt;&gt;&gt; # training dataset and `sample4.mydata.pt` and `sample5.mydata.pt` as the\n&gt;&gt;&gt; # validation dataset\n\n&gt;&gt;&gt; suffix_pickles = \"mydata.pt\"\n\n&gt;&gt;&gt; names_subset = {\n&gt;&gt;&gt;     Split.train: [sample1, sample2],\n&gt;&gt;&gt;     Split.val: [sample4, sample5],\n&gt;&gt;&gt; }\n\n&gt;&gt;&gt; # the following setting will attempt to create at least 5 tar files in\n&gt;&gt;&gt; # `/path/to/output/tars/dir/myshards-00000{0-5}.tar`\n\n&gt;&gt;&gt; n_tars_wds = 5\n&gt;&gt;&gt; prefix_tars_wds = \"myshards\"\n&gt;&gt;&gt; output_dir_tar_files = {\n        Split.train : \"/path/to/output/tars/dir-train\",\n        Split.val : \"/path/to/output/tars/dir-val\",\n        Split.test : \"/path/to/output/tars/dir-test\",\n    }\n\n&gt;&gt;&gt; # user can optionally customize the data processing routines and kwargs used\n&gt;&gt;&gt; # in the WebDataset and WebLoader (see the examples in `WebDataModule`)\n\n&gt;&gt;&gt; pipeline_wds = { Split.train: ... }\n\n&gt;&gt;&gt; pipeline_prebatch_wld = { Split.train: ... }\n\n&gt;&gt;&gt; kwargs_wds = { Split.train: ..., Split.val: ... }\n\n&gt;&gt;&gt; kwargs_wld = { Split.train: ..., Split.val: ... }\n\n&gt;&gt;&gt; invoke_wds = { Split.train: ..., Split.val: ... }\n\n&gt;&gt;&gt; invoke_wld = { Split.train: ..., Split.val: ... }\n\n&gt;&gt;&gt; # create the data module\n&gt;&gt;&gt; data_module = PickledDataWDS(\n&gt;&gt;&gt;     dir_pickles,\n&gt;&gt;&gt;     names_subset,\n&gt;&gt;&gt;     suffix_pickles, # `WebDataModule` args\n&gt;&gt;&gt;     output_dir_tar_files, # `WebDataModule` args\n&gt;&gt;&gt;     n_tars_wds=n_tars_wds,\n&gt;&gt;&gt;     prefix_tars_wds=prefix_tars_wds, # `WebDataModule` kwargs\n&gt;&gt;&gt;     pipeline_wds=pipeline_wds, # `WebDataModule` kwargs\n&gt;&gt;&gt;     pipeline_prebatch_wld=pipelines_wdl_batch, # `WebDataModule` kwargs\n&gt;&gt;&gt;     kwargs_wds=kwargs_wds, # `WebDataModule` kwargs\n&gt;&gt;&gt;     kwargs_wld=kwargs_wld, # `WebDataModule` kwargs\n&gt;&gt;&gt;     invoke_wds=invoke_wds, # `WebDataModule` kwargs\n&gt;&gt;&gt;     invoke_wld=invoke_wld, # `WebDataModule` kwargs\n&gt;&gt;&gt; )\n</code></pre> <p></p>"},{"location":"main/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#__init___1","title":"__init__","text":"<pre><code>def __init__(dir_pickles: str,\n             names_subset: Dict[Split, List[str]],\n             *args,\n             n_tars_wds: Optional[int] = None,\n             **kwargs) -&gt; None\n</code></pre> <p>Constructor.</p> <p>Arguments:</p> <ul> <li><code>dir_pickles</code> - input directory of pickled data files</li> <li><code>names_subset</code> - list of filename prefix of   the data samples to be loaded in the dataset and dataloader for   each of the split</li> <li><code>*args</code> - arguments passed to the parent WebDataModule</li> <li><code>n_tars_wds</code> - attempt to create at least this number of   webdataset shards</li> <li><code>**kwargs</code> - arguments passed to the parent WebDataModule</li> </ul> <p></p>"},{"location":"main/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#prepare_data_1","title":"prepare_data","text":"<pre><code>def prepare_data() -&gt; None\n</code></pre> <p>This is called only by the main process by the Lightning workflow.</p> <p>Do not rely on this data module object's state update here as there is no way to communicate the state update to other subprocesses. The nesting <code>pickles_to_tars</code> function goes through the data name prefixes in the different splits, read the corresponding pickled file and output a webdataset tar archive with the dict structure: {\"key\" : name.replace(\".\", \"-\"), suffix_pickles : pickled.dumps(data) }.</p>"},{"location":"main/examples/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Evo2</li> <li>MoCo</li> <li>SCDL</li> </ul>"},{"location":"main/examples/conftest/","title":"Conftest","text":"<p>SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved. SPDX-License-Identifier: LicenseRef-Apache2</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <pre><code>http://www.apache.org/licenses/LICENSE-2.0</code></pre> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> In\u00a0[\u00a0]: Copied! <pre>def pytest_collectstart(collector):\n    if collector.fspath and collector.fspath.ext == \".ipynb\":\n        collector.skip_compare += (\n            \"text/html\",\n            \"application/javascript\",\n            \"stderr\",\n        )\n</pre> def pytest_collectstart(collector):     if collector.fspath and collector.fspath.ext == \".ipynb\":         collector.skip_compare += (             \"text/html\",             \"application/javascript\",             \"stderr\",         )"},{"location":"main/examples/bionemo-evo2/fine-tuning-tutorial/","title":"Fine tuning tutorial","text":"In\u00a0[2]: Copied! <pre>%%capture\nimport os\n\nfrom bionemo.core.utils.subprocess_utils import run_subprocess_safely\n\n\nconcat_path = \"chr20_21_22.fa\"\nif not os.path.exists(concat_path):\n    !wget https://hgdownload.soe.ucsc.edu/goldenpath/hg38/chromosomes/chr20.fa.gz\n    !wget https://hgdownload.soe.ucsc.edu/goldenpath/hg38/chromosomes/chr21.fa.gz\n    !wget https://hgdownload.soe.ucsc.edu/goldenpath/hg38/chromosomes/chr22.fa.gz\n    !zcat chr20.fa.gz &gt; chr20.fa\n    !zcat chr21.fa.gz &gt; chr21.fa\n    !zcat chr22.fa.gz &gt; chr22.fa\n    !cat chr20.fa chr21.fa chr22.fa &gt; chr20_21_22.fa\n</pre> %%capture import os  from bionemo.core.utils.subprocess_utils import run_subprocess_safely   concat_path = \"chr20_21_22.fa\" if not os.path.exists(concat_path):     !wget https://hgdownload.soe.ucsc.edu/goldenpath/hg38/chromosomes/chr20.fa.gz     !wget https://hgdownload.soe.ucsc.edu/goldenpath/hg38/chromosomes/chr21.fa.gz     !wget https://hgdownload.soe.ucsc.edu/goldenpath/hg38/chromosomes/chr22.fa.gz     !zcat chr20.fa.gz &gt; chr20.fa     !zcat chr21.fa.gz &gt; chr21.fa     !zcat chr22.fa.gz &gt; chr22.fa     !cat chr20.fa chr21.fa chr22.fa &gt; chr20_21_22.fa In\u00a0[3]: Copied! <pre>full_fasta_path = os.path.abspath(concat_path)\noutput_dir = os.path.abspath(\"preprocessed_data\")\noutput_yaml = f\"\"\"\n- datapaths: [\"{full_fasta_path}\"]\n  output_dir: \"{output_dir}\"\n  output_prefix: chr20_21_22_uint8_distinct\n  train_split: 0.9\n  valid_split: 0.05\n  test_split: 0.05\n  overwrite: True\n  embed_reverse_complement: true\n  random_reverse_complement: 0.0\n  random_lineage_dropout: 0.0\n  include_sequence_id: false\n  transcribe: \"back_transcribe\"\n  force_uppercase: false\n  indexed_dataset_dtype: \"uint8\"\n  tokenizer_type: \"Byte-Level\"\n  vocab_file: null\n  vocab_size: null\n  merges_file: null\n  pretrained_tokenizer_model: null\n  special_tokens: null\n  fast_hf_tokenizer: true\n  append_eod: true\n  enforce_sample_length: null\n  ftfy: false\n  workers: 1\n  preproc_concurrency: 100000\n  chunksize: 25\n  drop_empty_sequences: true\n  nnn_filter: false  # If you split your fasta on NNN (in human these are contigs), then you should set this to true.\n  seed: 12342  # Not relevant because we are not using random reverse complement or lineage dropout.\n\"\"\"\nwith open(\"preprocess_config.yaml\", \"w\") as f:\n    print(output_yaml, file=f)\n</pre> full_fasta_path = os.path.abspath(concat_path) output_dir = os.path.abspath(\"preprocessed_data\") output_yaml = f\"\"\" - datapaths: [\"{full_fasta_path}\"]   output_dir: \"{output_dir}\"   output_prefix: chr20_21_22_uint8_distinct   train_split: 0.9   valid_split: 0.05   test_split: 0.05   overwrite: True   embed_reverse_complement: true   random_reverse_complement: 0.0   random_lineage_dropout: 0.0   include_sequence_id: false   transcribe: \"back_transcribe\"   force_uppercase: false   indexed_dataset_dtype: \"uint8\"   tokenizer_type: \"Byte-Level\"   vocab_file: null   vocab_size: null   merges_file: null   pretrained_tokenizer_model: null   special_tokens: null   fast_hf_tokenizer: true   append_eod: true   enforce_sample_length: null   ftfy: false   workers: 1   preproc_concurrency: 100000   chunksize: 25   drop_empty_sequences: true   nnn_filter: false  # If you split your fasta on NNN (in human these are contigs), then you should set this to true.   seed: 12342  # Not relevant because we are not using random reverse complement or lineage dropout. \"\"\" with open(\"preprocess_config.yaml\", \"w\") as f:     print(output_yaml, file=f) In\u00a0[4]: Copied! <pre>%%capture\n!preprocess_evo2 --config preprocess_config.yaml\n</pre> %%capture !preprocess_evo2 --config preprocess_config.yaml In\u00a0[5]: Copied! <pre># There should be a collection of bin/idx files created in the preprocessed_data directory.\n!ls -lh preprocessed_data/\n</pre> # There should be a collection of bin/idx files created in the preprocessed_data directory. !ls -lh preprocessed_data/ <pre>total 309M\ndrwxr-xr-x 3 ubuntu ubuntu 4.0K Mar 10 22:17 chr20_21_22_uint8_distinct_byte-level_test\n-rw-r--r-- 1 ubuntu ubuntu  90M Mar 10 23:07 chr20_21_22_uint8_distinct_byte-level_test.bin\n-rw-r--r-- 1 ubuntu ubuntu   82 Mar 10 23:07 chr20_21_22_uint8_distinct_byte-level_test.idx\ndrwxr-xr-x 3 ubuntu ubuntu 4.0K Mar 10 22:17 chr20_21_22_uint8_distinct_byte-level_train\n-rw-r--r-- 1 ubuntu ubuntu 123M Mar 10 23:06 chr20_21_22_uint8_distinct_byte-level_train.bin\n-rw-r--r-- 1 ubuntu ubuntu   82 Mar 10 23:07 chr20_21_22_uint8_distinct_byte-level_train.idx\ndrwxr-xr-x 3 ubuntu ubuntu 4.0K Mar 10 22:17 chr20_21_22_uint8_distinct_byte-level_val\n-rw-r--r-- 1 ubuntu ubuntu  97M Mar 10 23:06 chr20_21_22_uint8_distinct_byte-level_val.bin\n-rw-r--r-- 1 ubuntu ubuntu   82 Mar 10 23:07 chr20_21_22_uint8_distinct_byte-level_val.idx\n</pre> In\u00a0[6]: Copied! <pre>%%capture\nif not os.path.exists(\"nemo2_evo2_1b_8k\"):\n    !evo2_convert_to_nemo2 \\\n      --model-path hf://arcinstitute/savanna_evo2_1b_base \\\n      --model-size 1b --output-dir nemo2_evo2_1b_8k\n</pre> %%capture if not os.path.exists(\"nemo2_evo2_1b_8k\"):     !evo2_convert_to_nemo2 \\       --model-path hf://arcinstitute/savanna_evo2_1b_base \\       --model-size 1b --output-dir nemo2_evo2_1b_8k In\u00a0[11]: Copied! <pre>from pathlib import Path\n\n\noutput_pfx = str(Path(os.path.abspath(\"preprocessed_data\")) / \"chr20_21_22_uint8_distinct_byte-level\")\noutput_yaml = f\"\"\"\n- dataset_prefix: {output_pfx}_train\n  dataset_split: train\n  dataset_weight: 1.0\n- dataset_prefix: {output_pfx}_val\n  dataset_split: validation\n  dataset_weight: 1.0\n- dataset_prefix: {output_pfx}_test\n  dataset_split: test\n  dataset_weight: 1.0\n\"\"\"\nwith open(\"training_data_config.yaml\", \"w\") as f:\n    print(output_yaml, file=f)\n</pre> from pathlib import Path   output_pfx = str(Path(os.path.abspath(\"preprocessed_data\")) / \"chr20_21_22_uint8_distinct_byte-level\") output_yaml = f\"\"\" - dataset_prefix: {output_pfx}_train   dataset_split: train   dataset_weight: 1.0 - dataset_prefix: {output_pfx}_val   dataset_split: validation   dataset_weight: 1.0 - dataset_prefix: {output_pfx}_test   dataset_split: test   dataset_weight: 1.0 \"\"\" with open(\"training_data_config.yaml\", \"w\") as f:     print(output_yaml, file=f) <p>This next cell takes approximately 25 minutes to run on an RTX A6000 with <code>MAX_STEPS=100</code>. Each step takes about 9.5 seconds with the following configuration, so you can budget a desired number of max steps to try.</p> In\u00a0[12]: Copied! <pre>%%capture\nMAX_STEPS: int = 10 if FAST_CI_MODE else 100\nval_check_interval = min(int(MAX_STEPS // 2), 50)\nwarmup_steps = min(MAX_STEPS, 100)\n# For evo2 training and fine-tuning follow the same set of steps, so we use the same train_evo2 command.\n#  the big difference is the --ckpt-dir argument which points to a pre-existing checkpoint from some other training run.\n\nif FAST_CI_MODE:\n    model_subset_option = (\n        \"--num-layers 4 --hybrid-override-pattern SDH* --activation-checkpoint-recompute-num-layers 2\"\n    )\nelse:\n    # By default do 5 layers of activation checkpointing\n    model_subset_option = \"--activation-checkpoint-recompute-num-layers 5\"\ntrain_cmd = f\"\"\"train_evo2 \\\n    -d training_data_config.yaml \\\n    --dataset-dir ./preprocessed_data \\\n    --result-dir pretraining_demo \\\n    --experiment-name evo2 \\\n    --model-size 1b \\\n    --devices 1 \\\n    --num-nodes 1 \\\n    --seq-length 8192 \\\n    --micro-batch-size 2 \\\n    --lr 0.000015 \\\n    --min-lr 0.0000149 \\\n    --warmup-steps {warmup_steps} \\\n    --grad-acc-batches 4 \\\n    --max-steps {MAX_STEPS} \\\n    --ckpt-dir nemo2_evo2_1b_8k \\\n    --clip-grad 250 \\\n    --wd 0.001 \\\n    --attention-dropout 0.01 \\\n    --hidden-dropout 0.01 \\\n    --val-check-interval {val_check_interval} \\\n    {model_subset_option} \\\n    --create-tensorboard-logger \\\n    --ckpt-async-save\"\"\"\n\nprint(f\"Running command: {train_cmd}\")\n\nresult = run_subprocess_safely(train_cmd)\n</pre> %%capture MAX_STEPS: int = 10 if FAST_CI_MODE else 100 val_check_interval = min(int(MAX_STEPS // 2), 50) warmup_steps = min(MAX_STEPS, 100) # For evo2 training and fine-tuning follow the same set of steps, so we use the same train_evo2 command. #  the big difference is the --ckpt-dir argument which points to a pre-existing checkpoint from some other training run.  if FAST_CI_MODE:     model_subset_option = (         \"--num-layers 4 --hybrid-override-pattern SDH* --activation-checkpoint-recompute-num-layers 2\"     ) else:     # By default do 5 layers of activation checkpointing     model_subset_option = \"--activation-checkpoint-recompute-num-layers 5\" train_cmd = f\"\"\"train_evo2 \\     -d training_data_config.yaml \\     --dataset-dir ./preprocessed_data \\     --result-dir pretraining_demo \\     --experiment-name evo2 \\     --model-size 1b \\     --devices 1 \\     --num-nodes 1 \\     --seq-length 8192 \\     --micro-batch-size 2 \\     --lr 0.000015 \\     --min-lr 0.0000149 \\     --warmup-steps {warmup_steps} \\     --grad-acc-batches 4 \\     --max-steps {MAX_STEPS} \\     --ckpt-dir nemo2_evo2_1b_8k \\     --clip-grad 250 \\     --wd 0.001 \\     --attention-dropout 0.01 \\     --hidden-dropout 0.01 \\     --val-check-interval {val_check_interval} \\     {model_subset_option} \\     --create-tensorboard-logger \\     --ckpt-async-save\"\"\"  print(f\"Running command: {train_cmd}\")  result = run_subprocess_safely(train_cmd) In\u00a0[\u00a0]: Copied! <pre>assert result[\"returncode\"] == 0, result\n</pre> assert result[\"returncode\"] == 0, result <p>The plotting code is hidden in documentation for brevity. You can view the notebook on github, run it in jupyter-lab or launch the tutorial on brev.dev if you want to view the source.</p> <p>The following figures show various training metrics per step.</p> <ul> <li><code>reduced_train_loss</code> captures the training loss. On larger runs you want to see the loss drop to about 1.08 consistently for the 1b checkpoint.</li> <li><code>lr</code> shows the learning rate schedule for training. Typically we do a linear warmup schedule followed by an cosine decay. this small notebook tutorial just goes through the initial warmup period.</li> <li><code>grad_norm</code> shows the gradient norm of the full model. As the model fits the data better you should see this value drop down below 1.0 consistently.</li> <li><code>val_loss</code> shows the same kind of loss shown in <code>reduced_train_loss</code> but for a held-out set of validation samples. If you ever train the model a very long time and see this start to go up while the training loss continues to drop that's a sign of over-fitting. We have not yet seen this happen. Small fluctuations up and down are expected during training.</li> </ul> In\u00a0[13]: Copied! <pre># Get the TensorBoard event file for the training run\nlog_dirs = !find pretraining_demo/evo2/dev -name \"events.out.tfevents*\"\ntf_event_file = log_dirs[0]\n\n# Extract data from your event file\ndf = tensorboard_to_dataframe(tf_event_file)\n# You can uncomment and modify this to plot multiple metrics once you see what's available\nplot_multiple_training_metrics(df, [\"reduced_train_loss\", \"lr\", \"grad_norm\", \"val_loss\"])\n</pre> # Get the TensorBoard event file for the training run log_dirs = !find pretraining_demo/evo2/dev -name \"events.out.tfevents*\" tf_event_file = log_dirs[0]  # Extract data from your event file df = tensorboard_to_dataframe(tf_event_file) # You can uncomment and modify this to plot multiple metrics once you see what's available plot_multiple_training_metrics(df, [\"reduced_train_loss\", \"lr\", \"grad_norm\", \"val_loss\"]) <p>Now you have a checkpoint that you can try out in place of the converted evo2 checkpoint in the BRCA-1 tutorial (the path is displayed in the next code cell). To test your checkpoint, please supply the following path to the saved checkpoint produced by this notebook as the <code>--ckpt-dir {checkpoint_path}</code> argument to the <code>predict_evo2</code> command in the zero shot BRCA tutorial. For the 1b checkpoint you should see AUC above 0.73 if you successfully fine-tuned the checkpoint for your hardware, or to check that your hardware works with the converted checkpoint from hugging face as is.</p> <p>In our experience running this notebook for up to an hour on a single GPU is not sufficient to recover BF16 accuracy. We have more details about what did work in the Next Steps section below.</p> In\u00a0[14]: Copied! <pre>final_ckpt_paths = !ls -d pretraining_demo/evo2/checkpoints/*-last\nfinal_ckpt_path = final_ckpt_paths[-1]\nfinal_ckpt_path\n</pre> final_ckpt_paths = !ls -d pretraining_demo/evo2/checkpoints/*-last final_ckpt_path = final_ckpt_paths[-1] final_ckpt_path Out[14]: <pre>'pretraining_demo/default--val_loss=0.8664-epoch=0-consumed_samples=800.0-last'</pre> In\u00a0[2]: Copied! <pre># Display the example loss curve from a larger training run\nfrom IPython.display import Image, display\n\n\n# Load and display the image\ndisplay(Image(\"../assets/1b_finetuning_train_curve_500_steps_256gbs.png\", width=800))\n</pre> # Display the example loss curve from a larger training run from IPython.display import Image, display   # Load and display the image display(Image(\"../assets/1b_finetuning_train_curve_500_steps_256gbs.png\", width=800))"},{"location":"main/examples/bionemo-evo2/fine-tuning-tutorial/#fine-tuning-tutorial-for-evo2-adapt-the-1b-evo2-checkpoint-for-your-hardware","title":"Fine-tuning tutorial for Evo2: Adapt the 1b evo2 checkpoint for your hardware\u00b6","text":"<p>Deploy tutorial on brev.dev: </p>"},{"location":"main/examples/bionemo-evo2/fine-tuning-tutorial/#background-and-motivation","title":"Background and motivation\u00b6","text":"<p>To motivate this tutorial, we have noticed that the public evo2 checkpoint in hugging face for the 1b model is sensitive to <code>--fp8</code> status in training, the zero shot inference task, as demonstrated in the zero shot BRCA-1 notebook, produces near random AUCs if you do not use <code>--fp8</code>. If you want to infer or score new data, you need FP8 enabled since it was trained that way. Interestingly the <code>7b</code> checkpoint does not suffer from this limitation and seems robust to FP8 being activated or not. The consequence of this is that if you have older GPUs with a compute capability less than 8.9, which do not support FP8, then the output that you get from scoring sequences with sensitive checkpoints may not be biologically meaningful.</p> <p>We plan on making a <code>1b</code> parameter evo2 checkpoint available soon that has been fine-tuned to be robust to FP8 or BF16 inference in bionemo on NGC, but in the meantime this notebook tutorial outlines the steps for fine-tuning. The only difference between this notebook and what we did in production was to run these steps on more data on a slurm cluster to increase the global batch size. That said, if you run this for enough steps to get loss on the 1b checkpoint to the 1.08 range, you should have good luck with downstream sequence scoring tasks.</p>"},{"location":"main/examples/bionemo-evo2/fine-tuning-tutorial/#requirements","title":"Requirements\u00b6","text":"<p>This is a tutorial demonstrating how you can fine-tune Evo2 on new data and/or hardware. The tutorial should take slightly under 1 hour to run on an RTX A6000 in bf16 precision.</p> <p>As configured, this tutorial requires an NVIDIA GPU with approximately 45GB of ram. If you have multiple GPUs with less memory, or you are having trouble with CUDA OOM at the training step below, try reducing the <code>--micro-batch-size</code> and/or increasing the number of <code>--devices [int]</code> to match your setup and also setting <code>--tensor-parallel-size [int]</code> to the number of devices. This should split up most of the model evenly between your devices, which will require much less memory. When we train the 1b model in practice we typically have the micro batch size set to 8, and run without model parallelism on available devices to achieve the largest possible global batch size.</p>"},{"location":"main/examples/bionemo-evo2/fine-tuning-tutorial/#setup-training-data","title":"Setup training data\u00b6","text":"<p>Evo2 uses megatron style datasets behind the scenes with advanced support for randomly indexing into documents, and packing documents together into batches at scale. The file-formats backing these datasets is not a standard biological format like fasta for representing genomes. First we show how you can start from a fasta file and preprocess them into the required data format for downstream handling. High level the steps are as follows:</p> <ol> <li>Acquire fasta files locally, ideally in some shared cluster storage</li> <li>Write a config script defining how you want the processed files to be generated from the fasta files. This is where you specify top level train/validation/test splitting decisions.</li> <li>Call the actual <code>preprocess_evo2</code> script to generate the results.</li> </ol> <p>The next 4 cells go through this process on a set of smaller human chromosomes. At least 3 fasta records need to be present, one for the train, validation, and test split.</p>"},{"location":"main/examples/bionemo-evo2/fine-tuning-tutorial/#optional-specify-or-convert-initial-checkpoint","title":"[Optional] specify or convert initial checkpoint\u00b6","text":"<p>The main difference between pre-training and fine-tuning is whether or not you decide to start training the model with weights from a prior training run. For this tutorial we want to tune a <code>1b</code> checkpoint from hugging face that is known (at the time of this writing) to be sensitive to GPU architecture so that it will work with your architecture. We have a script that will download and convert a savanna format evo2 checkpoint from hugging face, and output that into a NeMo2 format checkpoint directory that can be used as the starting point for a fine-tuning run.</p>"},{"location":"main/examples/bionemo-evo2/fine-tuning-tutorial/#configure-the-training-dataset","title":"Configure the training dataset\u00b6","text":"<p>The next step is to configure your training dataset, in this case configuring the simple single-file example we output two steps ago in this tutorial.</p>"},{"location":"main/examples/bionemo-evo2/fine-tuning-tutorial/#next-steps","title":"Next steps\u00b6","text":"<p>On a small number of devices, or with the small demo fasta we provided in this tutorial, it's possible you are not at the needed 1.08 loss level to get good downstream accuracy out of this checkpoint. You can try increasing the <code>MAX_STEPS</code> parameter in the training cell, or running a larger cluster with more GPUs. The following loss curve was generated with a global batch size of 256 at 8192 context or approximately 2 million tokens per step. With that configuration we see a good loss of 1.08 after approximately 100 steps. The following figure shows our learning rate across the first 500 steps of fine-tuning with a global batch size of 256. Later on in this notebook we also show the slurm script to replicate this on your cluster.</p>"},{"location":"main/examples/bionemo-evo2/fine-tuning-tutorial/#how-we-fine-tuned-the-1b-checkpoint-for-bf16-accuracy","title":"How we fine-tuned the 1b checkpoint for bf16 accuracy\u00b6","text":"<p>An example of the full slurm script to run the above training curve on our infrastructure is as follows:</p> <p>First make a <code>~/.netrc</code> file with your wandb login info. You can also accomplish this by setting wandb ENV variables, assuming you want to log to wandb. If not you can pass the <code>--no-wandb</code> argument as part of the args to <code>train_evo2</code>:</p> <pre>machine api.wandb.ai\n  login user\n  password PASSWORD_HERE\n</pre> <p>Next, paste/edit the following sbatch script for your own configuration:</p> <pre># TODO: You may need to add more SBATCH configuration here specific to your cluster.\n#SBATCH --nodes=4                       # number of nodes\n#SBATCH --gpus-per-node=8\n#SBATCH --ntasks-per-node=8                 # n tasks per machine (one task per gpu) &lt;required&gt;\n#SBATCH --time=04:00:00                     # wall time  (8 for batch, backfill, 2 for batch_short)\n#SBATCH --mem=0                             # all mem avail\n#SBATCH --exclusive\nset -x\n# You may want to edit this file and/or add your own version to your mounts.\nCONFIG_PATH_IN_CONTAINER=/workspace/bionemo2/sub-packages/bionemo-evo2/examples/configs/full_pretrain_shortphase_config.yaml\n# You can build a `.sqsh` file with enroot which may be faster to load on each node rather than pulling down from NGC\nIMAGE_PATH=nvcr.io/nvidia/clara/bionemo-framework:nightly\nWANDB_PROJECT_NAME= # Set you wandb project here, or leave blank and add --no-wandb to the image\nMODEL_SIZE=1b  # change this to 7b_arc_longcontext etc. This version is different.\nCP_SIZE=1\nTP_SIZE=1\nPP_SIZE=1\nMICRO_BATCH_SIZE=8\nGRAD_ACC_BATCHES=1\nSEQ_LEN=8192\nMAX_STEPS=580000 # 8T tokens given 1024 nodes and 8192 seq length\nVAL_CHECK=500\nCLIP_GRAD=250  # Arc trained without gradient clipping. Set to a large value so megatron still logs grad_norm.\n# The following arguments will remove the EOD/PAD tokens from the loss, unlike how the original Evo2 model was trained.\n#  this does not impact downstream accuracy in our experience and is more standard.\nEXTRA_ARGS=\"--enable-preemption --ckpt-async-save --overlap-grad-reduce --clip-grad $CLIP_GRAD --eod-pad-in-loss-mask\"\nLR=0.000015\nMIN_LR=0.0000015\nWU_STEPS=100\nSEED=1234 \nWD=0.001\nADO=0.01\nHDO=0.01\nEXPERIMENT_NAME=fine_tune_evo2_1b_on_bf16\n# NCCL performance parameters\n# =========================\nexport TORCH_NCCL_AVOID_RECORD_STREAMS=1\n\n# Mounts\n# =========================\nDATA_PATH= # PATH to the directory that stores your data that you want to mount into the container\nDATA_MOUNT=/workspace/bionemo2/data  # or if you configure your data with a different base dir in the config, use that here\nRESULTS_PATH_CLUSTER= # Where do you want the results to land on your shared cluster storage\nRESULTS_PATH_IMAGE=/results/\nCKPT_MOUNT_CLUSTER= # Path to shared location on your cluster where the checkpoint files can be found\nCKPT_MOUNT_IMAGE=/checkpoints/  # pragma: allowlist secret  (for some reason this line flags a high entropy string check in CI)\nNETRC_PATH=$HOME/.netrc\nNETRC_MOUNT=/root/.netrc\n# TODO either move your config to one of the mounted paths or add your own mount to a location with your configs\n\nmkdir -p $RESULTS_PATH_CLUSTER\nMOUNTS=${DATA_PATH}:${DATA_MOUNT},${RESULTS_PATH_CLUSTER}:${RESULTS_PATH_IMAGE},${NETRC_PATH}:${NETRC_MOUNT},${CKPT_MOUNT_CLUSTER}:${CKPT_MOUNT_IMAGE},$HOME/.cache:/root/.cache\n# Generate (or retrieve) a unique, shared ID per run to handle restarts in W&amp;B and Tensorboard\n# =========================\nmkdir -p ${RESULTS_PATH_CLUSTER}\nif [ -f ${RESULTS_PATH_CLUSTER}/run.id ];\nthen\n    RUN_ID=$(&lt;${RESULTS_PATH_CLUSTER}/run.id)\nelse\n    array=()\n    for i in {a..z} {A..Z} {0..9};\n    do\n    array[$RANDOM]=$i\n    done\n    RUN_ID=$(printf %s ${array[@]::8})\n    echo $RUN_ID &gt; ${RESULTS_PATH_CLUSTER}/run.id\nfi\n# =========================\nread -r -d '' COMMAND &lt;&lt;EOF\necho \"*******STARTING********\" \\\n&amp;&amp; echo \"---------------\" \\\n&amp;&amp; echo \"Starting training\" \\\n&amp;&amp;  \\\ntrain_evo2 \\\n    -d $CONFIG_PATH_IN_CONTAINER \\\n    --num-nodes=${SLURM_JOB_NUM_NODES} \\\n    --ckpt-dir $CKPT_MOUNT_IMAGE/nemo2_evo2_1b_8k \\\n    --devices=${SLURM_NTASKS_PER_NODE} \\\n    --grad-acc-batches $GRAD_ACC_BATCHES \\\n    --max-steps=$MAX_STEPS \\\n    --seed $SEED \\\n    ${EXTRA_ARGS} \\\n    --wandb-run-id $RUN_ID \\\n    --wandb-project $WANDB_PROJECT_NAME \\\n    --lr $LR \\\n    --wd $WD \\\n    --activation-checkpoint-recompute-num-layers 5 \\\n    --min-lr $MIN_LR \\\n    --warmup-steps $WU_STEPS \\\n    --attention-dropout $ADO \\\n    --hidden-dropout $HDO \\\n    --limit-val-batches=20 \\\n    --val-check-interval=${VAL_CHECK} \\\n    --result-dir=$RESULTS_PATH_IMAGE \\\n    --seq-length=${SEQ_LEN} \\\n    --tensor-parallel-size=${TP_SIZE} \\\n    --context-parallel-size=${CP_SIZE} \\\n    --pipeline-model-parallel-size=${PP_SIZE} \\\n    --workers 8 \\\n    --micro-batch-size=${MICRO_BATCH_SIZE} \\\n    --model-size=${MODEL_SIZE}\nEOF\nsrun \\\n    --output ${RESULTS_PATH_CLUSTER}/slurm-%j.out \\\n    --error ${RESULTS_PATH_CLUSTER}/error-%j.out \\\n    --container-image=$IMAGE_PATH \\\n    --container-mounts ${MOUNTS} \\\n    bash -c \"${COMMAND}\"\nset +x\n</pre>"},{"location":"main/examples/bionemo-evo2/zeroshot_brca1/","title":"Zero-shot prediction of BRCA1 variant effects with Evo 2","text":"In\u00a0[1]: Copied! <pre>%%capture\n!pip install biopython openpyxl\nimport os\n\n\n# Runs a subset of the model layers to test that the notebook runs in CI, but the output will be incorrect.\nFAST_CI_MODE: bool = os.environ.get(\"FAST_CI_MODE\", False)\n</pre> %%capture !pip install biopython openpyxl import os   # Runs a subset of the model layers to test that the notebook runs in CI, but the output will be incorrect. FAST_CI_MODE: bool = os.environ.get(\"FAST_CI_MODE\", False) In\u00a0[2]: Copied! <pre>import glob\nimport gzip\nimport json\nimport math\nimport os\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport torch\nfrom Bio import SeqIO\nfrom sklearn.metrics import auc, roc_auc_score, roc_curve\n\nfrom bionemo.core.utils.subprocess_utils import run_subprocess_safely\n</pre> import glob import gzip import json import math import os from pathlib import Path  import matplotlib.pyplot as plt import pandas as pd import seaborn as sns import torch from Bio import SeqIO from sklearn.metrics import auc, roc_auc_score, roc_curve  from bionemo.core.utils.subprocess_utils import run_subprocess_safely <p>We start by loading a dataset from\u00a0Findlay et al. (2018), which contains experimentally measured function scores of 3,893\u00a0BRCA1\u00a0SNVs. These function scores reflect the extent by which the genetic variant has disrupted the protein's function, with lower scores indicating greater disruption. In this dataset, the SNVs are classified into three categories based on their function scores:\u00a0<code>LOF</code>\u00a0(loss-of-function),\u00a0<code>INT</code>\u00a0(intermediate), and\u00a0<code>FUNC</code>\u00a0(functional). We start by reading in this dataset.</p> <p>To keep the notebook streamlined, we've abstracted much of the preprocessing logic into accompanying scripts located in <code>brca1_utils</code>. This notebook can also be viewed here.</p> In\u00a0[4]: Copied! <pre>%%capture\n\n# Configuration parameters\nDATA_DIR = \"brca1\"\nSAMPLE_CONFIG = {\"sample_frac\": 0.05, \"balanced\": True, \"disable\": False, \"random_state\": 42}\n\n# 1. Download the necessary data files if not present\nexcel_path, genome_path = download_data(DATA_DIR)\nseq_chr17 = load_genome_sequence(genome_path)\n\n# 2. Load and preprocess BRCA1 data\nbrca1_df = load_brca1_data(excel_path)\n</pre> %%capture  # Configuration parameters DATA_DIR = \"brca1\" SAMPLE_CONFIG = {\"sample_frac\": 0.05, \"balanced\": True, \"disable\": False, \"random_state\": 42}  # 1. Download the necessary data files if not present excel_path, genome_path = download_data(DATA_DIR) seq_chr17 = load_genome_sequence(genome_path)  # 2. Load and preprocess BRCA1 data brca1_df = load_brca1_data(excel_path) <p>We then group the <code>FUNC</code> and <code>INT</code> classes of SNVs together into a single category (<code>FUNC/INT</code>).</p> <p>We build a function to parse the reference and variant sequences of a 8,192-bp window around the genomic position of each SNV, using the reference sequence of human chromosome 17 where BRCA1 is located.</p> <p>To make things run faster, we'll just look at a balanced sample of our data. If you want to run on the full dataset, set <code>disable_sample=True</code></p> In\u00a0[6]: Copied! <pre>OUTPUT_DIR = \"brca1_fasta_files\"\n\nbrca1_df = sample_data(\n    brca1_df,\n    sample_frac=SAMPLE_CONFIG[\"sample_frac\"],\n    balanced=SAMPLE_CONFIG[\"balanced\"],\n    disable=SAMPLE_CONFIG[\"disable\"],\n    random_state=SAMPLE_CONFIG[\"random_state\"],\n)\n\nbrca1_df.head(5)\n</pre> OUTPUT_DIR = \"brca1_fasta_files\"  brca1_df = sample_data(     brca1_df,     sample_frac=SAMPLE_CONFIG[\"sample_frac\"],     balanced=SAMPLE_CONFIG[\"balanced\"],     disable=SAMPLE_CONFIG[\"disable\"],     random_state=SAMPLE_CONFIG[\"random_state\"], )  brca1_df.head(5) Out[6]: chrom pos ref alt score class 0 17 41199726 T C 0.159762 FUNC/INT 1 17 41209074 T A -2.065569 LOF 2 17 41256913 A C -0.847753 FUNC/INT 3 17 41219631 T A -2.053739 LOF 4 17 41215965 G A -1.671525 LOF <p>Next, we'll write these to local <code>.fasta</code> files so we can use them for prediction below.</p> In\u00a0[8]: Copied! <pre>brca1_df = generate_fasta_files(brca1_df, seq_chr17, output_dir=OUTPUT_DIR)\n</pre> brca1_df = generate_fasta_files(brca1_df, seq_chr17, output_dir=OUTPUT_DIR) <pre>Total unique reference sequences: 79\nTotal unique variant sequences: 84\n</pre> <p>Then, we load Evo 2 1B model, loading the Evo 2 weights from hugging face.</p> <p>Note - for better performance, load the 7b model by setting <code>MODEL_SIZE=\"7b\"</code> which also works well GPUs that do not support FP8.</p> In\u00a0[9]: Copied! <pre>%%capture\nMODEL_SIZE = \"1b\"  # also try 7b if you have a GPU with more than 32GB of memory\n\n# Define checkpoint path\nif MODEL_SIZE == \"1b\":\n    from bionemo.core.data.load import load\n\n    #  This line will download the checkpoint from NGC to your $HOME/.cache/bionemo directory and return the path.\n    #  To do the same from the command line, use `CHECKPOINT_PATH=$(download_bionemo_data evo2/1b-8k-bf16:1.0)`\n    checkpoint_path = load(\"evo2/1b-8k-bf16:1.0\")\nelse:\n    checkpoint_path = Path(f\"nemo2_evo2_{MODEL_SIZE}_8k\")\n\n    # Check if the directory does not exist or is empty\n    if not checkpoint_path.exists() or not any(checkpoint_path.iterdir()):\n        !evo2_convert_to_nemo2 --model-path hf://arcinstitute/savanna_evo2_{MODEL_SIZE}_base --model-size {MODEL_SIZE} --output-dir nemo2_evo2_{MODEL_SIZE}_8k\n    else:\n        print(\"Checkpoint directory is not empty. Skipping command.\")\n</pre> %%capture MODEL_SIZE = \"1b\"  # also try 7b if you have a GPU with more than 32GB of memory  # Define checkpoint path if MODEL_SIZE == \"1b\":     from bionemo.core.data.load import load      #  This line will download the checkpoint from NGC to your $HOME/.cache/bionemo directory and return the path.     #  To do the same from the command line, use `CHECKPOINT_PATH=$(download_bionemo_data evo2/1b-8k-bf16:1.0)`     checkpoint_path = load(\"evo2/1b-8k-bf16:1.0\") else:     checkpoint_path = Path(f\"nemo2_evo2_{MODEL_SIZE}_8k\")      # Check if the directory does not exist or is empty     if not checkpoint_path.exists() or not any(checkpoint_path.iterdir()):         !evo2_convert_to_nemo2 --model-path hf://arcinstitute/savanna_evo2_{MODEL_SIZE}_base --model-size {MODEL_SIZE} --output-dir nemo2_evo2_{MODEL_SIZE}_8k     else:         print(\"Checkpoint directory is not empty. Skipping command.\") <p>Next, we score the likelihoods of the reference and variant sequences of each SNV.</p> In\u00a0[11]: Copied! <pre># Define output directories for prediction results\noutput_dir = Path(\"brca1_fasta_files\")\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Save reference and variant sequences to FASTA\nref_fasta_path = output_dir / \"brca1_reference_sequences.fasta\"\nvar_fasta_path = output_dir / \"brca1_variant_sequences.fasta\"\n\npredict_ref_dir = output_dir / \"reference_predictions\"\npredict_var_dir = output_dir / \"variant_predictions\"\npredict_ref_dir.mkdir(parents=True, exist_ok=True)\npredict_var_dir.mkdir(parents=True, exist_ok=True)\n\nfp8_supported, gpu_info = check_fp8_support()\nprint(f\"FP8 Support: {fp8_supported}\")\nprint(gpu_info)\n\n# Note: If FP8 is not supported, you may want to disable it in the model config\n# The Evo2 config has 'use_fp8_input_projections: True' by default\n\nif FAST_CI_MODE:\n    model_subset_option = \"--num-layers 4 --hybrid-override-pattern SDH*\"\nelse:\n    model_subset_option = \"\"\n\nfp8_option = \"--fp8\" if fp8_supported else \"\"\n\n# Update predict commands to run on the full dataset\npredict_ref_command = (\n    f\"predict_evo2 --fasta {ref_fasta_path} --ckpt-dir {checkpoint_path} \"\n    f\"--output-dir {predict_ref_dir} --model-size {MODEL_SIZE} --tensor-parallel-size 1  {model_subset_option} \"\n    f\"--pipeline-model-parallel-size 1 --context-parallel-size 1 --output-log-prob-seqs {fp8_option}\"\n)\n\npredict_var_command = (\n    f\"predict_evo2 --fasta {var_fasta_path} --ckpt-dir {checkpoint_path} \"\n    f\"--output-dir {predict_var_dir} --model-size {MODEL_SIZE} --tensor-parallel-size 1 {model_subset_option} \"\n    f\"--pipeline-model-parallel-size 1 --context-parallel-size 1 --output-log-prob-seqs {fp8_option}\"\n)\n</pre> # Define output directories for prediction results output_dir = Path(\"brca1_fasta_files\") output_dir.mkdir(parents=True, exist_ok=True)  # Save reference and variant sequences to FASTA ref_fasta_path = output_dir / \"brca1_reference_sequences.fasta\" var_fasta_path = output_dir / \"brca1_variant_sequences.fasta\"  predict_ref_dir = output_dir / \"reference_predictions\" predict_var_dir = output_dir / \"variant_predictions\" predict_ref_dir.mkdir(parents=True, exist_ok=True) predict_var_dir.mkdir(parents=True, exist_ok=True)  fp8_supported, gpu_info = check_fp8_support() print(f\"FP8 Support: {fp8_supported}\") print(gpu_info)  # Note: If FP8 is not supported, you may want to disable it in the model config # The Evo2 config has 'use_fp8_input_projections: True' by default  if FAST_CI_MODE:     model_subset_option = \"--num-layers 4 --hybrid-override-pattern SDH*\" else:     model_subset_option = \"\"  fp8_option = \"--fp8\" if fp8_supported else \"\"  # Update predict commands to run on the full dataset predict_ref_command = (     f\"predict_evo2 --fasta {ref_fasta_path} --ckpt-dir {checkpoint_path} \"     f\"--output-dir {predict_ref_dir} --model-size {MODEL_SIZE} --tensor-parallel-size 1  {model_subset_option} \"     f\"--pipeline-model-parallel-size 1 --context-parallel-size 1 --output-log-prob-seqs {fp8_option}\" )  predict_var_command = (     f\"predict_evo2 --fasta {var_fasta_path} --ckpt-dir {checkpoint_path} \"     f\"--output-dir {predict_var_dir} --model-size {MODEL_SIZE} --tensor-parallel-size 1 {model_subset_option} \"     f\"--pipeline-model-parallel-size 1 --context-parallel-size 1 --output-log-prob-seqs {fp8_option}\" ) <pre>FP8 Support: False\nDevice: NVIDIA RTX A6000, Compute Capability: 8.6\n</pre> <p>Score reference sequences:</p> In\u00a0[12]: Copied! <pre>%%capture\nprint(f\"Running command: {predict_ref_command}\")\n\nresult = run_subprocess_safely(predict_ref_command)\n</pre> %%capture print(f\"Running command: {predict_ref_command}\")  result = run_subprocess_safely(predict_ref_command) In\u00a0[\u00a0]: Copied! <pre>assert result[\"returncode\"] == 0, result\n</pre> assert result[\"returncode\"] == 0, result <p>Score variant sequences:</p> In\u00a0[13]: Copied! <pre>%%capture\nprint(f\"Running command: {predict_var_command}\")\n\nresult = run_subprocess_safely(predict_var_command)\n</pre> %%capture print(f\"Running command: {predict_var_command}\")  result = run_subprocess_safely(predict_var_command) In\u00a0[\u00a0]: Copied! <pre>assert result[\"returncode\"] == 0, result\n</pre> assert result[\"returncode\"] == 0, result <p>We calculate the change in likelihoods for each variant relative to the likelihood of their respective wild-type sequence.</p> <p>First, we load the prediction files and sequence id maps:</p> In\u00a0[\u00a0]: Copied! <pre># Find and load prediction files\nref_pred_files = glob.glob(os.path.join(predict_ref_dir, \"predictions__rank_*.pt\"))\nvar_pred_files = glob.glob(os.path.join(predict_var_dir, \"predictions__rank_*.pt\"))\n\n# Load sequence ID maps (maps sequence ID -&gt; prediction index)\nwith open(os.path.join(predict_ref_dir, \"seq_idx_map.json\"), \"r\") as f:\n    ref_seq_idx_map = json.load(f)\nwith open(os.path.join(predict_var_dir, \"seq_idx_map.json\"), \"r\") as f:\n    var_seq_idx_map = json.load(f)\n\n# Load predictions\nref_preds = torch.load(ref_pred_files[0])\nvar_preds = torch.load(var_pred_files[0])\n</pre> # Find and load prediction files ref_pred_files = glob.glob(os.path.join(predict_ref_dir, \"predictions__rank_*.pt\")) var_pred_files = glob.glob(os.path.join(predict_var_dir, \"predictions__rank_*.pt\"))  # Load sequence ID maps (maps sequence ID -&gt; prediction index) with open(os.path.join(predict_ref_dir, \"seq_idx_map.json\"), \"r\") as f:     ref_seq_idx_map = json.load(f) with open(os.path.join(predict_var_dir, \"seq_idx_map.json\"), \"r\") as f:     var_seq_idx_map = json.load(f)  # Load predictions ref_preds = torch.load(ref_pred_files[0]) var_preds = torch.load(var_pred_files[0]) <p>Then, calculate the delta score:</p> In\u00a0[15]: Copied! <pre># next, calculate change in likelihoods\nref_log_probs = []\nvar_log_probs = []\nfor _, row in brca1_df.iterrows():\n    ref_name = row[\"ref_fasta_name\"]\n    var_name = row[\"var_fasta_name\"]\n    ref_log_probs.append(ref_preds[\"log_probs_seqs\"][ref_seq_idx_map[ref_name]].item())\n    var_log_probs.append(var_preds[\"log_probs_seqs\"][var_seq_idx_map[var_name]].item())\nbrca1_df[\"ref_log_probs\"] = ref_log_probs\nbrca1_df[\"var_log_probs\"] = var_log_probs\n# ideally probability of a broken variant is lower than a good one. So a bad var - good ref is negative.\nbrca1_df[\"evo2_delta_score\"] = brca1_df[\"var_log_probs\"] - brca1_df[\"ref_log_probs\"]\nbrca1_df.head()\n</pre> # next, calculate change in likelihoods ref_log_probs = [] var_log_probs = [] for _, row in brca1_df.iterrows():     ref_name = row[\"ref_fasta_name\"]     var_name = row[\"var_fasta_name\"]     ref_log_probs.append(ref_preds[\"log_probs_seqs\"][ref_seq_idx_map[ref_name]].item())     var_log_probs.append(var_preds[\"log_probs_seqs\"][var_seq_idx_map[var_name]].item()) brca1_df[\"ref_log_probs\"] = ref_log_probs brca1_df[\"var_log_probs\"] = var_log_probs # ideally probability of a broken variant is lower than a good one. So a bad var - good ref is negative. brca1_df[\"evo2_delta_score\"] = brca1_df[\"var_log_probs\"] - brca1_df[\"ref_log_probs\"] brca1_df.head() Out[15]: chrom pos ref alt score class ref_fasta_name var_fasta_name ref_log_probs var_log_probs evo2_delta_score 0 17 41199726 T C 0.159762 FUNC/INT BRCA1_ref_pos_41199726_T_class_FUNC/INT BRCA1_var_pos_41199726_TtoC_class_FUNC/INT -0.952952 -0.953219 -0.000267 1 17 41209074 T A -2.065569 LOF BRCA1_ref_pos_41209074_T_class_LOF BRCA1_var_pos_41209074_TtoA_class_LOF -0.750379 -0.750438 -0.000059 2 17 41256913 A C -0.847753 FUNC/INT BRCA1_ref_pos_41256913_A_class_FUNC/INT BRCA1_var_pos_41256913_AtoC_class_FUNC/INT -0.798110 -0.799099 -0.000989 3 17 41219631 T A -2.053739 LOF BRCA1_ref_pos_41219631_T_class_LOF BRCA1_var_pos_41219631_TtoA_class_LOF -1.032214 -1.032696 -0.000482 4 17 41215965 G A -1.671525 LOF BRCA1_ref_pos_41215965_G_class_LOF BRCA1_var_pos_41215965_GtoA_class_LOF -0.860933 -0.861262 -0.000329 <p>This delta likelihood should be predictive of how disruptive the SNV is to the protein's function: the lower the delta, the more likely that the SNV is disruptive. We can show this by comparing the distributions of delta likelihoods for the two classes of SNVs (functional/intermediate vs loss-of-function).</p> In\u00a0[17]: Copied! <pre>plot_strip_with_means(brca1_df, x_col=\"evo2_delta_score\", class_col=\"class\")\n</pre> plot_strip_with_means(brca1_df, x_col=\"evo2_delta_score\", class_col=\"class\") <p>We can also calculate the area under the receiver operating characteristic curve (AUROC) of this zero-shot prediction method. Note that the results are nearly random unless you are on one of the following configurations:</p> <ul> <li><code>--fp8</code> on an fp8 enabled GPU with either the 1b or 7b models. The 40b likely works as well.</li> <li>the 7b model uniquely seems to work well without <code>--fp8</code> so if you are on an older device, the 7b model should produce robust results. Change the <code>MODEL_SIZE</code> earlier in this tutorial and rerun for good results in that case.</li> </ul> In\u00a0[18]: Copied! <pre># Calculate AUROC of zero-shot predictions\n#  class 1 is LOF which is the bad thing. That means we expect this to be more negative.\ny_true = brca1_df[\"class\"] == \"LOF\"\nauroc = roc_auc_score(y_true, -brca1_df[\"evo2_delta_score\"])\nprint(f\"Zero-shot prediction AUROC: {auroc:.2}\")\n</pre> # Calculate AUROC of zero-shot predictions #  class 1 is LOF which is the bad thing. That means we expect this to be more negative. y_true = brca1_df[\"class\"] == \"LOF\" auroc = roc_auc_score(y_true, -brca1_df[\"evo2_delta_score\"]) print(f\"Zero-shot prediction AUROC: {auroc:.2}\") <pre>Zero-shot prediction AUROC: 0.77\n</pre> In\u00a0[20]: Copied! <pre>plot_roc_curve(brca1_df)\n</pre> plot_roc_curve(brca1_df) In\u00a0[22]: Copied! <pre># Check if the AUC is a reasonable value for our CI suite when we run the full model\nassert FAST_CI_MODE or auroc &gt;= 0.73\n</pre> # Check if the AUC is a reasonable value for our CI suite when we run the full model assert FAST_CI_MODE or auroc &gt;= 0.73"},{"location":"main/examples/bionemo-evo2/zeroshot_brca1/#zero-shot-prediction-of-brca1-variant-effects-with-evo-2","title":"Zero-shot prediction of BRCA1 variant effects with Evo 2\u00b6","text":"<p>Deploy this tutorial on brev.dev: </p> <p>Note - this notebook is a reproduction of The Arc Institute\u2019s same-titled notebook here, using the BioNeMo 2 implementation of Evo2.</p> <p>Evo2 is a foundation AI model trained on 9.3 trillion DNA base pairs, predicting variant effects without prior tast-specific training.</p> <p>Without being explicitly trained on BRCA1 variants, we show Evo 2's ability to generalize across all life forms.</p> <p>The human\u00a0BRCA1\u00a0gene encodes for a protein that repairs damaged DNA (Moynahan et al., 1999). Certain variants of this gene have been associated with an increased risk of breast and ovarian cancers (Miki et al., 1994). Using Evo 2, we can predict whether a particular single nucleotide variant (SNV) of the\u00a0BRCA1\u00a0gene is likely to be harmful to the protein's function, and thus potentially increase the risk of cancer for the patient with the genetic variant.</p>"},{"location":"main/examples/bionemo-evo2/zeroshot_brca1/#load-evo-2-checkpoints","title":"Load Evo 2 Checkpoints\u00b6","text":""},{"location":"main/examples/bionemo-evo2/zeroshot_brca1/#score-sequences","title":"Score Sequences\u00b6","text":""},{"location":"main/examples/bionemo-evo2/zeroshot_brca1/#full-sample-performance","title":"Full Sample Performance\u00b6","text":"<p>The above analysis may have been performed on a subset of the available data.</p> <p>For comparison, the table below presents the AUROC scores for different model sizes trained on the full dataset (100% sample fraction).</p> Model Size Dataset Sample Fraction AUROC Evo 2 1B 100% 0.74 Evo 2 7B 100% 0.87"},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_cfm/","title":"Building Generative Models for Continuous Data via Continuous Interpolants","text":"<p>NOTE: it takes about 10 minutes to deploy this notebook as a Launchable. As of this writing, we are working on a free tier so a credit card may be required. You can reach out to your NVIDIA rep for credits.\"</p> In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport torch\nfrom sklearn.datasets import make_moons\n</pre> import matplotlib.pyplot as plt import torch from sklearn.datasets import make_moons In\u00a0[2]: Copied! <pre>def sample_moons(n, normalize=False):\n    x1, _ = make_moons(n_samples=n, noise=0.08)\n    x1 = torch.Tensor(x1)\n    x1 = x1 * 3 - 1\n    if normalize:\n        x1 = (x1 - x1.mean(0)) / x1.std(0) * 2\n    return x1\n</pre> def sample_moons(n, normalize=False):     x1, _ = make_moons(n_samples=n, noise=0.08)     x1 = torch.Tensor(x1)     x1 = x1 * 3 - 1     if normalize:         x1 = (x1 - x1.mean(0)) / x1.std(0) * 2     return x1 In\u00a0[3]: Copied! <pre>x1 = sample_moons(1000)\nplt.scatter(x1[:, 0], x1[:, 1])\n</pre> x1 = sample_moons(1000) plt.scatter(x1[:, 0], x1[:, 1]) Out[3]: <pre>&lt;matplotlib.collections.PathCollection at 0x7d23ff1b4a00&gt;</pre> In\u00a0[4]: Copied! <pre>dim = 2\nhidden_size = 64\nbatch_size = 256\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(dim + 1, hidden_size),\n    torch.nn.SELU(),\n    torch.nn.Linear(hidden_size, hidden_size),\n    torch.nn.SELU(),\n    torch.nn.Linear(hidden_size, hidden_size),\n    torch.nn.SELU(),\n    torch.nn.Linear(hidden_size, dim),\n)\noptimizer = torch.optim.Adam(model.parameters())\n</pre> dim = 2 hidden_size = 64 batch_size = 256 model = torch.nn.Sequential(     torch.nn.Linear(dim + 1, hidden_size),     torch.nn.SELU(),     torch.nn.Linear(hidden_size, hidden_size),     torch.nn.SELU(),     torch.nn.Linear(hidden_size, hidden_size),     torch.nn.SELU(),     torch.nn.Linear(hidden_size, dim), ) optimizer = torch.optim.Adam(model.parameters()) In\u00a0[5]: Copied! <pre>from bionemo.moco.distributions.prior import GaussianPrior\nfrom bionemo.moco.distributions.time import UniformTimeDistribution\nfrom bionemo.moco.interpolants import ContinuousFlowMatcher\n\n\nuniform_time = UniformTimeDistribution()\nsimple_prior = GaussianPrior()\nsigma = 0.1\ncfm = ContinuousFlowMatcher(\n    time_distribution=uniform_time, prior_distribution=simple_prior, sigma=sigma, prediction_type=\"velocity\"\n)\n# Place both the model and the interpolant on the same device\nDEVICE = \"cuda\"\nmodel = model.to(DEVICE)\ncfm = cfm.to_device(DEVICE)\n</pre> from bionemo.moco.distributions.prior import GaussianPrior from bionemo.moco.distributions.time import UniformTimeDistribution from bionemo.moco.interpolants import ContinuousFlowMatcher   uniform_time = UniformTimeDistribution() simple_prior = GaussianPrior() sigma = 0.1 cfm = ContinuousFlowMatcher(     time_distribution=uniform_time, prior_distribution=simple_prior, sigma=sigma, prediction_type=\"velocity\" ) # Place both the model and the interpolant on the same device DEVICE = \"cuda\" model = model.to(DEVICE) cfm = cfm.to_device(DEVICE) In\u00a0[6]: Copied! <pre>for k in range(20000):\n    optimizer.zero_grad()\n    shape = (batch_size, dim)\n    x0 = cfm.sample_prior(shape).to(DEVICE)\n    x1 = sample_moons(batch_size).to(DEVICE)\n\n    t = cfm.sample_time(batch_size)\n    xt = cfm.interpolate(x1, t, x0)\n    ut = cfm.calculate_target(x1, x0)\n\n    vt = model(torch.cat([xt, t[:, None]], dim=-1))\n    loss = cfm.loss(vt, ut, target_type=\"velocity\").mean()\n\n    loss.backward()\n    optimizer.step()\n\n    if (k + 1) % 5000 == 0:\n        print(f\"{k + 1}: loss {loss.item():0.3f}\")\n</pre> for k in range(20000):     optimizer.zero_grad()     shape = (batch_size, dim)     x0 = cfm.sample_prior(shape).to(DEVICE)     x1 = sample_moons(batch_size).to(DEVICE)      t = cfm.sample_time(batch_size)     xt = cfm.interpolate(x1, t, x0)     ut = cfm.calculate_target(x1, x0)      vt = model(torch.cat([xt, t[:, None]], dim=-1))     loss = cfm.loss(vt, ut, target_type=\"velocity\").mean()      loss.backward()     optimizer.step()      if (k + 1) % 5000 == 0:         print(f\"{k + 1}: loss {loss.item():0.3f}\") <pre>5000: loss 2.989\n10000: loss 2.999\n15000: loss 3.047\n20000: loss 3.151\n</pre> In\u00a0[7]: Copied! <pre>from bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule\n\n\ninference_sched = LinearInferenceSchedule(nsteps=100)\nschedule = inference_sched.generate_schedule().to(DEVICE)\ndts = inference_sched.discretize().to(DEVICE)\nschedule, dts\n</pre> from bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule   inference_sched = LinearInferenceSchedule(nsteps=100) schedule = inference_sched.generate_schedule().to(DEVICE) dts = inference_sched.discretize().to(DEVICE) schedule, dts Out[7]: <pre>(tensor([0.0000, 0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800,\n         0.0900, 0.1000, 0.1100, 0.1200, 0.1300, 0.1400, 0.1500, 0.1600, 0.1700,\n         0.1800, 0.1900, 0.2000, 0.2100, 0.2200, 0.2300, 0.2400, 0.2500, 0.2600,\n         0.2700, 0.2800, 0.2900, 0.3000, 0.3100, 0.3200, 0.3300, 0.3400, 0.3500,\n         0.3600, 0.3700, 0.3800, 0.3900, 0.4000, 0.4100, 0.4200, 0.4300, 0.4400,\n         0.4500, 0.4600, 0.4700, 0.4800, 0.4900, 0.5000, 0.5100, 0.5200, 0.5300,\n         0.5400, 0.5500, 0.5600, 0.5700, 0.5800, 0.5900, 0.6000, 0.6100, 0.6200,\n         0.6300, 0.6400, 0.6500, 0.6600, 0.6700, 0.6800, 0.6900, 0.7000, 0.7100,\n         0.7200, 0.7300, 0.7400, 0.7500, 0.7600, 0.7700, 0.7800, 0.7900, 0.8000,\n         0.8100, 0.8200, 0.8300, 0.8400, 0.8500, 0.8600, 0.8700, 0.8800, 0.8900,\n         0.9000, 0.9100, 0.9200, 0.9300, 0.9400, 0.9500, 0.9600, 0.9700, 0.9800,\n         0.9900], device='cuda:0'),\n tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n         0.0100], device='cuda:0'))</pre> In\u00a0[8]: Copied! <pre>inf_size = 1024\nsample = cfm.sample_prior((inf_size, 2)).to(DEVICE)  # Start with noise\ntrajectory = [sample]\nfor dt, t in zip(dts, schedule):\n    full_t = inference_sched.pad_time(inf_size, t, DEVICE)\n    vt = model(\n        torch.cat([sample, full_t[:, None]], dim=-1)\n    )  # calculate the vector field based on the definition of the model\n    sample = cfm.step(vt, sample, dt, full_t)\n    trajectory.append(sample)  # save the trajectory for plotting purposes\n</pre> inf_size = 1024 sample = cfm.sample_prior((inf_size, 2)).to(DEVICE)  # Start with noise trajectory = [sample] for dt, t in zip(dts, schedule):     full_t = inference_sched.pad_time(inf_size, t, DEVICE)     vt = model(         torch.cat([sample, full_t[:, None]], dim=-1)     )  # calculate the vector field based on the definition of the model     sample = cfm.step(vt, sample, dt, full_t)     trajectory.append(sample)  # save the trajectory for plotting purposes In\u00a0[9]: Copied! <pre>import matplotlib.pyplot as plt\n\n\ntraj = torch.stack(trajectory).cpu().detach().numpy()\nplot_limit = 1024\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(S)\")\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0] - 1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(0)\")\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\")\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> import matplotlib.pyplot as plt   traj = torch.stack(trajectory).cpu().detach().numpy() plot_limit = 1024 plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(S)\")  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0] - 1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(0)\")  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\") plt.legend() plt.xticks([]) plt.yticks([]) plt.show() In\u00a0[10]: Copied! <pre>inf_size = 1024\nsample = cfm.sample_prior((inf_size, 2)).to(DEVICE)\ntrajectory_stoch = [sample]\nvts = []\nfor dt, t in zip(dts, schedule):\n    current_time = inference_sched.pad_time(inf_size, t, DEVICE)  # torch.full((inf_size,), t).to(DEVICE)\n    vt = model(torch.cat([sample, current_time[:, None]], dim=-1))\n    sample = cfm.step_score_stochastic(vt, sample, dt, current_time, noise_temperature=1.0, gt_mode=\"tan\")\n    trajectory_stoch.append(sample)\n    vts.append(vt)\n</pre> inf_size = 1024 sample = cfm.sample_prior((inf_size, 2)).to(DEVICE) trajectory_stoch = [sample] vts = [] for dt, t in zip(dts, schedule):     current_time = inference_sched.pad_time(inf_size, t, DEVICE)  # torch.full((inf_size,), t).to(DEVICE)     vt = model(torch.cat([sample, current_time[:, None]], dim=-1))     sample = cfm.step_score_stochastic(vt, sample, dt, current_time, noise_temperature=1.0, gt_mode=\"tan\")     trajectory_stoch.append(sample)     vts.append(vt) In\u00a0[11]: Copied! <pre>traj = torch.stack(trajectory_stoch).cpu().detach().numpy()\nplot_limit = 1024\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(0)\")\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0] - 1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(1)\")\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\")\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.title(\"Stochastic score sampling Temperature = 1.0\")\nplt.show()\n</pre> traj = torch.stack(trajectory_stoch).cpu().detach().numpy() plot_limit = 1024 plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(0)\")  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0] - 1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\") # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(1)\")  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\") plt.legend() plt.xticks([]) plt.yticks([]) plt.title(\"Stochastic score sampling Temperature = 1.0\") plt.show() In\u00a0[12]: Copied! <pre>fmodel = torch.nn.Sequential(\n    torch.nn.Linear(dim + 1, hidden_size),\n    torch.nn.SELU(),\n    torch.nn.Linear(hidden_size, hidden_size),\n    torch.nn.SELU(),\n    torch.nn.Linear(hidden_size, hidden_size),\n    torch.nn.SELU(),\n    torch.nn.Linear(hidden_size, dim),\n).to(DEVICE)\ninf_size = 1024\nsample = cfm.sample_prior((inf_size, 2)).to(DEVICE)\ntrajectory2 = [sample]\nfor dt, t in zip(dts, schedule):\n    current_time = inference_sched.pad_time(inf_size, t, DEVICE)  # torch.full((inf_size,), t).to(DEVICE)\n    vt = fmodel(torch.cat([sample, current_time[:, None]], dim=-1))\n    sample = cfm.step(vt, sample, dt, current_time)\n    trajectory2.append(sample)\n</pre> fmodel = torch.nn.Sequential(     torch.nn.Linear(dim + 1, hidden_size),     torch.nn.SELU(),     torch.nn.Linear(hidden_size, hidden_size),     torch.nn.SELU(),     torch.nn.Linear(hidden_size, hidden_size),     torch.nn.SELU(),     torch.nn.Linear(hidden_size, dim), ).to(DEVICE) inf_size = 1024 sample = cfm.sample_prior((inf_size, 2)).to(DEVICE) trajectory2 = [sample] for dt, t in zip(dts, schedule):     current_time = inference_sched.pad_time(inf_size, t, DEVICE)  # torch.full((inf_size,), t).to(DEVICE)     vt = fmodel(torch.cat([sample, current_time[:, None]], dim=-1))     sample = cfm.step(vt, sample, dt, current_time)     trajectory2.append(sample) In\u00a0[13]: Copied! <pre>plot_limit = 1024\ntraj = torch.stack(trajectory2).cpu().detach().numpy()\n\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(0)\")\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0] - 1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(1)\")\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\")\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> plot_limit = 1024 traj = torch.stack(trajectory2).cpu().detach().numpy()  plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(0)\")  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0] - 1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(1)\")  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\") plt.legend() plt.xticks([]) plt.yticks([]) plt.show()"},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_cfm/#building-generative-models-for-continuous-data-via-continuous-interpolants","title":"Building Generative Models for Continuous Data via Continuous Interpolants\u00b6","text":""},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_cfm/#task-setup","title":"Task Setup\u00b6","text":"<p>To demonstrate how Conditional Flow Matching works we use sklearn to sample from and create custom 2D distriubtions.</p> <p>To start we define our \"dataloader\" so to speak. This is the '''sample_moons''' function.</p> <p>Next we define a custom PriorDistribution to enable the conversion of 8 equidistance gaussians to the moon distribution above.</p>"},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_cfm/#model-creation","title":"Model Creation\u00b6","text":"<p>Here we define a simple 4 layer MLP and define our optimizer</p>"},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_cfm/#continuous-flow-matching-interpolant","title":"Continuous Flow Matching Interpolant\u00b6","text":"<p>Here we import our desired interpolant objects.</p> <p>The continuous flow matcher and the desired time distribution.</p>"},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_cfm/#training-loop","title":"Training Loop\u00b6","text":""},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_cfm/#setting-up-generation","title":"Setting Up Generation\u00b6","text":"<p>Now we need to import the desired inference time schedule. This is what gives us the time values to iterate through to iteratively generate from our model.</p> <p>Here we show the output time schedule as well as the discretization between time points. We note that different inference time schedules may have different shapes resulting in non uniform dt</p>"},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_cfm/#sample-from-the-trained-model","title":"Sample from the trained model\u00b6","text":""},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_cfm/#sample-from-underlying-score-model","title":"Sample from underlying score model\u00b6","text":""},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_cfm/#low-temperature-sampling-is-a-heuristic-unclear-what-effects-it-has-on-the-final-distribution-intuitively-it-cuts-tails-and-focuses-more-on-the-mode-in-practice-who-knows-exactly-whats-the-final-effect","title":"low temperature sampling is a heuristic, unclear what effects it has on the final distribution. Intuitively, it cuts tails and focuses more on the mode, in practice who knows exactly what's the final effect.\u00b6","text":""},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_cfm/#gt_mode-is-a-hyperparameter-that-must-be-experimentally-chosen","title":"gt_mode is a hyperparameter that must be experimentally chosen\u00b6","text":""},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_cfm/#what-happens-if-you-just-sample-from-a-random-model","title":"What happens if you just sample from a random model?\u00b6","text":""},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_ddpm/","title":"Building Generative Models for Continuous Data via Continuous Interpolants","text":"<p>NOTE: it takes about 10 minutes to deploy this notebook as a Launchable. As of this writing, we are working on a free tier so a credit card may be required. You can reach out to your NVIDIA rep for credits.\"</p> In\u00a0[1]: Copied! <pre>import math\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom sklearn.datasets import make_moons\n</pre> import math  import matplotlib.pyplot as plt import torch from sklearn.datasets import make_moons In\u00a0[2]: Copied! <pre>def sample_moons(n, normalize=False):\n    x1, _ = make_moons(n_samples=n, noise=0.08)\n    x1 = torch.Tensor(x1)\n    x1 = x1 * 3 - 1\n    if normalize:\n        x1 = (x1 - x1.mean(0)) / x1.std(0) * 2\n    return x1\n</pre> def sample_moons(n, normalize=False):     x1, _ = make_moons(n_samples=n, noise=0.08)     x1 = torch.Tensor(x1)     x1 = x1 * 3 - 1     if normalize:         x1 = (x1 - x1.mean(0)) / x1.std(0) * 2     return x1 In\u00a0[3]: Copied! <pre>x1 = sample_moons(1000)\nplt.scatter(x1[:, 0], x1[:, 1])\n</pre> x1 = sample_moons(1000) plt.scatter(x1[:, 0], x1[:, 1]) Out[3]: <pre>&lt;matplotlib.collections.PathCollection at 0x7c5eb718cac0&gt;</pre> In\u00a0[4]: Copied! <pre>from typing import List\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Network(nn.Module):\n    def __init__(\n        self,\n        dim_in: int,\n        dim_out: int,\n        dim_hids: List[int],\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList(\n            [\n                TimeLinear(dim_in, dim_hids[0]),\n                *[TimeLinear(dim_hids[i - 1], dim_hids[i]) for i in range(1, len(dim_hids))],\n                TimeLinear(dim_hids[-1], dim_out),\n            ]\n        )\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor):\n        for i, layer in enumerate(self.layers):\n            x = layer(x, t)\n            if i &lt; len(self.layers) - 1:\n                x = F.relu(x)\n        return x\n\n\nclass TimeLinear(nn.Module):\n    def __init__(self, dim_in: int, dim_out: int):\n        super().__init__()\n        self.dim_in = dim_in\n        self.dim_out = dim_out\n\n        self.time_embedding = TimeEmbedding(dim_out)\n        self.fc = nn.Linear(dim_in, dim_out)\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor):\n        x = self.fc(x)\n        alpha = self.time_embedding(t).view(-1, self.dim_out)\n        return alpha * x\n\n\nclass TimeEmbedding(nn.Module):\n    # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\n    def __init__(self, hidden_size, frequency_embedding_size=256):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(frequency_embedding_size, hidden_size, bias=True),\n            nn.SiLU(),\n            nn.Linear(hidden_size, hidden_size, bias=True),\n        )\n        self.frequency_embedding_size = frequency_embedding_size\n\n    @staticmethod\n    def timestep_embedding(t, dim, max_period=10000):\n        \"\"\"\n        Create sinusoidal timestep embeddings.\n        :param t: a 1-D Tensor of N indices, one per batch element.\n                          These may be fractional.\n        :param dim: the dimension of the output.\n        :param max_period: controls the minimum frequency of the embeddings.\n        :return: an (N, D) Tensor of positional embeddings.\n        \"\"\"\n        # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\n        half = dim // 2\n        freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(\n            device=t.device\n        )\n        args = t[:, None].float() * freqs[None]\n        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n        if dim % 2:\n            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n        return embedding\n\n    def forward(self, t: torch.Tensor):\n        if t.ndim == 0:\n            t = t.unsqueeze(-1)\n        t_freq = self.timestep_embedding(t, self.frequency_embedding_size)\n        t_emb = self.mlp(t_freq)\n        return t_emb\n</pre> from typing import List  import torch.nn as nn import torch.nn.functional as F   class Network(nn.Module):     def __init__(         self,         dim_in: int,         dim_out: int,         dim_hids: List[int],     ):         super().__init__()         self.layers = nn.ModuleList(             [                 TimeLinear(dim_in, dim_hids[0]),                 *[TimeLinear(dim_hids[i - 1], dim_hids[i]) for i in range(1, len(dim_hids))],                 TimeLinear(dim_hids[-1], dim_out),             ]         )      def forward(self, x: torch.Tensor, t: torch.Tensor):         for i, layer in enumerate(self.layers):             x = layer(x, t)             if i &lt; len(self.layers) - 1:                 x = F.relu(x)         return x   class TimeLinear(nn.Module):     def __init__(self, dim_in: int, dim_out: int):         super().__init__()         self.dim_in = dim_in         self.dim_out = dim_out          self.time_embedding = TimeEmbedding(dim_out)         self.fc = nn.Linear(dim_in, dim_out)      def forward(self, x: torch.Tensor, t: torch.Tensor):         x = self.fc(x)         alpha = self.time_embedding(t).view(-1, self.dim_out)         return alpha * x   class TimeEmbedding(nn.Module):     # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py     def __init__(self, hidden_size, frequency_embedding_size=256):         super().__init__()         self.mlp = nn.Sequential(             nn.Linear(frequency_embedding_size, hidden_size, bias=True),             nn.SiLU(),             nn.Linear(hidden_size, hidden_size, bias=True),         )         self.frequency_embedding_size = frequency_embedding_size      @staticmethod     def timestep_embedding(t, dim, max_period=10000):         \"\"\"         Create sinusoidal timestep embeddings.         :param t: a 1-D Tensor of N indices, one per batch element.                           These may be fractional.         :param dim: the dimension of the output.         :param max_period: controls the minimum frequency of the embeddings.         :return: an (N, D) Tensor of positional embeddings.         \"\"\"         # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py         half = dim // 2         freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(             device=t.device         )         args = t[:, None].float() * freqs[None]         embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)         if dim % 2:             embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)         return embedding      def forward(self, t: torch.Tensor):         if t.ndim == 0:             t = t.unsqueeze(-1)         t_freq = self.timestep_embedding(t, self.frequency_embedding_size)         t_emb = self.mlp(t_freq)         return t_emb In\u00a0[5]: Copied! <pre>from bionemo.moco.distributions.prior import GaussianPrior\nfrom bionemo.moco.distributions.time import UniformTimeDistribution\nfrom bionemo.moco.interpolants import DDPM\nfrom bionemo.moco.schedules.inference_time_schedules import DiscreteLinearInferenceSchedule\nfrom bionemo.moco.schedules.noise.discrete_noise_schedules import (\n    DiscreteLinearNoiseSchedule,\n)\n\n\nDEVICE = \"cuda:0\"\nuniform_time = UniformTimeDistribution(discrete_time=True, nsteps=1000)\nsimple_prior = GaussianPrior()\nddpm = DDPM(\n    time_distribution=uniform_time,\n    prior_distribution=simple_prior,\n    prediction_type=\"noise\",\n    noise_schedule=DiscreteLinearNoiseSchedule(nsteps=1000),\n    device=DEVICE,\n)\n</pre> from bionemo.moco.distributions.prior import GaussianPrior from bionemo.moco.distributions.time import UniformTimeDistribution from bionemo.moco.interpolants import DDPM from bionemo.moco.schedules.inference_time_schedules import DiscreteLinearInferenceSchedule from bionemo.moco.schedules.noise.discrete_noise_schedules import (     DiscreteLinearNoiseSchedule, )   DEVICE = \"cuda:0\" uniform_time = UniformTimeDistribution(discrete_time=True, nsteps=1000) simple_prior = GaussianPrior() ddpm = DDPM(     time_distribution=uniform_time,     prior_distribution=simple_prior,     prediction_type=\"noise\",     noise_schedule=DiscreteLinearNoiseSchedule(nsteps=1000),     device=DEVICE, ) In\u00a0[6]: Copied! <pre># Place both the model and the interpolant on the same device\ndim = 2\nhidden_size = 128\nnum_hiddens = 3\nbatch_size = 256\nmodel = Network(dim_in=dim, dim_out=dim, dim_hids=[hidden_size] * num_hiddens)\noptimizer = torch.optim.Adam(model.parameters(), lr=1.0e-3)\nDEVICE = \"cuda\"\nmodel = model.to(DEVICE)\nddpm = ddpm.to_device(DEVICE)\nfor k in range(20000):\n    optimizer.zero_grad()\n    shape = (batch_size, dim)\n    x0 = ddpm.sample_prior(shape).to(DEVICE)\n    x1 = sample_moons(batch_size).to(DEVICE)\n\n    t = ddpm.sample_time(batch_size)\n    xt = ddpm.interpolate(x1, t, x0)\n\n    eps = model(xt, t)\n    loss = ddpm.loss(eps, x0, t).mean()\n\n    loss.backward()\n    optimizer.step()\n\n    if (k + 1) % 1000 == 0:\n        print(f\"{k + 1}: loss {loss.item():0.3f}\")\n</pre> # Place both the model and the interpolant on the same device dim = 2 hidden_size = 128 num_hiddens = 3 batch_size = 256 model = Network(dim_in=dim, dim_out=dim, dim_hids=[hidden_size] * num_hiddens) optimizer = torch.optim.Adam(model.parameters(), lr=1.0e-3) DEVICE = \"cuda\" model = model.to(DEVICE) ddpm = ddpm.to_device(DEVICE) for k in range(20000):     optimizer.zero_grad()     shape = (batch_size, dim)     x0 = ddpm.sample_prior(shape).to(DEVICE)     x1 = sample_moons(batch_size).to(DEVICE)      t = ddpm.sample_time(batch_size)     xt = ddpm.interpolate(x1, t, x0)      eps = model(xt, t)     loss = ddpm.loss(eps, x0, t).mean()      loss.backward()     optimizer.step()      if (k + 1) % 1000 == 0:         print(f\"{k + 1}: loss {loss.item():0.3f}\") <pre>1000: loss 0.343\n2000: loss 0.327\n3000: loss 0.299\n4000: loss 0.374\n5000: loss 0.309\n6000: loss 0.287\n7000: loss 0.276\n8000: loss 0.241\n9000: loss 0.343\n10000: loss 0.335\n11000: loss 0.292\n12000: loss 0.307\n13000: loss 0.332\n14000: loss 0.309\n15000: loss 0.322\n16000: loss 0.357\n17000: loss 0.285\n18000: loss 0.380\n19000: loss 0.357\n20000: loss 0.443\n</pre> In\u00a0[7]: Copied! <pre>x0 = ddpm.sample_prior(shape).to(DEVICE)\nx1 = sample_moons(batch_size).to(DEVICE)\nfor t in range(0, 900, 100):\n    tt = ddpm.sample_time(batch_size) * 0 + t\n    out = ddpm.interpolate(x1, tt, x0)\n    plt.scatter(out[:, 0].cpu().detach(), out[:, 1].cpu().detach())\n    plt.title(f\"Time = {t}\")\n    plt.show()\n</pre> x0 = ddpm.sample_prior(shape).to(DEVICE) x1 = sample_moons(batch_size).to(DEVICE) for t in range(0, 900, 100):     tt = ddpm.sample_time(batch_size) * 0 + t     out = ddpm.interpolate(x1, tt, x0)     plt.scatter(out[:, 0].cpu().detach(), out[:, 1].cpu().detach())     plt.title(f\"Time = {t}\")     plt.show() In\u00a0[8]: Copied! <pre>with torch.no_grad():\n    inf_size = 1024\n    schedule = DiscreteLinearInferenceSchedule(nsteps=1000, direction=\"diffusion\").generate_schedule(device=DEVICE)\n    sample = ddpm.sample_prior((inf_size, 2)).to(DEVICE)  # Start with noise\n    trajectory = [sample.cpu()]\n    for t in schedule:\n        full_t = torch.full((inf_size,), t).to(DEVICE)\n        eps_hat = model(sample, full_t)\n        sample = ddpm.step_noise(eps_hat, full_t, sample)\n        trajectory.append(sample.cpu())  # save the trajectory for plotting purposes\n</pre> with torch.no_grad():     inf_size = 1024     schedule = DiscreteLinearInferenceSchedule(nsteps=1000, direction=\"diffusion\").generate_schedule(device=DEVICE)     sample = ddpm.sample_prior((inf_size, 2)).to(DEVICE)  # Start with noise     trajectory = [sample.cpu()]     for t in schedule:         full_t = torch.full((inf_size,), t).to(DEVICE)         eps_hat = model(sample, full_t)         sample = ddpm.step_noise(eps_hat, full_t, sample)         trajectory.append(sample.cpu())  # save the trajectory for plotting purposes In\u00a0[9]: Copied! <pre>import matplotlib.pyplot as plt\n\n\ntraj = torch.stack(trajectory).numpy()\nplot_limit = 1024\n\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(S)\")\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0] - 1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(0)\")\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\")\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> import matplotlib.pyplot as plt   traj = torch.stack(trajectory).numpy() plot_limit = 1024  plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(S)\")  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0] - 1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(0)\")  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\") plt.legend() plt.xticks([]) plt.yticks([]) plt.show() In\u00a0[10]: Copied! <pre>with torch.no_grad():\n    inf_size = 1024\n    schedule = DiscreteLinearInferenceSchedule(nsteps=1000, direction=\"diffusion\").generate_schedule(device=DEVICE)\n    sample = ddpm.sample_prior((inf_size, 2)).to(DEVICE)  # Start with noise\n    trajectory = [sample.cpu()]\n    for t in schedule:\n        full_t = torch.full((inf_size,), t).to(DEVICE)\n        eps_hat = model(sample, full_t)  # calculate the vector field based on the definition of the model\n        sample = ddpm.step_ddim(eps_hat, full_t, sample)\n        trajectory.append(sample.cpu())  # save the trajectory for plotting purposes\n</pre> with torch.no_grad():     inf_size = 1024     schedule = DiscreteLinearInferenceSchedule(nsteps=1000, direction=\"diffusion\").generate_schedule(device=DEVICE)     sample = ddpm.sample_prior((inf_size, 2)).to(DEVICE)  # Start with noise     trajectory = [sample.cpu()]     for t in schedule:         full_t = torch.full((inf_size,), t).to(DEVICE)         eps_hat = model(sample, full_t)  # calculate the vector field based on the definition of the model         sample = ddpm.step_ddim(eps_hat, full_t, sample)         trajectory.append(sample.cpu())  # save the trajectory for plotting purposes In\u00a0[11]: Copied! <pre>traj = torch.stack(trajectory).cpu().detach().numpy()\nplot_limit = 1024\n\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(S)\")\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0] - 1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(0)\")\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\")\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> traj = torch.stack(trajectory).cpu().detach().numpy() plot_limit = 1024  plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(S)\")  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0] - 1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(0)\")  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\") plt.legend() plt.xticks([]) plt.yticks([]) plt.show() In\u00a0[12]: Copied! <pre>with torch.no_grad():\n    model = Network(dim_in=dim, dim_out=dim, dim_hids=[hidden_size] * num_hiddens).to(DEVICE)\n    inf_size = 1024\n    sample = ddpm.sample_prior((inf_size, 2)).to(DEVICE)\n    trajectory2 = [sample.cpu()]\n    for t in schedule:\n        full_t = torch.full((inf_size,), t).to(DEVICE)\n        vt = model(sample, full_t)  # calculate the vector field based on the definition of the model\n        sample = ddpm.step_noise(vt, full_t, sample)\n        trajectory2.append(sample.cpu())  #\nplot_limit = 1024\ntraj = torch.stack(trajectory2).cpu().detach().numpy()\n\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(0)\")\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0] - 1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(1)\")\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\")\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> with torch.no_grad():     model = Network(dim_in=dim, dim_out=dim, dim_hids=[hidden_size] * num_hiddens).to(DEVICE)     inf_size = 1024     sample = ddpm.sample_prior((inf_size, 2)).to(DEVICE)     trajectory2 = [sample.cpu()]     for t in schedule:         full_t = torch.full((inf_size,), t).to(DEVICE)         vt = model(sample, full_t)  # calculate the vector field based on the definition of the model         sample = ddpm.step_noise(vt, full_t, sample)         trajectory2.append(sample.cpu())  # plot_limit = 1024 traj = torch.stack(trajectory2).cpu().detach().numpy()  plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(0)\")  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0] - 1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(1)\")  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\") plt.legend() plt.xticks([]) plt.yticks([]) plt.show() In\u00a0[13]: Copied! <pre>from bionemo.moco.distributions.prior.continuous.gaussian import GaussianPrior\nfrom bionemo.moco.distributions.time.uniform import UniformTimeDistribution\nfrom bionemo.moco.interpolants.discrete_time.continuous.ddpm import DDPM\nfrom bionemo.moco.schedules.inference_time_schedules import DiscreteLinearInferenceSchedule\nfrom bionemo.moco.schedules.noise.discrete_noise_schedules import DiscreteLinearNoiseSchedule\n\n\nDEVICE = \"cuda:0\"\nuniform_time = UniformTimeDistribution(discrete_time=True, nsteps=1000)\nsimple_prior = GaussianPrior()\nddpm = DDPM(\n    time_distribution=uniform_time,\n    prior_distribution=simple_prior,\n    prediction_type=\"data\",\n    noise_schedule=DiscreteLinearNoiseSchedule(nsteps=1000),\n    device=DEVICE,\n)\n</pre> from bionemo.moco.distributions.prior.continuous.gaussian import GaussianPrior from bionemo.moco.distributions.time.uniform import UniformTimeDistribution from bionemo.moco.interpolants.discrete_time.continuous.ddpm import DDPM from bionemo.moco.schedules.inference_time_schedules import DiscreteLinearInferenceSchedule from bionemo.moco.schedules.noise.discrete_noise_schedules import DiscreteLinearNoiseSchedule   DEVICE = \"cuda:0\" uniform_time = UniformTimeDistribution(discrete_time=True, nsteps=1000) simple_prior = GaussianPrior() ddpm = DDPM(     time_distribution=uniform_time,     prior_distribution=simple_prior,     prediction_type=\"data\",     noise_schedule=DiscreteLinearNoiseSchedule(nsteps=1000),     device=DEVICE, ) In\u00a0[14]: Copied! <pre># Place both the model and the interpolant on the same device\ndim = 2\nhidden_size = 128\nnum_hiddens = 3\nbatch_size = 256\nmodel = Network(dim_in=dim, dim_out=dim, dim_hids=[hidden_size] * num_hiddens)\noptimizer = torch.optim.Adam(model.parameters(), lr=1.0e-3)\nDEVICE = \"cuda\"\nmodel = model.to(DEVICE)\nddpm = ddpm.to_device(DEVICE)\nfor k in range(20000):\n    optimizer.zero_grad()\n    shape = (batch_size, dim)\n    x0 = ddpm.sample_prior(shape).to(DEVICE)\n    x1 = sample_moons(batch_size).to(DEVICE)\n\n    t = ddpm.sample_time(batch_size)\n    xt = ddpm.interpolate(x1, t, x0)\n\n    x_hat = model(xt, t)\n    loss = ddpm.loss(x_hat, x1, t, weight_type=\"data_to_noise\").mean()\n\n    loss.backward()\n    optimizer.step()\n\n    if (k + 1) % 1000 == 0:\n        print(f\"{k + 1}: loss {loss.item():0.3f}\")\n</pre> # Place both the model and the interpolant on the same device dim = 2 hidden_size = 128 num_hiddens = 3 batch_size = 256 model = Network(dim_in=dim, dim_out=dim, dim_hids=[hidden_size] * num_hiddens) optimizer = torch.optim.Adam(model.parameters(), lr=1.0e-3) DEVICE = \"cuda\" model = model.to(DEVICE) ddpm = ddpm.to_device(DEVICE) for k in range(20000):     optimizer.zero_grad()     shape = (batch_size, dim)     x0 = ddpm.sample_prior(shape).to(DEVICE)     x1 = sample_moons(batch_size).to(DEVICE)      t = ddpm.sample_time(batch_size)     xt = ddpm.interpolate(x1, t, x0)      x_hat = model(xt, t)     loss = ddpm.loss(x_hat, x1, t, weight_type=\"data_to_noise\").mean()      loss.backward()     optimizer.step()      if (k + 1) % 1000 == 0:         print(f\"{k + 1}: loss {loss.item():0.3f}\") <pre>1000: loss 12.306\n2000: loss 2.511\n3000: loss 0.410\n4000: loss 0.751\n5000: loss 3.768\n6000: loss 0.476\n7000: loss 1.140\n8000: loss 0.258\n9000: loss 0.794\n10000: loss 0.454\n11000: loss 0.333\n12000: loss 5.036\n13000: loss 0.297\n14000: loss 0.572\n15000: loss 0.336\n16000: loss 0.578\n17000: loss 0.507\n18000: loss 1.017\n19000: loss 0.500\n20000: loss 0.308\n</pre> In\u00a0[15]: Copied! <pre>with torch.no_grad():\n    inf_size = 1024\n    sample = ddpm.sample_prior((inf_size, 2)).to(DEVICE)  # Start with noise\n    trajectory = [sample.cpu()]\n    for t in schedule:\n        full_t = torch.full((inf_size,), t).to(DEVICE)\n        x_hat = model(sample, full_t)  # calculate the vector field based on the definition of the model\n        sample = ddpm.step(x_hat, full_t, sample)\n        trajectory.append(sample.cpu())  # save the trajectory for plotting purposes\n</pre> with torch.no_grad():     inf_size = 1024     sample = ddpm.sample_prior((inf_size, 2)).to(DEVICE)  # Start with noise     trajectory = [sample.cpu()]     for t in schedule:         full_t = torch.full((inf_size,), t).to(DEVICE)         x_hat = model(sample, full_t)  # calculate the vector field based on the definition of the model         sample = ddpm.step(x_hat, full_t, sample)         trajectory.append(sample.cpu())  # save the trajectory for plotting purposes In\u00a0[16]: Copied! <pre>import matplotlib.pyplot as plt\n\n\ntraj = torch.stack(trajectory).cpu().detach().numpy()\nplot_limit = 1024\n\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(S)\")\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0] - 1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(0)\")\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\")\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> import matplotlib.pyplot as plt   traj = torch.stack(trajectory).cpu().detach().numpy() plot_limit = 1024  plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(S)\")  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0] - 1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(0)\")  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\") plt.legend() plt.xticks([]) plt.yticks([]) plt.show() In\u00a0[17]: Copied! <pre># Place both the model and the interpolant on the same device\ndim = 2\nhidden_size = 128\nnum_hiddens = 3\nbatch_size = 256\nmodel = Network(dim_in=dim, dim_out=dim, dim_hids=[hidden_size] * num_hiddens)\noptimizer = torch.optim.Adam(model.parameters(), lr=1.0e-3)\nDEVICE = \"cuda\"\nmodel = model.to(DEVICE)\nddpm = ddpm.to_device(DEVICE)\nfor k in range(20000):\n    optimizer.zero_grad()\n    shape = (batch_size, dim)\n    x0 = ddpm.sample_prior(shape).to(DEVICE)\n    x1 = sample_moons(batch_size).to(DEVICE)\n\n    t = ddpm.sample_time(batch_size)\n    xt = ddpm.interpolate(x1, t, x0)\n\n    x_hat = model(xt, t)\n    loss = ddpm.loss(x_hat, x1, t, weight_type=\"ones\").mean()\n\n    loss.backward()\n    optimizer.step()\n\n    if (k + 1) % 1000 == 0:\n        print(f\"{k + 1}: loss {loss.item():0.3f}\")\n</pre> # Place both the model and the interpolant on the same device dim = 2 hidden_size = 128 num_hiddens = 3 batch_size = 256 model = Network(dim_in=dim, dim_out=dim, dim_hids=[hidden_size] * num_hiddens) optimizer = torch.optim.Adam(model.parameters(), lr=1.0e-3) DEVICE = \"cuda\" model = model.to(DEVICE) ddpm = ddpm.to_device(DEVICE) for k in range(20000):     optimizer.zero_grad()     shape = (batch_size, dim)     x0 = ddpm.sample_prior(shape).to(DEVICE)     x1 = sample_moons(batch_size).to(DEVICE)      t = ddpm.sample_time(batch_size)     xt = ddpm.interpolate(x1, t, x0)      x_hat = model(xt, t)     loss = ddpm.loss(x_hat, x1, t, weight_type=\"ones\").mean()      loss.backward()     optimizer.step()      if (k + 1) % 1000 == 0:         print(f\"{k + 1}: loss {loss.item():0.3f}\") <pre>1000: loss 2.618\n2000: loss 2.448\n3000: loss 2.689\n4000: loss 2.352\n5000: loss 2.393\n6000: loss 2.802\n7000: loss 2.946\n8000: loss 2.774\n9000: loss 2.567\n10000: loss 2.632\n11000: loss 2.527\n12000: loss 2.403\n13000: loss 2.407\n14000: loss 2.594\n15000: loss 2.711\n16000: loss 2.562\n17000: loss 2.606\n18000: loss 2.592\n19000: loss 2.444\n20000: loss 2.642\n</pre> In\u00a0[18]: Copied! <pre>with torch.no_grad():\n    inf_size = 1024\n    sample = ddpm.sample_prior((inf_size, 2)).to(DEVICE)  # Start with noise\n    trajectory = [sample.cpu()]\n    for t in schedule:\n        full_t = torch.full((inf_size,), t).to(DEVICE)\n        x_hat = model(sample, full_t)  # calculate the vector field based on the definition of the model\n        sample = ddpm.step(x_hat, full_t, sample)\n        trajectory.append(sample.cpu())  # save the trajectory for plotting purposes\n</pre> with torch.no_grad():     inf_size = 1024     sample = ddpm.sample_prior((inf_size, 2)).to(DEVICE)  # Start with noise     trajectory = [sample.cpu()]     for t in schedule:         full_t = torch.full((inf_size,), t).to(DEVICE)         x_hat = model(sample, full_t)  # calculate the vector field based on the definition of the model         sample = ddpm.step(x_hat, full_t, sample)         trajectory.append(sample.cpu())  # save the trajectory for plotting purposes In\u00a0[19]: Copied! <pre>import matplotlib.pyplot as plt\n\n\ntraj = torch.stack(trajectory).cpu().detach().numpy()\nplot_limit = 1024\n\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(S)\")\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0] - 1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(0)\")\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\")\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> import matplotlib.pyplot as plt   traj = torch.stack(trajectory).cpu().detach().numpy() plot_limit = 1024  plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(S)\")  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0] - 1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(0)\")  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\") plt.legend() plt.xticks([]) plt.yticks([]) plt.show()"},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_ddpm/#building-generative-models-for-continuous-data-via-continuous-interpolants","title":"Building Generative Models for Continuous Data via Continuous Interpolants\u00b6","text":""},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_ddpm/#task-setup","title":"Task Setup\u00b6","text":"<p>To demonstrate how Conditional Flow Matching works we use sklearn to sample from and create custom 2D distriubtions.</p> <p>To start we define our \"dataloader\" so to speak. This is the '''sample_moons''' function.</p> <p>Next we define a custom PriorDistribution to enable the conversion of 8 equidistance gaussians to the moon distribution above.</p>"},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_ddpm/#model-creation","title":"Model Creation\u00b6","text":"<p>Here we define a simple 4 layer MLP and define our optimizer</p>"},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_ddpm/#ddpm-interpolant","title":"DDPM Interpolant\u00b6","text":""},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_ddpm/#note-ddpm-must-be-used-with-a-gaussian-prior","title":"note DDPM must be used with a Gaussian Prior.\u00b6","text":""},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_ddpm/#train-the-model","title":"Train the Model\u00b6","text":""},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_ddpm/#lets-vizualize-what-the-interpolation-looks-like-during-training-for-different-times","title":"Let's vizualize what the interpolation looks like during training for different times\u00b6","text":""},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_ddpm/#create-the-inference-time-schedule-and-sample-from-the-model","title":"Create the inference time schedule and sample from the model\u00b6","text":""},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_ddpm/#notice-that-his-yields-very-similar-results-to-using-the-underlying-score-function-in-the-stochastic-score-based-cfm-example","title":"Notice that his yields very similar results to using the underlying score function in the stochastic score based CFM example\u00b6","text":""},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_ddpm/#lets-try-other-sampling-functions","title":"Let's try other sampling functions\u00b6","text":""},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_ddpm/#what-happens-when-you-sample-from-an-untrained-model-with-ddpm","title":"What happens when you sample from an untrained model with DDPM\u00b6","text":""},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_ddpm/#now-lets-switch-the-parameterization-of-ddpm-from-noise-to-data","title":"Now let's switch the parameterization of DDPM from noise to data\u00b6","text":"<p>Here instead of training the model to learn the noise we want to learn the raw data. Both options are valid and the choice of which depends on the underlying modeling task.</p>"},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_ddpm/#let-us-first-train-the-model-with-a-weight-such-that-it-is-theoretically-equivalent-to-the-simple-noise-matching-loss-see-equation-9-from-httpsarxivorgpdf220200512","title":"Let us first train the model with a weight such that it is theoretically equivalent to the simple noise matching loss. See Equation 9 from https://arxiv.org/pdf/2202.00512\u00b6","text":""},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_ddpm/#now-let-us-train-with-no-loss-weighting-to-optimize-a-true-data-matching-loss-for-comparison","title":"Now let us train with no loss weighting to optimize a true data matching loss for comparison\u00b6","text":""},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_ddpm/#the-choice-in-data-vs-noise-and-variance-schedule-are-hyperparameters-that-must-be-tuned-to-each-task","title":"The choice in data vs noise and variance schedule are hyperparameters that must be tuned to each task\u00b6","text":""},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_ddpm/#many-of-these-choices-are-empirical-and-part-of-the-tuning-process-to-best-model-your-data-via-noise-data-or-even-velocity-prediction","title":"many of these choices are empirical and part of the tuning process to best model your data via noise, data, or even velocity prediction.\u00b6","text":""},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_vdm/","title":"Building Generative Models for Continuous Data via Continuous Interpolants","text":"<p>NOTE: it takes about 10 minutes to deploy this notebook as a Launchable. As of this writing, we are working on a free tier so a credit card may be required. You can reach out to your NVIDIA rep for credits.\"</p> In\u00a0[16]: Copied! <pre>import math\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom sklearn.datasets import make_moons\n</pre> import math  import matplotlib.pyplot as plt import torch from sklearn.datasets import make_moons In\u00a0[17]: Copied! <pre>def sample_moons(n, normalize=False):\n    x1, _ = make_moons(n_samples=n, noise=0.08)\n    x1 = torch.Tensor(x1)\n    x1 = x1 * 3 - 1\n    if normalize:\n        x1 = (x1 - x1.mean(0)) / x1.std(0) * 2\n    return x1\n</pre> def sample_moons(n, normalize=False):     x1, _ = make_moons(n_samples=n, noise=0.08)     x1 = torch.Tensor(x1)     x1 = x1 * 3 - 1     if normalize:         x1 = (x1 - x1.mean(0)) / x1.std(0) * 2     return x1 In\u00a0[18]: Copied! <pre>x1 = sample_moons(1000)\nplt.scatter(x1[:, 0], x1[:, 1])\n</pre> x1 = sample_moons(1000) plt.scatter(x1[:, 0], x1[:, 1]) Out[18]: <pre>&lt;matplotlib.collections.PathCollection at 0x72314e5c12a0&gt;</pre> In\u00a0[19]: Copied! <pre>from typing import List\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Network(nn.Module):\n    def __init__(\n        self,\n        dim_in: int,\n        dim_out: int,\n        dim_hids: List[int],\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList(\n            [\n                TimeLinear(dim_in, dim_hids[0]),\n                *[TimeLinear(dim_hids[i - 1], dim_hids[i]) for i in range(1, len(dim_hids))],\n                TimeLinear(dim_hids[-1], dim_out),\n            ]\n        )\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor):\n        for i, layer in enumerate(self.layers):\n            x = layer(x, t)\n            if i &lt; len(self.layers) - 1:\n                x = F.relu(x)\n        return x\n\n\nclass TimeLinear(nn.Module):\n    def __init__(self, dim_in: int, dim_out: int):\n        super().__init__()\n        self.dim_in = dim_in\n        self.dim_out = dim_out\n\n        self.time_embedding = TimeEmbedding(dim_out)\n        self.fc = nn.Linear(dim_in, dim_out)\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor):\n        x = self.fc(x)\n        alpha = self.time_embedding(t).view(-1, self.dim_out)\n        return alpha * x\n\n\nclass TimeEmbedding(nn.Module):\n    # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\n    def __init__(self, hidden_size, frequency_embedding_size=256):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(frequency_embedding_size, hidden_size, bias=True),\n            nn.SiLU(),\n            nn.Linear(hidden_size, hidden_size, bias=True),\n        )\n        self.frequency_embedding_size = frequency_embedding_size\n\n    @staticmethod\n    def timestep_embedding(t, dim, max_period=10000):\n        \"\"\"\n        Create sinusoidal timestep embeddings.\n        :param t: a 1-D Tensor of N indices, one per batch element.\n                          These may be fractional.\n        :param dim: the dimension of the output.\n        :param max_period: controls the minimum frequency of the embeddings.\n        :return: an (N, D) Tensor of positional embeddings.\n        \"\"\"\n        # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\n        half = dim // 2\n        freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(\n            device=t.device\n        )\n        args = t[:, None].float() * freqs[None]\n        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n        if dim % 2:\n            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n        return embedding\n\n    def forward(self, t: torch.Tensor):\n        if t.ndim == 0:\n            t = t.unsqueeze(-1)\n        t_freq = self.timestep_embedding(t, self.frequency_embedding_size)\n        t_emb = self.mlp(t_freq)\n        return t_emb\n</pre> from typing import List  import torch.nn as nn import torch.nn.functional as F   class Network(nn.Module):     def __init__(         self,         dim_in: int,         dim_out: int,         dim_hids: List[int],     ):         super().__init__()         self.layers = nn.ModuleList(             [                 TimeLinear(dim_in, dim_hids[0]),                 *[TimeLinear(dim_hids[i - 1], dim_hids[i]) for i in range(1, len(dim_hids))],                 TimeLinear(dim_hids[-1], dim_out),             ]         )      def forward(self, x: torch.Tensor, t: torch.Tensor):         for i, layer in enumerate(self.layers):             x = layer(x, t)             if i &lt; len(self.layers) - 1:                 x = F.relu(x)         return x   class TimeLinear(nn.Module):     def __init__(self, dim_in: int, dim_out: int):         super().__init__()         self.dim_in = dim_in         self.dim_out = dim_out          self.time_embedding = TimeEmbedding(dim_out)         self.fc = nn.Linear(dim_in, dim_out)      def forward(self, x: torch.Tensor, t: torch.Tensor):         x = self.fc(x)         alpha = self.time_embedding(t).view(-1, self.dim_out)         return alpha * x   class TimeEmbedding(nn.Module):     # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py     def __init__(self, hidden_size, frequency_embedding_size=256):         super().__init__()         self.mlp = nn.Sequential(             nn.Linear(frequency_embedding_size, hidden_size, bias=True),             nn.SiLU(),             nn.Linear(hidden_size, hidden_size, bias=True),         )         self.frequency_embedding_size = frequency_embedding_size      @staticmethod     def timestep_embedding(t, dim, max_period=10000):         \"\"\"         Create sinusoidal timestep embeddings.         :param t: a 1-D Tensor of N indices, one per batch element.                           These may be fractional.         :param dim: the dimension of the output.         :param max_period: controls the minimum frequency of the embeddings.         :return: an (N, D) Tensor of positional embeddings.         \"\"\"         # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py         half = dim // 2         freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(             device=t.device         )         args = t[:, None].float() * freqs[None]         embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)         if dim % 2:             embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)         return embedding      def forward(self, t: torch.Tensor):         if t.ndim == 0:             t = t.unsqueeze(-1)         t_freq = self.timestep_embedding(t, self.frequency_embedding_size)         t_emb = self.mlp(t_freq)         return t_emb In\u00a0[20]: Copied! <pre>from bionemo.moco.distributions.prior.continuous.gaussian import GaussianPrior\nfrom bionemo.moco.distributions.time import UniformTimeDistribution\nfrom bionemo.moco.interpolants import VDM\nfrom bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule\nfrom bionemo.moco.schedules.noise.continuous_snr_transforms import (\n    LinearLogInterpolatedSNRTransform,\n)\n\n\nDEVICE = \"cuda:0\"\nuniform_time = UniformTimeDistribution(discrete_time=False)\nsimple_prior = GaussianPrior()\nvdm = VDM(\n    time_distribution=uniform_time,\n    prior_distribution=simple_prior,\n    prediction_type=\"data\",\n    noise_schedule=LinearLogInterpolatedSNRTransform(),\n    device=DEVICE,\n)\nschedule = LinearInferenceSchedule(nsteps=1000, direction=\"diffusion\")\n</pre> from bionemo.moco.distributions.prior.continuous.gaussian import GaussianPrior from bionemo.moco.distributions.time import UniformTimeDistribution from bionemo.moco.interpolants import VDM from bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule from bionemo.moco.schedules.noise.continuous_snr_transforms import (     LinearLogInterpolatedSNRTransform, )   DEVICE = \"cuda:0\" uniform_time = UniformTimeDistribution(discrete_time=False) simple_prior = GaussianPrior() vdm = VDM(     time_distribution=uniform_time,     prior_distribution=simple_prior,     prediction_type=\"data\",     noise_schedule=LinearLogInterpolatedSNRTransform(),     device=DEVICE, ) schedule = LinearInferenceSchedule(nsteps=1000, direction=\"diffusion\") In\u00a0[21]: Copied! <pre># Place both the model and the interpolant on the same device\ndim = 2\nhidden_size = 128\nnum_hiddens = 3\nbatch_size = 256\nmodel = Network(dim_in=dim, dim_out=dim, dim_hids=[hidden_size] * num_hiddens)\nDEVICE = \"cuda\"\nmodel = model.to(DEVICE)\n</pre> # Place both the model and the interpolant on the same device dim = 2 hidden_size = 128 num_hiddens = 3 batch_size = 256 model = Network(dim_in=dim, dim_out=dim, dim_hids=[hidden_size] * num_hiddens) DEVICE = \"cuda\" model = model.to(DEVICE) In\u00a0[22]: Copied! <pre>optimizer = torch.optim.Adam(model.parameters(), lr=1.0e-3)\nfor k in range(20000):\n    optimizer.zero_grad()\n    shape = (batch_size, dim)\n    x0 = vdm.sample_prior(shape).to(DEVICE)\n    x1 = sample_moons(batch_size).to(DEVICE)\n\n    t = vdm.sample_time(batch_size)\n    xt = vdm.interpolate(x1, t, x0)\n\n    x_hat = model(xt, t)\n    loss = vdm.loss(x_hat, x1, t, weight_type=\"ones\").mean()\n\n    loss.backward()\n    optimizer.step()\n\n    if (k + 1) % 1000 == 0:\n        print(f\"{k + 1}: loss {loss.item():0.3f}\")\n</pre> optimizer = torch.optim.Adam(model.parameters(), lr=1.0e-3) for k in range(20000):     optimizer.zero_grad()     shape = (batch_size, dim)     x0 = vdm.sample_prior(shape).to(DEVICE)     x1 = sample_moons(batch_size).to(DEVICE)      t = vdm.sample_time(batch_size)     xt = vdm.interpolate(x1, t, x0)      x_hat = model(xt, t)     loss = vdm.loss(x_hat, x1, t, weight_type=\"ones\").mean()      loss.backward()     optimizer.step()      if (k + 1) % 1000 == 0:         print(f\"{k + 1}: loss {loss.item():0.3f}\") <pre>1000: loss 1.322\n2000: loss 1.147\n3000: loss 0.870\n4000: loss 1.033\n5000: loss 1.366\n6000: loss 1.261\n7000: loss 1.224\n8000: loss 1.150\n9000: loss 1.173\n10000: loss 1.269\n11000: loss 0.996\n12000: loss 1.193\n13000: loss 1.083\n14000: loss 1.047\n15000: loss 1.215\n16000: loss 1.110\n17000: loss 1.127\n18000: loss 1.243\n19000: loss 0.967\n20000: loss 1.264\n</pre> In\u00a0[23]: Copied! <pre>with torch.no_grad():\n    inf_size = 1024\n    sample = vdm.sample_prior((inf_size, 2)).to(DEVICE)  # Start with noise\n    trajectory = [sample.cpu()]\n    ts = schedule.generate_schedule()\n    dts = schedule.discretize()\n    for dt, t in zip(dts, ts):\n        full_t = torch.full((inf_size,), t).to(DEVICE)\n        x_hat = model(sample, full_t)  # calculate the vector field based on the definition of the model\n        sample = vdm.step(x_hat, full_t, sample, dt)\n        trajectory.append(sample.cpu())  # save the trajectory for plotting purposes\n</pre> with torch.no_grad():     inf_size = 1024     sample = vdm.sample_prior((inf_size, 2)).to(DEVICE)  # Start with noise     trajectory = [sample.cpu()]     ts = schedule.generate_schedule()     dts = schedule.discretize()     for dt, t in zip(dts, ts):         full_t = torch.full((inf_size,), t).to(DEVICE)         x_hat = model(sample, full_t)  # calculate the vector field based on the definition of the model         sample = vdm.step(x_hat, full_t, sample, dt)         trajectory.append(sample.cpu())  # save the trajectory for plotting purposes In\u00a0[24]: Copied! <pre>import matplotlib.pyplot as plt\n\n\ntraj = torch.stack(trajectory).cpu().detach().numpy()\nplot_limit = 1024\n\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(S)\")\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0] - 1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(0)\")\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\")\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> import matplotlib.pyplot as plt   traj = torch.stack(trajectory).cpu().detach().numpy() plot_limit = 1024  plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(S)\")  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0] - 1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(0)\")  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\") plt.legend() plt.xticks([]) plt.yticks([]) plt.show() <pre>/home/dreidenbach/mambaforge/envs/moco_bionemo/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n  fig.canvas.print_figure(bytes_io, **kw)\n</pre> In\u00a0[25]: Copied! <pre>with torch.no_grad():\n    inf_size = 1024\n    sample = vdm.sample_prior((inf_size, 2)).to(DEVICE)  # Start with noise\n    trajectory = [sample.cpu()]\n    ts = schedule.generate_schedule()\n    dts = schedule.discretize()\n    for dt, t in zip(dts, ts):\n        full_t = torch.full((inf_size,), t).to(DEVICE)\n        x_hat = model(sample, full_t)  # calculate the vector field based on the definition of the model\n        sample = vdm.step_ddim(x_hat, full_t, sample, dt)\n        trajectory.append(sample.cpu())  # save the trajectory for plotting purposes\n</pre> with torch.no_grad():     inf_size = 1024     sample = vdm.sample_prior((inf_size, 2)).to(DEVICE)  # Start with noise     trajectory = [sample.cpu()]     ts = schedule.generate_schedule()     dts = schedule.discretize()     for dt, t in zip(dts, ts):         full_t = torch.full((inf_size,), t).to(DEVICE)         x_hat = model(sample, full_t)  # calculate the vector field based on the definition of the model         sample = vdm.step_ddim(x_hat, full_t, sample, dt)         trajectory.append(sample.cpu())  # save the trajectory for plotting purposes In\u00a0[26]: Copied! <pre>import matplotlib.pyplot as plt\n\n\ntraj = torch.stack(trajectory).cpu().detach().numpy()\nplot_limit = 1024\n\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(S)\")\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0] - 1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(0)\")\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\")\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> import matplotlib.pyplot as plt   traj = torch.stack(trajectory).cpu().detach().numpy() plot_limit = 1024  plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(S)\")  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0] - 1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(0)\")  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\") plt.legend() plt.xticks([]) plt.yticks([]) plt.show() In\u00a0[27]: Copied! <pre>with torch.no_grad():\n    inf_size = 1024\n    sample = vdm.sample_prior((inf_size, 2)).to(DEVICE)  # Start with noise\n    trajectory = [sample.cpu()]\n    ts = schedule.generate_schedule()\n    dts = schedule.discretize()\n    for dt, t in zip(dts, ts):\n        full_t = torch.full((inf_size,), t).to(DEVICE)\n        x_hat = model(sample, full_t)  # calculate the vector field based on the definition of the model\n        # sample = vdm.step_hybrid_sde(x_hat, full_t, sample, dt)\n        sample = vdm.step_ode(x_hat, full_t, sample, dt)\n        trajectory.append(sample.cpu())  # save the trajectory for plotting purposes\n\ntraj = torch.stack(trajectory).cpu().detach().numpy()\nplot_limit = 1024\n\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(S)\")\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0] - 1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(0)\")\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\")\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> with torch.no_grad():     inf_size = 1024     sample = vdm.sample_prior((inf_size, 2)).to(DEVICE)  # Start with noise     trajectory = [sample.cpu()]     ts = schedule.generate_schedule()     dts = schedule.discretize()     for dt, t in zip(dts, ts):         full_t = torch.full((inf_size,), t).to(DEVICE)         x_hat = model(sample, full_t)  # calculate the vector field based on the definition of the model         # sample = vdm.step_hybrid_sde(x_hat, full_t, sample, dt)         sample = vdm.step_ode(x_hat, full_t, sample, dt)         trajectory.append(sample.cpu())  # save the trajectory for plotting purposes  traj = torch.stack(trajectory).cpu().detach().numpy() plot_limit = 1024  plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(S)\")  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0] - 1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(0)\")  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\") plt.legend() plt.xticks([]) plt.yticks([]) plt.show() In\u00a0[28]: Copied! <pre>with torch.no_grad():\n    inf_size = 1024\n    sample = vdm.sample_prior((inf_size, 2)).to(DEVICE)  # Start with noise\n    trajectory = [sample.cpu()]\n    ts = schedule.generate_schedule()\n    dts = schedule.discretize()\n    for dt, t in zip(dts, ts):\n        full_t = torch.full((inf_size,), t).to(DEVICE)\n        x_hat = model(sample, full_t)  # calculate the vector field based on the definition of the model\n        # sample = vdm.step_hybrid_sde(x_hat, full_t, sample, dt)\n        sample = vdm.step_ode(x_hat, full_t, sample, dt, temperature=1.5)\n        trajectory.append(sample.cpu())  # save the trajectory for plotting purposes\n\ntraj = torch.stack(trajectory).cpu().detach().numpy()\nplot_limit = 1024\n\n# Assuming traj is your tensor and traj.shape = (N, 2000, 2)\n# where N is the number of time points, 2000 is the number of samples at each time point, and 2 is for the x and y coordinates.\n\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(S)\")\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0] - 1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(0)\")\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\")\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> with torch.no_grad():     inf_size = 1024     sample = vdm.sample_prior((inf_size, 2)).to(DEVICE)  # Start with noise     trajectory = [sample.cpu()]     ts = schedule.generate_schedule()     dts = schedule.discretize()     for dt, t in zip(dts, ts):         full_t = torch.full((inf_size,), t).to(DEVICE)         x_hat = model(sample, full_t)  # calculate the vector field based on the definition of the model         # sample = vdm.step_hybrid_sde(x_hat, full_t, sample, dt)         sample = vdm.step_ode(x_hat, full_t, sample, dt, temperature=1.5)         trajectory.append(sample.cpu())  # save the trajectory for plotting purposes  traj = torch.stack(trajectory).cpu().detach().numpy() plot_limit = 1024  # Assuming traj is your tensor and traj.shape = (N, 2000, 2) # where N is the number of time points, 2000 is the number of samples at each time point, and 2 is for the x and y coordinates.  plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(S)\")  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0] - 1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(0)\")  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\") plt.legend() plt.xticks([]) plt.yticks([]) plt.show() In\u00a0[29]: Copied! <pre>with torch.no_grad():\n    inf_size = 1024\n    sample = vdm.sample_prior((inf_size, 2)).to(DEVICE)  # Start with noise\n    trajectory = [sample.cpu()]\n    ts = schedule.generate_schedule()\n    dts = schedule.discretize()\n    for dt, t in zip(dts, ts):\n        full_t = torch.full((inf_size,), t).to(DEVICE)\n        x_hat = model(sample, full_t)  # calculate the vector field based on the definition of the model\n        # sample = vdm.step_hybrid_sde(x_hat, full_t, sample, dt)\n        sample = vdm.step_ode(x_hat, full_t, sample, dt, temperature=0.5)\n        trajectory.append(sample.cpu())  # save the trajectory for plotting purposes\n\ntraj = torch.stack(trajectory).cpu().detach().numpy()\nplot_limit = 1024\n\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(S)\")\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0] - 1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(0)\")\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\")\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> with torch.no_grad():     inf_size = 1024     sample = vdm.sample_prior((inf_size, 2)).to(DEVICE)  # Start with noise     trajectory = [sample.cpu()]     ts = schedule.generate_schedule()     dts = schedule.discretize()     for dt, t in zip(dts, ts):         full_t = torch.full((inf_size,), t).to(DEVICE)         x_hat = model(sample, full_t)  # calculate the vector field based on the definition of the model         # sample = vdm.step_hybrid_sde(x_hat, full_t, sample, dt)         sample = vdm.step_ode(x_hat, full_t, sample, dt, temperature=0.5)         trajectory.append(sample.cpu())  # save the trajectory for plotting purposes  traj = torch.stack(trajectory).cpu().detach().numpy() plot_limit = 1024  plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(S)\")  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0] - 1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(0)\")  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\") plt.legend() plt.xticks([]) plt.yticks([]) plt.show() In\u00a0[30]: Copied! <pre>inf_size = 1024\nsample = vdm.sample_prior((inf_size, 2)).to(DEVICE)  # Start with noise\ntrajectory = [sample]\nts = schedule.generate_schedule()\ndts = schedule.discretize()\nfor dt, t in zip(dts, ts):\n    full_t = torch.full((inf_size,), t).to(DEVICE)\n    x_hat = model(sample, full_t)  # calculate the vector field based on the definition of the model\n    sample = vdm.step_hybrid_sde(x_hat, full_t, sample, dt)\n    # sample = vdm.step_ode(x_hat, full_t, sample, dt)\n    trajectory.append(sample)  # save the trajectory for plotting purposes\n\ntraj = torch.stack(trajectory).cpu().detach().numpy()\nplot_limit = 1024\n\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(S)\")\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0] - 1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(0)\")\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\")\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> inf_size = 1024 sample = vdm.sample_prior((inf_size, 2)).to(DEVICE)  # Start with noise trajectory = [sample] ts = schedule.generate_schedule() dts = schedule.discretize() for dt, t in zip(dts, ts):     full_t = torch.full((inf_size,), t).to(DEVICE)     x_hat = model(sample, full_t)  # calculate the vector field based on the definition of the model     sample = vdm.step_hybrid_sde(x_hat, full_t, sample, dt)     # sample = vdm.step_ode(x_hat, full_t, sample, dt)     trajectory.append(sample)  # save the trajectory for plotting purposes  traj = torch.stack(trajectory).cpu().detach().numpy() plot_limit = 1024  plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior sample z(S)\")  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0] - 1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label=\"z(0)\")  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label=\"Flow\") plt.legend() plt.xticks([]) plt.yticks([]) plt.show()"},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_vdm/#building-generative-models-for-continuous-data-via-continuous-interpolants","title":"Building Generative Models for Continuous Data via Continuous Interpolants\u00b6","text":""},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_vdm/#task-setup","title":"Task Setup\u00b6","text":"<p>To demonstrate how Conditional Flow Matching works we use sklearn to sample from and create custom 2D distriubtions.</p> <p>To start we define our \"dataloader\" so to speak. This is the '''sample_moons''' function.</p> <p>Next we define a custom PriorDistribution to enable the conversion of 8 equidistance gaussians to the moon distribution above.</p>"},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_vdm/#model-creation","title":"Model Creation\u00b6","text":"<p>Here we define a simple 4 layer MLP and define our optimizer</p>"},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_vdm/#now-lets-try-a-continuous-time-analog-interpolant-to-ddpm-called-vdm","title":"Now let's try a continuous time analog interpolant to DDPM called VDM\u00b6","text":""},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_vdm/#this-interpolant-was-used-in-chroma-and-is-described-in-great-detail-here-httpswwwbiorxivorgcontent10110120221201518682v1fullpdf","title":"This interpolant was used in Chroma and is described in great detail here https://www.biorxiv.org/content/10.1101/2022.12.01.518682v1.full.pdf\u00b6","text":""},{"location":"main/examples/bionemo-moco/continuous_data_interpolant_tutorial_vdm/#what-is-interesting-here-is-that-the-deterministic-sampling-of-ddim-best-recovers-the-flow-matching-ode-samples","title":"What is interesting here is that the deterministic sampling of DDIM best recovers the Flow Matching ODE samples\u00b6","text":""},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/","title":"Building Generative Models for Discrete Data via Discrete Interpolants","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\n\n\ntorch.cuda.manual_seed(42)\n</pre> import matplotlib.pyplot as plt import torch import torch.nn as nn from tqdm import tqdm   torch.cuda.manual_seed(42) In\u00a0[2]: Copied! <pre># training\nB = 32  # batch size\nD = 10  # dimension or sequence length, this is the number of discrete elements\nS = 2  # state space, binary so S=2\n# here we have a batch of 32 objects that consist of 10 binary variables.\n\n\nclass Model(nn.Module):\n    def __init__(self, D, S):\n        super().__init__()\n        self.embedding = nn.Embedding(S + 1, 16)\n        self.net = nn.Sequential(\n            nn.Linear(17 * D, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, S * D),\n        )\n\n    def forward(self, x, t):\n        B, D = x.shape\n        x_emb = self.embedding(x)  # (B, D, 16)\n        net_input = torch.cat([x_emb, t[:, None, None].repeat(1, D, 1)], dim=-1).reshape(B, -1)  # (B, D * 17)\n        return self.net(net_input).reshape(B, D, S)  # (B, D, S)\n</pre> # training B = 32  # batch size D = 10  # dimension or sequence length, this is the number of discrete elements S = 2  # state space, binary so S=2 # here we have a batch of 32 objects that consist of 10 binary variables.   class Model(nn.Module):     def __init__(self, D, S):         super().__init__()         self.embedding = nn.Embedding(S + 1, 16)         self.net = nn.Sequential(             nn.Linear(17 * D, 128),             nn.ReLU(),             nn.Linear(128, 128),             nn.ReLU(),             nn.Linear(128, S * D),         )      def forward(self, x, t):         B, D = x.shape         x_emb = self.embedding(x)  # (B, D, 16)         net_input = torch.cat([x_emb, t[:, None, None].repeat(1, D, 1)], dim=-1).reshape(B, -1)  # (B, D * 17)         return self.net(net_input).reshape(B, D, S)  # (B, D, S) In\u00a0[3]: Copied! <pre>from bionemo.moco.distributions.prior import DiscreteUniformPrior\nfrom bionemo.moco.distributions.time import UniformTimeDistribution\nfrom bionemo.moco.interpolants import DiscreteFlowMatcher\nfrom bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule\n\n\nB = 32  # batch size\nD = 10  # dimension\nS = 2  # state space\n\nDEVICE = \"cuda:0\"\nprior = DiscreteUniformPrior(num_classes=S)\ntime_distribution = UniformTimeDistribution()\ndfm = DiscreteFlowMatcher(time_distribution=time_distribution, prior_distribution=prior, device=DEVICE)\nschedule = LinearInferenceSchedule(nsteps=1000)\n</pre> from bionemo.moco.distributions.prior import DiscreteUniformPrior from bionemo.moco.distributions.time import UniformTimeDistribution from bionemo.moco.interpolants import DiscreteFlowMatcher from bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule   B = 32  # batch size D = 10  # dimension S = 2  # state space  DEVICE = \"cuda:0\" prior = DiscreteUniformPrior(num_classes=S) time_distribution = UniformTimeDistribution() dfm = DiscreteFlowMatcher(time_distribution=time_distribution, prior_distribution=prior, device=DEVICE) schedule = LinearInferenceSchedule(nsteps=1000) In\u00a0[4]: Copied! <pre>model = Model(D, S)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n</pre> model = Model(D, S) optimizer = torch.optim.Adam(model.parameters(), lr=1e-5) In\u00a0[5]: Copied! <pre>model = model.to(DEVICE)\nlosses = []\nfor _ in tqdm(range(50000)):\n    num_ones = torch.randint(0, D + 1, (B,))\n    x1 = (torch.arange(D)[None, :] &lt; num_ones[:, None]).long().to(DEVICE)\n    # x1 e.g. [1, 1, 1, 0, 0, 0, 0, 0, 0, 0] or [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n    optimizer.zero_grad()\n    x0 = dfm.sample_prior(x1.shape)  # B x D\n    t = dfm.sample_time(B)\n    xt = dfm.interpolate(x1, t, x0)\n    logits = model(xt, t)  # (B, D, S)\n    loss = dfm.loss(logits, x1, t).mean()\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n</pre> model = model.to(DEVICE) losses = [] for _ in tqdm(range(50000)):     num_ones = torch.randint(0, D + 1, (B,))     x1 = (torch.arange(D)[None, :] &lt; num_ones[:, None]).long().to(DEVICE)     # x1 e.g. [1, 1, 1, 0, 0, 0, 0, 0, 0, 0] or [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]     optimizer.zero_grad()     x0 = dfm.sample_prior(x1.shape)  # B x D     t = dfm.sample_time(B)     xt = dfm.interpolate(x1, t, x0)     logits = model(xt, t)  # (B, D, S)     loss = dfm.loss(logits, x1, t).mean()     loss.backward()     optimizer.step()     losses.append(loss.item()) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [00:53&lt;00:00, 937.54it/s]\n</pre> In\u00a0[6]: Copied! <pre>plt.plot(losses, label=\"Training Loss\", linestyle=\"-\", color=\"blue\", marker=\"o\")\nplt.xlabel(\"Step\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss\")\nplt.legend()\nplt.grid(True)\nplt.show()\n</pre> plt.plot(losses, label=\"Training Loss\", linestyle=\"-\", color=\"blue\", marker=\"o\") plt.xlabel(\"Step\") plt.ylabel(\"Loss\") plt.title(\"Training Loss\") plt.legend() plt.grid(True) plt.show() In\u00a0[7]: Copied! <pre>num_samples = 1000\nxt = dfm.sample_prior((num_samples, D))\nprint(xt.shape)\nts = schedule.generate_schedule(device=DEVICE)\ndts = schedule.discretize(device=DEVICE)\n</pre> num_samples = 1000 xt = dfm.sample_prior((num_samples, D)) print(xt.shape) ts = schedule.generate_schedule(device=DEVICE) dts = schedule.discretize(device=DEVICE) <pre>torch.Size([1000, 10])\n</pre> In\u00a0[8]: Copied! <pre>ts\n</pre> ts Out[8]: <pre>tensor([0.0000, 0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080,\n        0.0090, 0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160, 0.0170,\n        0.0180, 0.0190, 0.0200, 0.0210, 0.0220, 0.0230, 0.0240, 0.0250, 0.0260,\n        0.0270, 0.0280, 0.0290, 0.0300, 0.0310, 0.0320, 0.0330, 0.0340, 0.0350,\n        0.0360, 0.0370, 0.0380, 0.0390, 0.0400, 0.0410, 0.0420, 0.0430, 0.0440,\n        0.0450, 0.0460, 0.0470, 0.0480, 0.0490, 0.0500, 0.0510, 0.0520, 0.0530,\n        0.0540, 0.0550, 0.0560, 0.0570, 0.0580, 0.0590, 0.0600, 0.0610, 0.0620,\n        0.0630, 0.0640, 0.0650, 0.0660, 0.0670, 0.0680, 0.0690, 0.0700, 0.0710,\n        0.0720, 0.0730, 0.0740, 0.0750, 0.0760, 0.0770, 0.0780, 0.0790, 0.0800,\n        0.0810, 0.0820, 0.0830, 0.0840, 0.0850, 0.0860, 0.0870, 0.0880, 0.0890,\n        0.0900, 0.0910, 0.0920, 0.0930, 0.0940, 0.0950, 0.0960, 0.0970, 0.0980,\n        0.0990, 0.1000, 0.1010, 0.1020, 0.1030, 0.1040, 0.1050, 0.1060, 0.1070,\n        0.1080, 0.1090, 0.1100, 0.1110, 0.1120, 0.1130, 0.1140, 0.1150, 0.1160,\n        0.1170, 0.1180, 0.1190, 0.1200, 0.1210, 0.1220, 0.1230, 0.1240, 0.1250,\n        0.1260, 0.1270, 0.1280, 0.1290, 0.1300, 0.1310, 0.1320, 0.1330, 0.1340,\n        0.1350, 0.1360, 0.1370, 0.1380, 0.1390, 0.1400, 0.1410, 0.1420, 0.1430,\n        0.1440, 0.1450, 0.1460, 0.1470, 0.1480, 0.1490, 0.1500, 0.1510, 0.1520,\n        0.1530, 0.1540, 0.1550, 0.1560, 0.1570, 0.1580, 0.1590, 0.1600, 0.1610,\n        0.1620, 0.1630, 0.1640, 0.1650, 0.1660, 0.1670, 0.1680, 0.1690, 0.1700,\n        0.1710, 0.1720, 0.1730, 0.1740, 0.1750, 0.1760, 0.1770, 0.1780, 0.1790,\n        0.1800, 0.1810, 0.1820, 0.1830, 0.1840, 0.1850, 0.1860, 0.1870, 0.1880,\n        0.1890, 0.1900, 0.1910, 0.1920, 0.1930, 0.1940, 0.1950, 0.1960, 0.1970,\n        0.1980, 0.1990, 0.2000, 0.2010, 0.2020, 0.2030, 0.2040, 0.2050, 0.2060,\n        0.2070, 0.2080, 0.2090, 0.2100, 0.2110, 0.2120, 0.2130, 0.2140, 0.2150,\n        0.2160, 0.2170, 0.2180, 0.2190, 0.2200, 0.2210, 0.2220, 0.2230, 0.2240,\n        0.2250, 0.2260, 0.2270, 0.2280, 0.2290, 0.2300, 0.2310, 0.2320, 0.2330,\n        0.2340, 0.2350, 0.2360, 0.2370, 0.2380, 0.2390, 0.2400, 0.2410, 0.2420,\n        0.2430, 0.2440, 0.2450, 0.2460, 0.2470, 0.2480, 0.2490, 0.2500, 0.2510,\n        0.2520, 0.2530, 0.2540, 0.2550, 0.2560, 0.2570, 0.2580, 0.2590, 0.2600,\n        0.2610, 0.2620, 0.2630, 0.2640, 0.2650, 0.2660, 0.2670, 0.2680, 0.2690,\n        0.2700, 0.2710, 0.2720, 0.2730, 0.2740, 0.2750, 0.2760, 0.2770, 0.2780,\n        0.2790, 0.2800, 0.2810, 0.2820, 0.2830, 0.2840, 0.2850, 0.2860, 0.2870,\n        0.2880, 0.2890, 0.2900, 0.2910, 0.2920, 0.2930, 0.2940, 0.2950, 0.2960,\n        0.2970, 0.2980, 0.2990, 0.3000, 0.3010, 0.3020, 0.3030, 0.3040, 0.3050,\n        0.3060, 0.3070, 0.3080, 0.3090, 0.3100, 0.3110, 0.3120, 0.3130, 0.3140,\n        0.3150, 0.3160, 0.3170, 0.3180, 0.3190, 0.3200, 0.3210, 0.3220, 0.3230,\n        0.3240, 0.3250, 0.3260, 0.3270, 0.3280, 0.3290, 0.3300, 0.3310, 0.3320,\n        0.3330, 0.3340, 0.3350, 0.3360, 0.3370, 0.3380, 0.3390, 0.3400, 0.3410,\n        0.3420, 0.3430, 0.3440, 0.3450, 0.3460, 0.3470, 0.3480, 0.3490, 0.3500,\n        0.3510, 0.3520, 0.3530, 0.3540, 0.3550, 0.3560, 0.3570, 0.3580, 0.3590,\n        0.3600, 0.3610, 0.3620, 0.3630, 0.3640, 0.3650, 0.3660, 0.3670, 0.3680,\n        0.3690, 0.3700, 0.3710, 0.3720, 0.3730, 0.3740, 0.3750, 0.3760, 0.3770,\n        0.3780, 0.3790, 0.3800, 0.3810, 0.3820, 0.3830, 0.3840, 0.3850, 0.3860,\n        0.3870, 0.3880, 0.3890, 0.3900, 0.3910, 0.3920, 0.3930, 0.3940, 0.3950,\n        0.3960, 0.3970, 0.3980, 0.3990, 0.4000, 0.4010, 0.4020, 0.4030, 0.4040,\n        0.4050, 0.4060, 0.4070, 0.4080, 0.4090, 0.4100, 0.4110, 0.4120, 0.4130,\n        0.4140, 0.4150, 0.4160, 0.4170, 0.4180, 0.4190, 0.4200, 0.4210, 0.4220,\n        0.4230, 0.4240, 0.4250, 0.4260, 0.4270, 0.4280, 0.4290, 0.4300, 0.4310,\n        0.4320, 0.4330, 0.4340, 0.4350, 0.4360, 0.4370, 0.4380, 0.4390, 0.4400,\n        0.4410, 0.4420, 0.4430, 0.4440, 0.4450, 0.4460, 0.4470, 0.4480, 0.4490,\n        0.4500, 0.4510, 0.4520, 0.4530, 0.4540, 0.4550, 0.4560, 0.4570, 0.4580,\n        0.4590, 0.4600, 0.4610, 0.4620, 0.4630, 0.4640, 0.4650, 0.4660, 0.4670,\n        0.4680, 0.4690, 0.4700, 0.4710, 0.4720, 0.4730, 0.4740, 0.4750, 0.4760,\n        0.4770, 0.4780, 0.4790, 0.4800, 0.4810, 0.4820, 0.4830, 0.4840, 0.4850,\n        0.4860, 0.4870, 0.4880, 0.4890, 0.4900, 0.4910, 0.4920, 0.4930, 0.4940,\n        0.4950, 0.4960, 0.4970, 0.4980, 0.4990, 0.5000, 0.5010, 0.5020, 0.5030,\n        0.5040, 0.5050, 0.5060, 0.5070, 0.5080, 0.5090, 0.5100, 0.5110, 0.5120,\n        0.5130, 0.5140, 0.5150, 0.5160, 0.5170, 0.5180, 0.5190, 0.5200, 0.5210,\n        0.5220, 0.5230, 0.5240, 0.5250, 0.5260, 0.5270, 0.5280, 0.5290, 0.5300,\n        0.5310, 0.5320, 0.5330, 0.5340, 0.5350, 0.5360, 0.5370, 0.5380, 0.5390,\n        0.5400, 0.5410, 0.5420, 0.5430, 0.5440, 0.5450, 0.5460, 0.5470, 0.5480,\n        0.5490, 0.5500, 0.5510, 0.5520, 0.5530, 0.5540, 0.5550, 0.5560, 0.5570,\n        0.5580, 0.5590, 0.5600, 0.5610, 0.5620, 0.5630, 0.5640, 0.5650, 0.5660,\n        0.5670, 0.5680, 0.5690, 0.5700, 0.5710, 0.5720, 0.5730, 0.5740, 0.5750,\n        0.5760, 0.5770, 0.5780, 0.5790, 0.5800, 0.5810, 0.5820, 0.5830, 0.5840,\n        0.5850, 0.5860, 0.5870, 0.5880, 0.5890, 0.5900, 0.5910, 0.5920, 0.5930,\n        0.5940, 0.5950, 0.5960, 0.5970, 0.5980, 0.5990, 0.6000, 0.6010, 0.6020,\n        0.6030, 0.6040, 0.6050, 0.6060, 0.6070, 0.6080, 0.6090, 0.6100, 0.6110,\n        0.6120, 0.6130, 0.6140, 0.6150, 0.6160, 0.6170, 0.6180, 0.6190, 0.6200,\n        0.6210, 0.6220, 0.6230, 0.6240, 0.6250, 0.6260, 0.6270, 0.6280, 0.6290,\n        0.6300, 0.6310, 0.6320, 0.6330, 0.6340, 0.6350, 0.6360, 0.6370, 0.6380,\n        0.6390, 0.6400, 0.6410, 0.6420, 0.6430, 0.6440, 0.6450, 0.6460, 0.6470,\n        0.6480, 0.6490, 0.6500, 0.6510, 0.6520, 0.6530, 0.6540, 0.6550, 0.6560,\n        0.6570, 0.6580, 0.6590, 0.6600, 0.6610, 0.6620, 0.6630, 0.6640, 0.6650,\n        0.6660, 0.6670, 0.6680, 0.6690, 0.6700, 0.6710, 0.6720, 0.6730, 0.6740,\n        0.6750, 0.6760, 0.6770, 0.6780, 0.6790, 0.6800, 0.6810, 0.6820, 0.6830,\n        0.6840, 0.6850, 0.6860, 0.6870, 0.6880, 0.6890, 0.6900, 0.6910, 0.6920,\n        0.6930, 0.6940, 0.6950, 0.6960, 0.6970, 0.6980, 0.6990, 0.7000, 0.7010,\n        0.7020, 0.7030, 0.7040, 0.7050, 0.7060, 0.7070, 0.7080, 0.7090, 0.7100,\n        0.7110, 0.7120, 0.7130, 0.7140, 0.7150, 0.7160, 0.7170, 0.7180, 0.7190,\n        0.7200, 0.7210, 0.7220, 0.7230, 0.7240, 0.7250, 0.7260, 0.7270, 0.7280,\n        0.7290, 0.7300, 0.7310, 0.7320, 0.7330, 0.7340, 0.7350, 0.7360, 0.7370,\n        0.7380, 0.7390, 0.7400, 0.7410, 0.7420, 0.7430, 0.7440, 0.7450, 0.7460,\n        0.7470, 0.7480, 0.7490, 0.7500, 0.7510, 0.7520, 0.7530, 0.7540, 0.7550,\n        0.7560, 0.7570, 0.7580, 0.7590, 0.7600, 0.7610, 0.7620, 0.7630, 0.7640,\n        0.7650, 0.7660, 0.7670, 0.7680, 0.7690, 0.7700, 0.7710, 0.7720, 0.7730,\n        0.7740, 0.7750, 0.7760, 0.7770, 0.7780, 0.7790, 0.7800, 0.7810, 0.7820,\n        0.7830, 0.7840, 0.7850, 0.7860, 0.7870, 0.7880, 0.7890, 0.7900, 0.7910,\n        0.7920, 0.7930, 0.7940, 0.7950, 0.7960, 0.7970, 0.7980, 0.7990, 0.8000,\n        0.8010, 0.8020, 0.8030, 0.8040, 0.8050, 0.8060, 0.8070, 0.8080, 0.8090,\n        0.8100, 0.8110, 0.8120, 0.8130, 0.8140, 0.8150, 0.8160, 0.8170, 0.8180,\n        0.8190, 0.8200, 0.8210, 0.8220, 0.8230, 0.8240, 0.8250, 0.8260, 0.8270,\n        0.8280, 0.8290, 0.8300, 0.8310, 0.8320, 0.8330, 0.8340, 0.8350, 0.8360,\n        0.8370, 0.8380, 0.8390, 0.8400, 0.8410, 0.8420, 0.8430, 0.8440, 0.8450,\n        0.8460, 0.8470, 0.8480, 0.8490, 0.8500, 0.8510, 0.8520, 0.8530, 0.8540,\n        0.8550, 0.8560, 0.8570, 0.8580, 0.8590, 0.8600, 0.8610, 0.8620, 0.8630,\n        0.8640, 0.8650, 0.8660, 0.8670, 0.8680, 0.8690, 0.8700, 0.8710, 0.8720,\n        0.8730, 0.8740, 0.8750, 0.8760, 0.8770, 0.8780, 0.8790, 0.8800, 0.8810,\n        0.8820, 0.8830, 0.8840, 0.8850, 0.8860, 0.8870, 0.8880, 0.8890, 0.8900,\n        0.8910, 0.8920, 0.8930, 0.8940, 0.8950, 0.8960, 0.8970, 0.8980, 0.8990,\n        0.9000, 0.9010, 0.9020, 0.9030, 0.9040, 0.9050, 0.9060, 0.9070, 0.9080,\n        0.9090, 0.9100, 0.9110, 0.9120, 0.9130, 0.9140, 0.9150, 0.9160, 0.9170,\n        0.9180, 0.9190, 0.9200, 0.9210, 0.9220, 0.9230, 0.9240, 0.9250, 0.9260,\n        0.9270, 0.9280, 0.9290, 0.9300, 0.9310, 0.9320, 0.9330, 0.9340, 0.9350,\n        0.9360, 0.9370, 0.9380, 0.9390, 0.9400, 0.9410, 0.9420, 0.9430, 0.9440,\n        0.9450, 0.9460, 0.9470, 0.9480, 0.9490, 0.9500, 0.9510, 0.9520, 0.9530,\n        0.9540, 0.9550, 0.9560, 0.9570, 0.9580, 0.9590, 0.9600, 0.9610, 0.9620,\n        0.9630, 0.9640, 0.9650, 0.9660, 0.9670, 0.9680, 0.9690, 0.9700, 0.9710,\n        0.9720, 0.9730, 0.9740, 0.9750, 0.9760, 0.9770, 0.9780, 0.9790, 0.9800,\n        0.9810, 0.9820, 0.9830, 0.9840, 0.9850, 0.9860, 0.9870, 0.9880, 0.9890,\n        0.9900, 0.9910, 0.9920, 0.9930, 0.9940, 0.9950, 0.9960, 0.9970, 0.9980,\n        0.9990], device='cuda:0')</pre> In\u00a0[9]: Copied! <pre>LinearInferenceSchedule(nsteps=100, min_t=0, inclusive_end=False).generate_schedule()\n</pre> LinearInferenceSchedule(nsteps=100, min_t=0, inclusive_end=False).generate_schedule() Out[9]: <pre>tensor([0.0000, 0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800,\n        0.0900, 0.1000, 0.1100, 0.1200, 0.1300, 0.1400, 0.1500, 0.1600, 0.1700,\n        0.1800, 0.1900, 0.2000, 0.2100, 0.2200, 0.2300, 0.2400, 0.2500, 0.2600,\n        0.2700, 0.2800, 0.2900, 0.3000, 0.3100, 0.3200, 0.3300, 0.3400, 0.3500,\n        0.3600, 0.3700, 0.3800, 0.3900, 0.4000, 0.4100, 0.4200, 0.4300, 0.4400,\n        0.4500, 0.4600, 0.4700, 0.4800, 0.4900, 0.5000, 0.5100, 0.5200, 0.5300,\n        0.5400, 0.5500, 0.5600, 0.5700, 0.5800, 0.5900, 0.6000, 0.6100, 0.6200,\n        0.6300, 0.6400, 0.6500, 0.6600, 0.6700, 0.6800, 0.6900, 0.7000, 0.7100,\n        0.7200, 0.7300, 0.7400, 0.7500, 0.7600, 0.7700, 0.7800, 0.7900, 0.8000,\n        0.8100, 0.8200, 0.8300, 0.8400, 0.8500, 0.8600, 0.8700, 0.8800, 0.8900,\n        0.9000, 0.9100, 0.9200, 0.9300, 0.9400, 0.9500, 0.9600, 0.9700, 0.9800,\n        0.9900])</pre> In\u00a0[10]: Copied! <pre>for dt, t in zip(dts, ts):\n    t = schedule.pad_time(num_samples, t, DEVICE)\n    logits = model(xt, t)\n    xt = dfm.step(logits, t, xt, dt, stochasticity=0)\n</pre> for dt, t in zip(dts, ts):     t = schedule.pad_time(num_samples, t, DEVICE)     logits = model(xt, t)     xt = dfm.step(logits, t, xt, dt, stochasticity=0) In\u00a0[11]: Copied! <pre>counts = xt.cpu().sum(dim=1).float()\nplt.hist(counts.numpy(), bins=range(D + 2))\nplt.show()\n</pre> counts = xt.cpu().sum(dim=1).float() plt.hist(counts.numpy(), bins=range(D + 2)) plt.show() In\u00a0[12]: Copied! <pre>num_ones = torch.randint(0, D + 1, (1000,))\nx1 = (torch.arange(D)[None, :] &lt; num_ones[:, None]).long()\ncounts = x1.cpu().sum(dim=1).float()\nplt.hist(counts.numpy(), bins=range(D + 2))\nplt.show()\n</pre> num_ones = torch.randint(0, D + 1, (1000,)) x1 = (torch.arange(D)[None, :] &lt; num_ones[:, None]).long() counts = x1.cpu().sum(dim=1).float() plt.hist(counts.numpy(), bins=range(D + 2)) plt.show() In\u00a0[13]: Copied! <pre>x0 = dfm.sample_prior((10000, D))\ncounts = x0.cpu().sum(dim=1).float()\nplt.hist(counts.numpy(), bins=range(D + 2))\nplt.show()\n</pre> x0 = dfm.sample_prior((10000, D)) counts = x0.cpu().sum(dim=1).float() plt.hist(counts.numpy(), bins=range(D + 2)) plt.show() In\u00a0[14]: Copied! <pre># NBVAL_SKIP\nimport os\n\n\nos.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"0\"  # disable TF32 for numerical stability for blackwell testing\nfrom bionemo.moco.distributions.prior import DiscreteUniformPrior\nfrom bionemo.moco.distributions.time import UniformTimeDistribution\nfrom bionemo.moco.interpolants import D3PM\nfrom bionemo.moco.schedules.inference_time_schedules import DiscreteLinearInferenceSchedule\nfrom bionemo.moco.schedules.noise.discrete_noise_schedules import DiscreteCosineNoiseSchedule\n\n\nB = 32  # batch size\nD = 10  # dimension\nS = 2  # state space\n\nDEVICE = \"cuda:0\"\nprior = DiscreteUniformPrior(num_classes=S)\ntime_distribution = UniformTimeDistribution(discrete_time=True, nsteps=1000)\nnoise_schedule = DiscreteCosineNoiseSchedule(nsteps=1000)\nd3pm = D3PM(\n    time_distribution=time_distribution, prior_distribution=prior, noise_schedule=noise_schedule, device=DEVICE\n)\nschedule = DiscreteLinearInferenceSchedule(nsteps=1000, direction=\"diffusion\", device=DEVICE)\n</pre> # NBVAL_SKIP import os   os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"0\"  # disable TF32 for numerical stability for blackwell testing from bionemo.moco.distributions.prior import DiscreteUniformPrior from bionemo.moco.distributions.time import UniformTimeDistribution from bionemo.moco.interpolants import D3PM from bionemo.moco.schedules.inference_time_schedules import DiscreteLinearInferenceSchedule from bionemo.moco.schedules.noise.discrete_noise_schedules import DiscreteCosineNoiseSchedule   B = 32  # batch size D = 10  # dimension S = 2  # state space  DEVICE = \"cuda:0\" prior = DiscreteUniformPrior(num_classes=S) time_distribution = UniformTimeDistribution(discrete_time=True, nsteps=1000) noise_schedule = DiscreteCosineNoiseSchedule(nsteps=1000) d3pm = D3PM(     time_distribution=time_distribution, prior_distribution=prior, noise_schedule=noise_schedule, device=DEVICE ) schedule = DiscreteLinearInferenceSchedule(nsteps=1000, direction=\"diffusion\", device=DEVICE) In\u00a0[15]: Copied! <pre># NBVAL_SKIP\nmodel = Model(D, S)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\nd3pm.terminal_distribution  # here we can see as confirmation that the distribution we are diffusing from is binary\n</pre> # NBVAL_SKIP model = Model(D, S) optimizer = torch.optim.Adam(model.parameters(), lr=1e-5) d3pm.terminal_distribution  # here we can see as confirmation that the distribution we are diffusing from is binary Out[15]: <pre>tensor([0.5000, 0.5000])</pre> In\u00a0[16]: Copied! <pre># NBVAL_SKIP\nmodel = model.to(DEVICE)\nlosses = []\nfor _ in tqdm(range(50000)):\n    num_ones = torch.randint(0, D + 1, (B,))\n    x1 = (torch.arange(D)[None, :] &lt; num_ones[:, None]).long().to(DEVICE)\n    # x1 e.g. [1, 1, 1, 0, 0, 0, 0, 0, 0, 0] or [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n    optimizer.zero_grad()\n    # x0 = dfm.sample_prior(x1.shape) # B x D\n    t = d3pm.sample_time(B)\n    xt = d3pm.interpolate(x1, t)\n    logits = model(xt, t)  # (B, D, S)\n    loss = d3pm.loss(logits, x1, xt, t).mean()\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n</pre> # NBVAL_SKIP model = model.to(DEVICE) losses = [] for _ in tqdm(range(50000)):     num_ones = torch.randint(0, D + 1, (B,))     x1 = (torch.arange(D)[None, :] &lt; num_ones[:, None]).long().to(DEVICE)     # x1 e.g. [1, 1, 1, 0, 0, 0, 0, 0, 0, 0] or [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]     optimizer.zero_grad()     # x0 = dfm.sample_prior(x1.shape) # B x D     t = d3pm.sample_time(B)     xt = d3pm.interpolate(x1, t)     logits = model(xt, t)  # (B, D, S)     loss = d3pm.loss(logits, x1, xt, t).mean()     loss.backward()     optimizer.step()     losses.append(loss.item()) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [01:03&lt;00:00, 786.42it/s]\n</pre> In\u00a0[17]: Copied! <pre># NBVAL_SKIP\nplt.plot(losses, label=\"Training Loss\", linestyle=\"-\", color=\"blue\", marker=\"o\")\nplt.xlabel(\"Step\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss\")\nplt.legend()\nplt.grid(True)\nplt.ylim([0, 1])\n# plt.yscale('log')\nplt.show()\n</pre> # NBVAL_SKIP plt.plot(losses, label=\"Training Loss\", linestyle=\"-\", color=\"blue\", marker=\"o\") plt.xlabel(\"Step\") plt.ylabel(\"Loss\") plt.title(\"Training Loss\") plt.legend() plt.grid(True) plt.ylim([0, 1]) # plt.yscale('log') plt.show() In\u00a0[18]: Copied! <pre># NBVAL_SKIP\nts = schedule.generate_schedule()\nnum_samples = 1000\nxt = d3pm.sample_prior((num_samples, D))\nfor t in ts:\n    t = torch.full((xt.shape[0],), t).to(DEVICE)\n    logits = model(xt, t)\n    xt = d3pm.step(logits, t, xt)\n</pre> # NBVAL_SKIP ts = schedule.generate_schedule() num_samples = 1000 xt = d3pm.sample_prior((num_samples, D)) for t in ts:     t = torch.full((xt.shape[0],), t).to(DEVICE)     logits = model(xt, t)     xt = d3pm.step(logits, t, xt) In\u00a0[19]: Copied! <pre># NBVAL_SKIP\ncounts = xt.cpu().sum(dim=1).float()\nplt.hist(counts.numpy(), bins=range(D + 2))\nplt.show()\n</pre> # NBVAL_SKIP counts = xt.cpu().sum(dim=1).float() plt.hist(counts.numpy(), bins=range(D + 2)) plt.show() In\u00a0[20]: Copied! <pre># NBVAL_SKIP\nxt = d3pm.sample_prior((num_samples, D))\ncounts = xt.cpu().sum(dim=1).float()\nplt.hist(counts.numpy(), bins=range(D + 2))\nplt.show()\n</pre> # NBVAL_SKIP xt = d3pm.sample_prior((num_samples, D)) counts = xt.cpu().sum(dim=1).float() plt.hist(counts.numpy(), bins=range(D + 2)) plt.show() In\u00a0[21]: Copied! <pre># NBVAL_SKIP\nos.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"1\"  # reset to default for blackwell testing\n</pre> # NBVAL_SKIP os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"1\"  # reset to default for blackwell testing In\u00a0[22]: Copied! <pre>from bionemo.moco.distributions.prior import DiscreteMaskedPrior\nfrom bionemo.moco.distributions.time import UniformTimeDistribution\nfrom bionemo.moco.interpolants import MDLM\nfrom bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule\nfrom bionemo.moco.schedules.noise.continuous_noise_transforms import CosineExpNoiseTransform\n\n\nDEVICE = \"cuda:0\"\nprior = DiscreteMaskedPrior(num_classes=2, inclusive=False)\ntime_distribution = UniformTimeDistribution(discrete_time=False)\nnoise_schedule = CosineExpNoiseTransform()\nmdlm = MDLM(\n    time_distribution=time_distribution, prior_distribution=prior, noise_schedule=noise_schedule, device=DEVICE\n)\nschedule = LinearInferenceSchedule(direction=\"diffusion\", nsteps=1000)\n</pre> from bionemo.moco.distributions.prior import DiscreteMaskedPrior from bionemo.moco.distributions.time import UniformTimeDistribution from bionemo.moco.interpolants import MDLM from bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule from bionemo.moco.schedules.noise.continuous_noise_transforms import CosineExpNoiseTransform   DEVICE = \"cuda:0\" prior = DiscreteMaskedPrior(num_classes=2, inclusive=False) time_distribution = UniformTimeDistribution(discrete_time=False) noise_schedule = CosineExpNoiseTransform() mdlm = MDLM(     time_distribution=time_distribution, prior_distribution=prior, noise_schedule=noise_schedule, device=DEVICE ) schedule = LinearInferenceSchedule(direction=\"diffusion\", nsteps=1000) In\u00a0[23]: Copied! <pre>prior.num_classes  # The inclusive flag allows us to chose whether or not to add a dimension\n</pre> prior.num_classes  # The inclusive flag allows us to chose whether or not to add a dimension Out[23]: <pre>3</pre> In\u00a0[24]: Copied! <pre># training\nB = 32  # batch size\nD = 10  # dimension\nS = 3  # state space\n\nmodel = Model(D, S)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n\nmodel = model.to(DEVICE)\nlosses = []\nfor _ in tqdm(range(50000)):\n    num_ones = torch.randint(0, D + 1, (B,))\n    x1 = (torch.arange(D)[None, :] &lt; num_ones[:, None]).long().to(DEVICE)\n    # x1 e.g. [1, 1, 1, 0, 0, 0, 0, 0, 0, 0] or [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n    optimizer.zero_grad()\n    # x0 = dfm.sample_prior(x1.shape) # B x D\n    t = mdlm.sample_time(B)\n    xt = mdlm.interpolate(x1, t)\n    logits = model(xt, t)  # (B, D, S)\n    loss = mdlm.loss(logits, x1, xt, t).mean()\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n</pre> # training B = 32  # batch size D = 10  # dimension S = 3  # state space  model = Model(D, S) optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)  model = model.to(DEVICE) losses = [] for _ in tqdm(range(50000)):     num_ones = torch.randint(0, D + 1, (B,))     x1 = (torch.arange(D)[None, :] &lt; num_ones[:, None]).long().to(DEVICE)     # x1 e.g. [1, 1, 1, 0, 0, 0, 0, 0, 0, 0] or [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]     optimizer.zero_grad()     # x0 = dfm.sample_prior(x1.shape) # B x D     t = mdlm.sample_time(B)     xt = mdlm.interpolate(x1, t)     logits = model(xt, t)  # (B, D, S)     loss = mdlm.loss(logits, x1, xt, t).mean()     loss.backward()     optimizer.step()     losses.append(loss.item()) <pre> 55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 27457/50000 [00:54&lt;00:42, 535.47it/s]</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [01:35&lt;00:00, 521.23it/s]\n</pre> In\u00a0[25]: Copied! <pre>plt.plot(losses, label=\"Training Loss\", linestyle=\"-\", color=\"blue\", marker=\"o\")\nplt.xlabel(\"Step\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss\")\nplt.legend()\nplt.grid(True)\nplt.ylim([0, 1])\nplt.show()\n</pre> plt.plot(losses, label=\"Training Loss\", linestyle=\"-\", color=\"blue\", marker=\"o\") plt.xlabel(\"Step\") plt.ylabel(\"Loss\") plt.title(\"Training Loss\") plt.legend() plt.grid(True) plt.ylim([0, 1]) plt.show() In\u00a0[26]: Copied! <pre>num_samples = 1000\nxt = mdlm.sample_prior((num_samples, D))\ncounts = xt.flatten().cpu()\n\n# Compute frequency of each class index\nclass_counts = torch.bincount(counts)\n\n# Plotting\nplt.figure(figsize=(8, 5))\nplt.bar(range(len(class_counts)), class_counts.numpy(), color=\"red\")\nplt.xlabel(\"Class Index\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Discrete Distribution of Class Indices\")\nplt.xticks(range(len(class_counts)))  # Set x-ticks to class indices\nplt.show()\n</pre> num_samples = 1000 xt = mdlm.sample_prior((num_samples, D)) counts = xt.flatten().cpu()  # Compute frequency of each class index class_counts = torch.bincount(counts)  # Plotting plt.figure(figsize=(8, 5)) plt.bar(range(len(class_counts)), class_counts.numpy(), color=\"red\") plt.xlabel(\"Class Index\") plt.ylabel(\"Frequency\") plt.title(\"Discrete Distribution of Class Indices\") plt.xticks(range(len(class_counts)))  # Set x-ticks to class indices plt.show() In\u00a0[27]: Copied! <pre>ts = schedule.generate_schedule()\ndts = schedule.discretize()\nnum_samples = 1000\nxt = mdlm.sample_prior((num_samples, D))\nfor dt, t in zip(dts, ts):\n    t = torch.full((xt.shape[0],), t).to(DEVICE)\n    logits = model(xt, t)\n    xt = mdlm.step(logits, t, xt, dt)\n</pre> ts = schedule.generate_schedule() dts = schedule.discretize() num_samples = 1000 xt = mdlm.sample_prior((num_samples, D)) for dt, t in zip(dts, ts):     t = torch.full((xt.shape[0],), t).to(DEVICE)     logits = model(xt, t)     xt = mdlm.step(logits, t, xt, dt) In\u00a0[28]: Copied! <pre>counts = xt.flatten().cpu()\n\n# Compute frequency of each class index\nclass_counts = torch.bincount(counts)\n\n# Plotting\nplt.figure(figsize=(8, 5))\nplt.bar(range(len(class_counts)), class_counts.numpy(), color=\"green\")\nplt.xlabel(\"Class Index\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Discrete Distribution of Class Indices\")\nplt.xticks(range(len(class_counts)))  # Set x-ticks to class indices\nplt.show()\n</pre> counts = xt.flatten().cpu()  # Compute frequency of each class index class_counts = torch.bincount(counts)  # Plotting plt.figure(figsize=(8, 5)) plt.bar(range(len(class_counts)), class_counts.numpy(), color=\"green\") plt.xlabel(\"Class Index\") plt.ylabel(\"Frequency\") plt.title(\"Discrete Distribution of Class Indices\") plt.xticks(range(len(class_counts)))  # Set x-ticks to class indices plt.show() In\u00a0[29]: Copied! <pre>counts = xt.cpu().sum(dim=1).float()\nplt.hist(counts.numpy(), bins=range(D + 2))\nplt.show()\n</pre> counts = xt.cpu().sum(dim=1).float() plt.hist(counts.numpy(), bins=range(D + 2)) plt.show()"},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#building-generative-models-for-discrete-data-via-discrete-interpolants","title":"Building Generative Models for Discrete Data via Discrete Interpolants\u00b6","text":""},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#tutorial","title":"Tutorial\u00b6","text":"<p>This notebook walks through how to use 3 discrete data interpolants: (1) Discrete Flow Matching (2) Discrete Denoising Diffusion Probabilistic Models, and (3) Masked Diffusion Language Modeling</p>"},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#task","title":"Task\u00b6","text":"<p>here our object contains 10 binary elements with the goal distribution being a uniform distribution over the 10 elements.</p> <p>We initalize our interpolants with a binary uniform prior so on average each sample with have a value of 5 out of 10</p>"},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#define-the-model-architecture","title":"Define the Model Architecture\u00b6","text":""},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#define-the-discret-flow-matching-interpolant","title":"Define the Discret Flow Matching Interpolant\u00b6","text":""},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#train-dfm","title":"Train DFM\u00b6","text":""},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#sample-from-dfm","title":"Sample from DFM\u00b6","text":""},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#aside-on-low-temperature-sampling","title":"Aside on Low Temperature Sampling\u00b6","text":"<p>=====================================</p>"},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#temperature-in-sampling-distributions","title":"Temperature (<code>\u03c4</code>) in Sampling Distributions\u00b6","text":"<ul> <li>Definition: <code>\u03c4</code> is a hyperparameter that scales the logits (unnormalized log probabilities) of a categorical distribution before applying the softmax function to obtain the probabilities.</li> <li>Formulas:<ol> <li>Logits Scaling: <code>scaled_logits = logits / \u03c4</code></li> <li>Softmax: <code>probabilities = softmax(scaled_logits)</code></li> </ol> </li> </ul>"},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#effects-of-low-temperature-0-on-the-distribution","title":"Effects of Low Temperature (<code>\u03c4 \u2192 0</code>) on the Distribution\u00b6","text":""},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#characteristics","title":"Characteristics\u00b6","text":"<ul> <li>Peakiness Increases (Higher Confidence)<ul> <li>Distribution becomes more peaked around the mode (most likely outcome).</li> <li>Model becomes more confident in its top prediction.</li> </ul> </li> <li>Less Exploration, More Exploitation<ul> <li>Model is less likely to sample from less confident (lower probability) areas.</li> <li>Exploits the most likely outcome rather than exploring possibilities.</li> </ul> </li> <li>Convergence to Argmax<ul> <li>Sampling converges to selecting the argmax of the logits (highest logit value).</li> <li>Model always chooses the single most likely outcome, with no randomness.</li> </ul> </li> <li>Reduced Entropy<ul> <li>Entropy decreases, indicating less uncertainty.</li> <li>Reflects the distribution's increased peakiness around a single outcome.</li> </ul> </li> </ul>"},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#visual-illustration","title":"Visual Illustration\u00b6","text":"Temperature Distribution Over A, B, C Entropy &amp; Randomness High (<code>\u03c4 \u226b 1</code>) ~{0.33, 0.33, 0.33} Higher Entropy, More Randomness Low (<code>\u03c4 \u2248 0</code>) ~{0.99, 0.005, 0.005} Lower Entropy, Less Randomness"},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#practical-implications-of-low-temperature-sampling","title":"Practical Implications of Low Temperature Sampling\u00b6","text":""},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#advantages","title":"Advantages\u00b6","text":"<ul> <li>Faster Convergence in Training<ul> <li>Beneficial when quick convergence to an optimal solution is desired.</li> </ul> </li> <li>Clearer \"Best\" Predictions<ul> <li>Useful when clear indications of the model's most confident choice are needed.</li> </ul> </li> </ul>"},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#disadvantages","title":"Disadvantages\u00b6","text":"<ul> <li>Overconfidence<ul> <li>May lead to overestimation of the model's certainty in predictions.</li> </ul> </li> <li>Lack of Diversity<ul> <li>Results in repetitive or less innovative outcomes in applications requiring diverse outputs (e.g., text generation, game playing).</li> </ul> </li> </ul>"},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#understanding-stochasticity","title":"Understanding Stochasticity\u00b6","text":"<p>====================================================================</p>"},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#stochasticity-term-explanation","title":"Stochasticity Term Explanation\u00b6","text":"<p>The <code>stochasticity</code> term is a hyperparameter influencing the amount of noise added.</p>"},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#mask-prior-code-snippet","title":"Mask Prior Code Snippet\u00b6","text":"<pre>step_prob = (\n    dt * x_1_pred_prob * ((1 + stochasticity * t) / (1 - t)) * xt_is_mask\n    + dt * (1 - xt_is_mask) * mask_one_hot.view(1, 1, -1) * stochasticity * (t + dt &lt; 1).float()\n)\n</pre>"},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#uniform-prior-code-snippet","title":"Uniform Prior Code Snippet\u00b6","text":"<pre>step_prob = (\n    dt * x_1_pred_prob * ((1 + stochasticity + stochasticity * (S - 1) * t) / (1 - t))\n    + dt * pt_x1_eq_xt_prob * stochasticity\n)\n</pre>"},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#effects-of-stochasticity","title":"Effects of Stochasticity\u00b6","text":"<ul> <li>Value Range: 0 to N</li> <li>Exploration-Exploitation Tradeoff: Balances exploring new sequences (high stochasticity) and exploiting the most likely sequence (low stochasticity)</li> <li>Controls Masking and Unmasking for Masked Prior: Increasing the stochasticity control the unmasking and re-masking ratio.</li> </ul>"},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#generated-dfm-samples","title":"Generated DFM Samples\u00b6","text":""},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#ground-truth-distribution","title":"Ground Truth Distribution\u00b6","text":""},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#discrete-uniform-prior-distribution","title":"Discrete Uniform Prior Distribution\u00b6","text":""},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#we-see-that-with-dfm-we-are-able-to-approximate-the-ground-truth-distributionnow-lets-try-a-different-interpolant","title":"We see that with DFM we are able to approximate the ground truth distribution.Now let's try a different interpolant\u00b6","text":""},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#d3pm-interpolant","title":"D3PM Interpolant\u00b6","text":""},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#train-d3pm","title":"Train D3PM\u00b6","text":""},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#sample-from-d3pm","title":"Sample from D3PM\u00b6","text":""},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#d3pm-generated-distribution","title":"D3PM Generated Distribution\u00b6","text":""},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#d3pm-prior-distribution","title":"D3PM Prior Distribution\u00b6","text":""},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#now-lets-try-a-new-interpolant-and-a-new-prior","title":"Now let's try a new interpolant and a new prior\u00b6","text":""},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#mdlm-interpolant","title":"MDLM Interpolant\u00b6","text":""},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#train-mdlm","title":"Train MDLM\u00b6","text":""},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#visualize-the-mask-prior","title":"Visualize the MASK Prior\u00b6","text":""},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#sample-from-the-mdlm-trained-model","title":"Sample from the MDLM trained model\u00b6","text":""},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#visualize-the-class-breakdown-green-and-generated-samples-blue","title":"Visualize the class breakdown (green) and generated samples (blue)\u00b6","text":""},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#here-we-can-take-binary-data-and-rather-than-using-a-uniform-prior-introduce-a-mask-state-here-mdlm-trained-on-the-same-data-is-able-to-generate-the-desired-discrete-data-shown-ion-blue-although-starting-from-pure-mask-states-seen-in-red","title":"here we can take binary data and rather than using a uniform prior introduce a MASK state. Here MDLM trained on the same data is able to generate the desired discrete data shown ion blue although starting from pure MASK states seen in red.\u00b6","text":""},{"location":"main/examples/bionemo-moco/discrete_data_interpolant_tutorial/#these-3-cases-show-how-on-the-same-data-one-can-switch-between-various-diffusion-and-flow-matching-options-that-each-come-with-various-inference-time-sampling-abilities","title":"These 3 cases show how on the same data one can switch between various diffusion and flow matching options that each come with various inference time sampling abilities.\u00b6","text":""},{"location":"main/examples/bionemo-moco/entropic_time_scheduler_tutorial_cfm/","title":"Entropic Flow Matching for Optimal Time Scheduling","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport torch\nfrom sklearn.datasets import make_moons\n</pre> import matplotlib.pyplot as plt import torch from sklearn.datasets import make_moons In\u00a0[2]: Copied! <pre>dim = 2\nhidden_size = 64\nbatch_size = 2048\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(dim + 1, hidden_size),\n    torch.nn.SELU(),\n    torch.nn.Linear(hidden_size, hidden_size),\n    torch.nn.SELU(),\n    torch.nn.Linear(hidden_size, dim),\n)\noptimizer = torch.optim.Adam(model.parameters())\n</pre> dim = 2 hidden_size = 64 batch_size = 2048 model = torch.nn.Sequential(     torch.nn.Linear(dim + 1, hidden_size),     torch.nn.SELU(),     torch.nn.Linear(hidden_size, hidden_size),     torch.nn.SELU(),     torch.nn.Linear(hidden_size, dim), ) optimizer = torch.optim.Adam(model.parameters()) In\u00a0[3]: Copied! <pre>from bionemo.moco.distributions.prior import GaussianPrior\nfrom bionemo.moco.distributions.time import UniformTimeDistribution\nfrom bionemo.moco.interpolants import ContinuousFlowMatcher\n\n\nuniform_time = UniformTimeDistribution()\nsimple_prior = GaussianPrior()\nsigma = 0.1\ncfm = ContinuousFlowMatcher(\n    time_distribution=uniform_time, prior_distribution=simple_prior, sigma=sigma, prediction_type=\"velocity\"\n)\n# Place both the model and the interpolant on the same device\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = model.to(DEVICE)\ncfm = cfm.to_device(DEVICE)\n</pre> from bionemo.moco.distributions.prior import GaussianPrior from bionemo.moco.distributions.time import UniformTimeDistribution from bionemo.moco.interpolants import ContinuousFlowMatcher   uniform_time = UniformTimeDistribution() simple_prior = GaussianPrior() sigma = 0.1 cfm = ContinuousFlowMatcher(     time_distribution=uniform_time, prior_distribution=simple_prior, sigma=sigma, prediction_type=\"velocity\" ) # Place both the model and the interpolant on the same device DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" model = model.to(DEVICE) cfm = cfm.to_device(DEVICE) In\u00a0[4]: Copied! <pre>normalize = True\n\n\ndef sample_moons(n, normalize=False):\n    x1, _ = make_moons(n_samples=n, noise=0.05)\n    x1 = torch.Tensor(x1)\n    x1 = x1 * 3 - 1\n    if normalize:\n        x1 = (x1 - x1.mean(0)) / x1.std(0) * 2\n    return x1\n\n\nx1 = sample_moons(1000)\nplt.scatter(x1[:, 0], x1[:, 1])\n</pre> normalize = True   def sample_moons(n, normalize=False):     x1, _ = make_moons(n_samples=n, noise=0.05)     x1 = torch.Tensor(x1)     x1 = x1 * 3 - 1     if normalize:         x1 = (x1 - x1.mean(0)) / x1.std(0) * 2     return x1   x1 = sample_moons(1000) plt.scatter(x1[:, 0], x1[:, 1]) Out[4]: <pre>&lt;matplotlib.collections.PathCollection at 0x17f7c18e0&gt;</pre> In\u00a0[5]: Copied! <pre>for k in range(10_000):\n    optimizer.zero_grad()\n    shape = (batch_size, dim)\n    x0 = cfm.sample_prior(shape).to(DEVICE)\n    x1 = sample_moons(batch_size).to(DEVICE)\n\n    t = cfm.sample_time(batch_size)\n    xt = cfm.interpolate(x1, t, x0)\n    ut = cfm.calculate_target(x1, x0)\n\n    vt = model(torch.cat([xt, t[:, None]], dim=-1))\n    loss = cfm.loss(vt, ut, target_type=\"velocity\").mean()\n\n    loss.backward()\n    optimizer.step()\n\n    if (k + 1) % 500 == 0:\n        print(f\"{k + 1}: loss {loss.item():0.3f}\")\n</pre> for k in range(10_000):     optimizer.zero_grad()     shape = (batch_size, dim)     x0 = cfm.sample_prior(shape).to(DEVICE)     x1 = sample_moons(batch_size).to(DEVICE)      t = cfm.sample_time(batch_size)     xt = cfm.interpolate(x1, t, x0)     ut = cfm.calculate_target(x1, x0)      vt = model(torch.cat([xt, t[:, None]], dim=-1))     loss = cfm.loss(vt, ut, target_type=\"velocity\").mean()      loss.backward()     optimizer.step()      if (k + 1) % 500 == 0:         print(f\"{k + 1}: loss {loss.item():0.3f}\") <pre>500: loss 2.972\n1000: loss 3.096\n1500: loss 2.824\n2000: loss 2.925\n2500: loss 2.930\n3000: loss 2.835\n3500: loss 2.935\n4000: loss 2.947\n4500: loss 3.071\n5000: loss 2.917\n5500: loss 2.819\n6000: loss 2.932\n6500: loss 3.018\n7000: loss 2.756\n7500: loss 2.891\n8000: loss 2.871\n8500: loss 2.895\n9000: loss 2.839\n9500: loss 2.931\n10000: loss 2.910\n</pre> In\u00a0[6]: Copied! <pre>import matplotlib.pyplot as plt\nimport torch\n\nfrom bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule, TimeDirection\n</pre> import matplotlib.pyplot as plt import torch  from bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule, TimeDirection In\u00a0[7]: Copied! <pre>%%time\n# ---------- parameters ----------\ndim = 2\nshape = (batch_size, dim)\n_FLOW_STEPS = 100  # number of steps\ndisplay_int = 10  # show every n-th step\n# --------------------------------\n\n\ndef square_centre_limits(ax, pts, pad_frac: float = 0.05):\n    \"\"\"Make the axes square and centred on the data.\n\n    Args:\n        ax  : Matplotlib axis.\n        pts : (N, 2) tensor or ndarray on CPU.\n        pad_frac: Fractional padding added to the half-range.\n    \"\"\"\n    x, y = pts[:, 0], pts[:, 1]\n    x_mid, y_mid = (x.max() + x.min()) / 2, (y.max() + y.min()) / 2\n    half_range = max(x.max() - x.min(), y.max() - y.min()) / 2\n    half_range *= 1 + pad_frac  # add a small margin\n    ax.set_xlim(x_mid - half_range, x_mid + half_range)\n    ax.set_ylim(y_mid - half_range, y_mid + half_range)\n\n\n# define schedule\ninference_sched = LinearInferenceSchedule(nsteps=_FLOW_STEPS, direction=TimeDirection.UNIFIED)\nschedule = inference_sched.generate_schedule().to(DEVICE)  # len = _FLOW_STEPS\ndts = inference_sched.discretize().to(DEVICE)  # len = _FLOW_STEPS\n\n# always show t=0 and t=1\ndisplay_indices = sorted(set(range(0, _FLOW_STEPS + 1, display_int)) | {0, _FLOW_STEPS})\nn_plots = len(display_indices)\n\nwith torch.no_grad():\n    # start from the prior used in training\n    x = cfm.sample_prior(shape).to(DEVICE)\n\n    fig, axes = plt.subplots(1, n_plots, figsize=(4 * n_plots, 4))\n    for ax in axes:\n        ax.set_aspect(\"equal\", \"box\")\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n    plot_idx = 0\n    axes[plot_idx].scatter(x[:, 0].cpu(), x[:, 1].cpu(), s=2)\n    axes[plot_idx].set_title(\"t = 0.00\")\n    square_centre_limits(axes[plot_idx], x.cpu())\n\n    # sampling loop\n    for step, (dt, t) in enumerate(zip(dts, schedule)):\n        full_t = inference_sched.pad_time(batch_size, t, device=DEVICE)\n        v_t = model(torch.cat([x, full_t[:, None]], dim=-1))\n        x = cfm.step(v_t, x, dt, t=full_t)\n\n        # time after the step (always exists, even at the very end)\n        t_next = (t + dt).item()\n\n        if (step + 1) in display_indices:\n            plot_idx += 1\n            ax = axes[plot_idx]\n            ax.scatter(x[:, 0].cpu(), x[:, 1].cpu(), s=2)\n            ax.set_title(f\"t = {t_next:.2f}\")\n            square_centre_limits(ax, x.cpu())\n\nplt.tight_layout(pad=0.8)\nplt.show()\n</pre> %%time # ---------- parameters ---------- dim = 2 shape = (batch_size, dim) _FLOW_STEPS = 100  # number of steps display_int = 10  # show every n-th step # --------------------------------   def square_centre_limits(ax, pts, pad_frac: float = 0.05):     \"\"\"Make the axes square and centred on the data.      Args:         ax  : Matplotlib axis.         pts : (N, 2) tensor or ndarray on CPU.         pad_frac: Fractional padding added to the half-range.     \"\"\"     x, y = pts[:, 0], pts[:, 1]     x_mid, y_mid = (x.max() + x.min()) / 2, (y.max() + y.min()) / 2     half_range = max(x.max() - x.min(), y.max() - y.min()) / 2     half_range *= 1 + pad_frac  # add a small margin     ax.set_xlim(x_mid - half_range, x_mid + half_range)     ax.set_ylim(y_mid - half_range, y_mid + half_range)   # define schedule inference_sched = LinearInferenceSchedule(nsteps=_FLOW_STEPS, direction=TimeDirection.UNIFIED) schedule = inference_sched.generate_schedule().to(DEVICE)  # len = _FLOW_STEPS dts = inference_sched.discretize().to(DEVICE)  # len = _FLOW_STEPS  # always show t=0 and t=1 display_indices = sorted(set(range(0, _FLOW_STEPS + 1, display_int)) | {0, _FLOW_STEPS}) n_plots = len(display_indices)  with torch.no_grad():     # start from the prior used in training     x = cfm.sample_prior(shape).to(DEVICE)      fig, axes = plt.subplots(1, n_plots, figsize=(4 * n_plots, 4))     for ax in axes:         ax.set_aspect(\"equal\", \"box\")         ax.set_xticks([])         ax.set_yticks([])      plot_idx = 0     axes[plot_idx].scatter(x[:, 0].cpu(), x[:, 1].cpu(), s=2)     axes[plot_idx].set_title(\"t = 0.00\")     square_centre_limits(axes[plot_idx], x.cpu())      # sampling loop     for step, (dt, t) in enumerate(zip(dts, schedule)):         full_t = inference_sched.pad_time(batch_size, t, device=DEVICE)         v_t = model(torch.cat([x, full_t[:, None]], dim=-1))         x = cfm.step(v_t, x, dt, t=full_t)          # time after the step (always exists, even at the very end)         t_next = (t + dt).item()          if (step + 1) in display_indices:             plot_idx += 1             ax = axes[plot_idx]             ax.scatter(x[:, 0].cpu(), x[:, 1].cpu(), s=2)             ax.set_title(f\"t = {t_next:.2f}\")             square_centre_limits(ax, x.cpu())  plt.tight_layout(pad=0.8) plt.show() <pre>CPU times: user 522 ms, sys: 37.2 ms, total: 559 ms\nWall time: 260 ms\n</pre> In\u00a0[43]: Copied! <pre>import matplotlib.pyplot as plt\nimport torch\nfrom torch import Tensor\n\nfrom bionemo.moco.schedules.inference_time_schedules import EntropicInferenceSchedule, TimeDirection\n</pre> import matplotlib.pyplot as plt import torch from torch import Tensor  from bionemo.moco.schedules.inference_time_schedules import EntropicInferenceSchedule, TimeDirection In\u00a0[46]: Copied! <pre>%%time\n_FLOW_STEPS = 100\ndisplay_int = 10  # controls every \"n\" steps to display\nshape = (batch_size, dim)\n\n\n# Predictor function wrapper.\n# The scheduler needs a function `model(t, x)` and this wrapper handles the formatting.\ndef predictor_fn(t: Tensor, x: Tensor) -&gt; Tensor:\n    # Ensure t is broadcastable for concatenation\n    if t.ndim == 1:\n        t = t.unsqueeze(-1)\n    if t.shape[0] != x.shape[0]:\n        t = t.expand(x.shape[0], -1)\n\n    model_input = torch.cat([x, t], dim=-1)\n    return model(model_input)\n\n\ndef x_0_sampler_fn(n_samples: int) -&gt; Tensor:\n    return cfm.sample_prior((n_samples, dim))\n\n\ndef x_1_sampler_fn(n_samples: int) -&gt; Tensor:\n    return sample_moons(n_samples)\n\n\ninference_sched = EntropicInferenceSchedule(\n    predictor=predictor_fn,\n    x_0_sampler=x_0_sampler_fn,\n    x_1_sampler=x_1_sampler_fn,\n    nsteps=_FLOW_STEPS,\n    n_approx_entropy_points=30,  # More points -&gt; more accurate schedule, but slower to generate\n    batch_size=batch_size,\n    direction=TimeDirection.UNIFIED,\n    device=DEVICE,\n)\nprint(\"Generating entropic schedule...\")\nschedule = inference_sched.generate_schedule().to(DEVICE)\ndts = inference_sched.discretize().to(DEVICE)\nprint(\"Schedule generated.\")\n\ndisplay_indices = sorted(set(range(0, _FLOW_STEPS + 1, display_int)) | {0, _FLOW_STEPS})\nn_plots = len(display_indices)\n\nwith torch.no_grad():\n    x = cfm.sample_prior((batch_size, dim)).to(DEVICE)\n\n    fig, axes = plt.subplots(1, n_plots, figsize=(4 * n_plots, 4))\n    for ax in axes:\n        ax.set_aspect(\"equal\", \"box\")\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n    plot_idx = 0\n    axes[plot_idx].scatter(x[:, 0].cpu(), x[:, 1].cpu(), s=2)\n    axes[plot_idx].set_title(\"t = 0.00\")\n    square_centre_limits(axes[plot_idx], x.cpu())\n\n    # integration loop &amp; viz\n    for step, (dt, t) in enumerate(zip(dts, schedule)):\n        full_t = inference_sched.pad_time(batch_size, t, device=DEVICE)\n        v_t = model(torch.cat([x, full_t[:, None]], dim=-1))\n        x = cfm.step(v_t, x, dt, t=full_t)\n\n        t_next = (t + dt).item()\n\n        if (step + 1) in display_indices:\n            plot_idx += 1\n            ax = axes[plot_idx]\n            ax.scatter(x[:, 0].cpu(), x[:, 1].cpu(), s=2)\n            ax.set_title(f\"t = {t_next:.2f}\")\n            square_centre_limits(ax, x.cpu())\n\nplt.tight_layout(pad=0.8)\nplt.show()\n</pre> %%time _FLOW_STEPS = 100 display_int = 10  # controls every \"n\" steps to display shape = (batch_size, dim)   # Predictor function wrapper. # The scheduler needs a function `model(t, x)` and this wrapper handles the formatting. def predictor_fn(t: Tensor, x: Tensor) -&gt; Tensor:     # Ensure t is broadcastable for concatenation     if t.ndim == 1:         t = t.unsqueeze(-1)     if t.shape[0] != x.shape[0]:         t = t.expand(x.shape[0], -1)      model_input = torch.cat([x, t], dim=-1)     return model(model_input)   def x_0_sampler_fn(n_samples: int) -&gt; Tensor:     return cfm.sample_prior((n_samples, dim))   def x_1_sampler_fn(n_samples: int) -&gt; Tensor:     return sample_moons(n_samples)   inference_sched = EntropicInferenceSchedule(     predictor=predictor_fn,     x_0_sampler=x_0_sampler_fn,     x_1_sampler=x_1_sampler_fn,     nsteps=_FLOW_STEPS,     n_approx_entropy_points=30,  # More points -&gt; more accurate schedule, but slower to generate     batch_size=batch_size,     direction=TimeDirection.UNIFIED,     device=DEVICE, ) print(\"Generating entropic schedule...\") schedule = inference_sched.generate_schedule().to(DEVICE) dts = inference_sched.discretize().to(DEVICE) print(\"Schedule generated.\")  display_indices = sorted(set(range(0, _FLOW_STEPS + 1, display_int)) | {0, _FLOW_STEPS}) n_plots = len(display_indices)  with torch.no_grad():     x = cfm.sample_prior((batch_size, dim)).to(DEVICE)      fig, axes = plt.subplots(1, n_plots, figsize=(4 * n_plots, 4))     for ax in axes:         ax.set_aspect(\"equal\", \"box\")         ax.set_xticks([])         ax.set_yticks([])      plot_idx = 0     axes[plot_idx].scatter(x[:, 0].cpu(), x[:, 1].cpu(), s=2)     axes[plot_idx].set_title(\"t = 0.00\")     square_centre_limits(axes[plot_idx], x.cpu())      # integration loop &amp; viz     for step, (dt, t) in enumerate(zip(dts, schedule)):         full_t = inference_sched.pad_time(batch_size, t, device=DEVICE)         v_t = model(torch.cat([x, full_t[:, None]], dim=-1))         x = cfm.step(v_t, x, dt, t=full_t)          t_next = (t + dt).item()          if (step + 1) in display_indices:             plot_idx += 1             ax = axes[plot_idx]             ax.scatter(x[:, 0].cpu(), x[:, 1].cpu(), s=2)             ax.set_title(f\"t = {t_next:.2f}\")             square_centre_limits(ax, x.cpu())  plt.tight_layout(pad=0.8) plt.show() <pre>Generating entropic schedule...\nSchedule generated.\n</pre> <pre>CPU times: user 712 ms, sys: 156 ms, total: 869 ms\nWall time: 524 ms\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"main/examples/bionemo-moco/entropic_time_scheduler_tutorial_cfm/#entropic-flow-matching-for-optimal-time-scheduling","title":"Entropic Flow Matching for Optimal Time Scheduling\u00b6","text":"<p>Entropic Time Schedulers on Diffusion Models:</p> <p>Inspired by the work of Stancevic, D., &amp; Handke F., &amp; Ambrogioni, L. (2025). Entropic Time Schedulers for Generative Diffusion Models.</p> <p>We expand the work with a standard flow matching implementation and then augment it to estimate entropy production. #</p> <p>This entropy rate is then used to create an optimized, data-dependent time-stepping schedule for the generative process.</p> <p>Approach benefits:</p> <ul> <li>Improves inference performance</li> <li>Prevents oversampling of less-informative time steps</li> <li>Prevents undersampling of critical time windows</li> <li>Sample-eficient way to generate data</li> <li>Easily adapted into any model architecture leveraging flow-matching</li> </ul>"},{"location":"main/examples/bionemo-moco/entropic_time_scheduler_tutorial_cfm/#model-creation","title":"Model Creation\u00b6","text":"<p>Here we define a simple 4 layer MLP and define our optimizer</p>"},{"location":"main/examples/bionemo-moco/entropic_time_scheduler_tutorial_cfm/#target-data","title":"Target Data\u00b6","text":""},{"location":"main/examples/bionemo-moco/entropic_time_scheduler_tutorial_cfm/#training","title":"Training\u00b6","text":""},{"location":"main/examples/bionemo-moco/entropic_time_scheduler_tutorial_cfm/#linear-inference-schedule-example","title":"Linear Inference Schedule Example\u00b6","text":""},{"location":"main/examples/bionemo-moco/entropic_time_scheduler_tutorial_cfm/#entropic-time-scheduler","title":"Entropic Time Scheduler\u00b6","text":""},{"location":"main/examples/bionemo-moco/ot_sampler_tutorial/","title":"Optimal Transport Samplers Tutorial","text":"In\u00a0[1]: Copied! <pre>import copy\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom sklearn.datasets import make_moons\n\nfrom bionemo.moco.interpolants import EquivariantOTSampler, OTSampler\n</pre> import copy  import matplotlib.pyplot as plt import numpy as np import torch from sklearn.datasets import make_moons  from bionemo.moco.interpolants import EquivariantOTSampler, OTSampler In\u00a0[2]: Copied! <pre>def sample_moons(n, normalize=False):\n    x1, _ = make_moons(n_samples=n, noise=0.08)\n    x1 = torch.Tensor(x1)\n    x1 = x1 * 3 - 1\n    if normalize:\n        x1 = (x1 - x1.mean(0)) / x1.std(0) * 2\n    return x1\n\n\ndef sample_gaussian(n, dim=2):\n    return torch.randn(n, dim)\n</pre> def sample_moons(n, normalize=False):     x1, _ = make_moons(n_samples=n, noise=0.08)     x1 = torch.Tensor(x1)     x1 = x1 * 3 - 1     if normalize:         x1 = (x1 - x1.mean(0)) / x1.std(0) * 2     return x1   def sample_gaussian(n, dim=2):     return torch.randn(n, dim) In\u00a0[3]: Copied! <pre># Sample x0 and x1\nx1 = sample_moons(100, normalize=True).numpy()\nx0 = sample_gaussian(100).numpy()\n# Plot data points and linear interpolation\nplt.scatter(x1[:, 0], x1[:, 1], label=\"$x_0$\")\nplt.scatter(x0[:, 0], x0[:, 1], label=\"$x_1$\")\nx0 = np.asarray(x0)\nx1 = np.asarray(x1)\nfor i in range(len(x1)):\n    plt.plot([x0[i, 0], x1[i, 0]], [x0[i, 1], x1[i, 1]], color=\"k\", alpha=0.2)\nplt.legend()\n</pre> # Sample x0 and x1 x1 = sample_moons(100, normalize=True).numpy() x0 = sample_gaussian(100).numpy() # Plot data points and linear interpolation plt.scatter(x1[:, 0], x1[:, 1], label=\"$x_0$\") plt.scatter(x0[:, 0], x0[:, 1], label=\"$x_1$\") x0 = np.asarray(x0) x1 = np.asarray(x1) for i in range(len(x1)):     plt.plot([x0[i, 0], x1[i, 0]], [x0[i, 1], x1[i, 1]], color=\"k\", alpha=0.2) plt.legend() Out[3]: <pre>&lt;matplotlib.legend.Legend at 0x7690ce3f3d30&gt;</pre> In\u00a0[4]: Copied! <pre># Initialize the OTSampler\not_sampler = OTSampler(method=\"exact\", num_threads=1)\n# Sample new pairs from the OTSampler, mask is not used in this example\n# Replace is set to False, so no duplicates are allowed\n# Sort is set to \"x0\", so the order of output x0 is the same as input x0\not_sampled_x0, ot_sampled_x1, mask = ot_sampler.apply_augmentation(\n    torch.Tensor(x0), torch.Tensor(x1), mask=None, replace=False, sort=\"x0\"\n)\n# Convert the sampled tensors to numpy arrays\not_sampled_x0 = ot_sampled_x0.numpy()\not_sampled_x1 = ot_sampled_x1.numpy()\n</pre> # Initialize the OTSampler ot_sampler = OTSampler(method=\"exact\", num_threads=1) # Sample new pairs from the OTSampler, mask is not used in this example # Replace is set to False, so no duplicates are allowed # Sort is set to \"x0\", so the order of output x0 is the same as input x0 ot_sampled_x0, ot_sampled_x1, mask = ot_sampler.apply_augmentation(     torch.Tensor(x0), torch.Tensor(x1), mask=None, replace=False, sort=\"x0\" ) # Convert the sampled tensors to numpy arrays ot_sampled_x0 = ot_sampled_x0.numpy() ot_sampled_x1 = ot_sampled_x1.numpy() In\u00a0[5]: Copied! <pre># Plot data points and linear interpolation\nplt.scatter(ot_sampled_x1[:, 0], ot_sampled_x1[:, 1], label=\"$x_0$\")\nplt.scatter(ot_sampled_x0[:, 0], ot_sampled_x0[:, 1], label=\"$x_1$\")\nfor i in range(len(x1)):\n    plt.plot(\n        [ot_sampled_x0[i, 0], ot_sampled_x1[i, 0]], [ot_sampled_x0[i, 1], ot_sampled_x1[i, 1]], color=\"k\", alpha=0.2\n    )\nplt.legend()\n</pre> # Plot data points and linear interpolation plt.scatter(ot_sampled_x1[:, 0], ot_sampled_x1[:, 1], label=\"$x_0$\") plt.scatter(ot_sampled_x0[:, 0], ot_sampled_x0[:, 1], label=\"$x_1$\") for i in range(len(x1)):     plt.plot(         [ot_sampled_x0[i, 0], ot_sampled_x1[i, 0]], [ot_sampled_x0[i, 1], ot_sampled_x1[i, 1]], color=\"k\", alpha=0.2     ) plt.legend() Out[5]: <pre>&lt;matplotlib.legend.Legend at 0x7690c6597b80&gt;</pre> In\u00a0[6]: Copied! <pre>from bionemo.moco.distributions.prior import GaussianPrior\nfrom bionemo.moco.distributions.time import UniformTimeDistribution\nfrom bionemo.moco.interpolants import ContinuousFlowMatcher\n\n\ndef trainCFM(use_ot=False):\n    # Initialize model, optimizer, and flow matcher\n    dim = 2\n    hidden_size = 64\n    batch_size = 256\n    model = torch.nn.Sequential(\n        torch.nn.Linear(dim + 1, hidden_size),\n        torch.nn.SELU(),\n        torch.nn.Linear(hidden_size, hidden_size),\n        torch.nn.SELU(),\n        torch.nn.Linear(hidden_size, hidden_size),\n        torch.nn.SELU(),\n        torch.nn.Linear(hidden_size, dim),\n    )\n    optimizer = torch.optim.Adam(model.parameters())\n\n    uniform_time = UniformTimeDistribution()\n    moon_prior = GaussianPrior()\n    sigma = 0.1\n    cfm = ContinuousFlowMatcher(\n        time_distribution=uniform_time, prior_distribution=moon_prior, sigma=sigma, prediction_type=\"velocity\"\n    )\n\n    # Place both the model and the interpolant on the same device\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model = model.to(DEVICE)\n    cfm = cfm.to_device(DEVICE)\n\n    for k in range(10000):\n        optimizer.zero_grad()\n        shape = (batch_size, dim)\n        x0 = cfm.sample_prior(shape).to(DEVICE)\n        x1 = sample_moons(batch_size, normalize=False).to(DEVICE)\n        if use_ot:\n            x0, x1, mask = ot_sampler.apply_augmentation(x0, x1, mask=None, replace=False, sort=\"x0\")\n        t = cfm.sample_time(batch_size)\n        xt = cfm.interpolate(x1, t, x0)\n        ut = cfm.calculate_target(x1, x0)\n\n        vt = model(torch.cat([xt, t[:, None]], dim=-1))\n        loss = cfm.loss(vt, ut, target_type=\"velocity\").mean()\n\n        loss.backward()\n        optimizer.step()\n\n        if (k + 1) % 5000 == 0:\n            print(f\"{k + 1}: loss {loss.item():0.3f}\")\n    return model, cfm\n</pre> from bionemo.moco.distributions.prior import GaussianPrior from bionemo.moco.distributions.time import UniformTimeDistribution from bionemo.moco.interpolants import ContinuousFlowMatcher   def trainCFM(use_ot=False):     # Initialize model, optimizer, and flow matcher     dim = 2     hidden_size = 64     batch_size = 256     model = torch.nn.Sequential(         torch.nn.Linear(dim + 1, hidden_size),         torch.nn.SELU(),         torch.nn.Linear(hidden_size, hidden_size),         torch.nn.SELU(),         torch.nn.Linear(hidden_size, hidden_size),         torch.nn.SELU(),         torch.nn.Linear(hidden_size, dim),     )     optimizer = torch.optim.Adam(model.parameters())      uniform_time = UniformTimeDistribution()     moon_prior = GaussianPrior()     sigma = 0.1     cfm = ContinuousFlowMatcher(         time_distribution=uniform_time, prior_distribution=moon_prior, sigma=sigma, prediction_type=\"velocity\"     )      # Place both the model and the interpolant on the same device     DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"     model = model.to(DEVICE)     cfm = cfm.to_device(DEVICE)      for k in range(10000):         optimizer.zero_grad()         shape = (batch_size, dim)         x0 = cfm.sample_prior(shape).to(DEVICE)         x1 = sample_moons(batch_size, normalize=False).to(DEVICE)         if use_ot:             x0, x1, mask = ot_sampler.apply_augmentation(x0, x1, mask=None, replace=False, sort=\"x0\")         t = cfm.sample_time(batch_size)         xt = cfm.interpolate(x1, t, x0)         ut = cfm.calculate_target(x1, x0)          vt = model(torch.cat([xt, t[:, None]], dim=-1))         loss = cfm.loss(vt, ut, target_type=\"velocity\").mean()          loss.backward()         optimizer.step()          if (k + 1) % 5000 == 0:             print(f\"{k + 1}: loss {loss.item():0.3f}\")     return model, cfm In\u00a0[7]: Copied! <pre># Train a model with OT\not_model, ot_cfm = trainCFM(use_ot=True)\n# Train a model without OT\nno_ot_model, no_ot_cfm = trainCFM(use_ot=False)\n</pre> # Train a model with OT ot_model, ot_cfm = trainCFM(use_ot=True) # Train a model without OT no_ot_model, no_ot_cfm = trainCFM(use_ot=False) <pre>5000: loss 0.064\n10000: loss 0.067\n5000: loss 2.570\n10000: loss 3.204\n</pre> In\u00a0[8]: Copied! <pre># Set up the sampling time schedule\nfrom bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule\n\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ninference_sched = LinearInferenceSchedule(nsteps=100)\nschedule = inference_sched.generate_schedule().to(DEVICE)\ndts = inference_sched.discretize().to(DEVICE)\n</pre> # Set up the sampling time schedule from bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule   DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" inference_sched = LinearInferenceSchedule(nsteps=100) schedule = inference_sched.generate_schedule().to(DEVICE) dts = inference_sched.discretize().to(DEVICE) In\u00a0[9]: Copied! <pre># Sampling with the two trained models\ninf_size = 1024\not_sample = ot_cfm.sample_prior((inf_size, 2))  # Start with noise\nno_ot_sample = copy.deepcopy(ot_sample)  # Ensure the same starting point for both models\not_sample, no_ot_sample = ot_sample.to(DEVICE), no_ot_sample.to(DEVICE)\not_trajectory, no_ot_trajectory = [ot_sample], [no_ot_sample]\nfor dt, t in zip(dts, schedule):\n    full_t = torch.full((inf_size,), t).to(DEVICE)\n    ot_vt = ot_model(\n        torch.cat([ot_sample, full_t[:, None]], dim=-1)\n    )  # calculate the vector field based on the definition of the model\n    ot_sample = ot_cfm.step(ot_vt, ot_sample, dt, full_t)\n    no_ot_vt = no_ot_model(\n        torch.cat([no_ot_sample, full_t[:, None]], dim=-1)\n    )  # calculate the vector field based on the definition of the model\n    no_ot_sample = no_ot_cfm.step(no_ot_vt, no_ot_sample, dt, full_t)\n    ot_trajectory.append(ot_sample)  # save the trajectory for plotting purposes\n    no_ot_trajectory.append(no_ot_sample)  # save the trajectory for plotting purposes\n</pre> # Sampling with the two trained models inf_size = 1024 ot_sample = ot_cfm.sample_prior((inf_size, 2))  # Start with noise no_ot_sample = copy.deepcopy(ot_sample)  # Ensure the same starting point for both models ot_sample, no_ot_sample = ot_sample.to(DEVICE), no_ot_sample.to(DEVICE) ot_trajectory, no_ot_trajectory = [ot_sample], [no_ot_sample] for dt, t in zip(dts, schedule):     full_t = torch.full((inf_size,), t).to(DEVICE)     ot_vt = ot_model(         torch.cat([ot_sample, full_t[:, None]], dim=-1)     )  # calculate the vector field based on the definition of the model     ot_sample = ot_cfm.step(ot_vt, ot_sample, dt, full_t)     no_ot_vt = no_ot_model(         torch.cat([no_ot_sample, full_t[:, None]], dim=-1)     )  # calculate the vector field based on the definition of the model     no_ot_sample = no_ot_cfm.step(no_ot_vt, no_ot_sample, dt, full_t)     ot_trajectory.append(ot_sample)  # save the trajectory for plotting purposes     no_ot_trajectory.append(no_ot_sample)  # save the trajectory for plotting purposes In\u00a0[10]: Copied! <pre>ot_traj = torch.stack(ot_trajectory).cpu().detach().numpy()\nno_ot_traj = torch.stack(no_ot_trajectory).cpu().detach().numpy()\nn = 2000\n\n# Assuming traj is your tensor and traj.shape = (N, 2000, 2)\n# where N is the number of time points, 2000 is the number of samples at each time point, and 2 is for the x and y coordinates.\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 6))\n\n# Plot the first time point in black\nax[0].scatter(ot_traj[0, :n, 0], ot_traj[0, :n, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior z(S)\")\nax[1].scatter(no_ot_traj[0, :n, 0], no_ot_traj[0, :n, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior z(S)\")\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, ot_traj.shape[0] - 1):\n    ax[0].scatter(ot_traj[i, :n, 0], ot_traj[i, :n, 1], s=0.2, alpha=0.2, c=\"olive\", zorder=1)\n    ax[1].scatter(no_ot_traj[i, :n, 0], no_ot_traj[i, :n, 1], s=0.2, alpha=0.2, c=\"olive\", zorder=1)\n\n# Plot the last time point in blue\nax[0].scatter(ot_traj[-1, :n, 0], ot_traj[-1, :n, 1], s=4, alpha=1, c=\"blue\", label=\"z(0)\")\nax[1].scatter(no_ot_traj[-1, :n, 0], no_ot_traj[-1, :n, 1], s=4, alpha=1, c=\"blue\", label=\"z(0)\")\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nfor i in range(2):\n    ax[i].scatter([], [], s=2, alpha=1, c=\"olive\", label=\"Flow\")\n    ax[i].legend()\n    # ax[i].set_aspect('equal')\n    ax[i].set_xticks([])\n    ax[i].set_yticks([])\n    ax[i].set_xlim(-5, 6)\n    ax[i].set_ylim(-4, 5)\n    if i == 0:\n        ax[i].set_title(\"With OT\")\n    else:\n        ax[i].set_title(\"Without OT\")\nplt.subplots_adjust(wspace=0.05)\nplt.show()\n</pre> ot_traj = torch.stack(ot_trajectory).cpu().detach().numpy() no_ot_traj = torch.stack(no_ot_trajectory).cpu().detach().numpy() n = 2000  # Assuming traj is your tensor and traj.shape = (N, 2000, 2) # where N is the number of time points, 2000 is the number of samples at each time point, and 2 is for the x and y coordinates.  fig, ax = plt.subplots(1, 2, figsize=(12, 6))  # Plot the first time point in black ax[0].scatter(ot_traj[0, :n, 0], ot_traj[0, :n, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior z(S)\") ax[1].scatter(no_ot_traj[0, :n, 0], no_ot_traj[0, :n, 1], s=10, alpha=0.8, c=\"black\", label=\"Prior z(S)\")  # Plot all the rest of the time points except the first and last in olive for i in range(1, ot_traj.shape[0] - 1):     ax[0].scatter(ot_traj[i, :n, 0], ot_traj[i, :n, 1], s=0.2, alpha=0.2, c=\"olive\", zorder=1)     ax[1].scatter(no_ot_traj[i, :n, 0], no_ot_traj[i, :n, 1], s=0.2, alpha=0.2, c=\"olive\", zorder=1)  # Plot the last time point in blue ax[0].scatter(ot_traj[-1, :n, 0], ot_traj[-1, :n, 1], s=4, alpha=1, c=\"blue\", label=\"z(0)\") ax[1].scatter(no_ot_traj[-1, :n, 0], no_ot_traj[-1, :n, 1], s=4, alpha=1, c=\"blue\", label=\"z(0)\")  # Add a second legend for \"Flow\" since we can't label in the loop directly for i in range(2):     ax[i].scatter([], [], s=2, alpha=1, c=\"olive\", label=\"Flow\")     ax[i].legend()     # ax[i].set_aspect('equal')     ax[i].set_xticks([])     ax[i].set_yticks([])     ax[i].set_xlim(-5, 6)     ax[i].set_ylim(-4, 5)     if i == 0:         ax[i].set_title(\"With OT\")     else:         ax[i].set_title(\"Without OT\") plt.subplots_adjust(wspace=0.05) plt.show() In\u00a0[11]: Copied! <pre>first_points = no_ot_traj[0]\nlast_points = no_ot_traj[-1]\ndistances = ((last_points - first_points) ** 2).sum(-1)\naverage_distance = np.mean(distances)\n\nprint(f\"Average Distance between First and Last Points without OT: {average_distance.item()}\")\n\nfirst_points = ot_traj[0]\nlast_points = ot_traj[-1]\ndistances = ((last_points - first_points) ** 2).sum(-1)\naverage_distance = np.mean(distances)\n\nprint(f\"Average Distance between First and Last Points with OT: {average_distance.item()}\")\n</pre> first_points = no_ot_traj[0] last_points = no_ot_traj[-1] distances = ((last_points - first_points) ** 2).sum(-1) average_distance = np.mean(distances)  print(f\"Average Distance between First and Last Points without OT: {average_distance.item()}\")  first_points = ot_traj[0] last_points = ot_traj[-1] distances = ((last_points - first_points) ** 2).sum(-1) average_distance = np.mean(distances)  print(f\"Average Distance between First and Last Points with OT: {average_distance.item()}\") <pre>Average Distance between First and Last Points without OT: 3.3887150287628174\nAverage Distance between First and Last Points with OT: 3.6937265396118164\n</pre> In\u00a0[12]: Copied! <pre>def sum_of_squared_distances(trajectory):\n    \"\"\"Calculate the sum of squared distances from start to mid and mid to end of a trajectory.\n\n    Parameters:\n    - trajectory: A numpy array of shape (N, D) where N is the number of points\n                  in the trajectory and D is the dimensionality of the space.\n\n    Returns:\n    - Sum of squared distances (start to mid + mid to end).\n    \"\"\"\n    mid_idx = len(trajectory) // 2\n    start_point = trajectory[0]\n    mid_point = trajectory[mid_idx]\n    end_point = trajectory[-1]\n\n    start_to_mid_distance = np.linalg.norm(start_point - mid_point)\n    mid_to_end_distance = np.linalg.norm(mid_point - end_point)\n\n    return start_to_mid_distance**2 + mid_to_end_distance**2\n\n\n# Calculate and print sum of squared distances for both trajectories\nno_ot_sum_squared_distance = sum_of_squared_distances(no_ot_traj)\not_sum_squared_distance = sum_of_squared_distances(ot_traj)\n\nprint(\"Sum of Squared Distances (start to mid + mid to end):\")\nprint(f\"Without OT: {no_ot_sum_squared_distance:.4f}\")\nprint(f\"With OT: {ot_sum_squared_distance:.4f}\")\n</pre> def sum_of_squared_distances(trajectory):     \"\"\"Calculate the sum of squared distances from start to mid and mid to end of a trajectory.      Parameters:     - trajectory: A numpy array of shape (N, D) where N is the number of points                   in the trajectory and D is the dimensionality of the space.      Returns:     - Sum of squared distances (start to mid + mid to end).     \"\"\"     mid_idx = len(trajectory) // 2     start_point = trajectory[0]     mid_point = trajectory[mid_idx]     end_point = trajectory[-1]      start_to_mid_distance = np.linalg.norm(start_point - mid_point)     mid_to_end_distance = np.linalg.norm(mid_point - end_point)      return start_to_mid_distance**2 + mid_to_end_distance**2   # Calculate and print sum of squared distances for both trajectories no_ot_sum_squared_distance = sum_of_squared_distances(no_ot_traj) ot_sum_squared_distance = sum_of_squared_distances(ot_traj)  print(\"Sum of Squared Distances (start to mid + mid to end):\") print(f\"Without OT: {no_ot_sum_squared_distance:.4f}\") print(f\"With OT: {ot_sum_squared_distance:.4f}\") <pre>Sum of Squared Distances (start to mid + mid to end):\nWithout OT: 2318.0609\nWith OT: 1895.0890\n</pre> In\u00a0[13]: Copied! <pre># Define helper functions\ndef rotation_matrix(angle):\n    theta = (angle / 180.0) * np.pi\n    c, s = np.cos(theta), np.sin(theta)\n    return np.array([[c, -s], [s, c]])\n\n\ndef rotate(x, angle):\n    R = rotation_matrix(angle)\n    return x @ R.T\n\n\ndef plot_quadrilateral(x, axis, color=\"C0\", marker=\"o\", label=None):\n    assert x.shape == (4, 2)\n    axis.scatter(x[:, 0], x[:, 1], c=color, marker=marker, linewidths=1, edgecolors=\"k\", zorder=2, label=label)\n    for i in range(len(x)):\n        if i &lt; 3:\n            axis.plot([x[i, 0], x[i + 1, 0]], [x[i, 1], x[i + 1, 1]], c=color, zorder=1)\n        else:\n            axis.plot([x[i, 0], x[0, 0]], [x[i, 1], x[0, 1]], c=color, zorder=1)\n    return axis\n</pre> # Define helper functions def rotation_matrix(angle):     theta = (angle / 180.0) * np.pi     c, s = np.cos(theta), np.sin(theta)     return np.array([[c, -s], [s, c]])   def rotate(x, angle):     R = rotation_matrix(angle)     return x @ R.T   def plot_quadrilateral(x, axis, color=\"C0\", marker=\"o\", label=None):     assert x.shape == (4, 2)     axis.scatter(x[:, 0], x[:, 1], c=color, marker=marker, linewidths=1, edgecolors=\"k\", zorder=2, label=label)     for i in range(len(x)):         if i &lt; 3:             axis.plot([x[i, 0], x[i + 1, 0]], [x[i, 1], x[i + 1, 1]], c=color, zorder=1)         else:             axis.plot([x[i, 0], x[0, 0]], [x[i, 1], x[0, 1]], c=color, zorder=1)     return axis In\u00a0[14]: Copied! <pre># Initialize\nk0 = np.array(\n    [\n        [[-2, 0], [0, 1], [2, 0], [0, -1]],  # Rhombus\n        [[-1, 2], [-1, 4], [1, 4], [1, 2]],  # Square\n    ]\n)\nangles = [60, 25]\n\n# Rotate and shuffle samples in k0 to create k1\nk1 = np.array([rotate(k0[i], angles[i]) for i in [1, 0]])\nmarkers = [\"o\", \"s\"]\n\n# Translate k0 and k1\nk0 = np.array(k0) - 2\nk1 = np.array(k1) + 2\n</pre> # Initialize k0 = np.array(     [         [[-2, 0], [0, 1], [2, 0], [0, -1]],  # Rhombus         [[-1, 2], [-1, 4], [1, 4], [1, 2]],  # Square     ] ) angles = [60, 25]  # Rotate and shuffle samples in k0 to create k1 k1 = np.array([rotate(k0[i], angles[i]) for i in [1, 0]]) markers = [\"o\", \"s\"]  # Translate k0 and k1 k0 = np.array(k0) - 2 k1 = np.array(k1) + 2 In\u00a0[15]: Copied! <pre># Plot k0 and k1\nfig, ax = plt.subplots(1, 1, figsize=(5, 5))\nfor i in range(len(k0)):\n    plot_quadrilateral(k0[i], ax, color=\"C1\", marker=markers[i], label=\"$k_0^%d$\" % i)\n    plot_quadrilateral(k1[i], ax, color=\"C0\", marker=markers[i], label=\"$k_1^%d$\" % i)\n    # Calculate centroids of k0 and k1\n    centroid_k0 = np.mean(k0[i], axis=0)\n    centroid_k1 = np.mean(k1[i], axis=0)\n\n    # Plot a red line connecting the centroids\n    ax.plot(*zip(centroid_k0, centroid_k1), color=\"red\", linewidth=1, linestyle=\"--\")\nax.legend()\nax.set_aspect(\"equal\", adjustable=\"box\")\n</pre> # Plot k0 and k1 fig, ax = plt.subplots(1, 1, figsize=(5, 5)) for i in range(len(k0)):     plot_quadrilateral(k0[i], ax, color=\"C1\", marker=markers[i], label=\"$k_0^%d$\" % i)     plot_quadrilateral(k1[i], ax, color=\"C0\", marker=markers[i], label=\"$k_1^%d$\" % i)     # Calculate centroids of k0 and k1     centroid_k0 = np.mean(k0[i], axis=0)     centroid_k1 = np.mean(k1[i], axis=0)      # Plot a red line connecting the centroids     ax.plot(*zip(centroid_k0, centroid_k1), color=\"red\", linewidth=1, linestyle=\"--\") ax.legend() ax.set_aspect(\"equal\", adjustable=\"box\") In\u00a0[16]: Copied! <pre># Initialize the Kabsch OT Sampler\nkabsch_ot_sampler = EquivariantOTSampler(method=\"exact\", num_threads=1)\n# Sample new pairs from the EquivariantOTSampler, mask is not used in this example\n# Replace is set to False, so no duplicates are allowed\n# Sort is set to \"x0\", so the order of output x0 is the same as input x0\nkabsch_k0, kabsch_k1, mask = kabsch_ot_sampler.apply_augmentation(\n    torch.Tensor(k0), torch.Tensor(k1), mask=None, replace=False, sort=\"x0\"\n)\n# Convert the sampled tensors to numpy arrays\nkabsch_k0 = kabsch_k0.numpy()\nkabsch_k1 = kabsch_k1.numpy()\n</pre> # Initialize the Kabsch OT Sampler kabsch_ot_sampler = EquivariantOTSampler(method=\"exact\", num_threads=1) # Sample new pairs from the EquivariantOTSampler, mask is not used in this example # Replace is set to False, so no duplicates are allowed # Sort is set to \"x0\", so the order of output x0 is the same as input x0 kabsch_k0, kabsch_k1, mask = kabsch_ot_sampler.apply_augmentation(     torch.Tensor(k0), torch.Tensor(k1), mask=None, replace=False, sort=\"x0\" ) # Convert the sampled tensors to numpy arrays kabsch_k0 = kabsch_k0.numpy() kabsch_k1 = kabsch_k1.numpy() In\u00a0[17]: Copied! <pre># Plot newly sampled k0 and k1, note that k1 is rotated to match k0\nfig, ax = plt.subplots(1, 1, figsize=(5, 5))\nfor i in range(len(kabsch_k0)):\n    plot_quadrilateral(kabsch_k0[i], ax, color=\"C1\", marker=markers[i], label=\"$k_0^%d$\" % i)\n    plot_quadrilateral(kabsch_k1[i], ax, color=\"C0\", marker=markers[i], label=\"$k_1^%d$\" % i)\n    # Calculate centroids of k0 and k1\n    # Calculate centroids of k0 and k1\n    centroid_k0 = np.mean(kabsch_k0[i], axis=0)\n    centroid_k1 = np.mean(kabsch_k1[i], axis=0)\n\n    # Plot a red line connecting the centroids\n    ax.plot(*zip(centroid_k0, centroid_k1), color=\"red\", linewidth=1, linestyle=\"--\")\nax.legend()\nax.set_aspect(\"equal\", adjustable=\"box\")\n</pre> # Plot newly sampled k0 and k1, note that k1 is rotated to match k0 fig, ax = plt.subplots(1, 1, figsize=(5, 5)) for i in range(len(kabsch_k0)):     plot_quadrilateral(kabsch_k0[i], ax, color=\"C1\", marker=markers[i], label=\"$k_0^%d$\" % i)     plot_quadrilateral(kabsch_k1[i], ax, color=\"C0\", marker=markers[i], label=\"$k_1^%d$\" % i)     # Calculate centroids of k0 and k1     # Calculate centroids of k0 and k1     centroid_k0 = np.mean(kabsch_k0[i], axis=0)     centroid_k1 = np.mean(kabsch_k1[i], axis=0)      # Plot a red line connecting the centroids     ax.plot(*zip(centroid_k0, centroid_k1), color=\"red\", linewidth=1, linestyle=\"--\") ax.legend() ax.set_aspect(\"equal\", adjustable=\"box\")"},{"location":"main/examples/bionemo-moco/ot_sampler_tutorial/#optimal-transport-samplers-tutorial","title":"Optimal Transport Samplers Tutorial\u00b6","text":""},{"location":"main/examples/bionemo-moco/ot_sampler_tutorial/#task-setup","title":"Task Setup\u00b6","text":""},{"location":"main/examples/bionemo-moco/ot_sampler_tutorial/#demonstrating-the-effectiveness-of-ot-sampler-and-kabsch-based-equivariant-ot-sampler","title":"Demonstrating the effectiveness of OT sampler and Kabsch-based Equivariant OT sampler\u00b6","text":""},{"location":"main/examples/bionemo-moco/ot_sampler_tutorial/#1-we-will-start-with-the-ot-sampler-the-ot-sampler-is-an-implementation-of-the-ot-cfm-algorithm-proposed-by-tong-et-al-for-a-batch-of-randomly-sampled-noise-mathrmx_0-and-data-mathrmx_1-the-ot-sampler-will-sample-x_0-x_1-pairs-based-on-their-euclidean-distances-we-will-demonstrate-how-to-use-the-ot-sampler-with-a-simple-2d-example","title":"1. We will start with the OT sampler. The OT sampler is an implementation of the \"OT-CFM\" algorithm proposed by Tong et. al. For a batch of randomly sampled noise ($\\mathrm{x}_0$) and data ($\\mathrm{x}_1$), the OT sampler will sample $(x_0, x_1)$ pairs based on their Euclidean distances. We will demonstrate how to use the OT sampler with a simple 2D example.\u00b6","text":""},{"location":"main/examples/bionemo-moco/ot_sampler_tutorial/#11-sample-100-points-from-a-standard-gaussian-distribution-mathrmx_0-sim-pi_0-orange-colored-and-another-100-points-from-a-double-moon-shape-distribution-mathrmx_1-sim-pi_1-blue-colored-the-linear-interpolation-between-pairs-x_0i-x_1i-are-plotted-using-grey-lines","title":"1.1 Sample 100 points from a standard Gaussian distribution ($\\mathrm{x}_0 \\sim \\pi_0$, orange colored), and another 100 points from a double moon-shape distribution ($\\mathrm{x}_1 \\sim \\pi_1$, blue colored). The linear interpolation between pairs ($x_0^i, x_1^i$) are plotted using grey lines.\u00b6","text":""},{"location":"main/examples/bionemo-moco/ot_sampler_tutorial/#12-initialize-the-ot-sampler-and-sample-new-x_0-x_1-pairs-to-minimize-the-transport-cost-of-the-entire-batch-the-linear-interpolation-between-new-pairs-x_0i-x_1i-are-plotted-using-grey-lines-we-can-see-that-there-are-less-crossover-of-interpolation-trajectories-and-the-transport-cost-has-been-reduced","title":"1.2 Initialize the OT sampler and sample new $(x_0, x_1)$ pairs to minimize the transport cost of the entire batch. The linear interpolation between new pairs ($x_0^i, x_1^i$) are plotted using grey lines. We can see that there are less crossover of interpolation trajectories and the transport cost has been reduced.\u00b6","text":""},{"location":"main/examples/bionemo-moco/ot_sampler_tutorial/#13-lets-see-how-the-ot-can-help-in-conditional-flow-matching-training-we-will-train-two-models-one-with-ot-and-the-other-one-without-and-compare-the-flow-trajectory-during-sampling","title":"1.3 Let's see how the OT can help in conditional flow matching training. We will train two models, one with OT and the other one without, and compare the flow trajectory during sampling.\u00b6","text":"<p>Note the ContinuousFlowMatcher object can be initialized with any batch augmentation using the 'ot_type' parameter. For clarity we pull in our previosuly initialized OT Sampler.</p>"},{"location":"main/examples/bionemo-moco/ot_sampler_tutorial/#14-visualization-of-flow-trajectories-predicted-by-the-two-models-with-ot-left-the-flow-trajectory-is-straighter-thus-less-transport-cost-comapred-to-without-ot-right","title":"1.4 Visualization of flow trajectories predicted by the two models. With OT (left), the flow trajectory is straighter, thus less transport cost comapred to without OT (right).\u00b6","text":""},{"location":"main/examples/bionemo-moco/ot_sampler_tutorial/#2-we-will-then-introduce-the-kabsch-ot-sampler-the-kabsch-ot-sampler-is-an-implementation-of-the-equivariant-ot-algorithm-klein-et-al-for-a-batch-of-randomly-sampled-noise-mathrmx_0-and-data-mathrmx_1-the-kabsch-ot-sampler-will-sample-x_0-x_1-pairs-based-on-the-rmsd-after-aligning-zero-centered-x_0-x_1-using-kabsch-algorithm-we-will-demonstrate-how-to-use-the-kabsch-ot-sampler-with-a-simple-2d-example","title":"2. We will then introduce the Kabsch OT sampler. The Kabsch OT sampler is an implementation of the \"Equivariant OT\" algorithm (Klein et al.). For a batch of randomly sampled noise ($\\mathrm{x}_0$) and data ($\\mathrm{x}_1$), the Kabsch OT sampler will sample $(x_0, x_1)$ pairs based on the RMSD after aligning zero-centered $(x_0, x_1)$ using Kabsch algorithm. We will demonstrate how to use the Kabsch OT sampler with a simple 2D example.\u00b6","text":""},{"location":"main/examples/bionemo-moco/ot_sampler_tutorial/#21-initialize-mathrmk_0-which-contains-two-samples-k_00-is-a-rhombus-and-k_01-is-a-square-then-initialize-mathrmk_1-which-is-rotated-mathrmk_0-shuffle-the-order-of-mathrmk_1-so-k_10-is-rotated-square-and-k_11-is-rotated-rhombus-when-plotting-the-k_00-and-k_10-are-shown-with-circle-shaped-dots-while-k_01-and-k_11-are-shown-with-square-shaped-dots","title":"2.1 Initialize $\\mathrm{k}_0$ which contains two samples. $k_0^0$ is a rhombus and $k_0^1$ is a square. Then initialize $\\mathrm{k}_1$ which is rotated $\\mathrm{k}_0$. Shuffle the order of $\\mathrm{k}_1$ so $k_1^0$ is rotated square and $k_1^1$ is rotated rhombus. When plotting, the $k_0^0$ and $k_1^0$ are shown with circle-shaped dots while $k_0^1$ and $k_1^1$ are shown with square-shaped dots.\u00b6","text":""},{"location":"main/examples/bionemo-moco/ot_sampler_tutorial/#we-see-that-we-have-arbitraility-set-up-a-mismatch-the-orange-rhombus-with-circle-dots-is-tied-to-the-blue-rotated-square-with-circle-dots-we-can-use-equivariantot-to-fix-this","title":"We see that we have arbitraility set up a mismatch. The orange rhombus with circle dots is tied to the blue rotated square with circle dots. We can use EquivariantOT to fix this.\u00b6","text":""},{"location":"main/examples/bionemo-moco/ot_sampler_tutorial/#22-initialize-the-kabsch-based-equivariant-ot-sampler-and-sample-new-k_0-k_1-pairs-to-minimize-the-transport-cost-of-the-entire-batch-after-rotational-alignment-we-can-see-that-the-order-of-newly-sampled-mathrmk_1-has-changed-to-match-mathrmk_0-note-that-the-sampled-mathrmk_1-will-be-rotated-but-not-translated","title":"2.2 Initialize the Kabsch-based  Equivariant OT sampler and sample new $(k_0, k_1)$ pairs to minimize the transport cost of the entire batch after rotational alignment. We can see that the order of newly sampled $\\mathrm{k}_1$ has changed to match $\\mathrm{k}_0$. Note that the sampled $\\mathrm{k}_1$ will be rotated but not translated.\u00b6","text":""},{"location":"main/examples/bionemo-moco/ot_sampler_tutorial/#if-you-wanted-to-align-with-respect-to-rotations-and-translations-you-could-center-your-data-or-augment-the-equivariantot-object","title":"If you wanted to align with respect to rotations and translations you could center your data or augment the EquivariantOT object\u00b6","text":""},{"location":"main/examples/bionemo-scdl/example_notebook/","title":"Example notebook","text":"NOTE It takes about 10 minutes to deploy this notebook as a Launchable. As of this writing, we are working on a free tier so a credit card may be required. You can reach out to your NVIDIA rep for credits. In\u00a0[10]: Copied! <pre>import os\nimport tempfile\n\nimport pooch\nfrom torch.utils.data import DataLoader\n\nfrom bionemo.core import BIONEMO_CACHE_DIR\nfrom bionemo.scdl.io.single_cell_memmap_dataset import SingleCellMemMapDataset\nfrom bionemo.scdl.util.torch_dataloader_utils import collate_sparse_matrix_batch\n</pre> import os import tempfile  import pooch from torch.utils.data import DataLoader  from bionemo.core import BIONEMO_CACHE_DIR from bionemo.scdl.io.single_cell_memmap_dataset import SingleCellMemMapDataset from bionemo.scdl.util.torch_dataloader_utils import collate_sparse_matrix_batch <p>First, copy the input data. This can be done by copying https://datasets.cellxgene.cziscience.com/97e96fb1-8caf-4f08-9174-27308eabd4ea.h5ad to a directory named <code>hdf5s</code>.</p> In\u00a0[11]: Copied! <pre>input_data = pooch.retrieve(\n    \"https://datasets.cellxgene.cziscience.com/97e96fb1-8caf-4f08-9174-27308eabd4ea.h5ad\",\n    path=BIONEMO_CACHE_DIR / \"hdf5s\",\n    known_hash=\"a0728e13a421bbcd6b2718e1d32f88d0d5c7cb92289331e3f14a59b7c513b3bc\",\n)\n</pre> input_data = pooch.retrieve(     \"https://datasets.cellxgene.cziscience.com/97e96fb1-8caf-4f08-9174-27308eabd4ea.h5ad\",     path=BIONEMO_CACHE_DIR / \"hdf5s\",     known_hash=\"a0728e13a421bbcd6b2718e1d32f88d0d5c7cb92289331e3f14a59b7c513b3bc\", ) In\u00a0[12]: Copied! <pre># Create a SingleCellMemMapDataset\ndataset_temp_dir = tempfile.TemporaryDirectory()\ndataset_dir = os.path.join(dataset_temp_dir.name, \"97e_scmm\")\n\ndata = SingleCellMemMapDataset(dataset_dir, input_data)\n</pre> # Create a SingleCellMemMapDataset dataset_temp_dir = tempfile.TemporaryDirectory() dataset_dir = os.path.join(dataset_temp_dir.name, \"97e_scmm\")  data = SingleCellMemMapDataset(dataset_dir, input_data) In\u00a0[13]: Copied! <pre># Save the dataset to the disk.\ndata.save()\n</pre> # Save the dataset to the disk. data.save() Out[13]: <pre>True</pre> In\u00a0[14]: Copied! <pre># Reload the data\nreloaded_data = SingleCellMemMapDataset(dataset_dir)\n</pre> # Reload the data reloaded_data = SingleCellMemMapDataset(dataset_dir) <p>There are various numbers of columns per observation. However, for a batch size of 1 the data does not need to be collated. It will then be outputted in a torch tensor of shape (1, 2, num_obs) The first row of lengh num_obs contains the column pointers, and the second row contains the corresponding values.</p> In\u00a0[15]: Copied! <pre>model = lambda x: x  # noqa: E731\n\ndataloader = DataLoader(data, batch_size=1, shuffle=True, collate_fn=collate_sparse_matrix_batch)\nn_epochs = 1\nfor e in range(n_epochs):\n    for batch in dataloader:\n        model(batch)\n</pre> model = lambda x: x  # noqa: E731  dataloader = DataLoader(data, batch_size=1, shuffle=True, collate_fn=collate_sparse_matrix_batch) n_epochs = 1 for e in range(n_epochs):     for batch in dataloader:         model(batch) <p>The data can be collated with a batch size of 1 and must be collated with larger batch sizes. This will collate several sparse matrices into the CSR (Compressed Sparse Row) torch tensor format.</p> In\u00a0[16]: Copied! <pre>model = lambda x: x  # noqa: E731\n\ndataloader = DataLoader(data, batch_size=8, shuffle=True, collate_fn=collate_sparse_matrix_batch)\nn_epochs = 1\nfor e in range(n_epochs):\n    for batch in dataloader:\n        model(batch)\n</pre> model = lambda x: x  # noqa: E731  dataloader = DataLoader(data, batch_size=8, shuffle=True, collate_fn=collate_sparse_matrix_batch) n_epochs = 1 for e in range(n_epochs):     for batch in dataloader:         model(batch) <p>For some applications, we might want to also use the features. These can be specified by setting, in the get_row function, return_var_features = True for the per-gene (.var features), and return_obs_features = True for the per-cell (.obs features). By default, all features are returned, but the features can be specified with the var_feature_names and obs_feature_names arguments in get_row, which correspond to a list of the feature names to return.</p> In\u00a0[17]: Copied! <pre>for index in range(len(data)):\n    model(data.get_row(index, return_var_features=True, return_obs_features=True))\n</pre> for index in range(len(data)):     model(data.get_row(index, return_var_features=True, return_obs_features=True)) <p>Alternatively, if there are multiple AnnData files, they can be converted into a single SingleCellMemMapDataset. If the hdf5 directory has one or more AnnData files, the SingleCellCollection class crawls the filesystem to recursively find AnnData files (with the h5ad extension). The code below is in scripts/convert_h5ad_to_scdl.py. It will create a new dataset at example_dataset. This can also be called with the convert_h5ad_to_scdl command.</p> In\u00a0[18]: Copied! <pre># path to dir holding hdf5s data\nhdf5s = BIONEMO_CACHE_DIR / \"hdf5s\"\n\n# path to output dir where SCDataset will be stored\noutput_dir = os.path.join(\"scdataset_output\")\n</pre> # path to dir holding hdf5s data hdf5s = BIONEMO_CACHE_DIR / \"hdf5s\"  # path to output dir where SCDataset will be stored output_dir = os.path.join(\"scdataset_output\") In\u00a0[19]: Copied! <pre>from bionemo.scdl.io.single_cell_collection import SingleCellCollection\n\n\nwith tempfile.TemporaryDirectory() as temp_dir:\n    coll = SingleCellCollection(temp_dir)\n    coll.load_h5ad_multi(hdf5s, max_workers=4, use_processes=True)\n    coll.flatten(output_dir, destroy_on_copy=True)\n</pre> from bionemo.scdl.io.single_cell_collection import SingleCellCollection   with tempfile.TemporaryDirectory() as temp_dir:     coll = SingleCellCollection(temp_dir)     coll.load_h5ad_multi(hdf5s, max_workers=4, use_processes=True)     coll.flatten(output_dir, destroy_on_copy=True) In\u00a0[20]: Copied! <pre>dataset_temp_dir.cleanup()\n</pre> dataset_temp_dir.cleanup()"},{"location":"main/getting-started/","title":"Getting Started","text":""},{"location":"main/getting-started/#repository-structure","title":"Repository structure","text":""},{"location":"main/getting-started/#high-level-overview","title":"High level overview","text":"<p>This repository is structured as a meta-package that collects together many python packages. We designed in this way because this is how we expect our users to use bionemo, as a package that they themselves import and use in their own projects. By structuring code like this ourselves we ensure that bionemo developers follow similar patterns to our end users.</p> <p>Each model is stored in its own <code>sub-packages</code>. Some examples of models include:</p> <ul> <li><code>sub-packages/bionemo-example_model</code>: A minimal example MNIST model that demonstrates how you can write a lightweight   megatron model that doesn't actually support any megatron parallelism, but should run fine as long as you only use   data parallelism to train.</li> </ul> <p>There are also useful utility packages, for example:</p> <ul> <li><code>sub-packages/bionemo-scdl</code>: Single Cell Dataloader (SCDL) provides a dataset implementation that can be used by downstream   single-cell models in the bionemo package.</li> <li><code>sub-packages/bionemo-testing</code>: a suite of utilities that are useful in testing, think <code>torch.testing</code> or <code>np.testing</code>.</li> </ul> <p>Finally some of the packages represent common functions and abstract base classes that expose APIs that are useful for interacting with <code>NeMo2</code>. Some examples of these include:</p> <ul> <li><code>sub-packages/bionemo-core</code>: mostly just high level APIs</li> <li><code>sub-packages/bionemo-llm</code>: ABCs for code that multiple large language models (eg BERT variants) share.</li> </ul> <p>Documentation source is stored in <code>docs/</code></p> <p>The script for building a local docker container is <code>./launch.sh</code> which has some useful commands including:</p> <ul> <li><code>./launch.sh build</code> to build the container</li> <li><code>./launch.sh run</code> to get into a running container with reasonable settings for data/code mounts etc.</li> </ul>"},{"location":"main/getting-started/#more-detailed-structure-notes","title":"More detailed structure notes","text":"<pre><code>$ tree -C -I \"*.pyc\" -I \"test_data\" -I \"test_experiment\" -I \"test_finettune_experiment\" -I __pycache__ -I \"*.egg-info\" -I lightning_logs -I results -I data -I MNIST* -I 3rdparty\n.\n\u251c\u2500\u2500 CODE-REVIEW.md -&gt; docs/CODE-REVIEW.md\n\u251c\u2500\u2500 CODEOWNERS\n\u251c\u2500\u2500 CONTRIBUTING.md -&gt; docs/CONTRIBUTING.md\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 LICENSE\n\u2502   \u251c\u2500\u2500 license.txt\n\u2502   \u2514\u2500\u2500 third_party.txt\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 VERSION\n\u251c\u2500\u2500 ci\n\u2502   \u2514\u2500\u2500 scripts\n\u2502       \u251c\u2500\u2500 nightly_test.sh\n\u2502       \u251c\u2500\u2500 pr_test.sh\n\u2502       \u2514\u2500\u2500 static_checks.sh\n\u251c\u2500\u2500 docs\n\u2502   \u251c\u2500\u2500 CODE-REVIEW.md\n\u2502   \u251c\u2500\u2500 CONTRIBUTING.md\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 docs\n\u2502   \u2502   \u251c\u2500\u2500 assets\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 css\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 color-schemes.css\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 custom-material.css\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 fonts.css\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 images\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 favicon.png\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 logo-icon-black.svg\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 logo-white.svg\n\u2502   \u2502   \u251c\u2500\u2500 developer-guide\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 CODE-REVIEW.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 CONTRIBUTING.md\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 jupyter-notebooks.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 index.md\n\u2502   \u2502   \u2514\u2500\u2500 user-guide\n\u2502   \u2502       \u2514\u2500\u2500 index.md\n\u2502   \u251c\u2500\u2500 mkdocs.yml\n\u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2514\u2500\u2500 scripts\n\u2502       \u2514\u2500\u2500 gen_ref_pages.py\n\u251c\u2500\u2500 launch.sh\n\u251c\u2500\u2500 license_header\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 requirements-cve.txt\n\u251c\u2500\u2500 requirements-dev.txt\n\u251c\u2500\u2500 requirements-test.txt\n# \ud83d\udfe2 All work goes into `sub-packages`\n#  Sub-packages represent individually installable subsets of the bionemo codebase. We recommend that you\n#  create new sub-packages to track your experiments and save any updated models or utilities that you need.\n\u251c\u2500\u2500 sub-packages\n\u2502   \u251c\u2500\u2500 bionemo-core  # \ud83d\udfe2 bionemo-core, and bionemo-llm represent top level sub-packages that do not depend on others\n\u2502   \u2502   \u251c\u2500\u2500 LICENSE\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 setup.py\n\u2502   \u2502   \u251c\u2500\u2500 src  # \ud83d\udfe2 All sub-packages have a `src` and a `test` sub-directory.\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 bionemo\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 core\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 api.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 model\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2502   \u2514\u2500\u2500 config.py\n\u2502   \u2502   \u2502           \u2514\u2500\u2500 utils\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 batching_utils.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 dtypes.py\n\u2502   \u2502   \u2502               \u2514\u2500\u2500 random_utils.py\n\u2502   \u2502   \u2514\u2500\u2500 tests  # \ud83d\udfe2 Test files should be mirrored with `src` files, and have the same name other than `test_[file_name].py`\n\u2502   \u2502       \u2514\u2500\u2500 bionemo\n\u2502   \u2502           \u251c\u2500\u2500 core\n\u2502   \u2502           \u2514\u2500\u2500 pytorch\n\u2502   \u2502               \u2514\u2500\u2500 utils\n\u2502   \u2502                   \u2514\u2500\u2500 test_dtypes.py\n\u2502   \u251c\u2500\u2500 bionemo-example_model  # \ud83d\udfe2 a small example model that demonstrates how to write a megatron model from scratch and train on MNIST\n\u2502   \u2502   \u251c\u2500\u2500 LICENSE\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 _requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 setup.py\n\u2502   \u2502   \u251c\u2500\u2500 src\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 bionemo\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 example_model\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2514\u2500\u2500 lightning_basic.py\n\u2502   \u2502   \u2514\u2500\u2500 tests\n\u2502   \u2502       \u2514\u2500\u2500 bionemo\n\u2502   \u2502           \u2514\u2500\u2500 example_model\n\u2502   \u2502               \u2514\u2500\u2500 test_lightning_basic.py\n\u2502   \u251c\u2500\u2500 bionemo-llm  # \ud83d\udfe2 shared model code for LLM style models, eg BERT variants, transformer variants, etc.\n\u2502   \u2502   \u251c\u2500\u2500 LICENSE\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 _requirements-test.txt\n\u2502   \u2502   \u251c\u2500\u2500 _requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 setup.py\n\u2502   \u2502   \u251c\u2500\u2500 src\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 bionemo\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 llm\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 lightning.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 model\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 biobert\n\u2502   \u2502   \u2502           \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2502   \u2502   \u251c\u2500\u2500 lightning.py\n\u2502   \u2502   \u2502           \u2502   \u2502   \u251c\u2500\u2500 model.py\n\u2502   \u2502   \u2502           \u2502   \u2502   \u251c\u2500\u2500 testing_utils.py\n\u2502   \u2502   \u2502           \u2502   \u2502   \u2514\u2500\u2500 transformer_specs.py\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 config.py\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 layers.py\n\u2502   \u2502   \u2502           \u2502   \u2514\u2500\u2500 loss.py\n\u2502   \u2502   \u2502           \u2514\u2500\u2500 utils\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 datamodule_utils.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 iomixin_utils.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 logger_utils.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 remote.py\n\u2502   \u2502   \u2502               \u2514\u2500\u2500 weight_utils.py\n\u2502   \u2502   \u2514\u2500\u2500 tests\n\u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u2514\u2500\u2500 bionemo\n\u2502   \u2502           \u2514\u2500\u2500 llm\n\u2502   \u2502               \u251c\u2500\u2500 __init__.py\n\u2502   \u2502               \u251c\u2500\u2500 model\n\u2502   \u2502               \u2502   \u251c\u2500\u2500 biobert\n\u2502   \u2502               \u2502   \u2502   \u2514\u2500\u2500 test_transformer_specs.py\n\u2502   \u2502               \u2502   \u2514\u2500\u2500 test_loss.py\n\u2502   \u2502               \u251c\u2500\u2500 test_lightning.py\n\u2502   \u2502               \u2514\u2500\u2500 utils\n\u2502   \u2502                   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502                   \u251c\u2500\u2500 test_datamodule_utils.py\n\u2502   \u2502                   \u251c\u2500\u2500 test_iomixin_utils.py\n\u2502   \u2502                   \u2514\u2500\u2500 test_logger_utils.py\n\u2502   \u251c\u2500\u2500 bionemo-scdl  # \ud83d\udfe2\n\u2502   \u2502   \u251c\u2500\u2500 LICENSE\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 examples\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 example_notebook.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 setup.py\n\u2502   \u2502   \u251c\u2500\u2500 src\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 bionemo\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 scdl\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 api\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2502   \u2514\u2500\u2500 single_cell_row_dataset.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 index\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2502   \u2514\u2500\u2500 row_feature_index.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 io\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 single_cell_collection.py\n\u2502   \u2502   \u2502           \u2502   \u2514\u2500\u2500 single_cell_memmap_dataset.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 scripts\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2502   \u2514\u2500\u2500 convert_h5ad_to_scdl.py\n\u2502   \u2502   \u2502           \u2514\u2500\u2500 util\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 async_worker_queue.py\n\u2502   \u2502   \u2502               \u2514\u2500\u2500 torch_dataloader_utils.py\n\u2502   \u2502   \u2514\u2500\u2500 tests\n\u2502   \u2502       \u2514\u2500\u2500 bionemo\n\u2502   \u2502           \u2514\u2500\u2500 scdl\n\u2502   \u2502               \u251c\u2500\u2500 conftest.py\n\u2502   \u2502               \u251c\u2500\u2500 index\n\u2502   \u2502               \u2502   \u2514\u2500\u2500 test_row_feature_index.py\n\u2502   \u2502               \u251c\u2500\u2500 io\n\u2502   \u2502               \u2502   \u251c\u2500\u2500 test_single_cell_collection.py\n\u2502   \u2502               \u2502   \u2514\u2500\u2500 test_single_cell_memmap_dataset.py\n\u2502   \u2502               \u2514\u2500\u2500 util\n\u2502   \u2502                   \u251c\u2500\u2500 test_async_worker_queue.py\n\u2502   \u2502                   \u2514\u2500\u2500 test_torch_dataloader_utils.py\n\u2502   \u251c\u2500\u2500 bionemo-testing\n\u2502   \u2502   \u251c\u2500\u2500 LICENSE\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 _requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 setup.py\n\u2502   \u2502   \u251c\u2500\u2500 src\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 bionemo\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 testing\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 callbacks.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 harnesses\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2502   \u2514\u2500\u2500 stop_and_go.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 megatron_parallel_state_utils.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 testing_callbacks.py\n\u2502   \u2502   \u2502           \u2514\u2500\u2500 utils.py\n\u2502   \u2502   \u2514\u2500\u2500 tests\n\u2502   \u2502       \u2514\u2500\u2500 bionemo\n\u2502   \u2502           \u2514\u2500\u2500 testing\n\u2502   \u2502               \u2514\u2500\u2500 test_megatron_parallel_state_utils.py\n\u2502   \u2514\u2500\u2500 bionemo-webdatamodule\n\u2502       \u251c\u2500\u2500 LICENSE\n\u2502       \u251c\u2500\u2500 README.md\n\u2502       \u251c\u2500\u2500 pyproject.toml\n\u2502       \u251c\u2500\u2500 requirements.txt\n\u2502       \u251c\u2500\u2500 setup.py\n\u2502       \u251c\u2500\u2500 src\n\u2502       \u2502   \u2514\u2500\u2500 bionemo\n\u2502       \u2502       \u2514\u2500\u2500 webdatamodule\n\u2502       \u2502           \u251c\u2500\u2500 __init__.py\n\u2502       \u2502           \u251c\u2500\u2500 datamodule.py\n\u2502       \u2502           \u2514\u2500\u2500 utils.py\n\u2502       \u2514\u2500\u2500 tests\n\u2502           \u2514\u2500\u2500 bionemo\n\u2502               \u2514\u2500\u2500 webdatamodule\n\u2502                   \u251c\u2500\u2500 __init__.py\n\u2502                   \u251c\u2500\u2500 conftest.py\n\u2502                   \u2514\u2500\u2500 test_datamodule.py\n</code></pre>"},{"location":"main/getting-started/#installation","title":"Installation","text":""},{"location":"main/getting-started/#initializing-3rd-party-dependencies-as-git-submodules","title":"Initializing 3rd-party dependencies as git submodules","text":"<p>For development, the NeMo and Megatron-LM dependencies are vendored in the bionemo-2 repository workspace as git submodules. The pinned commits for these submodules represent the \"last-known-good\" versions of these packages that are confirmed to be working with bionemo2 (and those that are tested in CI).</p> <p>To initialize these sub-modules when cloning the repo, add the <code>--recursive</code> flag to the git clone command:</p> <pre><code>git clone --recursive git@github.com:NVIDIA/bionemo-framework.git\n</code></pre> <p>To download the pinned versions of these submodules within an existing git repository, run</p> <pre><code>git submodule update --init --recursive\n</code></pre>"},{"location":"main/getting-started/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Hardware and Software Prerequisites</li> <li>Access and Startup</li> <li>Initialization Guide</li> <li>Development</li> <li>Training Models</li> </ul>"},{"location":"main/getting-started/access-startup/","title":"Access and Startup BioNeMo Framework","text":"<p>The BioNeMo Framework is free-to-use and easily accessible. We recommend accessing the software through the BioNeMo Docker container, which provides a seamless and hassle-free way to develop and execute code. By using the Docker container, you can bypass the complexity of handling dependencies, ensuring that you have a consistent and reproducible environment for your projects.</p> <p>In this section of the documentation, we will guide you through the process of pulling the BioNeMo Docker container and setting up a local development environment. By following these steps, you will be able to quickly get started with the BioNeMo Framework and begin exploring its features and capabilities.</p>"},{"location":"main/getting-started/access-startup/#access-the-bionemo-framework","title":"Access the BioNeMo Framework","text":""},{"location":"main/getting-started/access-startup/#brevdev-access","title":"Brev.Dev Access","text":"<p>The BioNeMo Framework container can run in a brev.dev launchable: . It takes about 10 minutes to deploy this notebook as a Launchable. After launching the instance, launch a Terminal session in the Jupyter Lab UI.</p> <p>Notes:</p> <ul> <li>This links to the nightly release and may be out of sync with these docs.</li> <li>Access to Brev.Dev requires credit card information.</li> </ul>"},{"location":"main/getting-started/access-startup/#ngc-account-and-api-key-configuration","title":"NGC Account and API Key Configuration","text":"<p>You can also access the BioNeMo Framework container using a free NVIDIA GPU Cloud (NGC) account and an API key linked to that account.</p> <p>NGC is a portal of enterprise services, software, and support for artificial intelligence and high-performance computing (HPC) workloads. The BioNeMo Docker container is hosted on the NGC Container Registry. To pull and run a container from this registry, you will need to create a free NGC account and an API Key using the following steps:</p> <ol> <li> <p>Create a free account on NGC and log in.</p> </li> <li> <p>At the top right, click on the User &gt; Setup &gt; Generate API Key, then click + Generate API Key and    Confirm. Copy and store your API Key in a secure location.</p> </li> </ol> <p>You can now view the BioNeMo Framework container at this direct link in the NGC Catalog or by searching the NGC Catalog for \u201cBioNeMo Framework\u201d. You can also explore the other resources available to you in the catalog.</p>"},{"location":"main/getting-started/access-startup/#ngc-cli-configuration","title":"NGC CLI Configuration","text":"<p>The NGC Command Line Interface (CLI) is a command-line tool for managing resources in NGC, including datasets and model checkpoints. You can download the CLI on your local machine using the instructions on the NGC CLI website.</p> <p>Once you have installed the NGC CLI, run <code>ngc config set</code> at the command line to setup your NGC credentials:</p> <ul> <li>API key: Enter your API Key</li> <li>CLI output: Accept the default (ASCII format) by pressing <code>Enter</code></li> <li>org: Choose your preferred organization from the supplied list</li> <li>team: Choose the team to which you have been assigned from the supplied list</li> <li>ace : Choose an ACE, if applicable, otherwise press <code>Enter</code> to continue</li> </ul> <p>Note: The org and team commands are only relevant when pulling private containers/datasets from NGC created by you or your team. To access BioNeMo Framework, you can use the default value.</p>"},{"location":"main/getting-started/access-startup/#startup-instructions","title":"Startup Instructions","text":"<p>BioNeMo is compatible with a wide variety of computing environments, including both local workstations, data centers, and Cloud Service Providers (CSPs), such as Amazon Web Services, Microsoft Azure, Google Cloud Platform, and Oracle Cloud Infrastructure, and NVIDIA\u2019s own DGX Cloud.</p>"},{"location":"main/getting-started/access-startup/#running-the-container-on-a-local-machine","title":"Running the Container on a Local Machine","text":"<p>To run the BioNeMo Framework container on a local workstation:</p> <ol> <li>Pull the BioNeMo Framework container using the following command:</li> </ol> <pre><code>docker pull nvcr.io/nvidia/clara/bionemo-framework:nightly\n</code></pre> <ol> <li>Run it as you would a normal Docker container. For    example, to get basic shell access you can run the following command:</li> </ol> <pre><code>docker run --rm -it --gpus all \\\n  nvcr.io/nvidia/clara/bionemo-framework:nightly \\\n  /bin/bash\n</code></pre> <p>Because BioNeMo is distributed as a Docker container, standard arguments can be passed to the <code>docker run</code> command to alter the behavior of the container and its interactions with the host system. For more information on these arguments, refer to the Docker documentation.</p> <p>Refer to the next section, Initialization Guide, for useful <code>docker run</code> command variants for common workflows.</p>"},{"location":"main/getting-started/access-startup/#running-on-any-major-csp-with-the-nvidia-gpu-optimized-vmi","title":"Running on Any Major CSP with the NVIDIA GPU-Optimized VMI","text":"<p>The BioNeMo Framework container is supported on cloud-based GPU instances through the NVIDIA GPU-Optimized Virtual Machine Image (VMI), available for AWS, GCP, Azure, and OCI. NVIDIA VMIs are built on Ubuntu and provide a standardized operating system environment across cloud infrastructure for running NVIDIA GPU-accelerated software. These images are pre-configured with software dependencies, such as NVIDIA GPU drivers, Docker, and the NVIDIA Container Toolkit. For more information about NVIDIA VMIs, refer to the NGC Catalog.</p> <p>The general steps for launching the BioNeMo Framework container using a CSP are as follows:</p> <ol> <li>Launch a GPU-equipped instance running the NVIDIA GPU-Optimized VMI on your preferred CSP. Follow the instructions for    launching a GPU-equipped instance provided by your CSP.</li> <li>Connect to the running instance using SSH and run the BioNeMo Framework container exactly as outlined in the    Running the Container on a Local Machine section on    the Access and Startup page.</li> </ol>"},{"location":"main/getting-started/development/","title":"Development with BioNeMo","text":"<p>On this page, we will cover the organization of the codebase and the setup necessary for the two primary development workflows for users of the BioNeMo Framework: training and fine-tuning models. For both of these workflows, we recommend setting the <code>NGC_CLI_API_KEY</code> environment variable as discussed in the Initialization Guide. This environment variable is required by the script that will be used to download both model checkpoints and data from NGC to be used in these workflows.</p>"},{"location":"main/getting-started/development/#bionemo-code-overview","title":"BioNeMo Code Overview","text":"<p>The BioNeMo codebase is structured as a meta-package that collects together many Python packages. We designed BioNeMo this way with the expectation that users will import and use BioNeMo in their own projects. By structuring code this way, we ensure that BioNeMo developers follow similar patterns to those we expect of our end users.</p> <p>Each model is stored in its own subdirectory of <code>sub-packages</code>. Some examples of models include:</p> <ul> <li><code>bionemo-example_model</code>: A minimal example MNIST model that demonstrates how you can write a lightweight   Megatron model that does not actually support any megatron parallelism, but should run fine as long as you only use   data parallelism to train.</li> </ul> <p>We also include useful utility packages, for example:</p> <ul> <li><code>bionemo-scdl</code>: Single Cell Dataloader (SCDL) provides a dataset implementation that can be used by   downstream single-cell models in the bionemo package.</li> <li><code>bionemo-testing</code>: A suite of utilities that are useful in testing, think <code>torch.testing</code> or <code>np.testing</code>.</li> </ul> <p>Finally some of the packages represent common functions and abstract base classes that expose APIs that are useful for interacting with <code>NeMo2</code>. Some examples of these include:</p> <ul> <li><code>bionemo-core</code>: High-level APIs</li> <li><code>bionemo-llm</code>: Abstract base classes for code that multiple large language models (eg BERT variants) share.</li> </ul>"},{"location":"main/getting-started/development/#package-structure","title":"Package Structure","text":"<p>Within each of the Bionemo packages, a consistent structure is employed to facilitate organization and maintainability. The following components are present in each package:</p> <ul> <li><code>pyproject.toml</code>: Defines package metadata, including version, package name, and executable scripts to be installed.</li> <li><code>src</code>: Contains all source code for the package. Each package features a top-level <code>bionemo</code> folder, which serves   as the primary namespace for imports. During the build process, all <code>bionemo/*</code> source files are combined into a   single package, with unique subdirectory names appended to the <code>bionemo</code> directory.</li> <li><code>tests</code>: Houses all package tests. The convention for test files is to locate them in the same path as the   corresponding <code>src</code> file, but within the <code>tests</code> directory, with a <code>test_</code> prefix added to the test file name. For   example, to test a module-level file <code>src/bionemo/my_module</code>, a test file <code>tests/bionemo/test_my_module.py</code> should   be created. Similarly, to test a specific file <code>src/bionemo/my_module/my_file.py</code>, the test file should be named   <code>tests/bionemo/my_module/test_my_file.py</code>. Running <code>py.test sub-packages/my_package</code> will execute all tests within   the <code>tests</code> directory.</li> <li><code>examples</code>: Some packages include an <code>examples</code> directory containing Jupyter Notebook (<code>.ipynb</code>) files, which are   aggregated into the main documentation.</li> <li><code>README.md</code>: The core package README file serves as the primary documentation for each sub-package when uploaded   to PyPI.</li> <li><code>LICENSE</code>: For consistency, all Bionemo packages should utilize the Apache-2.0 license. By contributing code to   BioNeMo, you acknowledge permission for the code to be re-released under an Apache v2 license.</li> </ul>"},{"location":"main/getting-started/development/#model-training-process","title":"Model Training Process","text":"<p>Note</p> <p>See also Training Models</p> <p>The process for pretraining models from BioNeMo involves running scripts located in the <code>scripts</code> directory. Each script exposes a Command-Line Interface (CLI) that contains and documents the options available for that model.</p> <p>5D Parallel Training Moved to bionemo-recipes</p> <p>The 5D parallel training implementations for ESM-2 and Geneformer have been migrated to simplified TransformerEngine + FSDP implementations in bionemo-recipes. For training these models, please refer to the recipes in <code>bionemo-recipes/recipes/</code>.</p> <p>To pretrain a model, you need to run the corresponding script with the required parameters. For example, to pretrain ESM-2 models using the 5D parallel implementation, refer to the <code>esm2_native_te</code> recipe in bionemo-recipes.</p> <p>The scripts provide various options that can be customized for pretraining, such as:</p> <ul> <li>Data directories and paths</li> <li>Model checkpoint paths</li> <li>Experiment names for tracking</li> <li>Number of GPUs and nodes</li> <li>Validation check intervals</li> <li>Number of dataset workers</li> <li>Number of steps</li> <li>Sequence lengths</li> <li>Micro-batch sizes</li> <li>Limit on validation batches</li> </ul> <p>You can specify these options when running the script using command-line arguments. For each of the available scripts, you can use the <code>--help</code> option for an explanation of the available options for that model.</p> <p>For more information on pretraining a model, refer to the ESM-2 Pretraining Tutorial.</p>"},{"location":"main/getting-started/development/#fine-tuning","title":"Fine-Tuning","text":"<p>The model fine-tuning process involves downloading the required model checkpoints using the <code>download_bionemo_data</code> script. This script takes in the model name and version as arguments, along with the data source, which can be either <code>ngc</code> (default) or <code>pbss</code> for NVIDIA employees.</p> <p>To view a list of available resources (both model checkpoints and datasets), you can use the following command:</p> <pre><code>download_bionemo_data --list-resources\n</code></pre>"},{"location":"main/getting-started/development/#step-1-download-data-and-checkpoints","title":"Step 1: Download Data and Checkpoints","text":"<p>To download the data and checkpoints, use the following command:</p> <pre><code>export DATA_SOURCE=\"ngc\"\nMODEL_CKPT=$(download_bionemo_data &lt;model_name&gt;/&lt;checkpoint_name&gt;:&lt;version&gt; --source $DATA_SOURCE);\n</code></pre> <p>Replace <code>&lt;model_name&gt;</code> with the desired model (for example, <code>esm2</code>), <code>&lt;version&gt;</code> with the desired version, and <code>&lt;checkpoint_name&gt;</code> with the desired checkpoint name.</p> <p>Additionally, you can download available datasets from NGC using the following command, making similar substitutions as with the model checkpoint download command above:</p> <pre><code>TEST_DATA_DIR=$(download_bionemo_data &lt;model_name&gt;/testdata:&lt;version&gt; --source $DATA_SOURCE);\n</code></pre> <p>Alternatively, you can use your own data by configuring your container run with volume mounts as discussed in the Initialization Guide.</p>"},{"location":"main/getting-started/development/#step-2-adapt-the-training-process","title":"Step 2: Adapt the Training Process","text":"<p>Fine-tuning may involve specifying a different combination of model and loss than was used to train the initial version of the model. The fine-tuning steps will be application-specific, but a general set of steps include:</p> <ol> <li>Prepare your dataset: Collect and prepare your dataset, including the sequence data and target values. This step is    crucial to ensure that your dataset is in a format that can be used for training.</li> <li>Create a custom dataset class: Define a custom dataset class that can handle your specific data format. This class should    be able to initialize the dataset and retrieve individual data points.</li> <li>Create a datamodule: Define a datamodule that prepares the data for training. This includes tasks such as data loading,    tokenization, and batching.</li> <li>Fine-tune the model: Use a pre-trained model as a starting point and fine-tune it on your dataset. This involves    adjusting the model's parameters to fit your specific task and dataset.</li> <li>Configure the fine-tuning: Set various hyperparameters for the fine-tuning process, such as the batch size, number of    training steps, and learning rate. These hyperparameters can significantly affect the performance of the fine-tuned    model.</li> <li>Run inference: Once the model is fine-tuned, use it to make predictions on new, unseen data.</li> </ol> <p>For more information on fine-tuning a model, refer to the ESM-2 Fine-tuning Tutorial.</p>"},{"location":"main/getting-started/development/#advanced-developer-documentation","title":"Advanced Developer Documentation","text":"<p>For advanced development information (for example, developing the source code of BioNeMo), refer to the [README](https://github.com/NVIDIA/bionemo-framework) found on the main page of the BioNeMo GitHub Repository.</p>"},{"location":"main/getting-started/initialization-guide/","title":"Initialization Guide","text":"<p>Note</p> <pre><code>Prior to beginning this section, you must confirm that your computing platform meets or exceeds the prerequisites\noutlined in the [Hardware and Software Prerequisites](./pre-reqs.md) page and that you have already pulled and\nverified that you can run the BioNeMo container as outlined in the [Access and Startup](./access-startup.md) page.\n</code></pre> <p>At this point, you have successfully launched and run the Docker container. This section will guide you through setting up your host machine environment, suggest Docker commands for various common workflows, and explain helpful <code>docker run</code> options.</p>"},{"location":"main/getting-started/initialization-guide/#setting-up-your-host-machine-environment","title":"Setting Up Your Host Machine Environment","text":"<p>To effectively use the BioNeMo Framework, we recommend an organized environment configuration and directory structure. Specifically, we recommend having several cache directories per project. These directories will contain project files such as data, model checkpoints, training scripts, and outputs such as logs and predictions. To facilitate container set up, we recommend storing the paths to these directories in a <code>.env</code> file that can be referenced at container runtime. Below, we suggest useful environment variables to define in this file.</p>"},{"location":"main/getting-started/initialization-guide/#creating-a-env-file-for-first-time-setup","title":"Creating a .env File For First Time Setup","text":"<p>We recommend using a <code>.env</code> file in your local workspace to define environment variables. Specifically, the following variables are useful to include in your <code>.env</code> file:</p> <pre><code># Local Cache Directories\nLOCAL_RESULTS_PATH\nDOCKER_RESULTS_PATH\nLOCAL_DATA_PATH\nDOCKER_DATA_PATH\nLOCAL_MODELS_PATH\nDOCKER_MODELS_PATH\n\n# Desired Jupyter Port\nJUPYTER_PORT\n\n# NGC Configuration Settings\nNGC_CLI_API_KEY\nNGC_CLI_ORG\nNGC_CLI_TEAM\nNGC_CLI_FORMAT_TYPE\n\n# Weights and Biases API Key\nWANDB_API_KEY\n</code></pre> <p>For each of these variables, you can define them inside the <code>.env</code> file using <code>=</code>. For example, you can set the NGC API key using <code>NGC_CLI_API_KEY=&lt;your API key here&gt;</code>. You can then define these variables in your current shell using:</p> <pre><code>source .env\n</code></pre> <p>Running this command will make these variables available for use in the <code>docker run</code> command examples shown below.</p> <p>NGC Credentials Required for Data Download</p> <pre><code>Some of the credentials in the above `.env` file are optional for specific workflows. However, if you intend to use\ndata hosted on the NGC platform (for example, model checkpoints and example training data), you _must_ define both\nNGC_CLI_API_KEY and NGC_CLI_ORG at container run time. The easiest way to ensure these variables are set is to use\nthe `.env` file as shown here with your specific variable definitions.\n</code></pre> <p>Refer to the list below for an explanation of each of these variables:</p> <ul> <li><code>LOCAL_RESULTS_PATH</code> and <code>DOCKER_RESULTS_PATH</code>: Paths for storing results, with <code>LOCAL</code> referring to the path on the   local machine and <code>DOCKER</code> referring to the path inside the Docker container.</li> <li><code>LOCAL_DATA_PATH</code> and <code>DOCKER_DATA_PATH</code>: Paths for storing data, again with <code>LOCAL</code> and <code>DOCKER</code> distinctions.</li> <li><code>LOCAL_MODELS_PATH</code> and <code>DOCKER_MODELS_PATH</code>: Paths for storing machine learning models, with the same local and   Docker differences.</li> <li><code>JUPYTER_PORT</code>: The port number for a Jupyter Lab server. The default port is 8888.</li> <li><code>NGC_CLI_API_KEY</code>, <code>NGC_CLI_ORG</code>, <code>NGC_CLI_TEAM</code>, and <code>NGC_CLI_FORMAT_TYPE</code>: API key, organization, team, and format   type for the NVIDIA GPU Cloud (NGC) command-line interface (CLI).</li> <li><code>WANDB_API_KEY</code>: An API key for Weights and Biases (W&amp;B), a platform for machine learning experiment tracking and   visualization.</li> </ul> Weights and Biases Setup (WANDB_API_KEY, Optional) <pre><code>[Weights and Biases](https://wandb.ai/) (W&amp;B) is a machine learning operations platform that provides tools and\nservices to help machine learning practitioners build, train, and deploy models more efficiently. BioNeMo\nis built to work with W&amp;B and requires only simple setup steps to start tracking your experiments. To set up W&amp;B\ninside your container, follow the steps below:\n\n1. Sign up for an account at [Weights and Biases](https://wandb.ai/).\n2. Setup your [API Key](https://docs.wandb.ai/guides/track/public-api-guide#authentication) with W&amp;B.\n3. Set the `WANDB_API_KEY` variable in your `.env` in the same way as you set the previous environment variable\n    above.\n4. Set the environment variable inside your container using the `-e` option, as shown in the next section.\n</code></pre>"},{"location":"main/getting-started/initialization-guide/#starting-the-bionemo-container-for-common-workflows","title":"Starting the BioNeMo Container for Common Workflows","text":"<p>Below we describe some common BioNeMo workflows, including how to setup and run the container in each case. Each of the following examples will assume that you have local workspace directories as defined in your <code>.env</code> file shown above that you will attach to the container via volume mounts.</p>"},{"location":"main/getting-started/initialization-guide/#starting-a-shell-inside-the-container","title":"Starting a Shell Inside the Container","text":"<p>With a shell inside the BioNeMo Docker container, you can execute commands, edit files, and run applications as if you were working directly on the host machine. This self-contained environment allows you to work with your project's dependencies and configurations in isolation, ensuring consistent results and reproducibility. You can install packages, test and debug applications, and customize the environment to suit your needs.</p> <p>You can launch a Bash shell inside the BioNeMo container using the command below. Note that any files modified in the mounted directories while inside the container will persist on the host machine, but other modifications (such as installed software) will not.</p> <pre><code>docker run \\\n  --rm -it \\\n  --gpus all \\\n  --network host \\\n  --shm-size=4g \\\n  -e WANDB_API_KEY \\\n  -e NGC_CLI_API_KEY \\\n  -e NGC_CLI_ORG \\\n  -e NGC_CLI_TEAM \\\n  -e NGC_CLI_FORMAT_TYPE \\\n  -v $LOCAL_DATA_PATH:$DOCKER_DATA_PATH \\\n  -v $LOCAL_MODELS_PATH:$DOCKER_MODELS_PATH \\\n  -v $LOCAL_RESULTS_PATH:$DOCKER_RESULTS_PATH \\\n  nvcr.io/nvidia/clara/bionemo-framework:nightly \\\n  /bin/bash\n</code></pre> <ul> <li><code>--rm</code>: Removes the container when it exits.</li> <li><code>-it</code>: Allocates a pseudo-TTY and keeps the container running in the foreground.</li> <li><code>--gpus all</code>: Allocates all available GPUs on the host machine.</li> <li><code>--network host</code>: Allows the container to use the host's network stack, effectively sharing the host's network   namespace and allowing the container to access the host's network interfaces directly.</li> <li><code>--shm-size=4g</code>: Sets the size of the shared memory (/dev/shm) in the container to 4 gigabytes, which can be useful for applications that rely heavily on shared memory.</li> <li><code>-e &lt;VARIABLE&gt;</code>: Sets the environment variable inside the container, taking the value set on the host machine.</li> <li><code>-v &lt;LOCAL DIRECTORY&gt;:&lt;DOCKER DIRECTORY&gt;</code>: Mounts a volume from the host machine to the container.</li> <li><code>nvcr.io/nvidia/clara/bionemo-framework:nightly</code>: The path to the Docker image to use.</li> <li><code>/bin/bash</code>: The command to run inside the container, which starts a Bash shell.</li> </ul>"},{"location":"main/getting-started/initialization-guide/#running-a-model-training-script-inside-the-container","title":"Running a Model Training Script Inside the Container","text":"<p>Running a model training script inside the BioNeMo Docker container is the preferred workflow for model training. The container provides an encapsulated and reproducible training environment. By mounting a volume from the host machine, the output directory containing results such as logs and checkpoints can be persisted even after the container is removed. A training script can be run as in the example below. Replace <code>training.py</code> and option (for example, <code>--option1</code>) with the file name and relevant command line options, respectively.</p> <pre><code>docker run --rm -it --gpus all \\\n  -e NGC_CLI_API_KEY \\\n  -e WANDB_API_KEY \\\n  -v $LOCAL_DATA_PATH:$DOCKER_DATA_PATH \\\n  -v $LOCAL_MODELS_PATH:$DOCKER_MODELS_PATH \\\n  -v $LOCAL_RESULTS_PATH:$DOCKER_RESULTS_PATH \\\n  nvcr.io/nvidia/clara/bionemo-framework:nightly \\\n  python $DOCKER_RESULTS_PATH/training.py --option1 --option2 --output=$DOCKER_RESULTS_PATH\n</code></pre> <p>Many of the Docker run options are identical to the shell example above, with the exception of the command being run:</p> <ul> <li><code>python $DOCKER_RESULTS_PATH/training.py --option1 --option2 --output=$DOCKER_RESULTS_PATH</code>: The command to run inside the   container, which runs the <code>training.py</code> Python script with the specified command-line arguments.</li> </ul>"},{"location":"main/getting-started/initialization-guide/#running-jupyter-lab-inside-the-container","title":"Running Jupyter Lab Inside the Container","text":"<p>By starting a Jupyter Lab instance inside the BioNeMo Framework container, users can leverage the container's optimized environment for machine learning workloads to accelerate their data science workflows, while also benefiting from the interactive and collaborative features of Jupyter Lab. This allows users to seamlessly transition between data preparation, model development, and visualization, all within a single, streamlined environment. You can then launch the container. We recommend running the container in a Jupyter Lab environment using the command below:</p> <pre><code>docker run --rm -d --gpus all \\\n  -p $JUPYTER_PORT:$JUPYTER_PORT \\\n  -e NGC_CLI_API_KEY \\\n  -e WANDB_API_KEY \\\n  -v $LOCAL_DATA_PATH:$DOCKER_DATA_PATH \\\n  -v $LOCAL_MODELS_PATH:$DOCKER_MODELS_PATH \\\n  -v $LOCAL_RESULTS_PATH:$DOCKER_RESULTS_PATH \\\n  nvcr.io/nvidia/clara/bionemo-framework:nightly \\\n  jupyter lab \\\n    --allow-root \\\n    --ip=* \\\n    --port=$JUPYTER_PORT \\\n    --no-browser \\\n    --NotebookApp.token='' \\\n    --NotebookApp.allow_origin='*' \\\n    --ContentsManager.allow_hidden=True \\\n    --notebook-dir=$DOCKER_RESULTS_PATH\n</code></pre> <p>Refer to the guide below for an explanation of the recommended Jupyter Lab options:</p> <ul> <li><code>jupyter lab ...</code>: The command to run inside the container, which starts a Jupyter Lab server. The options are:</li> <li><code>--allow-root</code>: Allow the Jupyter Lab server to run as the root user.</li> <li><code>--ip=*</code>: Listen on all available network interfaces, which allows access from outside the container.</li> <li><code>--port=$JUPYTER_PORT</code>: Listen on port 8888.</li> <li><code>--no-browser</code>: Do not open a browser window automatically.</li> <li><code>--NotebookApp.token=''</code>: Set an empty token for the Jupyter Lab server (no authentication is required).</li> <li><code>--NotebookApp.allow_origin='*'</code>: Allow requests from any origin.</li> <li><code>--ContentsManager.allow_hidden=True</code>: Allow the contents manager to access hidden files and directories.</li> <li><code>--notebook-dir=$DOCKER_RESULTS_PATH</code>: Set the notebook directory to     <code>$DOCKER_RESULTS_PATH</code> inside the container.</li> </ul>"},{"location":"main/getting-started/initialization-guide/#common-docker-run-options","title":"Common <code>docker run</code> Options","text":"<p>Below we explain some common <code>docker run</code> options and how to use them as part of your BioNeMo development workflows.</p>"},{"location":"main/getting-started/initialization-guide/#mounting-volumes-with-the-v-option","title":"Mounting Volumes with the <code>-v</code> Option","text":"<p>The <code>-v</code> allows you to mount a host machine's directory as a volume inside the container. This enables data persistence even after the container is deleted or restarted. In the context of machine learning workflows, leveraging the <code>-v</code> option is essential for maintaining a local cache of datasets, model weights, and results on the host machine such that they can persist after the container terminates and be reused across container runs.</p> <p>Syntax:</p> <pre><code>docker run -v &lt;host_directory&gt;:&lt;container_directory&gt; &lt;image_name&gt;\n</code></pre> <p>Example:</p> <pre><code>docker run -v /path/to/local/cache:/workspace/bionemo2/cache \\\n    nvcr.io/nvidia/clara/bionemo-framework:nightly\n</code></pre> <p>In this example, the <code>/path/to/local/cache</code> directory on the host machine is mounted as a volume at <code>/workspace/bionemo2/cache</code> inside the container.</p>"},{"location":"main/getting-started/initialization-guide/#setting-environment-variables-with-the-e-option","title":"Setting Environment Variables with the <code>-e</code> Option","text":"<p>The <code>-e</code> option allows you to set environment variables inside the container. You can use this option to define variables that will be available to the application running inside the container.</p> <p>Example:</p> <pre><code>docker run -e MY_VAR=value -e ANOTHER_VAR=another_value \\\n    nvcr.io/nvidia/clara/bionemo-framework:nightly\n</code></pre> <ul> <li><code>-e MY_VAR=value</code> sets the <code>MY_VAR</code> environment variable to <code>value</code> inside the container.</li> <li><code>-e ANOTHER_VAR=another_value</code> sets the <code>ANOTHER_VAR</code> environment variable to <code>another_value</code> inside the container.</li> </ul> <p>You can set multiple environment variables by repeating the <code>-e</code> option. The values of these variables will be available to the application running inside the container, allowing you to customize its behavior.</p> <p>Note that you can also use shell variables and command substitutions to set environment variables dynamically. For example:</p> <pre><code>MY_EXTERNAL_VAR=external_value\ndocker run -e MY_INTERNAL_VAR=$MY_EXTERNAL_VAR \\\n    nvcr.io/nvidia/clara/bionemo-framework:nightly\n</code></pre> <p>In this example, the <code>MY_INTERNAL_VAR</code> environment variable inside the container will be set to the value of the <code>MY_EXTERNAL_VAR</code> shell variable on the host machine.</p>"},{"location":"main/getting-started/initialization-guide/#setting-user-and-group-ids-with-the-u-option","title":"Setting User and Group IDs with the <code>-u</code> Option","text":"<p>The <code>-u</code> option sets the user and group IDs to use for the container process. By matching the IDs of the user on the host machine, the user inside the container will have identical permissions for reading and writing files in the mounted volumes as the user that ran the command. You can use command substitutions to automatically retrieve your user and group IDs.</p> <p>Example:</p> <pre><code>docker run -u $(id -u):$(id -g) \\\n    nvcr.io/nvidia/clara/bionemo-framework:nightly\n</code></pre> <ul> <li><code>$(id -u)</code> is a command substitution that executes the id -u command and captures its output. <code>id -u</code> prints the   effective user ID of the current user.</li> <li><code>$(id -g)</code> is another command substitution that executes the <code>id -g</code> command and captures its output. <code>id -g</code> prints   the effective group ID of the current user.</li> </ul>"},{"location":"main/getting-started/pre-reqs/","title":"Hardware and Software Prerequisites for BioNeMo Framework","text":"<p>Before you begin using the BioNeMo Framework, ensure the hardware and software prerequisites outlined below are met.</p>"},{"location":"main/getting-started/pre-reqs/#hardware-prerequisites","title":"Hardware Prerequisites","text":"<p>The BioNeMo Framework is compatible with environments that have access to NVIDIA GPUs. <code>bfloat16</code> precision requires an Ampere generation GPU or higher (Compute Capability \u22658.0). You may be able to run BioNeMo on GPUs without <code>bfloat16</code>, but this use-case is not supported by the development team.</p>"},{"location":"main/getting-started/pre-reqs/#gpu-support-matrix","title":"GPU Support Matrix","text":"<p>The following datacenter and desktop GPUs have Compute Capability \u22658.0 and are supported hardware for BioNeMo:</p> GPU Compute Capability Support H100 9.0 Full L4 8.9 Full L40 8.9 Full A100 8.0 Full A40 8.6 Full A30 8.0 Full A10 8.6 Full A16 8.6 Full A2 8.6 Full RTX 6000 8.9 Full RTX A6000 8.6 Full RTX A5000 8.6 Full RTX A4000 8.6 Full"},{"location":"main/getting-started/pre-reqs/#software-prerequisites","title":"Software Prerequisites","text":"<p>The BioNeMo Framework is supported on x86 Linux systems.</p> <p>Ensure that the following are installed in your desired execution environment:</p> <ul> <li>Appropriate GPU drivers (minimum version: 560; lower versions may be compatible, but cannot be guaranteed)</li> <li>Docker (with GPU support, Docker Engine 19.03 or above)</li> <li>NVIDIA Container Toolkit   to allow Docker to access the GPUs</li> </ul>"},{"location":"main/getting-started/training-models/","title":"Training Models","text":"<p>5D Parallel Training Moved to bionemo-recipes</p> <p>The 5D parallel training implementations for ESM-2 and Geneformer have been migrated to simplified TransformerEngine + FSDP implementations in bionemo-recipes. For training these models, please refer to the recipes in <code>bionemo-recipes/recipes/</code> (e.g., <code>esm2_native_te</code>, <code>geneformer_native_te_mfsdp_fp8</code>). The instructions below work for bionemo-framework container releases \\&lt;2.7.1.</p>"},{"location":"main/getting-started/training-models/#pydantic-configuration","title":"Pydantic Configuration","text":"<p>BioNeMo 2 provides two entrypoints for models with both argparse and pydantic. Both documented in the <code>Models</code> section below. Pydantic based configuration is designed to accept a configuration yaml file as input, along with context-specific arguments (e.g., should we resume from existing checkpoints?). These YAML configs go through a Pydantic Validator, in this case referred to as <code>MainConfig</code>. This Config is composed of several other Pydantic models, see the class definition for details. To pre-populate a config with reasonable defaults for various standard models, we provide 'recipes.' These are simple methods that instantiate the config object and then serialize it to a YAML configuration file. From this file, you may either submit it directly, or modify the various parameters to meet your use case. For example, Weights and biases, devices, precision, and dataset options are all extremely useful to modify. Then, you would submit this config for training.</p> <p>These two workflows are packaged as executables when esm2 or geneformer are installed with pip. These commands will appear as:</p> <pre><code>bionemo-geneformer-recipe\nbionemo-esm2-recipe\nbionemo-geneformer-train\nbionemo-esm2-train\n</code></pre>"},{"location":"main/getting-started/training-models/#esm-2","title":"ESM-2","text":""},{"location":"main/getting-started/training-models/#running","title":"Running","text":"<p>First off, we have a utility function for downloading full/test data and model checkpoints called <code>download_bionemo_data</code> that our following examples currently use. This will download the object if it is not already on your local system, and then return the path either way. For example if you run this twice in a row, you should expect the second time you run it to return the path almost instantly.</p> <p>NOTE: NVIDIA employees should use <code>pbss</code> rather than <code>ngc</code> for the data source.</p> <pre><code>export MY_DATA_SOURCE=\"ngc\"\n</code></pre> <p>or for NVIDIA internal employees with new data etc:</p> <pre><code>export MY_DATA_SOURCE=\"pbss\"\n</code></pre> <pre><code># The fastest transformer engine environment variables in testing were the following two\nTEST_DATA_DIR=$(download_bionemo_data esm2/testdata_esm2_pretrain:2.0 --source $MY_DATA_SOURCE); \\\nESM2_650M_CKPT=$(download_bionemo_data esm2/650m:2.0 --source $MY_DATA_SOURCE); \\\n\ntrain_esm2     \\\n    --train-cluster-path ${TEST_DATA_DIR}/2024_03_sanity/train_clusters_sanity.parquet     \\\n    --train-database-path ${TEST_DATA_DIR}/2024_03_sanity/train_sanity.db     \\\n    --valid-cluster-path ${TEST_DATA_DIR}/2024_03_sanity/valid_clusters.parquet     \\\n    --valid-database-path ${TEST_DATA_DIR}/2024_03_sanity/validation.db     \\\n    --result-dir ./results     \\\n    --experiment-name test_experiment     \\\n    --num-gpus 1  \\\n    --num-nodes 1 \\\n    --val-check-interval 10 \\\n    --num-dataset-workers 1 \\\n    --num-steps 10 \\\n    --max-seq-length 1024 \\\n    --limit-val-batches 2 \\\n    --micro-batch-size 2 \\\n    --restore-from-checkpoint-path ${ESM2_650M_CKPT}\n</code></pre>"},{"location":"main/getting-started/training-models/#running-with-pydantic-configs","title":"Running with Pydantic configs","text":"<p>Alternatively, we provide a validated and serialized configuration file entrypoint for executing the same workflow. These can be generated using the <code>bionemo-esm2-recipe</code> entrypoints. Recipes are available for 8m, 650m, and 3b ESM2 models. You may select which preset config to use by setting the <code>--recipe</code> parameter. The output is then a serialized configuration file that may be used in the associated <code>bionemo-esm2-train</code> commands.</p> <pre><code># The fastest transformer engine environment variables in testing were the following two\nTEST_DATA_DIR=$(download_bionemo_data esm2/testdata_esm2_pretrain:2.0 --source $MY_DATA_SOURCE); \\\nbionemo-esm2-recipe \\\n--train-cluster-path ${TEST_DATA_DIR}/2024_03_sanity/train_clusters_sanity.parquet     \\\n--train-database-path ${TEST_DATA_DIR}/2024_03_sanity/train_sanity.db     \\\n--valid-cluster-path ${TEST_DATA_DIR}/2024_03_sanity/valid_clusters.parquet     \\\n--valid-database-path ${TEST_DATA_DIR}/2024_03_sanity/validation.db     \\\n--result-dir ./results     \\\n--dest my_config.yaml\\\n--recipe esm2_8m_recipe\n</code></pre> <p>\u26a0\ufe0f IMPORTANT: Inspect and edit the contents of the outputted my_config.yaml as you see fit</p> <p>NOTE: To continue training from an existing checkpoint, simply pass in the path --initial-ckpt-path to the recipe command. This will populate the YAML with the correct field to ensure pretraining is initialized from an existing checkpoint.</p> <p>To submit a training job with the passed config, first update the yaml file with any additional execution parameters of your choosing: number of devices, workers, steps, etc. Second, invoke our training entrypoint. To do this, we need three things:</p> <ul> <li>Configuration file, the YAML produced by the previous step</li> <li>Model config type, in this case the pretraining config. This will validate the arguments in the config YAML against   those required for pretraining. Alternatively, things like fine-tuning with custom task heads may be specified here.   This allows for mixing/matching Data Modules with various tasks.</li> <li>Data Config type, this specifies how to parse, validate, and prepare the DataModule. This may change depending on task,   for example, pretraining ESM2 uses a protein cluster oriented sampling method. In the case of inference or fine-tuning   a pretrained model, a simple fasta file may be sufficient. There is a one-to-one relationship between DataConfig types   and DataModule types.</li> </ul> <p>\u26a0\ufe0f Warning: This setup does NO configuration of Weights and Biases. Edit your config YAML and populate it with your WandB details.</p> <pre><code>bionemo-esm2-train \\\n--data-config-cls bionemo.esm2.run.config_models.ESM2DataConfig \\\n--model-config-cls bionemo.esm2.run.config_models.ExposedESM2PretrainConfig \\\n--config my_config.yaml\n</code></pre> <p>NOTE: both data-config-cls and model-config-cls have default values corresponding to ESM2DataConfig and ExposedESM2PretrainingConfig</p> <p>DataConfigCls and ModelConfigCls can also refer to locally defined types by the user. As long as python knows how to import the specified path, they may be configured. For example, you may have a custom Dataset/DataModule that you would like to mix with an existing recipe. In this case, you define a DataConfig object with the generic specified as your DataModule type, and then pass in the config type to the training recipe.</p>"},{"location":"main/getting-started/training-models/#geneformer","title":"Geneformer","text":""},{"location":"main/getting-started/training-models/#running_1","title":"Running","text":"<p>Similar to ESM-2, you can download the dataset and checkpoint through our utility function.</p> <pre><code>TEST_DATA_DIR=$(download_bionemo_data single_cell/testdata-20241203 --source $MY_DATA_SOURCE); \\\nGENEFORMER_10M_CKPT=$(download_bionemo_data geneformer/10M_240530:2.0 --source $MY_DATA_SOURCE); \\\ntrain_geneformer     \\\n    --data-dir ${TEST_DATA_DIR}/cellxgene_2023-12-15_small_processed_scdl    \\\n    --result-dir ./results     \\\n    --restore-from-checkpoint-path ${GENEFORMER_10M_CKPT} \\\n    --experiment-name test_experiment     \\\n    --num-gpus 1  \\\n    --num-nodes 1 \\\n    --val-check-interval 10 \\\n    --num-dataset-workers 0 \\\n    --num-steps 55 \\\n    --seq-length 128 \\\n    --limit-val-batches 2 \\\n    --micro-batch-size 2\n</code></pre> <p>To fine-tune, you need to specify a different combination of model and loss. Pass the path to the outputted config file from the previous step as the <code>--restore-from-checkpoint-path</code>, and also change <code>--training-model-config-class</code> to the newly created model-config-class.</p> <p>While no CLI option currently exists to hot swap in different data modules and processing functions now, you could copy the <code>sub-projects/bionemo-geneformer/geneformer/scripts/train_geneformer.py</code> and modify the DataModule class that gets initialized.</p> <p>Simple fine-tuning example (NOTE: please change <code>--restore-from-checkpoint-path</code> to be the checkpoint directory path that was output last by the previous train run)</p> <pre><code>TEST_DATA_DIR=$(download_bionemo_data single_cell/testdata-20241203 --source $MY_DATA_SOURCE); \\\ntrain_geneformer     \\\n    --data-dir ${TEST_DATA_DIR}/cellxgene_2023-12-15_small_processed_scdl    \\\n    --result-dir ./results     \\\n    --experiment-name test_finettune_experiment     \\\n    --num-gpus 1  \\\n    --num-nodes 1 \\\n    --val-check-interval 10 \\\n    --num-dataset-workers 0 \\\n    --num-steps 55 \\\n    --seq-length 128 \\\n    --limit-val-batches 2 \\\n    --micro-batch-size 2 \\\n    --training-model-config-class FineTuneSeqLenBioBertConfig \\\n    --restore-from-checkpoint-path results/test_experiment/dev/checkpoints/test_experiment--val_loss=4.3506-epoch=1-last\n</code></pre>"},{"location":"main/getting-started/training-models/#running-with-pydantic-configs_1","title":"Running with Pydantic configs","text":"<p>Alternatively, we provide a validated and serialized configuration file entrypoint for executing the same workflow. Recipes are available for 10m, and 106m geneformer models. Additionally we provide an example recipe of finetuning, where the objective is to 'regress' on token IDs rather than the traditional masked language model approach. In practice, you will likely need to implement your own DataModule, DataConfig, and Finetuning model. You can use the same overall approach, but with customizations for your task.</p> <pre><code>TEST_DATA_DIR=$(download_bionemo_data single_cell/testdata-20241203 --source $MY_DATA_SOURCE); \\\nbionemo-geneformer-recipe \\\n    --recipe 10m-pretrain \\\n    --dest my_config.json \\\n    --data-path ${TEST_DATA_DIR}/cellxgene_2023-12-15_small_processed_scdl \\\n    --result-dir ./results\n</code></pre> <p>\u26a0\ufe0f IMPORTANT: Inspect and edit the contents of the outputted my_config.yaml as you see fit</p> <p>NOTE: To pretrain from an existing checkpoint, simply pass in the path --initial-ckpt-path to the recipe command. This will populate the YAML with the correct field to ensure pretraining is initialized from an existing checkpoint.</p> <p>To submit a training job with the passed config, first update the yaml file with any additional execution parameters of your choosing: number of devices, workers, steps, etc. Second, invoke our training entrypoint. To do this, we need three things:</p> <ul> <li>Configuration file, the YAML produced by the previous step</li> <li>Model config type, in this case the pretraining config. This will validate the arguments in the config YAML against   those required for pretraining. Alternatively, things like fine-tuning with custom task heads may be specified here.   This allows for mixing/matching Data Modules with various tasks.</li> <li>Data Config type, this specifies how to parse, validate, and prepare the DataModule. This may change depending on task,   for example, while fine-tuning you may want to use a custom Dataset/DataModule that includes PERTURB-seq. In this case,   the default pretraining DataConfig and DataModule will be insufficient. See ESM2 for additional example use cases.</li> </ul> <p>\u26a0\ufe0f Warning: This setup does NO configuration of Weights and Biases. Edit your config YAML and populate it with your WandB details.</p> <pre><code>bionemo-geneformer-train \\\n--data-config-cls bionemo.geneformer.run.config_models.GeneformerPretrainingDataConfig \\\n--model-config-cls bionemo.geneformer.run.config_models.ExposedGeneformerPretrainConfig \\\n--config my_config.yaml\n</code></pre> <p>NOTE: both data-config-cls and model-config-cls have default values corresponding to GeneformerPretrainingDataConfig and ExposedGeneformerPretrainConfig</p> <p>DataConfigCls and ModelConfigCls can also refer to locally defined types by the user. As long as python knows how to import the specified path, they may be configured. For example, you may have a custom Dataset/DataModule that you would like to mix with an existing recipe. In this case, you define a DataConfig object with the generic specified as your DataModule type, and then pass in the config type to the training recipe.</p>"},{"location":"main/getting-started/training-models/#weights-and-biases-tricks-and-tips","title":"Weights and Biases Tricks and Tips","text":""},{"location":"main/getting-started/training-models/#trainerglobal-step","title":"Trainer/Global Step","text":"<p>At some point you may encounter some funny plots inside the Weights and Biases charts having to do with <code>trainer/global_steps</code>. An oscillation pattern like this might be present. . This is actually due to an interaction between Pytorch Lightning and Weights and Biases. The issue is that during validation, the <code>validation_step</code> will be used as <code>trainer.global_step</code>. It will not impact model performance, accuracy, or the learning rate scheduler. Moreover, there is also another column called <code>global_step</code> that will reflect the accurate step counts over time.</p>"},{"location":"main/getting-started/using-slurm/","title":"BioNeMo Training Scripts for SLURM","text":"<p>This guide provides example SLURM scripts for running BioNeMo training jobs on HPC systems. The scripts are configured for the DGX cloud SLURM environment. While some settings are specific to our cluster resources, these scripts can be adapted for other HPC environments.</p> <p>For a list of general SLURM commands, check out the SLURM Quick Start User Guide.</p>"},{"location":"main/getting-started/using-slurm/#example-sbatch-script","title":"Example SBATCH Script","text":"<p>Below is a sample SLURM batch script that runs BioNeMo Evo 2 training in a containerized environment. The script demonstrates how to:</p> <ul> <li>Configure SLURM job parameters (nodes, partitions, etc.)</li> <li>Set up training hyperparameters</li> <li>Mount data and model directories</li> <li>Launch distributed training</li> </ul> <p>We'll walk through each section of the script below.</p> <pre><code>#!/bin/bash\n# SLURM directives\n# =========================\n#SBATCH --account=[INSERT ACCOUNT HERE]\n#SBATCH --nodes=1\n#SBATCH --partition=[INSERT PARTITIONs HERE]\n#SBATCH --ntasks-per-node=8\n#SBATCH --time=00:15:00\n#SBATCH --mem=0\n#SBATCH --job-name=[INSERT JOB NAME HERE]\n#SBATCH --mail-type=FAIL\n#SBATCH --exclusive\nset -x # Enable debugging (prints executed commands)\n# Job-specific parameters\n# =========================\nIMAGE_NAME=[INSERT IMAGE NAME HERE]\nEXPERIMENT_NAME=[INSERT PROJECT NAME HERE]\nMODEL_SIZE=7b\nCP_SIZE=1\nTP_SIZE=1\nPP_SIZE=1\nMICRO_BATCH_SIZE=2\nGRAD_ACC_BATCHES=1\nSEQ_LEN=8192\nMAX_STEPS=100\nVAL_CHECK=50\nCLIP_GRAD=250\nEXTRA_ARGS=\"--enable-preemption --use-megatron-comm-overlap-llama3-8k --ckpt-async-save --overlap-grad-reduce --clip-grad $CLIP_GRAD --eod-pad-in-loss-mask\"\nEXTRA_ARG_DESC=\"BF16_perf_cg250_continue\"\nLR=0.0003\nMIN_LR=0.00003\nWU_STEPS=2500\n# 0xDEADBEEF\nSEED=1234\nWD=0.1\nADO=0.01\nHDO=0.01\n# Mounts\n# =========================\nDATA_PATH=/lustre/.../[INSERT DATA PATH HERE]\nDATA_MOUNT=/workspace/bionemo2/data\nMODEL_PATH=/lustre/.../[INSERT MODEL PATH HERE]\nMODEL_MOUNT=/workspace/bionemo2/model\nRESULTS_PATH=$MODEL_PATH/experiments/${EXPERIMENT_NAME}\nmkdir -p $RESULTS_PATH\nMOUNTS=${DATA_PATH}:${DATA_MOUNT},${MODEL_PATH}:${MODEL_MOUNT},$HOME/.cache:/root/.cache\n# Training command\n# =========================\nread -r -d '' COMMAND &lt;&lt;EOF\necho \"*******STARTING********\" \\\n&amp;&amp; echo \"---------------\" \\\n&amp;&amp; echo \"Starting training\" \\\n&amp;&amp;  \\\npython /workspace/bionemo2/sub-packages/bionemo-evo2/src/bionemo/evo2/run/train.py \\\n    -d /workspace/bionemo2/sub-packages/bionemo-evo2/examples/configs/full_pretrain_shortphase_config.yaml \\\n    --num-nodes=${SLURM_JOB_NUM_NODES} \\\n    --devices=${SLURM_NTASKS_PER_NODE} \\\n    --grad-acc-batches $GRAD_ACC_BATCHES \\\n    --max-steps=$MAX_STEPS \\\n    --seed $SEED \\\n    ${EXTRA_ARGS} \\\n    --no-wandb \\\n    --lr $LR \\\n    --wd $WD \\\n    --min-lr $MIN_LR \\\n    --warmup-steps $WU_STEPS \\\n    --attention-dropout $ADO \\\n    --hidden-dropout $HDO \\\n    --limit-val-batches=20 \\\n    --val-check-interval=${VAL_CHECK} \\\n    --experiment-dir=/workspace/bionemo2/model/checkpoints/${EXPERIMENT_NAME} \\\n    --seq-length=${SEQ_LEN} \\\n    --tensor-parallel-size=${TP_SIZE} \\\n    --context-parallel-size=${CP_SIZE} \\\n    --pipeline-model-parallel-size=${PP_SIZE} \\\n    --workers 8 \\\n    --micro-batch-size=${MICRO_BATCH_SIZE} \\\n    --model-size=${MODEL_SIZE}\nEOF\nsrun \\\n    --output ${RESULTS_PATH}/slurm-%j.out \\\n    --error ${RESULTS_PATH}/error-%j.out \\\n    --container-image=$IMAGE_NAME \\\n    --container-mounts ${MOUNTS} \\\n    bash -c \"${COMMAND}\"\nset +x # Disable debugging\n</code></pre>"},{"location":"main/getting-started/using-slurm/#slurm-directives-resource-allocation","title":"SLURM Directives (Resource Allocation)","text":"<p>After the first shebang line, you'll need to add some <code>#SBATCH</code> directives to define how SLURM manages the job. Some of these are user-specific or project-specific, so you'll need to ask your sysadmin for the correct values.</p> Directive Description <code>#SBATCH --account=[INSERT ACCOUNT HERE]</code> Specifies the SLURM account for billing. <code>#SBATCH --nodes=1</code> Requests 1 compute node. <code>#SBATCH --partition=[INSERT PARTITIONs HERE]</code> Specifies job queue (partition). <code>#SBATCH --ntasks-per-node=8</code> Requests 8 tasks per node (often maps to GPUs/CPUs). <code>#SBATCH --time=00:15:00</code> Limits execution time to 15 minutes. <code>#SBATCH --mem=0</code> Uses default max memory available. <code>#SBATCH --job-name=[INSERT JOB NAME HERE]</code> Names the job for tracking. <code>#SBATCH --mail-type=FAIL</code> Sends an email if the job fails. <code>#SBATCH --exclusive</code> Ensures the job has exclusive access to the node. <p>Tip: You can check partition limits and node availability using <code>sinfo</code></p> <p>These must be specified before the job-specific parameters below. They will be passed to our script as SLURM-provided variables, e.g. <code>${SLURM_JOB_NUM_NODES}</code>.</p>"},{"location":"main/getting-started/using-slurm/#job-specific-parameters","title":"Job-Specific Parameters","text":"<p>These environment variables configure training. They can be whatever you want. Here we set various parameters that we will use in the training script below.</p> <p>It's not necessary to specify these as variables, you could just specify them directly in the training script below, but specifying them as variables makes the script more readable and easier to modify.</p> <pre><code>IMAGE_NAME=[INSERT QA IMAGE NAME HERE]\nEXPERIMENT_NAME=[INSERT QA PROJECT HERE]\nSEQ_LEN=8192\nMAX_STEPS=100\n...\nADO=0.01\nHDO=0.01\n</code></pre>"},{"location":"main/getting-started/using-slurm/#mounts","title":"Mounts","text":"<p>SLURM jobs run in an HPC environment where filesystems are often shared across nodes.</p> <p>In our case, we store stuff in the Lustre file system directories, so you'll want to mount the correct Lustre path to the correct container path. The Lustre filesystem is mounted in the <code>/lustre</code> directory, and the container is mounted in the <code>/workspace</code> directory.</p> <p>To specify a mount, first specify the Lustre path, then the container path, separated by a <code>:</code>. You can specify multiple mounts by separating them with commas.</p> <p>Once again, we specify the paths as variables for readability and ease of modification.</p> <pre><code>DATA_PATH=/lustre/.../[INSERT DATA PATH HERE]\nDATA_MOUNT=/workspace/bionemo2/data\nMODEL_PATH=/lustre/.../[INSERT MODEL PATH HERE]\nMODEL_MOUNT=/workspace/bionemo2/model\nRESULTS_PATH=$MODEL_PATH/experiments/${EXPERIMENT_NAME}\nmkdir -p $RESULTS_PATH\nMOUNTS=${DATA_PATH}:${DATA_MOUNT},${MODEL_PATH}:${MODEL_MOUNT},$HOME/.cache:/root/.cache\n</code></pre> <p>Note that paths on EOS and ORD are different, so you'll want to mount the correct Lustre path to the correct container path.</p> <p>Pay special attention to <code>RESULTS_PATH</code>, as this will be the location of your experiment results. It's common to set this <code>EXPERIMENT_NAME</code> specific to the parameters of the experiment you're running, e.g.</p> <pre><code>EXPERIMENT_NAME=EVO2_SEQLEN${SEQ_LEN}_PP${PP_SIZE}_TP${TP_SIZE}_CP${CP_SIZE}_LR${LR}_MINLR${MIN_LR}_WU${WU_STEPS}_GA${GRAD_ACC_BATCHES}_...\n</code></pre>"},{"location":"main/getting-started/using-slurm/#training-command-execution","title":"Training Command Execution","text":"<p>After setting up all parameters and mounts, the training script is launched within the SLURM job using a compound command. This command string\u2014stored in the <code>COMMAND</code> variable\u2014calls the Python training script with all the environment-specific arguments and hyperparameters defined earlier.</p> <pre><code>python /workspace/bionemo2/sub-packages/bionemo-evo2/src/bionemo/evo2/run/train.py \\\n    -d /workspace/bionemo2/sub-packages/bionemo-evo2/examples/configs/full_pretrain_shortphase_config.yaml \\\n    --num-nodes=${SLURM_JOB_NUM_NODES} \\\n    --devices=${SLURM_NTASKS_PER_NODE} \\\n    --grad-acc-batches $GRAD_ACC_BATCHES \\\n    --max-steps=$MAX_STEPS \\\n    --seed $SEED \\\n    ...\n    --model-size=${MODEL_SIZE}\nEOF\n</code></pre> <p>In the command above, we invoke the training script with all the environment-specific arguments and hyperparameters defined earlier, including the SLURM-provided variables and our own parameters such as gradient accumulation, learning rate, etc.</p> <p>The command is executed in a containerized environment to ensure that dependencies and runtime conditions remain consistent. This is accomplished by using the <code>srun</code> command with container options:</p> <pre><code>srun --output ${RESULTS_PATH}/slurm-%j.out \\\n    --error ${RESULTS_PATH}/error-%j.out \\\n    --container-image=$IMAGE_NAME \\\n    --container-mounts ${MOUNTS} \\\n    bash -c \"${COMMAND}\"\n</code></pre> <ul> <li><code>srun</code>initiates the job across allocated resources.</li> <li><code>--container-image</code> flag ensures that the job runs with the correct environment. Here you can point to an image or a <code>.sqsh</code> file.</li> <li><code>--container-mounts</code> flag maps the Lustre file system directories to the container\u2019s workspace, as we specified above, ensuring that data, models, and results are accessible.</li> <li>Log Redirection: the <code>--error</code> and <code>--output</code> flags redirect standard output and error messages to dedicated log files (<code>slurm-%j.out</code> and <code>error-%j.out</code>) under the results directory.</li> </ul> <p>To access logs, you can use the following commands:</p> <pre><code># Standard output logs\ncat ${RESULTS_PATH}/slurm-&lt;JOB_ID&gt;.out\n\n# Error logs\ncat ${RESULTS_PATH}/error-&lt;JOB_ID&gt;.out\n</code></pre>"},{"location":"main/getting-started/using-slurm/#using-the-sbatch-script","title":"Using The Sbatch Script","text":"<p>To kick off the job, you can submit the script using <code>sbatch</code>:</p> <pre><code>sbatch my_training_script.sbatch\n</code></pre> <p>This will submit the job to the SLURM scheduler, and you can check the status of the job using <code>squeue</code>:</p> <pre><code>squeue -u $USER\n</code></pre>"},{"location":"main/recipes/","title":"BioNeMo Recipes","text":"<p>BioNeMo Recipes provides an easy path for the biological foundation model training community to scale up transformer-based models efficiently. Rather than offering a batteries-included training framework, BioNeMo Recipes provide model checkpoints with TransformerEngine (TE) layers and training recipes that demonstrate how to achieve maximum throughput with popular open-source frameworks and fully sharded data parallel (FSDP) scale-out.</p>"},{"location":"main/recipes/#overview","title":"Overview","text":"<p>The biological AI community actively prototypes model architectures and needs tooling that prioritizes extensibility, interoperability, and ease-of-use, alongside performance. BioNeMo Recipes addresses this by offering:</p> <ul> <li>Flexible scaling: Scales from single-GPU prototyping to multi-node training without complex parallelism configurations</li> <li>Framework compatibility: Works with popular frameworks like HuggingFace Accelerate, PyTorch Lightning, and vanilla PyTorch</li> <li>Performance optimization: Leverages TransformerEngine and megatron-FSDP for state-of-the-art training efficiency</li> <li>Research-friendly: Contains hackable and readable code that researchers can easily adapt for their experiments</li> </ul>"},{"location":"main/recipes/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>  Training benchmarks for ESM-2 using the <code>esm2_native_te</code> recipe. </p>"},{"location":"main/recipes/#use-cases","title":"Use Cases","text":"<p>The use cases of BioNeMO Recipes include:</p> <ul> <li>Foundation Model Developers: AI researchers and ML engineers developing novel biological foundation models who need to scale up prototypes efficiently</li> <li>Foundation Model Customizers: Domain scientists looking to fine-tune existing models with proprietary data for drug discovery and biological research</li> </ul>"},{"location":"main/recipes/#supported-recipes-and-models","title":"Supported Recipes and Models","text":"Directory Description FSDP BF16 FP8<sup>[1]</sup> THD FP8 + THD MXFP8<sup>[2]</sup> NVFP4<sup>[2]</sup> CP <code>models/amplify</code>, available on Hugging Face TE accelerated protein BERT, Amgen \u2705 \u2705 \u274c \u274c \ud83d\udea7 \u274c \u274c \u274c <code>models/esm2</code>, available on Hugging Face TE accelerated protein BERT, Meta \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 \u2705 <code>models/llama3</code> TE accelerated Llama 3, Meta \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 <code>recipes/</code><code>esm2_native_te</code> Recipe for <code>esm2/amplify</code> + native PyTorch mFSDP, FSDP2 \u2705 \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 \ud83d\udea7 <code>recipes/</code><code>llama3_native_te</code> Recipe for <code>llama3</code> + native PyTorch FSDP2 \u2705 \u2705 \u2705 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 <code>recipes/</code><code>esm2_accelerate_te</code> Recipe for <code>esm2/amplify</code> TE + HF Accelerate \ud83d\udea7 \u2705 \u2705 \ud83d\udea7 \u274c \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 <code>recipes/</code><code>codonfm_ptl_te</code> Recipe for CodonFM's Encodon using TE \ud83d\udea7 \u2705 \ud83d\udea7 \u2705 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 <code>recipes/</code><code>geneformer_native_te_mfsdp_fp8</code> Recipe for geneformer HF model mFSDP \u2705 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 <code>recipes/</code><code>vit</code> Recipe for vision transformer mFSDP \u2705 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 <p>\u2705: Supported  \ud83d\udea7: Under development, will be supported soon  \u274c: Not supported </p> <p>Abbreviations:</p> <ul> <li>FSDP: Fully sharded data parallel. In <code>bionemo-recipes</code>, we focus on pytorch native FSDP2 and megatron-FSDP(mFSDP) support.</li> <li>BF16: brain-float 16, a common 16 bit float format for deep learning.</li> <li>FP8<sup>[1]</sup>: 8-bit floating point, a compact format for weights allowing for faster training and inference.</li> <li>MXFP8<sup>[2]</sup>: Multi Scale 8-bit floating point, as compact as FP8 but with better numerical precision.</li> <li>NVFP4<sup>[2]</sup>: NVIDIA 4-bit floating point, faster than FP8, retaining accuracy using multi-scale.</li> <li>THD: Total Heads Dimension, also known as \"sequence packing\". A way to construct a batch with sequences of different lengths so there are no pads, which results in no compute wasted on computing attention for padding tokens. This is in contrast to Batch Sequence Head Dimension (BSHD) format, which uses pads to create a rectangular batch.</li> <li>CP: Context parallel, also known as sequence parallel. A way to distribute the memory required to process long sequences across multiple GPUs. For more information, refer to context parallel</li> </ul> <p>[1]: Requires compute capability 9.0 and above (Hopper+)  [2]: Requires compute capability 10.0 and 10.3 (Blackwell), 12.0 support pending </p>"},{"location":"main/recipes/#repository-structure","title":"Repository Structure","text":"<p>This repository contains two types of components:</p>"},{"location":"main/recipes/#models-models","title":"Models (<code>models/</code>)","text":"<p>Huggingface-compatible <code>PreTrainedModel</code> classes that use TransformerEngine layers internally. These are designed to be:</p> <ul> <li>Distributed through Hugging Face Hub: Pre-converted checkpoints available at huggingface.co/nvidia</li> <li>Drop-in replacements: Compatible with <code>AutoModel.from_pretrained()</code> without additional dependencies</li> <li>Performance optimized: Leverage TransformerEngine features like FP8 training and context parallelism</li> </ul> <p>Example models include ESM-2, Geneformer, and AMPLIFY.</p>"},{"location":"main/recipes/#recipes-recipes","title":"Recipes (<code>recipes/</code>)","text":"<p>Self-contained training examples demonstrating best practices for scaling biological foundation models. Each recipe is a complete Docker container with:</p> <ul> <li>Framework examples: Vanilla PyTorch, HuggingFace Accelerate, PyTorch Lightning</li> <li>Feature demonstrations: FP8 training, megatron-FSDP, context parallelism, sequence packing</li> <li>Scaling strategies: Single-GPU to multi-node training patterns</li> <li>Benchmarked performance: Validated throughput and convergence metrics</li> </ul> <p>Recipes are not pip-installable packages but serve as reference implementations that users can adapt for their own research.</p>"},{"location":"main/recipes/#quick-start","title":"Quick Start","text":"<p>This section describe how you can get started with BioNeMo Recipes.</p>"},{"location":"main/recipes/#loading-models","title":"Loading Models","text":"<p>Run the following to load the BioNeMo model.</p> <pre><code>from transformers import AutoModel, AutoTokenizer\n\n# Load a BioNeMo model directly from Hugging Face\nmodel = AutoModel.from_pretrained(\"nvidia/AMPLIFY_120M\")\ntokenizer = AutoTokenizer.from_pretrained(\"nvidia/AMPLIFY_120M\")\n</code></pre>"},{"location":"main/recipes/#running-recipes","title":"Running Recipes","text":"<p>Build and run recipes with the following.</p> <pre><code># Navigate to a recipe\ncd recipes/esm2_native_te_mfsdp\n\n# Build and run\ndocker build -t esm2_recipe .\ndocker run --rm -it --gpus all esm2_recipe python train.py\n</code></pre>"},{"location":"main/recipes/#setting-up-the-development-environment","title":"Setting Up the Development Environment","text":"<ol> <li>Install pre-commit hooks:</li> </ol> <pre><code>pre-commit install\n</code></pre> <p>Run hooks manually:</p> <pre><code>pre-commit run --all-files\n</code></pre> <ol> <li>Test your changes:    Each model and recipe has its own build and test setup following this pattern:</li> </ol> <pre><code>cd models/my_model  # or recipes/my_recipe\ndocker build . -t my_tag\ndocker run --rm -it --gpus all my_tag pytest -v .\n</code></pre>"},{"location":"main/recipes/#coding-guidelines","title":"Coding Guidelines","text":"<p>BioNeMo Recipes prioritize readability and simplicity over comprehensive feature coverage:</p> <ul> <li>KISS (Keep It Simple) over DRY (Don't Repeat Yourself): It's better to have clear, duplicated code than complex   abstractions</li> <li>One thing well: Each recipe should demonstrate specific features clearly rather than trying to cover everything</li> <li>Self-contained: Recipes cannot depend on cutting-edge code from other parts of the repository</li> </ul>"},{"location":"main/recipes/#testing-strategy","title":"Testing Strategy","text":"<p>BioNeMo Reciptes use a three-tier testing approach:</p>"},{"location":"main/recipes/#l0-tests-pre-merge","title":"L0 Tests (Pre-merge)","text":"<ul> <li>Purpose: Fast validation that code works</li> <li>Runtime: \\&lt;10 minutes, single GPU</li> <li>Frequency: Run automatically on PRs</li> <li>Scope: Basic functionality, checkpoint creation/loading</li> </ul>"},{"location":"main/recipes/#l1-tests-performance-monitoring","title":"L1 Tests (Performance Monitoring)","text":"<ul> <li>Purpose: Performance benchmarking and partial convergence validation</li> <li>Runtime: Up to 4 hours, up to 16 GPUs</li> <li>Frequency: Nightly/weekly</li> <li>Scope: Throughput metrics, scaling validation</li> </ul>"},{"location":"main/recipes/#l2-tests-release-validation","title":"L2 Tests (Release Validation)","text":"<ul> <li>Purpose: Full convergence and large-scale validation</li> <li>Runtime: Multiple days, hundreds of GPUs</li> <li>Frequency: Monthly or before releases</li> <li>Scope: Complete model convergence, cross-platform validation</li> </ul>"},{"location":"main/recipes/#adding-new-components","title":"Adding New Components","text":"<p>With BioNeMo Recipes, you can add new components including models and recipes.</p>"},{"location":"main/recipes/#adding-a-new-model","title":"Adding a New Model","text":"<p>Models should be pip-installable packages that can export checkpoints to Hugging Face. Refer to the models README for detailed guidelines on:</p> <ul> <li>Package structure and conventions</li> <li>Checkpoint export procedures</li> <li>Testing requirements</li> <li>CI/CD integration</li> </ul>"},{"location":"main/recipes/#adding-a-new-recipe","title":"Adding a New Recipe","text":"<p>Recipes should be self-contained Docker environments demonstrating specific training patterns. Refer to the recipes README for guidance on:</p> <ul> <li>Directory structure and naming</li> <li>Hydra configuration management</li> <li>Docker best practices</li> <li>SLURM integration examples</li> </ul>"},{"location":"main/recipes/#cicd-contract","title":"CI/CD Contract","text":"<p>All components must pass this basic validation:</p> <pre><code>docker build -t {component_tag} .\ndocker run --rm -it --gpus all {component_tag} pytest -v .\n</code></pre>"},{"location":"main/recipes/#running-cicd","title":"Running CI/CD","text":"<p>To run the CI/CD pipeline locally, run the following command:</p> <pre><code>./ci/build_and_test.py\n</code></pre>"},{"location":"main/recipes/#performance-expectations","title":"Performance Expectations","text":"<p>We aim to provide the fastest available training implementations for biological foundation models, with documented benchmarks across NVIDIA hardware (A100, H100, H200, B100, B200, etc.).</p>"},{"location":"main/recipes/#contributing","title":"Contributing","text":"<p>We welcome contributions that advance the state of biological foundation model training. Ensure your contributions:</p> <ul> <li>Follow our coding guidelines emphasizing clarity</li> <li>Include appropriate tests (L0 minimum, L1/L2 as applicable)</li> <li>Provide clear documentation and examples</li> <li>Maintain compatibility with our supported frameworks</li> </ul> <p>For detailed contribution guidelines, refer to our individual component READMEs:</p> <ul> <li>Models Development Guide</li> <li>Recipes Development Guide</li> </ul>"},{"location":"main/recipes/#license","title":"License","text":"<p>[Add appropriate license information]</p>"},{"location":"main/recipes/#support","title":"Support","text":"<p>For technical support and questions:</p> <ul> <li>Check existing issues before opening a new one</li> <li>Review our training recipes for implementation examples</li> <li>Consult the TransformerEngine and megatron-FSDP documentation for underlying technologies</li> </ul>"},{"location":"main/recipes/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Home</li> <li>Models</li> <li>Recipes</li> </ul>"},{"location":"main/recipes/models/","title":"Models Directory","text":"<p>This directory contains HuggingFace-compatible model implementations that use TransformerEngine layers internally. These models are designed to be distributed through the Hugging Face Hub and serve as drop-in replacements for standard transformer models with enhanced performance.</p>"},{"location":"main/recipes/models/#overview","title":"Overview","text":"<p>Models in this directory are not intended to be pip-installed directly. Instead, they serve as:</p> <ul> <li>Reference implementations of biological foundation models using TransformerEngine</li> <li>Conversion utilities for transforming existing model checkpoints to TE-compatible format</li> <li>Export tools for preparing model releases on the Hugging Face Hub</li> </ul> <p>Users will typically interact with these models by loading pre-converted checkpoints directly from the Hugging Face Hub using standard transformers APIs.</p>"},{"location":"main/recipes/models/#adding-a-new-model","title":"Adding a New Model","text":""},{"location":"main/recipes/models/#minimum-requirements","title":"Minimum Requirements","text":"<p>To add a new model to this directory, you must provide:</p>"},{"location":"main/recipes/models/#1-golden-value-tests","title":"1. Golden Value Tests","text":"<ul> <li>Accuracy validation: Tests that demonstrate the converted TE model produces identical outputs to the source/reference model</li> <li>Numerical precision: Verify outputs match within acceptable tolerance (typically <code>rtol=1e-5, atol=1e-8</code>)</li> <li>Multiple test cases: Cover different input shapes, batch sizes, and edge cases</li> </ul>"},{"location":"main/recipes/models/#2-state-dict-conversion-functions","title":"2. State Dict Conversion Functions","text":"<ul> <li><code>convert_hf_to_te()</code>: Function to convert HuggingFace model state_dict to TransformerEngine format</li> <li><code>convert_te_to_hf()</code>: Function to convert TransformerEngine state_dict back to HuggingFace format</li> <li>Bidirectional validation: Tests ensuring round-trip conversion preserves model weights</li> </ul>"},{"location":"main/recipes/models/#3-checkpoint-export-script","title":"3. Checkpoint Export Script","text":"<ul> <li><code>export.py</code>: Script that packages all necessary files for Hugging Face Hub upload</li> <li>Complete asset bundling: Must include all required files, refer to Export Requirements</li> <li>Automated process: Should be runnable with minimal manual intervention</li> </ul>"},{"location":"main/recipes/models/#4-open-source-license","title":"4. Open Source License","text":"<ul> <li>Approved license: Must include an approved open-source license (MIT, Apache 2.0, etc.)</li> <li>License compatibility: Ensure license is compatible with source model and dependencies</li> <li>Clear attribution: Proper attribution to original model authors if applicable</li> </ul>"},{"location":"main/recipes/models/#directory-structure","title":"Directory Structure","text":"<p>Each model should follow this standardized layout:</p> <pre><code>models/{model_name}/\n\u251c\u2500\u2500 Dockerfile                   # Container definition for testing\n\u251c\u2500\u2500 .dockerignore                # Docker ignore patterns\n\u251c\u2500\u2500 pyproject.toml               # Python package configuration\n\u251c\u2500\u2500 README.md                    # Model-specific documentation\n\u251c\u2500\u2500 model_readme.template        # Detailed model card for Hub upload\n\u251c\u2500\u2500 export.py                    # Checkpoint export utilities\n\u251c\u2500\u2500 LICENSE                      # Open source license file\n\u251c\u2500\u2500 src/                         # Source code directory\n\u2502   \u2514\u2500\u2500 {model_name}/           # Package directory\n\u2502       \u251c\u2500\u2500 __init__.py         # Package initialization\n\u2502       \u251c\u2500\u2500 {model_name}_te.py  # TransformerEngine model implementation\n\u2502       \u251c\u2500\u2500 convert.py          # HF \u2194 TE conversion utilities\n\u2502       \u2514\u2500\u2500 modeling_{...}.py   # Additional model-specific modules\n\u2514\u2500\u2500 tests/                       # Test suite\n    \u251c\u2500\u2500 conftest.py              # Pytest configuration and fixtures\n    \u251c\u2500\u2500 test_golden_values.py    # Golden value validation tests\n    \u251c\u2500\u2500 test_conversion.py       # State dict conversion tests\n    \u2514\u2500\u2500 test_checkpoint.py       # Save/load functionality tests\n</code></pre>"},{"location":"main/recipes/models/#implementation-guidelines","title":"Implementation Guidelines","text":""},{"location":"main/recipes/models/#model-implementation-model_name_tepy","title":"Model Implementation (<code>{model_name}_te.py</code>)","text":"<p>Your TransformerEngine model should:</p> <ul> <li>Inherit from <code>PreTrainedModel</code>: Follow HuggingFace conventions for model structure</li> <li>Use TE layers: Replace standard PyTorch layers with TransformerEngine equivalents</li> <li>Maintain API compatibility: Support the same forward pass signature as the original model</li> <li>Include configuration: Provide a configuration class that extends <code>PretrainedConfig</code></li> </ul> <pre><code>from transformers import PreTrainedModel, PretrainedConfig\nfrom transformer_engine.pytorch import TransformerLayer\n\n\nclass MyModelTEConfig(PretrainedConfig):\n    model_type = \"my_model_te\"\n    # ... configuration parameters\n\n\nclass MyModelTE(PreTrainedModel):\n    config_class = MyModelTEConfig\n\n    def __init__(self, config):\n        super().__init__(config)\n        # Initialize with TE layers\n\n    def forward(self, input_ids, attention_mask=None, **kwargs):\n        # Forward pass implementation\n        pass\n</code></pre>"},{"location":"main/recipes/models/#conversion-functions-convertpy","title":"Conversion Functions (<code>convert.py</code>)","text":"<p>Implement bidirectional conversion between HuggingFace and TransformerEngine state dictionaries. We use a module adapted from the nemo.lightning.io.apply_transforms function to handle the conversion.</p> <pre><code>def convert_hf_to_te(model_hf: nn.Module, **config_kwargs) -&gt; nn.Module:\n    \"\"\"Convert HuggingFace model to TransformerEngine format.\"\"\"\n    te_config = MyModelTEConfig(**model_hf.config.to_dict(), **config_kwargs)\n    with init_empty_weights():\n        model_te = MyModelTE(te_config, dtype=te_config.dtype)\n\n    output_model = io.apply_transforms(model_hf, model_te, ...)\n    return output_model\n\n\ndef convert_te_to_hf(\n    hf_model_tag: str, model_te: nn.Module, **config_kwargs\n) -&gt; nn.Module:\n    \"\"\"Convert TransformerEngine model to HuggingFace format.\"\"\"\n    with init_empty_weights():\n        model_hf = AutoModel.from_pretrained(hf_model_tag)\n\n    output_model = io.apply_transforms(model_te, model_hf, ...)\n\n    return output_model\n</code></pre>"},{"location":"main/recipes/models/#testing-requirements","title":"Testing Requirements","text":""},{"location":"main/recipes/models/#golden-value-tests-test_golden_valuespy","title":"Golden Value Tests (<code>test_golden_values.py</code>)","text":"<pre><code>import pytest\nimport torch\nfrom transformers import AutoModel\nfrom src.my_model.my_model_te import MyModelTE\nfrom src.my_model.convert import convert_hf_to_te\n\n\ndef test_model_outputs_match_reference():\n    \"\"\"Test that TE model outputs match reference HF model.\"\"\"\n    # Load reference model\n    reference_model = AutoModel.from_pretrained(\"original/model\")\n\n    # Create TE model with converted weights\n    te_model = MyModelTE.from_pretrained(\"original/model\")\n\n    # Test with various inputs\n    test_inputs = [...]  # Different input shapes and types\n\n    for inputs in test_inputs:\n        with torch.no_grad():\n            ref_output = reference_model(**inputs)\n            te_output = te_model(**inputs)\n\n        torch.testing.assert_close(\n            te_output.last_hidden_state,\n            ref_output.last_hidden_state,\n            rtol=1e-5,\n            atol=1e-8,\n        )\n</code></pre>"},{"location":"main/recipes/models/#conversion-tests-test_conversionpy","title":"Conversion Tests (<code>test_conversion.py</code>)","text":"<pre><code>def test_bidirectional_conversion():\n    \"\"\"Test that state dict conversion is bidirectional.\"\"\"\n    # Load original state dict\n    original_state_dict = ...\n\n    # Convert HF -&gt; TE -&gt; HF\n    te_state_dict = convert_hf_to_te(original_state_dict)\n    recovered_state_dict = convert_te_to_hf(te_state_dict)\n\n    # Verify weights are preserved\n    for key in original_state_dict:\n        torch.testing.assert_close(original_state_dict[key], recovered_state_dict[key])\n</code></pre>"},{"location":"main/recipes/models/#checkpoint-tests-test_checkpointpy","title":"Checkpoint Tests (<code>test_checkpoint.py</code>)","text":"<pre><code>def test_save_and_load_pretrained():\n    \"\"\"Test that model can be saved and loaded with HF APIs.\"\"\"\n    model = MyModelTE.from_pretrained(\"original/model\")\n\n    # Save model\n    model.save_pretrained(\"./test_checkpoint\")\n\n    # Load model\n    loaded_model = MyModelTE.from_pretrained(\"./test_checkpoint\")\n\n    # Verify models are equivalent\n    # ... test logic\n</code></pre>"},{"location":"main/recipes/models/#export-requirements","title":"Export Requirements","text":"<p>The <code>export.py</code> script must bundle all necessary assets for Hugging Face Hub upload:</p>"},{"location":"main/recipes/models/#required-files","title":"Required Files","text":"<ol> <li>Model weights: <code>pytorch_model.bin</code> or <code>model.safetensors</code></li> <li>Configuration: <code>config.json</code> with proper <code>auto_map</code> section</li> <li>Model code: All source files needed to instantiate the model</li> <li>Tokenizer: <code>tokenizer.json</code>, <code>tokenizer_config.json</code>, <code>vocab.txt</code>, etc.</li> <li>Model Card: <code>model_readme.template</code> model card</li> <li>License: <code>LICENSE</code> file with approved open-source license</li> <li>Requirements: <code>requirements.txt</code> for any additional dependencies</li> </ol>"},{"location":"main/recipes/models/#configjson-auto-map","title":"Config.json Auto Map","text":"<p>Ensure the exported <code>config.json</code> includes the auto_map section:</p> <pre><code>{\n    \"auto_map\": {\n        \"AutoModel\": \"modeling_my_model.MyModelTE\",\n        \"AutoConfig\": \"configuration_my_model.MyModelTEConfig\"\n    }\n}\n</code></pre>"},{"location":"main/recipes/models/#export-script-template","title":"Export Script Template","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"Export script for MyModel checkpoint.\"\"\"\n\nimport os\nimport shutil\nfrom pathlib import Path\nfrom transformers import AutoTokenizer\nfrom src.my_model.my_model_te import MyModelTE\n\n\ndef export_checkpoint(output_dir: str):\n    \"\"\"Export complete checkpoint for Hugging Face Hub.\"\"\"\n    output_path = Path(output_dir)\n    output_path.mkdir(exist_ok=True)\n\n    # Load and save model\n    model = MyModelTE.from_pretrained(\"source/checkpoint\")\n    model.save_pretrained(output_path)\n\n    # Save tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\"source/checkpoint\")\n    tokenizer.save_pretrained(output_path)\n\n    # Copy source code files\n    src_files = [\"modeling_my_model.py\", \"configuration_my_model.py\"]\n    for file in src_files:\n        shutil.copy(f\"src/my_model/{file}\", output_path / file)\n\n    # Copy documentation and license\n    shutil.copy(\n        \"model_readme.md\", output_path / \"README.md\"\n    )  # Or alternative template-based creation of README.md\n    shutil.copy(\"LICENSE\", output_path / \"LICENSE\")\n\n    print(f\"Checkpoint exported to {output_path}\")\n\n\nif __name__ == \"__main__\":\n    export_checkpoint(\"./exported_checkpoint\")\n</code></pre>"},{"location":"main/recipes/models/#cicd-integration","title":"CI/CD Integration","text":"<p>Each model must pass the standard CI/CD contract:</p> <pre><code>cd models/my_model\ndocker build -t my_model_test .\ndocker run --rm -it --gpus all my_model_test pytest -v .\n</code></pre> <p>The Docker container should:</p> <ul> <li>Install all required dependencies</li> <li>Run the complete test suite</li> <li>Validate that all conversion and export functionality works</li> <li>Complete in reasonable time for CI/CD pipelines</li> </ul>"},{"location":"main/recipes/models/#examples","title":"Examples","text":"<p>For reference implementations, see existing models in this directory:</p> <ul> <li><code>esm2/</code>: Protein language model with bidirectional conversion</li> <li><code>amplify/</code>: DNA foundation model with comprehensive export utilities</li> <li><code>geneformer/</code>: Single-cell gene expression model</li> </ul>"},{"location":"main/recipes/models/#license-requirements","title":"License Requirements","text":"<p>Ensure your exported model is packaged with a LICENSE file containing an approved open-source license for external distribution.</p>"},{"location":"main/recipes/models/#support","title":"Support","text":"<p>For questions about adding new models:</p> <ol> <li>Review existing model implementations for examples</li> <li>Check the main project README for general guidelines</li> <li>Ensure all tests pass before submitting contributions</li> </ol> <p>Remember: The goal is to provide high-performance, easy-to-use biological foundation models that researchers can readily adopt and adapt for their work.</p>"},{"location":"main/recipes/models/amplify/amplify/","title":"AMPLIFY Optimized with NVIDIA TransformerEngine","text":"<p>This folder contains source code and tests for an AMPLIFY model that inherits from the transformers <code>PreTrainedModel</code> class and uses TransformerEngine layers. Users do not need to install this package directly, but can load the model directly from HuggingFace Hub using the standard transformers API. For more information, refer to Inference Examples.</p>"},{"location":"main/recipes/models/amplify/amplify/#feature-support","title":"Feature support","text":"<p>The AMPLIFY implementation natively supports the following TransformerEngine-provided optimizations:</p> Feature Support FP8 \ud83d\udea7 Under development MXFP8 \u274c Not currently supported Sequence Packing / THD input format \ud83d\udea7 Under development FP8 with THD input format \ud83d\udea7 Under development Import from HuggingFace checkpoints \u2705 Supported Export to HuggingFace checkpoints \ud83d\udea7 Under development <p>Refer to BioNeMo Recipes for more details on how to use these features to accelerate model training and inference.</p>"},{"location":"main/recipes/models/amplify/amplify/#links-to-hf-checkpoints","title":"Links to HF checkpoints","text":"<p>Pre-trained AMPLIFY models are available on HuggingFace as part of the NVIDIA BioNeMo collection on the HuggingFace Hub:</p> <p>Available Models:</p> <ul> <li><code>nvidia/AMPLIFY_120M</code> (120M parameters)</li> <li><code>nvidia/AMPLIFY_350M</code> (350M parameters)</li> </ul>"},{"location":"main/recipes/models/amplify/amplify/#runtime-requirements","title":"Runtime Requirements","text":"<p>We recommend using the latest NVIDIA PyTorch container for optimal performance and compatibility. Refer to the provided Dockerfile for details.</p>"},{"location":"main/recipes/models/amplify/amplify/#inference-examples","title":"Inference Examples","text":"<p>Quick start example using HuggingFace transformers:</p> <pre><code>from transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained(\"nvidia/AMPLIFY_120M\")\ntokenizer = AutoTokenizer.from_pretrained(\"nvidia/AMPLIFY_120M\")\n\n# Example protein sequence\nprotein_sequence = (\n    \"MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTL\"\n    \"VTTFSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLV\"\n    \"NRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLAD\"\n    \"HYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\"\n)\n\ninputs = tokenizer(protein_sequence, return_tensors=\"pt\")\noutput = model(**inputs)\n</code></pre>"},{"location":"main/recipes/models/amplify/amplify/#recipe-links","title":"Recipe Links","text":"<p>Training recipes are available in the <code>bionemo-recipes/recipes/</code> directory. AMPLIFY can be trained using the same recipes as ESM-2, simply by switching the model_tag to reference the AMPLIFY model, such as <code>nvidia/AMPLIFY_120M</code>, and changing the dataset as appropriate.</p> <ul> <li>esm2_native_te - Demonstrates training with a simple native PyTorch training   loop.</li> <li>esm2_accelerate_te - Trains the model using HuggingFace   Accelerate.</li> </ul>"},{"location":"main/recipes/models/amplify/amplify/#commands-for-converting-checkpoints","title":"Commands for converting checkpoints","text":""},{"location":"main/recipes/models/amplify/amplify/#hf-transformers-to-te-conversion","title":"HF Transformers to TE conversion","text":"<p>Generate converted AMPLIFY checkpoints from existing HuggingFace transformers checkpoints:</p> <pre><code>mkdir -p checkpoint_export\ndocker build -t amplify .\ndocker run --rm -it --gpus all \\\n  -v $PWD/checkpoint_export/:/workspace/bionemo/checkpoint_export \\\n  -v $HOME/.cache/huggingface/:/root/.cache/huggingface \\\n  amplify python export.py\n</code></pre>"},{"location":"main/recipes/models/amplify/amplify/#te-to-hf-transformers-conversion","title":"TE to HF Transformers conversion","text":"<p>(Coming soon)</p>"},{"location":"main/recipes/models/amplify/amplify/#developer-guide","title":"Developer Guide","text":""},{"location":"main/recipes/models/amplify/amplify/#running-tests","title":"Running tests","text":"<p>To run tests locally, run <code>recipes_local_test.py</code> from the repository root with the model directory as an argument.</p> <pre><code>./ci/scripts/recipes_local_test.py bionemo-recipes/models/amplify/\n</code></pre>"},{"location":"main/recipes/models/amplify/amplify/#development-container","title":"Development container","text":"<p>To use the provided devcontainer, use \"Dev Containers: Reopen in Container\" from the VSCode menu, and choose the \"BioNeMo Recipes Dev Container\" option. To run the tests inside the container, first install the model package in editable mode with <code>pip install -e .</code>, then run <code>pytest -v .</code> in the model directory.</p>"},{"location":"main/recipes/models/amplify/amplify/#deploying-converted-checkpoints-to-huggingface-hub","title":"Deploying converted checkpoints to HuggingFace Hub","text":"<p>After running the checkpoint conversion steps listed in Commands for converting checkpoints, you can deploy the converted checkpoints to the HuggingFace Hub by running the following command:</p> <pre><code>huggingface-cli upload nvidia/${MODEL_NAME} $PWD/checkpoint_export/${MODEL_NAME}\n</code></pre> <p>Or, upload all models at once with:</p> <pre><code>for dir in *; do huggingface-cli upload nvidia/$(basename \"$dir\") \"$dir/\"; done\n</code></pre> <p>z</p>"},{"location":"main/recipes/models/esm2/esm2/","title":"ESM-2 Optimized with NVIDIA TransformerEngine","text":"<p>This folder contains source code and tests for an ESM-2 model that inherits from the transformers <code>PreTrainedModel</code> class and uses TransformerEngine layers. Users don't need to install this package directly, but can load the model directly from HuggingFace Hub using the standard transformers API. For more information, refer to Inference Examples.</p>"},{"location":"main/recipes/models/esm2/esm2/#feature-support","title":"Feature support","text":"<p>The ESM-2 implementation natively supports the following TransformerEngine-provided optimizations:</p> Feature Support FP8 \u2705 Supported on compute capacity 9.0 and above (Hopper+) MXFP8 \u2705 Supported on compute capacity 10.0 and 10.3 (Blackwell), 12.0 support pending Sequence Packing / THD input format \u2705 Supported FP8 with THD input format \u2705 Supported where FP8 is supported Import from HuggingFace checkpoints \u2705 Supported Export to HuggingFace checkpoints \u2705 Supported <p>Refer to BioNemo Recipes for more details on how to use these features to accelerate model training and inference.</p>"},{"location":"main/recipes/models/esm2/esm2/#links-to-hf-checkpoints","title":"Links to HF checkpoints","text":"<p>Pre-trained ESM-2 models converted from the original Facebook weights are available on HuggingFace as part of the NVIDIA BioNeMo collection on the HuggingFace Hub:</p> <p>Available Models:</p> <ul> <li><code>nvidia/esm2_t6_8M_UR50D</code> (8M parameters)</li> <li><code>nvidia/esm2_t12_35M_UR50D</code> (35M parameters)</li> <li><code>nvidia/esm2_t30_150M_UR50D</code> (150M parameters)</li> <li><code>nvidia/esm2_t33_650M_UR50D</code> (650M parameters)</li> <li><code>nvidia/esm2_t36_3B_UR50D</code> (3B parameters)</li> <li><code>nvidia/esm2_t48_15B_UR50D</code> (15B parameters)</li> </ul>"},{"location":"main/recipes/models/esm2/esm2/#runtime-requirements","title":"Runtime Requirements","text":"<p>We recommend using the latest NVIDIA PyTorch container for optimal performance and compatibility. Refer to the provided Dockerfile for details.</p>"},{"location":"main/recipes/models/esm2/esm2/#inference-examples","title":"Inference Examples","text":"<p>Quick start example using HuggingFace transformers:</p> <pre><code>from transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained(\"nvidia/esm2_t6_8M_UR50D\")\ntokenizer = AutoTokenizer.from_pretrained(\"nvidia/esm2_t6_8M_UR50D\")\n\ngfp_P42212 = (\n    \"MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTL\"\n    \"VTTFSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLV\"\n    \"NRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLAD\"\n    \"HYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\"\n)\n\ninputs = tokenizer(gfp_P42212, return_tensors=\"pt\")\noutput = model(**inputs)\n</code></pre>"},{"location":"main/recipes/models/esm2/esm2/#recipe-links","title":"Recipe Links","text":"<p>Training recipes are available in the <code>bionemo-recipes/recipes/</code> directory:</p> <ul> <li>esm2_native_te - Demonstrates training with a simple native PyTorch training   loop.</li> <li>esm2_accelerate_te - Trains the model using HuggingFace   Accelerate.</li> </ul>"},{"location":"main/recipes/models/esm2/esm2/#converting-between-model-formats","title":"Converting Between Model Formats","text":"<p>This section explains how to convert between Hugging Face Transformers and Transformer Engine (TE) ESM2 model formats. The process demonstrates bidirectional conversion: from Transformers to TE format for optimized inference, and back to Hugging Face Transformers format for sharing and deployment. The workflow involves several key steps:</p>"},{"location":"main/recipes/models/esm2/esm2/#converting-from-hf-transformers-to-te","title":"Converting from HF Transformers to TE","text":"<pre><code>from transformers import AutoModelForMaskedLM\n\nfrom esm.convert import convert_esm_hf_to_te\n\nhf_model = AutoModelForMaskedLM.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\nte_model = convert_esm_hf_to_te(hf_model)\nte_model.save_pretrained(\"/path/to/te_checkpoint\")\n</code></pre> <p>This loads the pre-trained ESM2 model that will serve as our reference for comparison.</p>"},{"location":"main/recipes/models/esm2/esm2/#converting-from-te-back-to-hf-transformers","title":"Converting from TE back to HF Transformers","text":"<pre><code>from esm.convert import convert_esm_te_to_hf\nfrom esm.modeling_esm_te import NVEsmForMaskedLM\n\nte_model = NVEsmForMaskedLM.from_pretrained(\"/path/to/te_checkpoint\")\nhf_model = convert_esm_te_to_hf(te_model)\nhf_model.save_pretrained(\"/path/to/hf_checkpoint\")\n</code></pre>"},{"location":"main/recipes/models/esm2/esm2/#loading-and-testing-the-exported-model","title":"Loading and Testing the Exported Model","text":"<p>Load the exported model and perform validation:</p> <pre><code>from transformers import AutoTokenizer\n\nmodel_hf_exported = AutoModelForMaskedLM.from_pretrained(\"/path/to/hf_checkpoint\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n</code></pre>"},{"location":"main/recipes/models/esm2/esm2/#validating-converted-models","title":"Validating Converted Models","text":"<p>To validate the converted models, refer to the commands in Inference Examples above to load and test both the original and converted models to ensure loss and logit values are similar. Additionally, refer to the golden value tests in test_modeling_esm_te.py and test_convert.py.</p>"},{"location":"main/recipes/models/esm2/esm2/#developer-guide","title":"Developer Guide","text":""},{"location":"main/recipes/models/esm2/esm2/#running-tests","title":"Running tests","text":"<p>To run tests locally, run <code>recipes_local_test.py</code> from the repository root with the model directory as an argument.</p> <pre><code>./ci/scripts/recipes_local_test.py bionemo-recipes/models/esm2/\n</code></pre>"},{"location":"main/recipes/models/esm2/esm2/#development-container","title":"Development container","text":"<p>To use the provided devcontainer, use \"Dev Containers: Reopen in Container\" from the VSCode menu, and choose the \"BioNeMo Recipes Dev Container\" option. To run the tests inside the container, first install the model package in editable mode with <code>pip install -e .</code>, then run <code>pytest -v .</code> in the model directory.</p>"},{"location":"main/recipes/models/esm2/esm2/#deploying-converted-checkpoints-to-huggingface-hub","title":"Deploying converted checkpoints to HuggingFace Hub","text":"<p>First, generate converted ESM-2 checkpoints from existing HuggingFace transformers checkpoints:</p> <pre><code>mkdir -p checkpoint_export\ndocker build -t esm2 .\ndocker run --rm -it --gpus all \\\n  -v $PWD/checkpoint_export/:/workspace/bionemo/checkpoint_export \\\n  -v $HOME/.cache/huggingface/:/root/.cache/huggingface \\\n  esm2 python export.py\n</code></pre> <p>Now deploy the converted checkpoints to the HuggingFace Hub by running the following command for each model:</p> <pre><code>huggingface-cli upload nvidia/${MODEL_NAME} $PWD/checkpoint_export/${MODEL_NAME}\n</code></pre> <p>You can also upload all models at once with:</p> <pre><code>cd checkpoint_export\nfor dir in */; do hf upload --repo-type model nvidia/$(basename \"$dir\") \"$dir/\"; done\n</code></pre>"},{"location":"main/recipes/models/geneformer/geneformer/","title":"Geneformer Implemented with Transformer Engine","text":"<p>This repository contains an optimized implementation of Geneformer using NVIDIA Transformer Engine (TE) layers for improved performance on NVIDIA GPUs.</p>"},{"location":"main/recipes/models/geneformer/geneformer/#overview","title":"Overview","text":"<p>Geneformer is a transformer-based model pre-trained on large-scale single-cell transcriptomic data for learning context-aware gene embeddings. This implementation leverages NVIDIA's Transformer Engine to provide:</p> <ul> <li>Accelerated inference with optimized CUDA kernels</li> <li>Memory efficiency through fused operations</li> <li>FP8 precision support for faster computation (on supported hardware)</li> <li>Full compatibility with original Hugging Face checkpoints</li> </ul>"},{"location":"main/recipes/models/geneformer/geneformer/#available-model-variants","title":"Available Model Variants","text":"Model Parameters Input Size Vocabulary Training Data <code>Geneformer-V1-10M</code> 10M 2048 ~25K genes ~30M human single cells <code>Geneformer-V2-104M</code> 104M 4096 ~20K genes ~104M human single cells <code>Geneformer-V2-316M</code> 316M 4096 ~20K genes ~104M human single cells <code>Geneformer-V2-104M_CLcancer</code> 104M 4096 ~20K genes ~104M human single cells + cancer cell lines"},{"location":"main/recipes/models/geneformer/geneformer/#quick-start","title":"Quick Start","text":""},{"location":"main/recipes/models/geneformer/geneformer/#converting-models-to-te-format","title":"Converting Models to TE Format","text":"<p>Use the export script to convert Geneformer models to the optimized Transformer Engine format:</p> <p>Convert a specific model:</p> <pre><code>python export.py --model Geneformer-V1-10M\n</code></pre> <p>Convert all available models:</p> <pre><code>python export.py\n</code></pre> <p>Specify a custom output directory:</p> <pre><code>python export.py --model Geneformer-V2-104M --output-path /path/to/output\n</code></pre> <p>Using Docker:</p> <pre><code>docker run --rm -it --gpus all \\\n  -v /path/to/checkpoint_export/:/workspace/checkpoint_export \\\n  -v $HOME/.cache/huggingface/:/root/.cache/huggingface \\\n  geneformer python export.py --model Geneformer-V2-104M --output-path /workspace/checkpoint_export\n</code></pre>"},{"location":"main/recipes/models/geneformer/geneformer/#converting-between-model-formats","title":"Converting Between Model Formats","text":"<p>Geneformer supports bidirectional conversion between Hugging Face Transformers format and the optimized Transformer Engine format.</p>"},{"location":"main/recipes/models/geneformer/geneformer/#loading-a-pre-trained-model","title":"Loading a Pre-trained Model","text":"<p>Load any Geneformer model variant from Hugging Face:</p> <pre><code>from transformers import AutoModelForMaskedLM\n\n# Load the default model (Geneformer-V2-316M)\nmodel = AutoModelForMaskedLM.from_pretrained(\"ctheodoris/Geneformer\")\n\n# Or load a specific variant\nmodel = AutoModelForMaskedLM.from_pretrained(\n    \"ctheodoris/Geneformer\", subfolder=\"Geneformer-V2-104M\"\n)\n</code></pre>"},{"location":"main/recipes/models/geneformer/geneformer/#converting-from-hf-transformers-to-te","title":"Converting from HF Transformers to TE","text":"<p>For more control over the conversion process, you can use the low-level conversion API:</p> <pre><code>from transformers import AutoModelForMaskedLM\nfrom geneformer.convert import convert_geneformer_hf_to_te\n\n# Load the original HF model\nmodel_hf = AutoModelForMaskedLM.from_pretrained(\n    \"ctheodoris/Geneformer\", subfolder=\"Geneformer-V2-104M\"\n)\n\n# Convert to TE format\nmodel_te = convert_geneformer_hf_to_te(model_hf)\n\n# Save the TE model\nmodel_te.save_pretrained(\"./te_checkpoint\")\n</code></pre>"},{"location":"main/recipes/models/geneformer/geneformer/#converting-from-te-back-to-hf-transformers","title":"Converting from TE back to HF Transformers","text":"<p>Convert TE-optimized checkpoints back to standard Hugging Face format:</p>"},{"location":"main/recipes/models/geneformer/geneformer/#using-the-high-level-export-api-recommended","title":"Using the High-Level Export API (Recommended)","text":"<pre><code>from geneformer.export import export_te_checkpoint\n\n# Convert TE checkpoint back to HF format\nexport_te_checkpoint(\n    te_checkpoint_path=\"./te_checkpoint\", output_path=\"./hf_checkpoint\"\n)\n</code></pre> <p>This will:</p> <ol> <li>Load the TE checkpoint</li> <li>Unpack fused QKV parameters</li> <li>Convert to standard HF format</li> <li>Save as a standard Hugging Face checkpoint</li> </ol>"},{"location":"main/recipes/models/geneformer/geneformer/#using-the-low-level-conversion-api","title":"Using the Low-Level Conversion API","text":"<pre><code>from geneformer import BertForMaskedLM\nfrom geneformer.convert import convert_geneformer_te_to_hf\n\n# Load the TE model\nmodel_te = BertForMaskedLM.from_pretrained(\"./te_checkpoint\")\n\n# Convert back to HF format\nmodel_hf = convert_geneformer_te_to_hf(model_te)\n\n# Save as HF checkpoint\nmodel_hf.save_pretrained(\"./hf_checkpoint\")\n</code></pre>"},{"location":"main/recipes/models/geneformer/geneformer/#what-happens-during-conversion","title":"What Happens During Conversion?","text":"<p>HF \u2192 TE Conversion:</p> <ul> <li>QKV weights are packed into fused parameters for efficient attention computation</li> <li>Layer structure is adapted to use TE's optimized CUDA kernels</li> <li>Configuration is extended with TE-specific settings (dtype, layer config, etc.)</li> </ul> <p>TE \u2192 HF Conversion:</p> <ul> <li>Fused QKV parameters are unpacked to separate Q, K, V weights</li> <li>Layer structure is converted back to standard BERT format</li> <li>TE-specific configuration options are removed</li> </ul>"},{"location":"main/recipes/models/geneformer/geneformer/#installation","title":"Installation","text":""},{"location":"main/recipes/models/geneformer/geneformer/#using-pip","title":"Using pip","text":"<pre><code>cd bionemo-recipes/models/geneformer\npip install -e .\n</code></pre>"},{"location":"main/recipes/models/geneformer/geneformer/#for-development","title":"For Development","text":"<pre><code>cd bionemo-recipes/models/geneformer\npip install -e .[test]\n</code></pre>"},{"location":"main/recipes/models/geneformer/geneformer/#testing","title":"Testing","text":""},{"location":"main/recipes/models/geneformer/geneformer/#running-tests-with-docker","title":"Running Tests with Docker","text":"<pre><code>docker build -t geneformer .\ndocker run --rm -it --gpus all geneformer pytest tests/\n</code></pre>"},{"location":"main/recipes/models/geneformer/geneformer/#running-tests-locally","title":"Running Tests Locally","text":"<pre><code>cd bionemo-recipes/models/geneformer\npytest tests/\n</code></pre> <p>Install development dependencies:</p> <pre><code>cd bionemo-recipes/models/geneformer\nPIP_CONSTRAINT= pip install -e .[test]\n</code></pre>"},{"location":"main/recipes/models/geneformer/geneformer/#license","title":"License","text":"<p>This project is licensed under the Apache License 2.0. See the LICENSE file for details.</p>"},{"location":"main/recipes/models/llama3/llama3/","title":"\ud83d\udea7 Llama-3.1 Optimized with NVIDIA TransformerEngine","text":"<p>This folder contains source code and tests for an Llama-3.1 model that inherits from the transformers <code>PreTrainedModel</code> class and uses TransformerEngine layers.</p> <p>This folder is currently work in progress and is not yet ready for general use.</p>"},{"location":"main/recipes/recipes/","title":"Recipes Directory","text":"<p>This directory contains self-contained training examples that demonstrate best practices for scaling biological foundation models using TransformerEngine and megatron-FSDP). Each recipe is a complete Docker environment with benchmarked training scripts that users can learn from and adapt for their own research.</p>"},{"location":"main/recipes/recipes/#philosophy","title":"Philosophy","text":"<p>Recipes are designed as educational reference implementations rather than production training frameworks. Our guiding principles:</p>"},{"location":"main/recipes/recipes/#code-as-documentation","title":"Code as Documentation","text":"<p>Users will read your code far more often than they execute it directly. Prioritize clarity and educational value:</p> <ul> <li>Write code that demonstrates training features and techniques</li> <li>Include detailed comments explaining design decisions</li> <li>Make trade-offs explicit in your implementation</li> <li>Minimize branching logic and complex abstractions</li> </ul>"},{"location":"main/recipes/recipes/#self-contained-simplicity","title":"Self-Contained Simplicity","text":"<p>Each recipe is a completely isolated environment:</p> <ul> <li>No shared dependencies between recipes</li> <li>No bionemo-recipes imports from other parts of bionemo-recipes</li> <li>Everything needed to run training is included in the recipe directory</li> <li>Pinned dependencies for reproducible results. Eventually we will use a uv lockfile to make automated package updates easier.</li> </ul>"},{"location":"main/recipes/recipes/#kiss-keep-it-simple-over-dry-dont-repeat-yourself","title":"KISS (Keep It Simple) over DRY (Don't Repeat Yourself)","text":"<p>Prioritize readability and educational value over code reuse:</p> <ul> <li>Duplicate code between recipes is fine if it makes the training script more readable</li> <li>Keep it simple - users should understand the full training loop at a glance</li> <li>One concept per recipe - don't try to demonstrate every feature in one script. If multiple   features share most of their common infrastructure, use a single recipe folder and multiple   <code>train_{feature}.py</code> entrypoints.</li> </ul>"},{"location":"main/recipes/recipes/#adding-a-new-recipe","title":"Adding a New Recipe","text":""},{"location":"main/recipes/recipes/#recipe-naming-convention","title":"Recipe Naming Convention","text":"<p>Follow this naming pattern to clearly communicate what your recipe demonstrates:</p> <pre><code>{model_name}_{training_framework}_{key_features}/\n</code></pre> <p>Examples:</p> <ul> <li><code>esm2_native_te_mfsdp/</code> - ESM-2 with vanilla PyTorch, TransformerEngine, and megatron-fsdp</li> <li><code>geneformer_lightning_context_parallel/</code> - Geneformer with PyTorch Lightning and context parallelism</li> </ul>"},{"location":"main/recipes/recipes/#required-directory-structure","title":"Required Directory Structure","text":"<p>Each recipe must follow this example layout:</p> <pre><code>recipes/{recipe_name}/\n\u251c\u2500\u2500 README.md                                 # Recipe-specific documentation\n\u251c\u2500\u2500 Dockerfile                                # Self-contained training environment\n\u251c\u2500\u2500 .dockerignore                             # Docker build optimization\n\u251c\u2500\u2500 .ruff.toml                                # Code formatting configuration\n\u251c\u2500\u2500 requirements.txt                          # Pinned Python dependencies\n\u251c\u2500\u2500 hydra_config/                             # Training configuration management\n\u2502   \u251c\u2500\u2500 defaults.yaml                         # Shared configuration defaults\n\u2502   \u251c\u2500\u2500 L0_sanity.yaml                        # Fast CI/CD test config\n\u2502   \u251c\u2500\u2500 L1_{model_size}_perf.yaml             # Performance benchmark config\n\u2502   \u251c\u2500\u2500 L1_{model_size}_partial_conv.yaml     # Partial convergence test config (optional)\n\u2502   \u2514\u2500\u2500 L2_{model_size}_full_conv.yaml        # Full convergence test config (optional)\n\u251c\u2500\u2500 train.py                                  # Main training entrypoint\n\u251c\u2500\u2500 train_{feature}.py                        # Alternative training scripts (optional)\n\u251c\u2500\u2500 test_train.py                             # L0 and L1 test suite\n\u251c\u2500\u2500 modeling_{model}.py                       # Model definition (if needed)\n\u251c\u2500\u2500 dataset.py                                # Data loading utilities\n\u251c\u2500\u2500 train.parquet                             # Small test dataset (&lt; 5MB)\n\u2514\u2500\u2500 slurm.sh                                  # Example multi-node SLURM script\n</code></pre>"},{"location":"main/recipes/recipes/#implementation-requirements","title":"Implementation Requirements","text":""},{"location":"main/recipes/recipes/#self-contained-docker-environment","title":"Self-Contained Docker Environment","text":"<p>Your <code>Dockerfile</code> should create a complete, reproducible training environment:</p> <pre><code>FROM nvcr.io/nvidia/pytorch:26.01-py3\n\n# Install dependencies with caching for faster builds\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=requirements.txt,target=/requirements.txt \\\n    PIP_CONSTRAINT= pip install -r /requirements.txt\n\n# Set workspace to avoid pytest conflicts\nWORKDIR /workspace/bionemo\nCOPY . .\n\n# Default command for interactive development\nCMD [\"/bin/bash\"]\n</code></pre> <p>Key requirements:</p> <ul> <li>Use the latest NVIDIA PyTorch base image unless specific requirements dictate otherwise</li> <li>Pin all dependencies in <code>requirements.txt</code> for reproducibility</li> <li>Include everything needed to run training without external dependencies</li> <li>Optimize for Docker layer caching</li> </ul>"},{"location":"main/recipes/recipes/#readable-training-scripts","title":"Readable Training Scripts","text":"<p>Your <code>train.py</code> should be educational and self-explanatory:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nESM-2 training with TransformerEngine and megatron-fsdp.\n\nThis script demonstrates how to:\n1. Load and prepare biological sequence data\n2. Initialize ESM-2 with TransformerEngine layers\n3. Configure megatron-fsdp for memory-efficient multi-GPU training\n4. Implement a training loop with proper checkpointing\n\nKey design decisions:\n- We use megatron-fsdp ZeRO-3 for maximum memory efficiency\n- TransformerEngine FP8 is enabled for H100+ hardware\n- Context parallelism handles long biological sequences\n\"\"\"\n\nimport hydra\nfrom omegaconf import DictConfig\nimport torch\nfrom torch.distributed import init_process_group, destroy_process_group\n\n\n@hydra.main(config_path=\"hydra_config\", config_name=\"L0_sanity\", version_base=\"1.2\")\ndef main(args: DictConfig):\n    \"\"\"Main training entrypoint.\"\"\"\n\n    # 1. Initialize distributed training\n    init_process_group(backend=\"nccl\")\n\n    # 2. Load model and data (with clear explanations)\n    model = load_model(args.model_config)\n    dataloader = create_dataloader(args.data_config)\n\n    # 3. Configure optimization strategy\n    optimizer = setup_optimizer(model, args.optimizer_config)\n\n    # 4. Training loop with checkpointing\n    train_loop(model, dataloader, optimizer, args)\n\n    destroy_process_group()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Key principles:</p> <ul> <li>Comprehensive docstrings explaining what the recipe demonstrates</li> <li>Inline comments for non-obvious design decisions</li> <li>Modular functions with clear responsibilities</li> <li>Error handling for common failure modes</li> <li>Progress logging so users understand training status</li> </ul>"},{"location":"main/recipes/recipes/#hydra-configuration-management","title":"Hydra Configuration Management","text":"<p>Use Hydra for clean, hierarchical configuration management:</p>"},{"location":"main/recipes/recipes/#defaultsyaml-shared-configuration","title":"<code>defaults.yaml</code> - Shared Configuration","text":"<pre><code># Model configuration\nmodel:\n  name: esm2_t12_35M_UR50D\n  use_te: true\n  fp8_training: false\n\n# Training configuration\ntraining:\n  micro_batch_size: 4\n  gradient_accumulation_steps: 1\n  num_train_steps: 1000\n  save_interval: 100\n\n# Optimization configuration\noptimizer:\n  type: adamw\n  lr: 1e-4\n  weight_decay: 0.01\n  betas: [0.9, 0.98]\n\n# Distributed training\ndistributed:\n  backend: nccl\n  mfsdp:\n    enable: true\n    sharding_strategy: zero3\n\n# Logging\nwandb:\n  project: bionemo-recipes\n  mode: online\n</code></pre>"},{"location":"main/recipes/recipes/#l0_sanityyaml-fast-development-testing","title":"<code>L0_sanity.yaml</code> - Fast Development Testing","text":"<pre><code>defaults:\n  - defaults\n\n# Override for fast testing\nmodel:\n  name: esm2_t6_8M_UR50D  # Smallest model for speed\n\ntraining:\n  micro_batch_size: 2\n  num_train_steps: 5      # Minimal steps for CI/CD\n  save_interval: 5\n\nwandb:\n  mode: offline           # No external logging in CI\n</code></pre>"},{"location":"main/recipes/recipes/#l1_benchmarkyaml-performance-validation","title":"<code>L1_benchmark.yaml</code> - Performance Validation","text":"<pre><code>defaults:\n  - defaults\n\n# Configuration for performance benchmarking\nmodel:\n  name: esm2_t12_35M_UR50D\n  fp8_training: true      # Enable FP8 for performance\n\ntraining:\n  micro_batch_size: 8\n  num_train_steps: 100    # Enough steps for stable metrics\n\nwandb:\n  name: \"esm2_mfsdp_benchmark\"\n  tags: [\"L1\", \"benchmark\", \"performance\"]\n</code></pre> <p>Whenever possible, initialize objects by directly passing config objects as <code>**kwargs</code>:</p> <pre><code>model = MyModel(**config.model_kwargs)\noptimizer = AdamW(**config.optimizer_kwargs)\n</code></pre>"},{"location":"main/recipes/recipes/#comprehensive-testing","title":"Comprehensive Testing","text":"<p>Ensure the following tests are done when implementing.</p>"},{"location":"main/recipes/recipes/#l0-tests-fast-cicd-validation","title":"L0 Tests - Fast CI/CD Validation","text":"<p>Your <code>test_train.py</code> must include L0 tests that run quickly in CI/CD. Include both tests that execute <code>main</code> in the same process, as well as tests that call torchrun / accelerate launch as we expect a user to do.</p> <pre><code>def test_train_main_in_same_process(monkeypatch, session_temp_dir: Path):\n    \"\"\"Test that train.py runs successfully with sanity config and creates expected outputs.\"\"\"\n\n    # Get the recipe directory\n    recipe_dir = Path(__file__).parent\n\n    # Set required environment variables for distributed training\n    monkeypatch.setenv(\"LOCAL_RANK\", \"0\")\n    monkeypatch.setenv(\"RANK\", \"0\")\n    monkeypatch.setenv(\"WORLD_SIZE\", \"1\")\n    monkeypatch.setenv(\"MASTER_ADDR\", \"localhost\")\n    monkeypatch.setenv(\"MASTER_PORT\", \"29500\")\n    monkeypatch.setenv(\"WANDB_MODE\", \"disabled\")\n    monkeypatch.setenv(\"ACCELERATE_MIXED_PRECISION\", \"fp8\")\n    monkeypatch.setenv(\"ACCELERATE_FP8_BACKEND\", \"TE\")\n\n    with initialize_config_dir(\n        config_dir=str(recipe_dir / \"hydra_config\"), version_base=\"1.2\"\n    ):\n        sanity_config = compose(\n            config_name=\"L0_sanity\",\n            overrides=[f\"trainer.output_dir={session_temp_dir}\"],\n        )\n\n    main(sanity_config)\n\n\n@pytest.mark.parametrize(\n    \"accelerate_config\", [\"some_config.yaml\", \"some_other_config.yaml\"]\n)\ndef test_accelerate_launch(accelerate_config, tmp_path):\n    \"\"\"Test that accelerate launch runs successfully.\"\"\"\n    # Run 'accelerate launch train.py' as a subprocess\n    subprocess.run(\n        [\n            sys.executable,\n            \"-m\",\n            \"accelerate.commands.launch\",\n            \"--config_file\",\n            str(accelerate_config_path),\n            \"train.py\",\n            \"--config-name\",\n            \"L0_sanity\",\n            f\"trainer.output_dir={tmp_path}\",\n        ],\n        cwd=recipe_dir,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        text=True,\n        check=True,\n        timeout=240,\n    )\n</code></pre>"},{"location":"main/recipes/recipes/#l1-benchmark-tests","title":"L1 Benchmark Tests","text":"<p>L1 tests should be specified using a <code>L1_{model_name}_{test_type}.yaml</code> config file. These should be workflows that will be launched using a common SLURM or Lepton batch script, complete in under 4 hours, and have a clear set of performance metrics to validate.</p>"},{"location":"main/recipes/recipes/#cluster-agnostic-slurm-script","title":"Cluster-Agnostic SLURM Script","text":"<p>Provide a reference SLURM script that works across different cluster configurations:</p> <pre><code>#!/bin/bash\n#SBATCH --nodes=2                         # number of nodes\n#SBATCH --ntasks-per-node=1               # n tasks per machine (one task per gpu) &lt;required&gt;\n#SBATCH --gpus-per-node=8\n#SBATCH --time=01:00:00                   # wall time\n#SBATCH --mem=0                           # all mem avail\n\nset -x -e\nulimit -c 0\n\nexport GPUS_PER_NODE=8\nexport CMD=\"accelerate launch \\\n    --config_file accelerate_config/some_config.yaml \\\n    --machine_rank \"\\$SLURM_NODEID\" \\\n    --num_machines \"$SLURM_NNODES\" \\\n    --main_process_ip \"\\$SLURM_SRUN_COMM_HOST\" \\\n    --main_process_port 12340 \\\n    --num_processes \"$(( $SLURM_NNODES * $GPUS_PER_NODE ))\" \\\n    train.py\n\"\n\n# Mount a persistent cache directory to cache dataset downloads and transformations.\nexport CACHE_DIR=&lt;cache_dir&gt;\n\nsrun \\\n  --container-image=&lt;image_name&gt; \\\n  --container-mounts=${PWD}:/workspace/bionemo,$HOME/.netrc:/root/.netrc,$CACHE_DIR:/root/.cache/huggingface \\\n  bash -c \"$CMD\"\n</code></pre> <p>SLURM Script Requirements:</p> <ul> <li>Don't expose any internal cluster-specific hardware details, environment variables, or file paths.</li> <li>Demonstrate how to properly format and launch a multi-node SLURM job with the given framework's   launcher (e.g. <code>accelerate launch</code>, <code>torchrun</code>, etc.).</li> </ul>"},{"location":"main/recipes/recipes/#quality-standards","title":"Quality Standards","text":""},{"location":"main/recipes/recipes/#documentation-requirements","title":"Documentation Requirements","text":"<p>Each recipe must include a detailed README.md covering:</p> <ul> <li>What it demonstrates: Clear statement of the training techniques shown</li> <li>Hardware requirements: Minimum and recommended GPU configurations</li> <li>Performance expectations: Benchmark results on reference hardware</li> <li>Configuration options: How to modify the recipe for different use cases</li> <li>Troubleshooting: Common issues and solutions</li> </ul>"},{"location":"main/recipes/recipes/#performance-benchmarking","title":"Performance Benchmarking","text":"<p>Document performance metrics for your recipe, for example:</p> <pre><code>## Performance Benchmarks\n\n### Single Node (8x H100)\n- **Throughput**: 2,500 tokens/sec\n- **Memory Usage**: 45GB per GPU\n- **Model**: ESM-2 650M parameters\n- **Batch Size**: 32 (micro_batch_size=4, gradient_accumulation=8)\n\n### Multi Node (2x8 H100)\n- **Throughput**: 4,800 tokens/sec\n- **Scaling Efficiency**: 96%\n- **Network**: InfiniBand\n</code></pre>"},{"location":"main/recipes/recipes/#cicd-integration","title":"CI/CD Integration","text":"<p>Each recipe must pass the standard test contract, which is a simple pytest invocation:</p> <pre><code>cd recipes/my_recipe\ndocker build -t my_recipe .\ndocker run --rm -it --gpus all my_recipe pytest -v .\n</code></pre> <p>L0 tests run automatically on every PR and must complete in under 10 minutes. L1 tests run nightly on dedicated hardware and can take up to 4 hours.</p>"},{"location":"main/recipes/recipes/#examples","title":"Examples","text":"<p>For reference implementations, examine existing recipes:</p> <ul> <li><code>esm2_native_te_mfsdp/</code>: Comprehensive example showing vanilla PyTorch with TE and megatron-fsdp</li> <li><code>geneformer_lightning_context_parallel/</code>: PyTorch Lightning with context parallelism for long sequences</li> </ul>"},{"location":"main/recipes/recipes/#best-practices","title":"Best Practices","text":""},{"location":"main/recipes/recipes/#what-makes-a-great-recipe","title":"What Makes a Great Recipe","text":"<ul> <li>Educational value: Users learn something new about scaling biological models</li> <li>Production relevance: Techniques are applicable to real research workflows</li> <li>Performance validation: Benchmarked results demonstrate clear benefits</li> <li>Adaptation friendly: Users can easily modify for their specific needs</li> </ul>"},{"location":"main/recipes/recipes/#common-pitfalls-to-avoid","title":"Common Pitfalls to Avoid","text":"<ul> <li>Over-abstraction: Don't hide important details behind complex abstractions</li> <li>Feature creep: Resist adding every possible feature to one recipe</li> <li>Brittle dependencies: Pin versions and test with fresh environments</li> </ul> <p>Remember: A great recipe teaches users how to scale their biological foundation models effectively. Focus on clarity, education, and practical applicability over comprehensive use case coverage or error handling.</p>"},{"location":"main/recipes/recipes/codonfm_ptl_te/codonfm_ptl_te/","title":"Codonfm ptl te","text":"<p>Disclaimer: This is an isolated model recipe based on PyTorch Lightning, which requires its own dockerized environment -- in the local folder - to be run successfully.</p>"},{"location":"main/recipes/recipes/codonfm_ptl_te/codonfm_ptl_te/#codon-fm-foundation-models-for-codon-sequences","title":"Codon FM: Foundation Models for Codon Sequences","text":"<p>Codon FM is a fully open-source suite of foundation models trained directly on codon sequences to learn contextual codon representations and enable downstream codon-aware tasks. This is a TransformerEngine accelerated reproduction of https://github.com/NVIDIA-Digital-Bio/CodonFM, which was published in https://research.nvidia.com/labs/dbr/assets/data/manuscripts/nv-codonfm-preprint.pdf. The research repository contains exact code used in the original scientific exploration, while this repository contains performance accelerations and maintenance updates for community reuse. We release the entire codebase, pre-training/finetuning scripts, evaluation jupyter notebooks, dockerized environments, experiment templates, and downloadable pre-trained model weights\u2014under an open license for transparent and reproducible use. Our primary model family, EnCodon, uses masked language modeling over codons with scalable architectures (80M, 600M, 1B) and efficient memory-mapped data pipelines.</p>"},{"location":"main/recipes/recipes/codonfm_ptl_te/codonfm_ptl_te/#origin","title":"Origin","text":"<p>This recipe offers NVIDIA Transformer Engine (TE) accelerated code for training and inference in addition to the original PyTorch workflow. Hence, the folder structure and most of the code is copied from the original PyTorch based research repository https://github.com/NVIDIA-Digital-Bio/CodonFM, based on the paper https://research.nvidia.com/labs/dbr/assets/data/manuscripts/nv-codonfm-preprint.pdf. We also provide a checkpoint conversion script between PyTorch and TransformerEngine architecture.</p>"},{"location":"main/recipes/recipes/codonfm_ptl_te/codonfm_ptl_te/#pre-trained-models","title":"Pre-Trained Models","text":"<p>The table below summarizes the set of open source pre-trained weights currently made available. All of the training scripts are contained in the directory <code>experiment_scripts/pretraining/encodon_filtered/</code>.</p> Model Variant Hidden size Layers Heads Intermediate Script Original Checkpoint TransformerEngine Checkpoint EnCodon 80M MLM (random p=0.15) 1024 6 8 4096 <code>mlm/encodon_80m.sh</code> Link Link EnCodon 600M MLM (random p=0.15) 2048 12 16 8192 <code>mlm/encodon_600m.sh</code> Link Link EnCodon 1B MLM (random p=0.15) 2048 18 16 8192 <code>mlm/encodon_1b.sh</code> Link Link EnCodon 1B (CDSWT) MLM (codon frequency-weighted) 2048 18 16 8192 <code>cdswt/encodon_1b.sh</code> Link Link"},{"location":"main/recipes/recipes/codonfm_ptl_te/codonfm_ptl_te/#repository-structure","title":"Repository Structure","text":"<p>High-level overview (NerdTree-style):</p> <pre><code>codonfm_ptl_te/\n\u251c\u2500\u2500 src/ \u2014 core library and CLI entrypoints\n\u2502   \u251c\u2500\u2500 runner.py \u2014 entry for pretrain/finetune/eval\n\u2502   \u251c\u2500\u2500 config.py \u2014 model/data/trainer configs\n\u2502   \u251c\u2500\u2500 tasks.py \u2014 pretraining/finetuning/eval tasks\n\u2502   \u251c\u2500\u2500 models/ \u2014 model definitions and components\n\u2502   \u2502   \u251c\u2500\u2500 encodon_pl.py - PyTorch Lightning module of Pytorch Encodon model\n\u2502   \u2502   \u2514\u2500\u2500 encodon_te_pl.py - PyTorch Lightning module of TE Encodon model\n\u2502   \u251c\u2500\u2500 data/ \u2014 datamodules, datasets, preprocess\n\u2502   \u2502   \u2514\u2500\u2500 preprocess/ \u2014 item level process items\n\u2502   \u251c\u2500\u2500 inference/ \u2014 inference wrappers and prediction definitions\n\u2502   \u251c\u2500\u2500 tokenizer/ \u2014 codon tokenizer and mappings\n\u2502   \u2514\u2500\u2500 utils/ \u2014 logging, schedulers, writers, helpers\n\u251c\u2500\u2500 experiment_scripts/ \u2014 launch scripts\n\u2502   \u251c\u2500\u2500 pretraining/ \u2014 EnCodon pretraining\n\u2502   \u2514\u2500\u2500 finetuning/ \u2014 task-specific finetuning\n\u251c\u2500\u2500 data_scripts/ \u2014 data download and curation tools\n\u251c\u2500\u2500 notebooks/ \u2014 analysis and evaluation notebooks\n\u251c\u2500\u2500 codonfm_ckpt_te_conversion.py \u2014 checkpoint conversion between PyTorch and TE\n\u251c\u2500\u2500 Dockerfile \u2014 Dockerfile used to create the docker container\n\u251c\u2500\u2500 run_dev.sh - bash script to build and launch docker container\n\u251c\u2500\u2500 pyproject.toml \u2014 project file used for creating the codon-fm-te pip package\n\u251c\u2500\u2500 README.md \u2014 repo guide\n\u2514\u2500\u2500 LICENSE \u2014 license\n</code></pre>"},{"location":"main/recipes/recipes/codonfm_ptl_te/codonfm_ptl_te/#nvidia-transformerengine-optimization-benchmarks","title":"NVIDIA TransformerEngine Optimization Benchmarks","text":"<p>Several Encodon model versions are benchmarked: The first is the original research code - PyTorch transformer layers using Xformers library's attention function. The second switches Xformers with PyTorch's native Scaled Dot Product Attention (SDPA)implementation, which does not affect checkpoint compatibility to the original research code. The third is the codebase in this repository which uses TransformerEngine transformer layers. The variants change the training/inference speeds while the model scientific benchmarks and accuracy is unchanged.</p> <p>The SPDA and TransformerEngine implementations are available in this codebase:</p> <ol> <li>The default is the PyTorch native transformer based model with SDPA attention implementation.</li> <li> <p>Transformer Engine (TE) acceleration that is enabled with <code>--use_transformer_engine</code> in <code>runner.py</code>. This can also be seen below in our sample commands. Moreover, if you would like to increase training performance, enable THD sequence packing, use <code>--attn_input_format=thd</code> and <code>--collate_fn=thd</code>. For more information on sequence packing refer to this link. The custom TE-based model definition is located here <code>src/models/components/encodon_te_layer.py</code> and encapsulated within the <code>TETransformerLayer</code>. There are two \"flavors\" of TE Encodon models available:</p> </li> <li> <p>Exact: An exact reproduction of the original research code architecture</p> </li> <li>Non-Exact: A variant that uses a different implementation of a transformer that is native to the TE library (differing in LayerNorms), and gives similar scientific accuracy but with a simpler and fewer lines-of-code implementation of the model.   The default and recommended version is the \"exact\" version, which is the default and can be toggled using the environment variable <code>CODON_FM_TE_IMPL=exact</code>.</li> </ol> Advanced: \"Non-exact\" TE Implementation (Optional)  We also present the ability to utilize a simpler model architecture that directly employs Transformer Engine's `TransformerLayer`. This implementation will not directly match the PyTorch (baseline) model (1) but it is simpler to use. To use it please set `export CODON_FM_TE_IMPL=nonexact`. Checkpoints cannot be converted from (1) to this model. This is more for educational purposes to show users the minimal code changes to lead to a TE-accelerated model. We verified that despite the slight architectural difference, this model converges on par with the original architecture.   <p></p> <p>The training step speedups for the 80M Encodon model when both Transformer Engine (TE) and Sequence Packing (THD) are applied compared to the Xformers based model are shown below. We benchmarked on NVIDIA H100 80GB HBM3 GPUs using a micro batch-size is 32. The training step speedups for the 1B Encodon model are on a micro batch-size of 4.</p> <p></p> <p>For inferencing, we can also demonstrate acceleration when using each models TE counterpart. Thus, a 1.4X speedup in this chart shows how much faster the TE version of the model is over the original baseline PyTorch SDPA model. </p>"},{"location":"main/recipes/recipes/codonfm_ptl_te/codonfm_ptl_te/#quickstart","title":"Quickstart","text":"<p>To run the scripts in this repository, we recommend using the provided Docker setup.</p>"},{"location":"main/recipes/recipes/codonfm_ptl_te/codonfm_ptl_te/#1-clone-the-repository","title":"1. Clone the repository","text":"<pre><code>git clone https://github.com/NVIDIA/bionemo-framework/tree/main\ncd bionemo-recipes/recipes/codonfm_ptl_te\n</code></pre>"},{"location":"main/recipes/recipes/codonfm_ptl_te/codonfm_ptl_te/#2-docker-setup","title":"2. Docker Setup","text":"<p>The fastest way to get up and running with CodonFM is through the Docker setup below. This is an interactive development environment, you can build and launch a container that mounts your local repository. This allows you to edit code locally and run it inside the container.</p> <p>To build and launch the development container, simply run the following from the root folder:</p> <pre><code>bash run_dev.sh\n</code></pre> <p>This script will:</p> <ol> <li>Build the development Docker image using the <code>development</code> target in the <code>Dockerfile</code>.</li> <li>Pass your user and group IDs to the container to avoid permission issues with mounted files.</li> <li>Stop and remove any existing container with the same name.</li> <li>Launch a new container with your local code mounted at <code>/workspace</code>, GPU access, host networking, and common directories for data and SSH keys.</li> </ol> <p>You can also customize the data and checkpoint directory paths by passing arguments:</p> <pre><code>bash run_dev.sh --data-dir /path/to/your/data --checkpoints-dir /path/to/your/checkpoints\n</code></pre> <p>You will be dropped into a <code>bash</code> shell inside the container as a non-root user.</p> <p>You can also use the VSCode <code>./.devcontainer</code>. Ensure you mount your data and checkpoints by editing <code>./devcontainer/devcontainer.json</code>.</p>"},{"location":"main/recipes/recipes/codonfm_ptl_te/codonfm_ptl_te/#evaluation-notebooks","title":"Evaluation Notebooks","text":"<p>A series of notebooks are provided in the notebooks directory show casing multiple use cases such as zero-shot variant prediction and finetuning on downstream tasks. The following is a brief overview:</p> Notebook Description 00-Mutation-Datasets-Preprocessing.ipynb Prepare and harmonize mutation datasets used across evaluations. Prerequisite for <code>0-Zero-Shot-Mutation-Variant-CancerHotspot.ipynb</code>, <code>1-Zero-Shot-Mutation-Variant-DDD-ASD.ipynb</code>, <code>2-Zero-Shot-Mutation-Variant-Clinvar-Alphamissense.ipynb</code>, <code>3-Zero-Shot-Mutation-Variant-Clinvar-Synonymous.ipynb</code>. 0-Zero-Shot-Mutation-Variant-CancerHotspot.ipynb Zero-shot variant effect scoring on Cancer Hotspots. 1-Zero-Shot-Mutation-Variant-DDD-ASD.ipynb Zero-shot scoring on Deciphering Developmental Disorders (DDD) and autism spectrum disorder (ASD) cohort study, which catalogs genetic mutations linked to rare pediatric and developmental diseases, to evaluate separation of healthy versus disease coh on coding sequence context. 2-Zero-Shot-Mutation-Variant-Clinvar-Alphamissense.ipynb Zero-shot evaluation on ClinVar missense variants classifying benign vs. pathogenic 3-Zero-Shot-Mutation-Variant-Clinvar-Synonymous.ipynb Zero-shot evaluation on ClinVar synonymous variants evaluating how the models separate benign versus pathogenic synonymous mutations. 4-EnCodon-Downstream-Task-riboNN.ipynb Predicts ribosome profiling signal intensity along coding sequences, evaluating how well models capture translation efficiency and codon-level regulation from sequence context. 5-EnCodon-Downstream-Task-mRFP-expression.ipynb Predicts fluorescent protein expression levels (mRFP) from coding sequences, testing how accurately models capture codon-dependent effects on translation efficiency and protein abundance. 6-EnCodon-Downstream-Task-mRNA-stability.ipynb Predicts mRNA stability from coding sequences evaluating how the models associate codon composition with stability of mRNA."},{"location":"main/recipes/recipes/codonfm_ptl_te/codonfm_ptl_te/#data-preparation","title":"Data Preparation","text":""},{"location":"main/recipes/recipes/codonfm_ptl_te/codonfm_ptl_te/#pre-training-dataset","title":"Pre-training Dataset","text":"<p>In order to create the data required for pretraining, follow the guidance outlined in data_scripts/data_curation/README.</p>"},{"location":"main/recipes/recipes/codonfm_ptl_te/codonfm_ptl_te/#evaluation-datasets","title":"Evaluation Datasets","text":"<ul> <li>mRFP expression and mRNA stability:</li> <li>Open and run the notebooks <code>notebooks/5-EnCodon-Downstream-Task-mRFP-expression.ipynb</code> and <code>notebooks/6-EnCodon-Downstream-Task-mRNA-stability.ipynb</code>. These notebooks contain cells that download/prepare the datasets and guide you through executing the evaluations end-to-end.</li> <li>Mean translation efficiency prediction task:</li> <li>Open and run the notebook <code>notebooks/4-EnCodon-Downstream-Task-riboNN.ipynb</code>. It will download/prepare the downstream dataset and guide you through finetuning on this downstream task.</li> <li>Synonymous, DDD/ASD, and Cancer Hotspot variant datasets:</li> <li>Follow <code>notebooks/00-Mutation-Datasets-Preprocessing.ipynb</code>. This notebook includes a cell that lists the required input files (with expected names/locations) and outlines how to process them into harmonized formats.</li> <li>After preprocessing, use the task-specific notebooks in <code>notebooks/</code> (fir example, <code>0-...CancerHotspot.ipynb</code> and <code>1-...DDD-ASD.ipynb</code>), which consume the harmonized outputs produced by the preprocessing notebook.</li> </ul>"},{"location":"main/recipes/recipes/codonfm_ptl_te/codonfm_ptl_te/#running-trainingfinetuningevaluation","title":"Running Training/Finetuning/Evaluation","text":"<p>The main entry point is <code>src/runner.py</code> which supports three modes:</p>"},{"location":"main/recipes/recipes/codonfm_ptl_te/codonfm_ptl_te/#pre-training","title":"Pre-training","text":"<p>The explicit scripts used to train the released checkpoints are referenced in Pre-trained Models.</p> <pre><code>- If `--use_transformer_engine` is added TransformerEngine will be used, otherwise it will default to PyTorchs Scaled Dot Product Attention (SDPA).\n- For some hardware devices, there may be issues with Transformer Engine's fused attention kernel and sequence packing (THD). To disable this kernel, use `export NVTE_FUSED_ATTN=0`.\n</code></pre> <pre><code>python -m src.runner pretrain \\\n    --out_dir &lt;output_dir&gt; \\\n    --exp_name &lt;experiment_name&gt; \\\n    --model_name &lt;model_size&gt; \\\n    --data_path &lt;path_to_data&gt; \\\n    --process_item mlm_memmap \\\n    --dataset_name CodonMemmapDataset \\\n    --lr &lt;learning_rate&gt; \\\n    --num_gpus &lt;num_gpus&gt; \\\n    --num_nodes &lt;num_nodes&gt; \\\n    --collate_fn &lt;thd/bshd&gt; \\\n    --attn_input_format &lt;thd/bshd&gt; \\\n    [--use_transformer_engine]\n</code></pre> <p>Optional path overrides:</p> <pre><code>  --out_dir &lt;dir&gt;\n  --checkpoints_dir &lt;dir&gt;\n  --pretrained_ckpt_path &lt;path&gt;\n</code></pre> <p>For multi-node execution consider using <code>torchrun</code>.</p> <pre><code>export NUM_GPUS=$(nvidia-smi --query-gpu=gpu_name --format=csv,noheader | wc -l)\ntorchrun \\\n    --nnodes=$NNODES \\\n    --nproc_per_node=$NUM_GPUS \\\n    --node_rank=$NODE_RANK \\\n    --master_addr=$MASTER_ADDR \\\n    --master_port=$MASTER_PORT \\\n  -m src.runner pretrain \\\n    --out_dir &lt;output_dir&gt; \\\n    --exp_name &lt;experiment_name&gt; \\\n    --model_name &lt;model_size&gt; \\\n    --data_path &lt;path_to_data&gt; \\\n    --process_item mlm_memmap \\\n    --dataset_name CodonMemmapDataset \\\n    --lr &lt;learning_rate&gt; \\\n    --num_gpus $NUM_GPUS \\\n    --num_nodes $NNODES \\\n    --collate_fn &lt;thd/bshd&gt; \\\n    --attn_input_format &lt;thd/bshd&gt; \\\n    [--use_transformer_engine]\n</code></pre> <p>Available <code>--process_item</code> options:</p> <ul> <li><code>mlm_memmap</code>: Constructs MLM training examples using memory-mapped data input format.</li> <li><code>mutation_pred_mlm</code>: Constructs mutation prediction scoring input for the model using ref/alt/mut pos</li> <li><code>mutation_pred_likelihood</code>: Constructs input sentence with alt mutation at input to be scored by the model.</li> <li><code>codon_sequence</code>: Constructs a codon sequence that can be input into the model.</li> </ul> <p>Available <code>--dataset_name</code> options:</p> <ul> <li><code>CodonMemmapDataset</code>: Dataset to support memory-mapped pre-training dataset used for pre-training</li> <li><code>MutationDataset</code>: Dataset for mutation prediction</li> <li><code>CodonBertDataset</code>: Dataset to ingest codon sequences.</li> </ul>"},{"location":"main/recipes/recipes/codonfm_ptl_te/codonfm_ptl_te/#fine-tuning","title":"Fine-tuning","text":"<p>The publicly available checkpoints can be finetuned using the finetuning options.</p> <p>Available finetuning options:</p> <p>Refer to example script at <code>experiment_scripts/pretraining/encodon_filtered/finetuning/</code>.</p> <ul> <li><code>lora</code>: Fine-tunes low-rank adapters within a pretrained model added to each transformer layer to reduce training cost and memory usage.</li> <li><code>head_only_random</code>: Trains a randomly initialized output head while the remainder of the model is kept frozen.</li> <li><code>head_only_pretrained</code>: Trains a pretrained output head while the remainder of the model is kept frozen.</li> <li><code>full</code>: Fine-tunes all parameters of the model end-to-end</li> </ul> <p>This is an example commandline for running finetuning:</p> <pre><code>python -m src.runner finetune \\\n    --out_dir &lt;output_dir&gt; \\\n    --exp_name &lt;experiment_name&gt; \\\n    --model_name &lt;model_size&gt; \\\n    --pretrained_ckpt_path &lt;path_to_pretrained_checkpoint&gt; \\\n    --data_path &lt;path_to_data&gt; \\\n    --process_item mutation_pred_mlm \\\n    --dataset_name MutationDataset \\\n    --finetune_strategy &lt;strategy&gt; \\\n    [--use_transformer_engine]\n</code></pre>"},{"location":"main/recipes/recipes/codonfm_ptl_te/codonfm_ptl_te/#evaluation","title":"Evaluation","text":"<p>The publicly available checkpoints can be used to launch scientific evaluation and benchmarking.</p> <p>Available tasks</p> <ul> <li><code>mutation_prediction</code>: Scores a specified mutation with ref-vs-alt codon log-likelihood ratio.</li> <li><code>masked_language_modeling</code>: Predicts masked codon tokens from surrounding sequence context.</li> <li><code>fitness_prediction</code>: Estimates sequence fitness as the mean log-likelihood of the sequence as predicted by the model.</li> <li><code>embedding_prediction</code>: Extracts encoder CLS embeddings for each input.</li> <li><code>downstream_prediction</code>: Uses the downstream cross-attention head for task-specific classification/regression.</li> </ul> <p>This is an example commandline for running evaluation:</p> <pre><code>python -m src.runner eval \\\n    --out_dir &lt;output_dir&gt; \\\n    --exp_name &lt;experiment_name&gt; \\\n    --model_name &lt;model_size&gt; \\\n    --checkpoint_path &lt;path_to_checkpoint&gt; \\\n    --data_path &lt;path_to_data&gt; \\\n    --task_type &lt;task_type&gt; \\\n    --predictions_output_dir &lt;output_directory&gt;\n    [--use_transformer_engine]\n</code></pre>"},{"location":"main/recipes/recipes/codonfm_ptl_te/codonfm_ptl_te/#checkpoint-conversion-between-pytorch-and-te","title":"Checkpoint conversion between PyTorch and TE","text":"<p>codonfm_ckpt_te_conversion.py will convert PyTorch-native Encodon checkpoint TE and back, refer to Pre-trained Models.</p>"},{"location":"main/recipes/recipes/codonfm_ptl_te/codonfm_ptl_te/#using-weights-and-biases-with-codonfm","title":"Using Weights and Biases With CodonFM","text":"<p>CodonFM can log all training and validation metrics to Weights &amp; Biases (WandB), which requires an account. To use alternative solutions other than WandB, you can change the logging destination in encodon_pl.py::training_step and encodon_te_pl.py::training_step.</p> <p>To use WandB with CodonFM, set your Weights &amp; Biases API key for logging inside the running container.</p> <pre><code># WANDB key (optional; only needed if enabling --enable_wandb)\nexport WANDB_API_KEY=your_wandb_api_key\n</code></pre> <p>Alternatively, add your login info to <code>~/.netrc</code>.</p> <p>When launching runs, enable WandB logging by passing <code>--enable_wandb</code> and providing <code>--project_name</code> and <code>--entity</code>. If these are omitted, WandB logging will be skipped.</p>"},{"location":"main/recipes/recipes/codonfm_ptl_te/codonfm_ptl_te/#experiment-launch-scripts","title":"Experiment launch scripts","text":"<p>Experiment launch scripts for reproducing pretraining and fine-tuning are under <code>experiment_scripts/</code>.</p> <ul> <li>Pretraining scripts: <code>experiment_scripts/pretraining/encodon_filtered/</code></li> <li>Fine-tuning templates: <code>experiment_scripts/finetuning/</code></li> </ul>"},{"location":"main/recipes/recipes/codonfm_ptl_te/codonfm_ptl_te/#license","title":"License","text":"<p>Refer to LICENSE.</p>"},{"location":"main/recipes/recipes/esm2_accelerate_te/esm2_accelerate_te/","title":"TransformerEngine-accelerated ESM-2 training with Hugging Face Trainer","text":"<p>This folder demonstrates how to train TE-accelerated ESM-2 using the Hugging Face Transformers Trainer class and Hugging Face Accelerate, including sequence packing and FP8 precision, using distributed training frameworks like FSDP and DeepSpeed.</p>"},{"location":"main/recipes/recipes/esm2_accelerate_te/esm2_accelerate_te/#how-to-use-this-recipe","title":"How to use this recipe","text":"<p>This folder contains an independent, minimal training example. It does not depend on any other code in the top-level bionemo-framework repository. You can download a zipped directory of this folder alone by clicking here.</p>"},{"location":"main/recipes/recipes/esm2_accelerate_te/esm2_accelerate_te/#how-to-deploy-this-recipe-on-cloud-providers","title":"How to deploy this recipe on cloud providers","text":"<p>\ud83d\udea7 Under development</p>"},{"location":"main/recipes/recipes/esm2_accelerate_te/esm2_accelerate_te/#supported-models-and-training-features","title":"Supported Models and Training Features","text":"Model BF16 FP8<sup>[1]</sup> THD Input Format FP8 with THD Input Format MXFP8<sup>[2]</sup> Context Parallelism ESM-2 \u2705 \u2705 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \u274c AMPLIFY \u2705 \u274c \ud83d\udea7 \u274c \u274c \u274c <p>\u2705: Supported  \ud83d\udea7: Under development  \u274c: Not supported </p> <p>[1]: Requires compute capacity 9.0 and above (Hopper+)  [2]: Requires compute capacity 10.0 and 10.3 (Blackwell), 12.0 support pending </p>"},{"location":"main/recipes/recipes/esm2_accelerate_te/esm2_accelerate_te/#distributed-training","title":"Distributed Training","text":"<p>This recipe leverages Hugging Face Accelerate for distributed training, which supports multiple distributed training frameworks through configuration files:</p> <ul> <li>Distributed Data Parallel (DDP), shown in <code>accelerate_config/default.yaml</code></li> <li>Fully Sharded Data Parallel (FSDP), shown in   <code>accelerate_config/fsdp1_te.yaml</code> and <code>accelerate_config/fsdp1_hf.yaml</code> (depending on whether the model is   TransformerEngine-accelerated or not)</li> <li>Fully Sharded Data Parallel 2 (FSDP2), shown in   <code>accelerate_config/fsdp2_te.yaml</code> and <code>accelerate_config/fsdp2_hf.yaml</code></li> </ul> <p>The training strategy is configured through Accelerate's configuration system rather than separate training scripts.</p>"},{"location":"main/recipes/recipes/esm2_accelerate_te/esm2_accelerate_te/#commands-to-launch-training","title":"Commands to Launch Training","text":""},{"location":"main/recipes/recipes/esm2_accelerate_te/esm2_accelerate_te/#single-process-training","title":"Single Process Training","text":"<p>To run single-process training on one GPU:</p> <pre><code>python train.py --config-name=L0_sanity\n</code></pre> <p>To train the AMPLIFY model instead of ESM-2, use the <code>L0_sanity_amplify</code> config:</p> <pre><code>python train.py --config-name=L0_sanity_amplify\n</code></pre>"},{"location":"main/recipes/recipes/esm2_accelerate_te/esm2_accelerate_te/#multi-process-training","title":"Multi-Process Training","text":"<p>To run distributed training using Accelerate's launch command:</p> <pre><code>accelerate launch --config_file accelerate_config/fsdp2_te.yaml \\\n    --num_processes 2 train.py \\\n    --config-name=L0_sanity\n</code></pre>"},{"location":"main/recipes/recipes/esm2_accelerate_te/esm2_accelerate_te/#multi-node-training","title":"Multi-Node Training","text":"<p>For multi-node training, configure your accelerate setup with the appropriate machine rank, main process IP, and port:</p> <pre><code>accelerate launch --config_file accelerate_config/fsdp2_te.yaml \\\n    --main_process_ip 192.168.20.1 \\\n    --main_process_port 9898 \\\n    --num_machines 2 \\\n    --machine_rank 0 \\\n    train.py --config-name=L0_sanity\n</code></pre> <p>Refer to <code>slurm.sh</code> for an example SLURM script.</p>"},{"location":"main/recipes/recipes/esm2_accelerate_te/esm2_accelerate_te/#fp8-training","title":"FP8 Training","text":"<p>FP8 precision is enabled with an accelerate config file, shown in <code>accelerate_config/fp8.yaml</code>.</p> <pre><code>accelerate launch --config_file accelerate_config/fp8.yaml \\\n    train.py --config-name L0_sanity\n</code></pre>"},{"location":"main/recipes/recipes/esm2_accelerate_te/esm2_accelerate_te/#torch-dynamo-torchcompile-support","title":"Torch Dynamo (torch.compile) Support","text":"<p>An example accelerate config file, shown in <code>accelerate_config/dynamo.yaml</code>, is provided for torch.compile support.</p> <pre><code>accelerate launch --config_file accelerate_config/dynamo.yaml \\\n    train.py --config-name L0_sanity\n</code></pre>"},{"location":"main/recipes/recipes/esm2_accelerate_te/esm2_accelerate_te/#known-limitations","title":"Known Limitations","text":"<ul> <li>Combining FP8 and FSDP1 or FSDP2 does not seem to be supported currently.</li> </ul>"},{"location":"main/recipes/recipes/esm2_accelerate_te/esm2_accelerate_te/#saving-and-loading-checkpoints","title":"Saving and Loading Checkpoints","text":"<p>The Hugging Face Trainer automatically handles checkpointing based on the <code>TrainingArguments</code> configuration. Checkpointing behavior is controlled through the trainer configuration in your hydra config.</p>"},{"location":"main/recipes/recipes/esm2_accelerate_te/esm2_accelerate_te/#enabling-checkpoints","title":"Enabling Checkpoints","text":"<p>To enable checkpoint saving, ensure that <code>trainer.output_dir</code> is set to a writable directory. Checkpointing frequency is controlled by the <code>trainer.save_steps</code> configuration parameter.</p> <pre><code>accelerate launch train.py --config-name L0_sanity \\\n  trainer.output_dir=/path/to/ckpt_dir \\\n  trainer.save_steps=100\n</code></pre>"},{"location":"main/recipes/recipes/esm2_accelerate_te/esm2_accelerate_te/#resuming-from-checkpoints","title":"Resuming from Checkpoints","text":"<p>The Trainer automatically detects and resumes from the latest checkpoint in the output directory. You can also specify a specific checkpoint:</p> <pre><code>accelerate launch train.py --config-name L0_sanity \\\n  trainer.output_dir=/path/to/ckpt_dir \\\n  trainer.resume_from_checkpoint=/path/to/specific/checkpoint\n</code></pre>"},{"location":"main/recipes/recipes/esm2_accelerate_te/esm2_accelerate_te/#evaluation","title":"Evaluation","text":"<p>Configure evaluation strategy and frequency through TrainingArguments:</p> <pre><code>accelerate launch train.py --config-name L0_sanity \\\n  trainer.eval_strategy=steps \\\n  trainer.eval_steps=500 \\\n</code></pre>"},{"location":"main/recipes/recipes/esm2_accelerate_te/esm2_accelerate_te/#running-inference-with-the-trained-model","title":"Running Inference with the Trained Model","text":"<p>Models trained with this recipe can be loaded using standard Hugging Face methods. The final model is saved in a format compatible with the <code>AutoModel.from_pretrained</code> method:</p> <pre><code>from transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained(\"path/to/checkpoint\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n\ngfp_P42212 = (\n    \"MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTL\"\n    \"VTTFSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLV\"\n    \"NRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLAD\"\n    \"HYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\"\n)\n\ninputs = tokenizer(gfp_P42212, return_tensors=\"pt\")\nmodel.eval()\noutput = model(**inputs)\n</code></pre>"},{"location":"main/recipes/recipes/esm2_accelerate_te/esm2_accelerate_te/#performance","title":"Performance","text":"<p>\ud83d\udea7 Under development</p>"},{"location":"main/recipes/recipes/esm2_accelerate_te/esm2_accelerate_te/#references","title":"References","text":"<ul> <li>ESM-2 Training with Native PyTorch</li> <li>Hugging Face Trainer Documentation</li> <li>Hugging Face Accelerate Documentation</li> </ul>"},{"location":"main/recipes/recipes/esm2_accelerate_te/esm2_accelerate_te/#developer-guide","title":"Developer Guide","text":""},{"location":"main/recipes/recipes/esm2_accelerate_te/esm2_accelerate_te/#running-tests","title":"Running Tests","text":"<p>To run tests locally, run <code>recipes_local_test.py</code> from the repository root with the recipe directory as an argument.</p> <pre><code>./ci/scripts/recipes_local_test.py bionemo-recipes/recipes/esm2_accelerate_te/\n</code></pre> <p>Tests should be kept relatively fast, using the smallest model and number of training steps required to validate the feature. Hardware requirements beyond those used in CI, like a single L4, should be annotated with pytest.mark.requires, such as <code>requires_fp8</code> and <code>requires_multi_gpu</code>.</p>"},{"location":"main/recipes/recipes/esm2_accelerate_te/esm2_accelerate_te/#development-container","title":"Development Container","text":"<p>To use the provided devcontainer, use \"Dev Containers: Reopen in Container\" from the VSCode menu, and choose the \"BioNeMo Recipes Dev Container\" option. To run the tests inside the container, run <code>pytest -v .</code> in the recipe directory.</p>"},{"location":"main/recipes/recipes/esm2_accelerate_te/esm2_accelerate_te/#hydra-tips","title":"Hydra Tips","text":"<p>Hydra is a powerful configuration management library for Python. This recipe uses Hydra to manage training configurations, allowing for easy modification of training hyper-parameters and model settings.</p> <p>Configuration parameters can be overridden from the command line, for example:</p> <pre><code>accelerate launch train.py --config-name L0_sanity fp8_config.enabled=true trainer.learning_rate=2e-5\n</code></pre>"},{"location":"main/recipes/recipes/esm2_native_te/esm2_native_te/","title":"TransformerEngine-accelerated ESM-2 training with native PyTorch training loop","text":"<p>This folder demonstrates how to train TE-accelerated ESM-2 with a native PyTorch training loop, including sequence packing and FP8 precision, using fully sharded data parallel (FSDP) for distributed training.</p>"},{"location":"main/recipes/recipes/esm2_native_te/esm2_native_te/#how-to-use-this-recipe","title":"How to use this recipe","text":"<p>This folder contains an independent, minimal training example. It does not depend on any other code in the top-level bionemo-framework repository. You can download a zipped directory of this folder alone by clicking here.</p>"},{"location":"main/recipes/recipes/esm2_native_te/esm2_native_te/#how-to-deploy-this-recipe-on-cloud-providers","title":"How to deploy this recipe on cloud providers","text":"<p>\ud83d\udea7 Under development</p>"},{"location":"main/recipes/recipes/esm2_native_te/esm2_native_te/#supported-models-and-training-features","title":"Supported Models and Training Features","text":"Model BF16 FP8<sup>[1]</sup> THD Input Format FP8 with THD Input Format MXFP8<sup>[2]</sup> Context Parallelism ESM-2 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 AMPLIFY \u2705 \u274c \ud83d\udea7 \u274c \u274c \ud83d\udea7 <p>\u2705: Supported  \ud83d\udea7: Under development  \u274c: Not supported </p> <p>[1]: Requires compute capability 9.0 and above (Hopper+)  [2]: Requires compute capability 10.0 and 10.3 (Blackwell), 12.0 support pending </p>"},{"location":"main/recipes/recipes/esm2_native_te/esm2_native_te/#installing-dependencies","title":"Installing Dependencies","text":"<p>The easiest way to get started with this recipe is to use the provided Dockerfile, which uses the latest NVIDIA PyTorch base image to provide optimized versions of PyTorch and TransformerEngine. To build the container, run:</p> <pre><code>docker build -t esm2_native_te .\n</code></pre> <p>To run the container, run:</p> <pre><code>docker run -it --gpus all --network host --ipc=host --rm -v ${PWD}:/workspace/bionemo esm2_native_te /bin/bash\n</code></pre> <p>Alternatively, the dependencies can be installed manually in an environment with CUDA support. Refer to Dockerfile.cuda for the process of installing dependencies in a fresh python environment (for example, CUDA 13.0):</p> <pre><code>uv venv --python 3.12 --seed /workspace/.venv\nsource /workspace/.venv/bin/activate\nuv pip install torch==2.9.0 --index-url https://download.pytorch.org/whl/cu130\nuv pip install wheel packaging psutil\npip install --no-build-isolation \"flash-attn&gt;=2.1.1,&lt;=2.8.1\"\npip install --no-build-isolation transformer-engine[pytorch]==2.9.0\nuv pip install -r /requirements.txt\n</code></pre> <p>To build and run the CUDA base container, run:</p> <pre><code>docker build -t esm2_native_te_cuda -f Dockerfile.cuda .\ndocker run -it --gpus all --network host --ipc=host --rm -v ${PWD}:/workspace/bionemo esm2_native_te_cuda /bin/bash -c \"pytest -v .\"\n</code></pre>"},{"location":"main/recipes/recipes/esm2_native_te/esm2_native_te/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>Note: \"compiled\" refers to <code>torch.compile</code>. \"fa2\" is FlashAttention2. Recently, we measured 2800 tokens/second/GPU training speed on H100 with HuggingFace Transformers's ESM-2 implementation of THD sequence packing, however we have not been able to make this configuration work on Blackwell and this work is still in progress.</p>"},{"location":"main/recipes/recipes/esm2_native_te/esm2_native_te/#distributed-training","title":"Distributed Training","text":"<p>This recipe supports distributed training using DDP, FSDP2, and Megatron-FSDP, shown in three separate training entrypoints:</p> <ul> <li>Distributed Data Parallel (DDP), shown in <code>train_ddp.py</code></li> <li>Fully Sharded Data Parallel 2 (FSDP2), shown in <code>train_fsdp2.py</code></li> <li>Megatron-FSDP (mFSDP), shown in <code>train_mfsdp.py</code></li> </ul>"},{"location":"main/recipes/recipes/esm2_native_te/esm2_native_te/#commands-to-launch-training","title":"Commands to Launch Training","text":"<p>To run single-process training on one GPU, run:</p> <pre><code>python train_ddp.py  # or train_fsdp2.py / train_mfsdp.py\n</code></pre> <p>To run multi-process training locally on 2+ GPUs, run:</p> <pre><code>torchrun --nproc_per_node=2 train_fsdp2.py  # or train_mfsdp.py / train_ddp.py\n</code></pre> <p>Multi-Node training is supported with all three strategies, refer to <code>slurm.sh</code> for an example SLURM script.</p>"},{"location":"main/recipes/recipes/esm2_native_te/esm2_native_te/#fp8-training","title":"FP8 Training","text":"<p>To run training with FP8, enable it by overriding the <code>fp8_config.enabled=true</code> configuration parameter. Additional FP8 configuration parameters, including switching to <code>MXFP8BlockScaling</code>, can be set using the hydra configuration.</p> <pre><code>python train_fsdp2.py --config-name L0_sanity fp8_config.enabled=true\n</code></pre>"},{"location":"main/recipes/recipes/esm2_native_te/esm2_native_te/#fp8-debugging","title":"FP8 Debugging","text":"<p>We also provide a mechanism to receive tensor data related to FP8 layers during training which may include activations, weights and gradients.</p> <p>To enable this please select the following config options.</p> <pre><code>python train_fsdp2.py \\\nfp8_stats_config.enabled=True # whether to log stats or not\nfp8_stats_config.fp8_log_dir=./logs/fp8_stats_logs_dummy # where to store the logs\nfp8_stats_config.fp8_stats_file=./fp8_debugging_stats.yaml # specifies what stats you want to run. Currently this is saved in this yaml file.\nfp8_config.enabled=True # set this to use FP8 otherwise stats logging won't work\n</code></pre> <p>Note: This feature is available for the <code>train_ddp</code> and the <code>train_fsdp2</code> scripts. It is not yet available for <code>train_mfsdp</code>.</p> <p>The config file structure fp8_debugging_stats.yaml is explained in the NVIDIA Transformer Engine config file documentation in more detail. Below we will cover some very basic elements of the file structure.</p> <p>This comes as a performance cost that is dependent on the <code>freq</code> parameter mentioned above. <code>freq=1</code> collects stats on every step which in our experiments caused a ~29% decrease in throughput (executed on a single RTX 5090). We recommend using <code>freq&gt;=10</code> to reduce this performance hit.</p>"},{"location":"main/recipes/recipes/esm2_native_te/esm2_native_te/#sequence-packing-thd-input-format","title":"Sequence Packing (THD input format)","text":"<p>Sequence packing is handled using a padding-free collator (in <code>collator.py</code>) that provides input arguments, such as <code>cu_seq_lens_q</code>), needed for padding-free attention. To enable sequence packing, set <code>use_sequence_packing=true</code> in the hydra configuration.</p> <pre><code>python train_fsdp2.py --config-name L0_sanity use_sequence_packing=true\n</code></pre>"},{"location":"main/recipes/recipes/esm2_native_te/esm2_native_te/#fp8-and-sequence-packing","title":"FP8 and Sequence Packing","text":"<p>To combine FP8 training with sequence packing, the number of unpadded input tokens must be a multiple of 16. The data collator will automatically pad packed sequences to the maximum number of tokens per batch.</p> <pre><code>python train_fsdp2.py --config-name L0_sanity \\\n  fp8_config.enabled=true \\\n  use_sequence_packing=true\n</code></pre>"},{"location":"main/recipes/recipes/esm2_native_te/esm2_native_te/#context-parallelism","title":"Context Parallelism","text":"<p>We provide a training script train_ddp_cp and a sample config L0_sanity_cp that uses context parallelism.</p> <p>In the config, the argument <code>--cp_size</code> allows the user to set the size of the context parallel distributed group. When paired with Distributed Data Parallelism (DDP), the number of context parallel groups will be determined by <code>world_size//cp_size</code>.</p> <p>Thus, if a user has 8 processes and sets <code>cp_size=2</code> they will have <code>2</code> CP groups and <code>4</code> DDP groups. During dataloading we make no assumptions about the data pipeline being deterministic or not. DDP groups will provide unique data while CP groups will contain replicates of that data.</p> <p>For example, if we have 2 DDP groups and 2 CP groups. Each DDP group will have a unique dataloader DP0 for DDP group 0 and DP1 for DDP group 1. CP works by running something called ring attention, which expects tokens to live on each device in a particular layout. For this CP implementation we use something called Dual Chunk Swapping. If DP0 outputs sequence <code>1 2 3 4 5 6 7 8</code> and DP1 outputs <code>9 10 11 12 13 14 15 16</code> then when we run through the <code>CPAwareDataloader</code> defined in datasets, the dataloader will create CP shards from that DP group as follows:</p> <pre><code>      |   DP0   |    DP1        |\n  CP0 | 1,2,7,8 | 9, 10, 15, 16 |\n  CP1 | 3,4,5,6 | 11, 12, 13, 14|\n</code></pre> <p>You may notice these shards and wonder why they are the way they are. The reason is that CP groups are sharded using slices. The full input sequence (such as <code>1 2 3 4 5 6 7</code>) is sliced into <code>2 * cp_size</code> groups. Then CP0 takes the first and last slice, while CP1 takes the middle slices, of each sequence.</p> <p>In this example, we only show one sequence but its important to note that slicing takes place on every sequence, so if a second sequence is also available, that will be sliced in the same manner. CP0 will take the first and last slice of every sequence, while CP1 will take the middle slices of each sequence.</p>"},{"location":"main/recipes/recipes/esm2_native_te/esm2_native_te/#comparing-against-the-hf-transformers-reference-implementation","title":"Comparing Against the HF Transformers Reference Implementation","text":"<p>To launch training with the ESM-2 model as implemented in HF Transformers, pass a <code>facebook/esm2</code> checkpoint as the model tag:</p> <pre><code>python train_fsdp2.py --config-name L0_sanity model_tag=facebook/esm2_t6_8M_UR50D\n</code></pre>"},{"location":"main/recipes/recipes/esm2_native_te/esm2_native_te/#downloading-pre-training-data-for-offline-training","title":"Downloading Pre-Training Data For Offline Training","text":"<p>An example pre-training dataset for ESM-2 is available in the <code>nvidia/esm2_uniref_pretraining_data</code> Hugging Face dataset. This dataset can be streamed from the Hugging Face Hub by using the following.</p> <pre><code>&gt;&gt;&gt; from datasets import load_dataset\n&gt;&gt;&gt; dataset = load_dataset('nvidia/esm2_uniref_pretraining_data', split='train', streaming=True)\n&gt;&gt;&gt; print(next(iter(dataset)))\n{'sequence': 'MSPRRTGGARPPGPCTPCGPRPRCPSRRSAAARPAPSAAPARRARPGRRPGCRPGTDCPGTARRPGGGP...',\n 'ur50_id': 'UniRef50_A0A081XN86',\n 'ur90_id': 'UniRef90_UPI002FBE17D9'}\n</code></pre> <p>For large-scale training, the dataset should be downloaded locally with the huggingface CLI, with appropriate values set for <code>HF_HOME</code> and <code>HF_TOKEN</code> environment variables. Use <code>uv tool install huggingface_hub</code> to install the CLI if not already installed.</p> <pre><code>export HF_TOKEN=&lt;your_huggingface_token&gt;\nhf download nvidia/esm2_uniref_pretraining_data --repo-type dataset --local-dir /path/to/download/directory\n# Test to ensure the dataset can be loaded correctly\npython -c \"import datasets; datasets.load_dataset('/path/to/download/directory', split='train', streaming=True)\"\n</code></pre> <p>Pass the downloaded dataset directory to the training script as the <code>dataset.path</code> configuration parameter.</p> <pre><code>HF_DATASETS_OFFLINE=1 python train_fsdp2.py --config-name L0_sanity \\\n  dataset.load_dataset_kwargs.path=/path/to/download/directory\n</code></pre>"},{"location":"main/recipes/recipes/esm2_native_te/esm2_native_te/#saving-and-loading-checkpoints","title":"Saving and Loading Checkpoints","text":"<p>To enable checkpoint saving, ensure that <code>checkpoint.ckpt_dir</code> is set to a writable directory. Checkpointing frequency is controlled by the <code>checkpoint.save_every_n_steps</code> configuration parameter.</p> <pre><code>python train_fsdp2.py --config-name L0_sanity \\\n  checkpoint.ckpt_dir=/path/to/ckpt_dir \\\n  checkpoint.save_every_n_steps=100\n</code></pre> <p>To enable checkpoint loading, set <code>checkpoint.resume_from_checkpoint=true</code> to resume from the latest checkpoint.</p> <pre><code>python train_fsdp2.py --config-name L0_sanity \\\n  checkpoint.ckpt_dir=/path/to/ckpt_dir \\\n  checkpoint.resume_from_checkpoint=true\n</code></pre> <p>We also show how to export a final model at the end of training, which is suitable for uploading to the Hugging Face Hub or for local inference as a more durable format than torch distributed checkpoints. To enable this, set <code>checkpoint.save_final_model=true</code> in the hydra configuration. The resulting model will be saved to the <code>final_model</code> directory within the checkpoint directory.</p> <p>Checkpointing is implemented for all three strategies, see <code>checkpoint.py</code> for more details.</p>"},{"location":"main/recipes/recipes/esm2_native_te/esm2_native_te/#saving-dataloader-state-with-statefuldataloader","title":"Saving Dataloader State with <code>StatefulDataLoader</code>","text":"<p>These examples show how to save and resume your dataloader by passing the dataloader instance to our <code>save_checkpoint_*</code> and <code>load_checkpoint_*</code> functions using the <code>StatefulDataLoader</code> class from <code>torchdata</code>. See <code>checkpoint.py</code> for implementation details.</p> <p>For references on <code>StatefulDataLoader</code> and it's integration with <code>datasets</code> see:</p> <ul> <li>https://github.com/meta-pytorch/data/tree/main/torchdata/stateful_dataloader</li> <li>https://huggingface.co/docs/datasets/en/stream#save-a-dataset-checkpoint-and-resume-iteration</li> </ul> <p>Known limitations:</p> <ul> <li>When loading the dataloader from a saved checkpoint, you must provide the same <code>num_workers</code> that you used to save the   dataloader state, because state is saved at the worker-level.</li> <li>Moreover, dataloader state is saved on a per-rank basis. So if you resume training and load the dataloader with a   different amount of nodes / gpus that was used when you saved the dataloader the state will not resume perfectly.</li> </ul>"},{"location":"main/recipes/recipes/esm2_native_te/esm2_native_te/#running-inference-with-the-trained-model","title":"Running Inference with the Trained Model","text":"<p>Models can be loaded from the final checkpoint directory using the <code>AutoModel.from_pretrained</code> method. For example:</p> <pre><code>from transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained(\"path/to/final_model\")\ntokenizer = AutoTokenizer.from_pretrained(\"...\")\n\ngfp_P42212 = (\n    \"MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTL\"\n    \"VTTFSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLV\"\n    \"NRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLAD\"\n    \"HYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\"\n)\n\ninputs = tokenizer(gfp_P42212, return_tensors=\"pt\")\nmodel.eval()\noutput = model(**inputs)\n</code></pre>"},{"location":"main/recipes/recipes/esm2_native_te/esm2_native_te/#performance","title":"Performance","text":"<p>\ud83d\udea7 Under development</p>"},{"location":"main/recipes/recipes/esm2_native_te/esm2_native_te/#reference","title":"Reference","text":"<ul> <li>ESM-2 Training with Accelerate</li> </ul>"},{"location":"main/recipes/recipes/esm2_native_te/esm2_native_te/#developer-guide","title":"Developer Guide","text":""},{"location":"main/recipes/recipes/esm2_native_te/esm2_native_te/#running-tests","title":"Running Tests","text":"<p>To run tests locally, run <code>recipes_local_test.py</code> from the repository root with the recipe directory as an argument.</p> <pre><code>./ci/scripts/recipes_local_test.py bionemo-recipes/recipes/esm2_native_te/\n</code></pre> <p>Tests should be kept relatively fast, using the smallest model and number of training steps required to validate the feature. Hardware requirements beyond those used in CI (e.g., a single L4) should be annotated with pytest.mark.requires, e.g. <code>requires_fp8</code> and <code>requires_multi_gpu</code>.</p>"},{"location":"main/recipes/recipes/esm2_native_te/esm2_native_te/#development-container","title":"Development Container","text":"<p>To use the provided devcontainer, use \"Dev Containers: Reopen in Container\" from the VSCode menu, and choose the \"BioNeMo Recipes Dev Container\" option. To run the tests inside the container, run <code>pytest -v .</code> in the recipe directory.</p>"},{"location":"main/recipes/recipes/esm2_native_te/esm2_native_te/#hydra-tips","title":"Hydra Tips","text":"<p>Hydra is a powerful configuration management library for Python. This recipe uses Hydra to manage training configurations, allowing for easy modification of training hyper-parameters and model settings.</p> <p>Configuration parameters can be overridden from the command line. For example, <code>python train_fsdp2.py --config-name L0_sanity fp8_config.enabled=true</code>.</p> <p>For verbose logging, use the hydra command line override <code>hydra.verbose=true</code>, see https://hydra.cc/docs/tutorials/basic/running_your_app/logging/ for more details.</p>"},{"location":"main/recipes/recipes/esm2_peft_te/esm2_peft_te/","title":"PEFT Fine-tuning with TransformerEngine-accelerated ESM-2","text":"<p>This folder demonstrates how to fine-tune a TransformerEngine-accelerated ESM-2 model using PEFT.</p> <p>Note: This recipe is a work in progress, and currently only demonstrates basic support for LoRA fine-tuning and TransformerEngine layers. Refer to <code>bionemo-recipes/models/esm2/tests/test_peft.py</code> for additional information and known limitations.</p>"},{"location":"main/recipes/recipes/evo2_megatron/evo2_megatron/","title":"Evo2 Recipe","text":"<p>This recipe is work-in-progress rewrite of the nemo2 based bionemo/evo2 package into a self-contained training repository that makes use of megatron bridge.</p>"},{"location":"main/recipes/recipes/evo2_megatron/evo2_megatron/#installation","title":"Installation","text":"<pre><code># 1. Create venv (CRITICAL: include system packages so it sees the container's PyTorch)\nexport UV_LINK_MODE=copy\nuv venv --system-site-packages --seed /workspace/.venv\n\n# 2. Activate the environment\nsource /workspace/.venv/bin/activate\npip freeze | grep transformer_engine &gt; pip-constraints.txt\nuv pip install -r build_requirements.txt --no-build-isolation  # some extra requirements are needed for building\nuv pip install -c pip-constraints.txt -e . --no-build-isolation\n</code></pre>"},{"location":"main/recipes/recipes/evo2_megatron/evo2_megatron/#usage","title":"Usage","text":""},{"location":"main/recipes/recipes/evo2_megatron/evo2_megatron/#example-job","title":"Example job","text":"<pre><code># 3. Run an example job\n## 2. if on a6000s, you may need to disable p2p to avoid crashing\nexport NCCL_P2P_DISABLE=1\n## 3. Run the job:\ntorchrun --nproc-per-node 2 --no-python \\\n  train_evo2 \\\n  --hf-tokenizer-model-path tokenizers/nucleotide_fast_tokenizer_256 \\\n  --model-size striped_hyena_1b_nv_parallel --max-steps 12 --eval-interval 10 \\\n  --eval-iters 3 --mock-data \\\n  --micro-batch-size 16 --global-batch-size 32 --seq-length 1024 \\\n  --tensor-model-parallel 1 \\\n  --use-precision-aware-optimizer --dataset-seed 33 \\\n  --seed 41 --spike-no-more-embedding-init \\\n  --no-weight-decay-embeddings --cross-entropy-loss-fusion \\\n  --align-param-gather --overlap-param-gather  --grad-reduce-in-fp32 \\\n  --decay-steps 100 --warmup-steps 10 \\\n  --mixed-precision-recipe bf16_with_fp8_current_scaling_mixed \\\n  --no-fp32-residual-connection --activation-checkpoint-recompute-num-layers 1 \\\n  --attention-dropout 0.001 --hidden-dropout 0.001 \\\n  --eod-pad-in-loss-mask --enable-preemption \\\n  --log-interval 5 --debug-ddp-parity-freq 10 \\\n  --result-dir tmpfp8 --no-renormalize-loss\n</code></pre>"},{"location":"main/recipes/recipes/evo2_megatron/evo2_megatron/#example-fine-tune-from-an-existing-checkpoint","title":"Example fine-tune from an existing checkpoint","text":"<p>First convert the checkpoint from nemo2 format (temporary step until we upload the new files)</p> <p>Good checkpoint names to try are:</p> <ul> <li>evo2/1b-8k-bf16:1.0 (model_size: 1b)</li> <li>evo2/7b-1m:1.0 (model_size: 7b_arc_longcontext)</li> <li>evo2/40b-1m-fp8-bf16:1.0 (model_size: 40b_arc_longcontext)</li> </ul> <p>Other than the 7b version, the other two are checkpoints fine-tuned by the BioNeMo team to support both FP8 and BF16 precision. The 7b version worked well on both FP8 and BF16 out of the box so it was not fine-tuned further. If you do want to use one of the FP8 sensitive checkpoints, like <code>evo2/40b-1m</code> then be sure to add the <code>--vortex-style-fp8</code> option to the checkpoint conversion step below. Also note that although 8k versions of the 7b and 40b checkpoints exist, it is advisable to use the longer context versions since they were trained further and still run on shorter inputs.</p> <p>See <code>download_bionemo_data --list-resources</code> for other checkpoint options and a list of available downloadable resources.</p> <pre><code>CKPT_NAME=evo2/1b-8k-bf16:1.0\nCKPT_OUT_DIR=evo2_1b_8k_bf16_mbridge\nevo2_convert_nemo2_to_mbridge \\\n  --mixed-precision-recipe bf16_with_fp8_current_scaling_mixed \\\n  --tokenizer-path tokenizers/nucleotide_fast_tokenizer_512 \\\n  --model-size 1b \\\n  --seq-length 8192 \\\n  --nemo2-ckpt-dir $(download_bionemo_data $CKPT_NAME) \\\n  --mbridge-ckpt-dir $CKPT_OUT_DIR\n</code></pre> <p>Now run like before, but include the fine-tuned checkpoint directory you converted in the previous step with <code>--finetune-ckpt-dir $CKPT_OUT_DIR</code>. Also if you have problems with <code>bf16_with_fp8_current_scaling_mixed</code> try <code>bf16_mixed</code>.</p> <pre><code>torchrun --nproc-per-node 2 --no-python \\\n  train_evo2 \\\n  --hf-tokenizer-model-path tokenizers/nucleotide_fast_tokenizer_512 \\\n  --model-size 1b --max-steps 12 --eval-interval 10 \\\n  --eval-iters 3 --mock-data \\\n  --micro-batch-size 16 --global-batch-size 32 --seq-length 1024 \\\n  --tensor-model-parallel 1 \\\n  --use-precision-aware-optimizer --dataset-seed 33 \\\n  --seed 41 \\\n  --cross-entropy-loss-fusion \\\n  --align-param-gather --overlap-param-gather  --grad-reduce-in-fp32 \\\n  --decay-steps 100 --warmup-steps 10 \\\n  --mixed-precision-recipe bf16_with_fp8_current_scaling_mixed \\\n  --no-fp32-residual-connection --activation-checkpoint-recompute-num-layers 1 \\\n  --attention-dropout 0.001 --hidden-dropout 0.001 \\\n  --eod-pad-in-loss-mask --enable-preemption \\\n  --log-interval 5 --debug-ddp-parity-freq 10 \\\n  --result-dir tmpfp8-ft-example --no-renormalize-loss \\\n  --finetune-ckpt-dir $CKPT_OUT_DIR\n</code></pre>"},{"location":"main/recipes/recipes/evo2_megatron/evo2_megatron/#where-do-the-custom-command-line-programs-come-from","title":"Where do the custom command line programs come from?","text":"<p>See <code>pyproject.toml</code> for where runnable programs like <code>train_evo2</code> and <code>evo2_convert_nemo2_to_mbridge</code> are implemented in code.</p>"},{"location":"main/recipes/recipes/evo2_megatron/evo2_megatron/#docker-build","title":"Docker build","text":"<pre><code>docker build -t evo2_megatron_recipe-$(git rev-parse --short HEAD) .\n</code></pre>"},{"location":"main/recipes/recipes/evo2_megatron/evo2_megatron/#performance-and-accuracy-comparisons","title":"Performance and accuracy comparisons","text":"<p>NOTE: this section is largely a work in progress. This reflects the most updated information, but may not reflect the current state of the code base at any given time.</p>"},{"location":"main/recipes/recipes/evo2_megatron/evo2_megatron/#training-accuracy-convergence","title":"Training accuracy convergence","text":"<p>We ran a 12 hour 48 H100 GPU training run to compare megatron bridge with nemo2. We found that FP8 current scaling converges by around the 5,000th step to the bf16 lines. And that bf16 is comparable with nemo2. Interestingly in nemo2 bf16 and fp8 followed nearly identical trajectories for the first 5k steps as well. Note that in a typical training run we are performing over 100k steps, so different behavior in the first 5k steps is less worrisome if the endpoints are comparable.</p> <p></p>"},{"location":"main/recipes/recipes/evo2_megatron/evo2_megatron/#training-performance-comparisons","title":"Training performance comparisons","text":"<p>FP8 current scaling which is supposed to have better convergence properties than delayed scaling, performs nearly as well as delayed scaling in mbridge. Even leaving multiple transformer layers in bf16 precision trains faster than fp8 delayed scaling in nemo2.</p> Evo2 1B Run Seconds per step (lower is better) Tokens/sec/GPU Global Batch Size Number of GPUs Vocab Size MBridge BF16 6.10 26,859 960 48 256 MBridge FP8 (delayed) 5.38 30,453 960 48 256 MBridge FP8 (current) 5.44 28,755 960 48 512 MBridge FP8 (current first/last two layers bf16) 5.47 28,598 960 48 512 Nemo2 FP8 (delayed) 6.18 26,511 960 48 512 <p>Activation memory optimizations have enabled context parallelism to work better with evo2 style models in our mbridge implementation than the previous nemo2 implementation. Since TP requires more node to node communication, you generally want to limit TP to your fastest interconnects, which are typically configured in nodes of 8 GPUs. Evo2 would previously OOM with these more ideal configurations, requiring much larger than typical levels of TP to handle long context training. With our latest changes to the evo2 forward pass, we can now handle more typical TP vs CP configurations. This enables significantly faster step timing at long context, as well as demonstrating up to 2M context length. We have currently demonstrated small training runs at 2M context on only 512 H100 GPUs for the 40b parameter model.</p> Configuration Precision TP CP Number of Nodes Number of GPUs Context Length Global Batch Size Seconds per Step NeMo2 fp8-delayed 64 2 32 256 1M 2 44 NeMo2 fp8-delayed 8 16 32 256 1M 2 OOM MBridge Optimized bf16 8 16 32 256 1M 2 30 2M Stress Test bf16 8 32 64 512 2M 2 48"},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/","title":"FP8 Training Analyzer - User Guide","text":"<p>A model-agnostic tool for analyzing FP8 quantization logs and visualizing gradient underflows during training.</p>"},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#what-does-this-tool-do","title":"\ud83c\udfaf What Does This Tool Do?","text":"<p>The FP8 Analyzer helps you diagnose training issues caused by FP8 quantization by:</p> <ol> <li>Parsing training logs - Extracts FP8 metrics from your training runs</li> <li>Auto-detecting model architecture - Identifies encoder layers, head layers, and embeddings</li> <li>Generating publication-quality heatmaps - Visualizes gradient underflows across all model components over time</li> <li>Exporting structured data - Saves metrics to CSV for further analysis</li> </ol>"},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#key-features","title":"Key Features","text":"<p>\u2705 Model-agnostic - Works with any transformer architecture (ESM, BERT, GPT, T5, etc.) \u2705 Automatic component detection - No configuration needed \u2705 Beautiful visualizations - 600 DPI publication-ready heatmaps \u2705 Easy comparison - Run on multiple experiments with suffixes</p>"},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#how-to-use-it","title":"How to use it","text":"<p>Before using this tool, you must first gather FP8 statistics during training. We currently support the following models and training scripts.</p> Model DDP FSDP2 MFSDP ESM2 \u2713 \u2713 \u2717 LLAMA3 \u2713 \u2713 \u2717 <p>To gather FP8 statistics for analysis, refer to the model-specific documentation (e.g., ESM2 FP8 Debugging) or add these arguments to your training command:</p> <pre><code>python train_fsdp2.py \\\nfp8_stats_config.enabled=True # whether to log stats or not\nfp8_stats_config.fp8_log_dir=./logs/fp8_stats_logs_dummy # where to store the logs\nfp8_stats_config.fp8_stats_file=./fp8_stats.yaml # specifies what stats you want to run. Currently this is saved in this yaml file.\nfp8_config.enabled=True # set this to use FP8 otherwise stats logging won't work\n</code></pre> <p>Once the run is completed. The <code>fp8_stats_config.fp8_log_dir</code> should have several directories under it. It should look like this</p> <pre><code>\u2514\u2500\u2500 rank_0\n    \u251c\u2500\u2500 nvdlfw_inspect_logs\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 nvdlfw_inspect_globalrank-0.log\n    \u2514\u2500\u2500 nvdlfw_inspect_statistics_logs\n        \u2514\u2500\u2500 nvdlfw_inspect_globalrank-0.log\n</code></pre> <p>Here we can see that there are directories for each rank. This is intended in case one wants to do a rank-by-rank analysis. As we can see, there are <code>inspect_logs</code> and <code>inspect_statistics_logs</code>. The <code>inspect_logs</code> will tell you what layer names are being tracked as well as what tensor values are being logged. The <code>inspect_statistics_logs</code> holds the actual stats for the runs, which should have a value for every tracked tensor at iterations specified by the <code>freq</code> parameter in the log config file (specified by <code>fp8_stats_config.fp8_stats_file</code>).</p>"},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#sample-output","title":"\ud83d\udcca Sample Output","text":""},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#example-full-fp8-run-encoder-head-in-fp8","title":"Example: Full FP8 Run (Encoder + Head in FP8)","text":"<p>Command:</p> <pre><code>python3 analyze_and_create_heatmap.py fp8logswithhead\n</code></pre> <p>Result: </p> <p>What you see:</p> <ul> <li>35 components: 33 encoder layers + 2 head layers (Dense, Decoder)</li> <li>Red/orange bands: Critical underflows in early layers (2-5) and late layer (33)</li> <li>Head layers affected: Both Dense (2.3%) and Decoder (2.5%) show underflows</li> <li>White separator line: Divides encoder layers from head layers</li> <li>U-shape pattern: Middle layers (7-28) are fine, but edges suffer</li> <li>Max underflow: 5.89% at Layer 33</li> <li>Yellow boxes: Highlight the 5 worst components</li> </ul> <p>Interpretation: This is a problematic run where FP8 quantization causes significant gradient underflows throughout the model. The head being in FP8 amplifies problems in the encoder. Early layers (2-5) suffer from vanishing gradients, while late layers (32-33) and head layers receive noisy gradients from FP8 quantization.</p>"},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#1-run-the-analyzer","title":"1. Run the Analyzer","text":"<pre><code>python3 analyze_and_create_heatmap.py &lt;log_directory&gt;\n</code></pre>"},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#2-view-output","title":"2. View Output","text":"<p>The script generates two files:</p> <pre><code>analysis_output/csv_data/rank_0_metrics.csv\nheatmap_visualization/heatmap_highres.png\n</code></pre> <p>Open the PNG to see your heatmap!</p>"},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#understanding-the-heatmap","title":"\ud83d\udcd6 Understanding the Heatmap","text":""},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#color-scale","title":"Color Scale","text":"Color Underflow % Meaning \ud83d\udfe2 Green &lt; 0.5% \u2705 Acceptable - Normal quantization noise \ud83d\udfe1 Yellow 0.5-2% \u26a0\ufe0f Warning - Monitor but not critical \ud83d\udfe0 Orange 2-4% \ud83d\udd36 Critical - Significant learning signal loss \ud83d\udd34 Red &gt; 4% \u274c Severe - Major training instability risk"},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#visual-elements","title":"Visual Elements","text":"<ol> <li>Yellow boxes - Highlight the 5 worst components (&gt;2% underflows)</li> <li>White separator lines - Divide component groups (Encoder | Head | Embedding)</li> <li>Cyan vertical line - Marks iteration 3000 (common divergence point)</li> <li>Side labels - Show component groups (ENCODER, HEAD)</li> <li>Summary box (top-right) - Key statistics</li> </ol>"},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#interpreting-patterns","title":"Interpreting Patterns","text":""},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#good-pattern","title":"\u2705 Good Pattern","text":"<pre><code>Most layers: Green\nFew yellow spots: Acceptable\nMax &lt; 2%: Safe to continue training\n</code></pre>"},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#warning-pattern","title":"\u26a0\ufe0f Warning Pattern","text":"<pre><code>Some layers: Orange (2-4%)\nIsolated to 1-3 layers: Monitor closely\nMax &lt; 4%: Consider adjusting FP8 settings\n</code></pre>"},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#bad-pattern","title":"\u274c Bad Pattern","text":"<pre><code>Multiple layers: Red (&gt;4%)\nU-shape (early + late): Gradient flow issues\nMax &gt; 5%: High risk of divergence\n</code></pre>"},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#common-scenarios","title":"\ud83d\udd0d Common Scenarios","text":""},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#scenario-1-u-shape-pattern-early-late-layers","title":"Scenario 1: U-Shape Pattern (Early + Late Layers)","text":"<p>What it looks like:</p> <ul> <li>Layers 1-5: Red/Orange</li> <li>Layers 6-28: Green</li> <li>Layers 29-33: Red/Orange</li> <li>Head: Red/Orange</li> </ul> <p>Why it happens:</p> <ul> <li>Early layers: Far from loss, gradients shrink through backprop (vanishing gradient)</li> <li>Late layers: Close to loss but receive noisy gradients from head</li> <li>Middle layers: Goldilocks zone - far enough to have stable gradients, close enough to receive clean signal</li> </ul> <p>Solution:</p> <pre><code># Keep problematic layers in higher precision\nfp8_skip_layers = [\n    \"layers.1\",\n    \"layers.2\",\n    \"layers.3\",  # Early layers\n    \"layers.31\",\n    \"layers.32\",\n    \"layers.33\",  # Late layers\n    \"lm_head.dense\",\n    \"lm_head.decoder\",  # Head\n]\n</code></pre>"},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#scenario-2-head-only-problem","title":"Scenario 2: Head-Only Problem","text":"<p>What it looks like:</p> <ul> <li>Encoder layers: Mostly green</li> <li>Head (Dense/Decoder): Red/Orange</li> </ul> <p>Why it happens:</p> <ul> <li>Head has small vocabulary weight matrix with large dynamic range</li> <li>Gradients from cross-entropy loss can be very small or very large</li> <li>FP8 struggles with this high dynamic range</li> </ul> <p>Solution:</p> <pre><code># Keep head in BF16\nfp8_enabled = True\nfp8_skip_layers = [\"lm_head.dense\", \"lm_head.decoder\"]\n</code></pre> <p>Expected improvement: 34-57% reduction in encoder underflows (validated!)</p>"},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#reading-the-log-output","title":"\ud83d\udcd0 Reading the Log Output","text":"<p>When you run the script, you'll see:</p> <pre><code>INFO - ================================================================================\nINFO - MODEL-AGNOSTIC FP8 LOG ANALYZER &amp; HEATMAP GENERATOR\nINFO - ================================================================================\nINFO - Log directory: fp8logswithhead\nINFO - Output suffix: '_fp8head' (if provided)\nINFO - ================================================================================\n\nINFO - ================================================================================\nINFO - PARSING MODEL ARCHITECTURE\nINFO - ================================================================================\nINFO - Metadata: fp8logswithhead/rank_0/nvdlfw_inspect_logs/nvdlfw_inspect_globalrank-0.log\nINFO - Found 373 layer names\n\nINFO - Model Structure:\nINFO -   Encoder layers: 33\nINFO -     Range: Layer 1 to 33\nINFO -   Head layers: 3\nINFO -     - model.lm_head\nINFO -     - model.lm_head.dense\nINFO -     - model.lm_head.decoder\n\nINFO - ================================================================================\nINFO - PARSING LOG FILE\nINFO - ================================================================================\nINFO - File: fp8logswithhead/.../nvdlfw_inspect_globalrank-0.log\nINFO - Processed 500,000 lines...\nINFO - Total lines: 6,013,920\nINFO - Metrics extracted: 6,013,920\nINFO - Iteration range: 0 to 7369\n\nINFO - ================================================================================\nINFO - AUTO-DETECTING COMPONENTS\nINFO - ================================================================================\nINFO - Found 68 gradient underflow metrics\n\nINFO - Component Summary:\nINFO -   Encoder: 33 components\nINFO -   Head: 2 components\nINFO -     - Decoder\nINFO -     - Dense\n\nINFO - ================================================================================\nINFO - CREATING HEATMAP\nINFO - ================================================================================\nINFO - Components: 35\nINFO - Data points: 257,950\nINFO - Heatmap dimensions: 35 components \u00d7 121 time points\n\nINFO - \u2728 Saved heatmap: heatmap_visualization/heatmap_highres_fp8head.png\nINFO -    Max underflow: 5.89%\nINFO -    Critical components (&gt;2%): 5\n\nINFO - ================================================================================\nINFO - \u2705 COMPLETE\nINFO - ================================================================================\n</code></pre> <p>Key metrics to watch:</p> <ul> <li>Max underflow: Should be &lt; 2% ideally, &lt; 4% acceptable</li> <li>Critical components: Fewer is better</li> <li>Iteration range: Ensure you have enough data</li> </ul>"},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#advanced-analysis","title":"\ud83d\udd2c Advanced Analysis","text":""},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#comparing-multiple-runs","title":"Comparing Multiple Runs","text":"<p>To compare different experiments, run the analyzer in separate directories or rename the output files after each run:</p> <pre><code># Run 1: Analyze and save results\npython3 analyze_and_create_heatmap.py logs_fp8_full\nmv heatmap_visualization/heatmap_highres.png heatmap_visualization/run1_fp8.png\nmv analysis_output/csv_data/rank_0_metrics.csv analysis_output/csv_data/run1_fp8.csv\n\n# Run 2: Analyze next experiment\npython3 analyze_and_create_heatmap.py logs_bf16_head\nmv heatmap_visualization/heatmap_highres.png heatmap_visualization/run2_bf16.png\nmv analysis_output/csv_data/rank_0_metrics.csv analysis_output/csv_data/run2_bf16.csv\n</code></pre> <p>Then compare the heatmaps side-by-side!</p>"},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#extracting-specific-metrics","title":"Extracting Specific Metrics","text":"<p>The CSV output contains all metrics:</p> <pre><code>import pandas as pd\n\n# Load data\ndf = pd.read_csv(\"analysis_output/csv_data/rank_0_metrics.csv\")\n\n# Get Layer 33 underflows over time\nlayer33 = df[\n    df[\"metric_name\"]\n    == \"model.esm.encoder.layers.33.self_attention.layernorm_qkv_gradient_underflows%\"\n]\n\n# Plot\nimport matplotlib.pyplot as plt\n\nplt.plot(layer33[\"iteration\"], layer33[\"value\"])\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Gradient Underflow %\")\nplt.title(\"Layer 33 Gradient Underflows\")\nplt.show()\n</code></pre>"},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#faq","title":"\u2753 FAQ","text":""},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#q-what-if-my-model-has-a-different-architecture","title":"Q: What if my model has a different architecture?","text":"<p>A: The script auto-detects layers! It works with:</p> <ul> <li>ESM: <code>model.esm.encoder.layers.N</code></li> <li>BERT: <code>model.encoder.layer.N</code></li> <li>GPT: <code>model.transformer.layers.N</code></li> <li>Custom: Any pattern with <code>.layers.N.</code> or <code>.layer.N.</code></li> </ul>"},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#q-why-are-some-layers-missing-in-my-heatmap","title":"Q: Why are some layers missing in my heatmap?","text":"<p>A: If you used <code>fp8_skip_layers</code>, those layers won't be in FP8 and won't have underflow metrics logged. This is expected!</p>"},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#q-whats-a-good-underflow-percentage","title":"Q: What's a \"good\" underflow percentage?","text":"<p>A:</p> <ul> <li>&lt; 0.5%: Excellent</li> <li>0.5-1%: Good</li> <li>1-2%: Acceptable</li> <li>2-4%: Concerning</li> <li>&gt; 4%: Critical - action needed</li> </ul>"},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#q-can-i-change-the-color-scale","title":"Q: Can I change the color scale?","text":"<p>A: Yes! Edit the <code>max_val</code> variable in the <code>create_heatmap</code> function in <code>analyze_and_create_heatmap.py</code>:</p> <pre><code>max_val = min(6, pivot_sample.values.max())  # Change 6 to your max\n</code></pre>"},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#q-how-do-i-export-to-pdf","title":"Q: How do I export to PDF?","text":"<p>A: Use ImageMagick or Preview.app:</p> <pre><code># macOS\nopen heatmap_visualization/heatmap_highres.png\n# File \u2192 Export as PDF\n\n# Linux\nconvert heatmap_visualization/heatmap_highres.png output.pdf\n</code></pre>"},{"location":"main/recipes/recipes/fp8_analysis/fp8_analysis/#key-takeaways","title":"\ud83c\udf93 Key Takeaways","text":"<ol> <li>Gradient underflows % shows how much learning signal is lost - Keep it under 2%</li> <li>U-shape pattern indicates vanishing/noisy gradients - Fix by keeping problematic layers in BF16</li> <li>Head precision matters - BF16 head reduces encoder underflows by 34-57%</li> <li>Compare runs visually - Side-by-side heatmaps quickly show improvements</li> <li>Early detection is key - Run this analyzer frequently during training</li> </ol> <p>Need help? Check the example heatmaps above or refer to the FAQ section!</p> <p>Generated: January 13, 2026</p>"},{"location":"main/recipes/recipes/geneformer_native_te_mfsdp_fp8/geneformer_native_te_mfsdp_fp8/","title":"Geneformer native te mfsdp fp8","text":"<p>\ud83e\udd16 AI AGENTS: START HERE \u2192 AGENT_DOCUMENTATION.md</p>"},{"location":"main/recipes/recipes/geneformer_native_te_mfsdp_fp8/geneformer_native_te_mfsdp_fp8/#important-for-ai-agents","title":"\u26a0\ufe0f IMPORTANT FOR AI AGENTS \u26a0\ufe0f","text":"<p>DO NOT proceed without reading AI_DOCUMENTATION.md first. This file contains comprehensive documentation specifically designed for AI agents. Please see gitingest.txt for the complete codebase.</p>"},{"location":"main/recipes/recipes/geneformer_native_te_mfsdp_fp8/geneformer_native_te_mfsdp_fp8/#geneformer-pretraining-with-mfsdp-and-a-custom-pytorch-training-loop","title":"Geneformer Pretraining with mfsdp and a custom pytorch training loop.","text":"<p>The code runs inside of a container. To construct this container refer to container build and container run. In this folder, we supply a pretraining script capable of training several variants of Geneformer. Those variants are located in our hydra_config. This code was forked from the original geneformer repository, and enhanced to increase its performance.</p> <p>Geneformer is a BERT-based transformer pretrained on single-cell transcriptomes. For more information, refer to the nature paper here.</p>"},{"location":"main/recipes/recipes/geneformer_native_te_mfsdp_fp8/geneformer_native_te_mfsdp_fp8/#training-commands","title":"Training Commands","text":""},{"location":"main/recipes/recipes/geneformer_native_te_mfsdp_fp8/geneformer_native_te_mfsdp_fp8/#basic-training","title":"Basic Training","text":"<pre><code># Single GPU training\ntorchrun --nproc_per_node=1 train.py --config-name &lt;config_name&gt;\n\n# Multi-GPU training (single node)\ntorchrun --nproc_per_node=&lt;num_gpus&gt; train.py --config-name &lt;config_name&gt;\n\n# Quick sanity check with included dataset\ntorchrun --nproc_per_node=1 train.py\n</code></pre> <p>Note: The config name is the filename without <code>.yaml</code> extension (for example, <code>4b</code> for <code>4b.yaml</code>).</p>"},{"location":"main/recipes/recipes/geneformer_native_te_mfsdp_fp8/geneformer_native_te_mfsdp_fp8/#advanced-configuration","title":"Advanced Configuration","text":"<p>You can override config parameters directly from the command line. The most common options are:</p> <ul> <li><code>model.use_te_layers=True/False</code> - Enable/disable Transformer Engine layers</li> <li><code>training.use_fp8=True/False</code> - Enable/disable FP8 precision (requires H100+ GPUs)</li> <li><code>training.use_mfsdp=True/False</code> - Enable/disable mfsdp distributed training</li> </ul>"},{"location":"main/recipes/recipes/geneformer_native_te_mfsdp_fp8/geneformer_native_te_mfsdp_fp8/#examples","title":"Examples","text":"<pre><code># Run 106M model with default settings\ntorchrun --nproc_per_node=1 train.py --config-name 106m\n\n# Run 106M model without Transformer Engine layers\ntorchrun --nproc_per_node=2 train.py --config-name 106m model.use_te_layers=False\n\n# Run 4B model with FP8 and mfsdp enabled\ntorchrun --nproc_per_node=4 train.py --config-name 4b training.use_fp8=True training.use_mfsdp=True\n</code></pre>"},{"location":"main/recipes/recipes/geneformer_native_te_mfsdp_fp8/geneformer_native_te_mfsdp_fp8/#configuration-files","title":"Configuration Files","text":"<p>We provide pre-built configuration files for different Geneformer model sizes. Each configuration supports optional features like Transformer Engine and FP8 precision.</p>"},{"location":"main/recipes/recipes/geneformer_native_te_mfsdp_fp8/geneformer_native_te_mfsdp_fp8/#available-models","title":"Available Models","text":"<ul> <li>10M parameters: <code>10m.yaml</code></li> <li>106M parameters: <code>106m.yaml</code></li> <li>4B parameters: <code>4b.yaml</code></li> </ul>"},{"location":"main/recipes/recipes/geneformer_native_te_mfsdp_fp8/geneformer_native_te_mfsdp_fp8/#testing-configuration","title":"Testing Configuration","text":"<pre><code># Test your configuration settings\npython train.py --config-name l0_sanity --cfg all\n</code></pre> <p>Important: FP8 precision requires GPU compute capability \u2265 8.9 (H100+ GPUs). Disable FP8 mode on older hardware.</p> <pre><code># Model configuration for 4B parameter model\nmodel:  # A group of parameters related to the model\n  attention_probs_dropout_prob: 0.02  # Dropout probability applied to attention weights to prevent overfitting\n  hidden_act: relu  # Activation function used in the feedforward network (relu, gelu, swish, etc.)\n  hidden_dropout_prob: 0.02  # Dropout probability applied to hidden states throughout the model\n  hidden_size: 2560  # The main dimensionality of the model's hidden representation. Input/output dimension of the attention layers.\n  initializer_range: 0.02  # Standard deviation for weight initialization (controls how weights are randomly initialized)\n  intermediate_size: 10240  # The width / expanded dimension used inside the feedforward network (FFN).\n  layer_norm_eps: 1.0e-12  # Small epsilon value added to layer normalization for numerical stability\n  max_position_embeddings: 2048  # Maximum sequence length the model can handle (positional encoding limit)\n  micro_batch_size: 10  # The batch size per each device\n  model_type: bert  # The architecture of the transformer\n  num_attention_heads: 40  # Number of parallel attention heads in multi-head attention (must divide hidden_size evenly)\n  num_hidden_layers: 36  # Number of transformer layers stacked in the model (depth of the network)\n  pad_token_id: 0  # Token ID used for padding sequences to the same length\n  seq_length: 2048  # Maximum sequence length for training (should be &lt;= max_position_embeddings)\n  use_te_layers: true  # Whether or not to use transformer engine layers in the model. If set to false we will use regular vanilla bert.\n  vocab_size: 25426  # Size of the vocabulary (number of unique tokens the model can process)\n\n# Training configuration\ntraining:\n  learning_rate: 1e-4  # The learning rate for the optimizer\n  num_train_steps: 1000  # Total number of training steps to perform.\n  num_workers: 4  # Number of worker processes for data loading (parallelizes data preprocessing)\n  mlm_probability: 0.15  # Probability of masking tokens for masked language modeling (typically 15%)\n  use_fp8: true  # Set to true to enable FP8 training\n  wandb_init_args:\n    name: \"geneformer-4b-te\"  # Name of the experiment run for tracking in Weights &amp; Biases\n    project: \"bionemo-recipes\"  # Project name to organize runs in Weights &amp; Biases\n  checkpoint_dir: \"/workspace/bionemo/checkpoints/sanity_te\" # Where you want to save your checkpoints.\n  save_every_n_steps: 50 # What interval you want to save checkpoints at.\n  resume_from_checkpoint: true  # if you want to resume from a checkpoint. If true, we will load the checkpoint with the highest \"step count\" from the \"checkpoint_dir\".\n\n# Data configuration\ndata:\n  path: \"/workspace/data/Genecorpus-30M/genecorpus_1M_samples.parquet\"  # Path to the training dataset file\n</code></pre> <p>For detailed model-specific configuration files, refer to the hydra_config/model directory. Some example configs have already been provided such as You can find the full configuration for the 4B parameter model in <code>hydra_config/model/4b.yaml</code>.</p>"},{"location":"main/recipes/recipes/geneformer_native_te_mfsdp_fp8/geneformer_native_te_mfsdp_fp8/#checkpoint-management","title":"Checkpoint Management","text":"<p>Training jobs often run for many hours and may need to be stopped and restarted. This implementation provides built-in checkpoint support for seamless training resumption.</p> <p>Checkpoint Behavior:</p> <ul> <li>Saving: Checkpoints are automatically saved every <code>save_every_n_steps</code> to the <code>checkpoint_dir</code></li> <li>Resuming: Set <code>resume_from_checkpoint: true</code> to automatically resume from the latest checkpoint (latest == highest step count)</li> <li>Fresh start: Set <code>resume_from_checkpoint: false</code> to start training from step 0</li> </ul> <p>When resuming, training will start at the step count where the most recent checkpoint was saved and continue until <code>num_train_steps</code> is reached. If no valid checkpoint is found, training starts from step 0.</p> <p>Checkpoint resuming is supported for both mfsdp (distributed checkpoints) and DDP (single-file checkpoints) configurations.</p>"},{"location":"main/recipes/recipes/geneformer_native_te_mfsdp_fp8/geneformer_native_te_mfsdp_fp8/#safetensors-export","title":"Safetensors Export","text":"<p>At the end of training, the model is automatically exported in safetensors format to the <code>final_model</code> directory within your checkpoint directory. This export works for both mfsdp and vanilla DDP training configurations.</p> <p>Export Location:</p> <pre><code>{checkpoint_dir}/final_model/\n\u251c\u2500\u2500 model.safetensors      # Model weights in safetensors format\n\u251c\u2500\u2500 config.json           # Model configuration\n</code></pre> <p>How it works:</p> <ul> <li>For mfsdp: Parameters are gathered from all processes, then saved by rank 0</li> <li>For DDP: The underlying model is unwrapped and saved by rank 0</li> <li>Export happens automatically after training completes successfully</li> </ul>"},{"location":"main/recipes/recipes/geneformer_native_te_mfsdp_fp8/geneformer_native_te_mfsdp_fp8/#loading-exported-models","title":"Loading Exported Models","text":"<p>You can load the exported model using <code>BertForMaskedLM.from_pretrained()</code> for inference or further fine-tuning:</p> <pre><code>from modeling_bert_te import BertForMaskedLM\nimport torch\n\n# Load the trained model\nmodel_path = \"/workspace/bionemo/checkpoints/your_run/final_model\"\nmodel = BertForMaskedLM.from_pretrained(\n    model_path, torch_dtype=torch.bfloat16, trust_remote_code=True\n)\n\n# Example 1: Model inference\nmodel.eval()\nwith torch.no_grad():\n    # Your input tokens here\n    input_ids = torch.tensor([[1, 2, 3, 4, 5]])  # Replace with actual token IDs\n    outputs = model(input_ids)\n    predictions = outputs.prediction_logits\n\n# Example 2: Continue fine-tuning\nmodel.train()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n# Your fine-tuning loop here\nfor batch in your_dataloader:\n    outputs = model(**batch)\n    loss = outputs.loss\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n</code></pre> <p>Benefits of Safetensors:</p> <ul> <li>Fast loading: Faster than pickle-based formats</li> <li>Safe: No arbitrary code execution risks</li> <li>Memory efficient: Zero-copy loading when possible</li> <li>Cross-platform: Works across different PyTorch versions</li> </ul>"},{"location":"main/recipes/recipes/geneformer_native_te_mfsdp_fp8/geneformer_native_te_mfsdp_fp8/#containers","title":"Containers","text":""},{"location":"main/recipes/recipes/geneformer_native_te_mfsdp_fp8/geneformer_native_te_mfsdp_fp8/#container-build","title":"Container Build","text":"<p>This folder contains its own Dockerfile and requirements used for creating your workload environment. If you want to create your own container, you should run the following BUILD command:</p> <pre><code>docker build -t &lt;imagename&gt; .\n</code></pre> <p>where <code>.</code> is expected to be a folder containing the <code>Dockerfile</code>.</p>"},{"location":"main/recipes/recipes/geneformer_native_te_mfsdp_fp8/geneformer_native_te_mfsdp_fp8/#dependencies","title":"Dependencies","text":"<p>Our main dependency is the pytorch container specified inside the Dockerfile. Other than that we have pip packages listed inside requirements.txt for python specific packages.</p>"},{"location":"main/recipes/recipes/geneformer_native_te_mfsdp_fp8/geneformer_native_te_mfsdp_fp8/#container-run","title":"Container Run","text":"<p>Configure dataset paths in your hydra config's <code>data.path</code> variable using absolute paths.</p> <p>Example run command:</p> <pre><code>export CONTAINER_NAME=bionemo-recipe-geneformer\nexport DATA_SOURCE=/path/to/your/data\nexport DATA_PATH=/workspace/data\n\ndocker run -it --gpus all --network host --ipc=host \\\n  --ulimit memlock=-1 --ulimit stack=67108864 --rm \\\n  -v $DATA_SOURCE:$DATA_PATH \\\n  $CONTAINER_NAME /bin/bash\n</code></pre>"},{"location":"main/recipes/recipes/geneformer_native_te_mfsdp_fp8/geneformer_native_te_mfsdp_fp8/#wandb","title":"WandB","text":"<p>We support full integration with weights and biases. To use this, the environment variable:nter</p> <pre><code>export WANDB_API_KEY=&lt;yourapikey&gt;\n</code></pre> <p>Also, enter your experiment name and project in the hydra config section <code>wandb_init_args</code>.</p>"},{"location":"main/recipes/recipes/geneformer_native_te_mfsdp_fp8/geneformer_native_te_mfsdp_fp8/#dataset","title":"Dataset","text":"<p>This repository has two files associated with our dataset. There is a parquet file with 500 samples of tokenized data that originated from the HF Geneformer. Additionally, there is a <code>vocab.txt</code> file that holds the full vocabulary.</p>"},{"location":"main/recipes/recipes/llama3_native_te/llama3_native_te/","title":"Macro Syntax Error","text":"<p>File: <code>main/recipes/recipes/llama3_native_te/llama3_native_te.md</code></p> <p>Line 162 in Markdown file: Missing end of comment tag <pre><code>## Training on Natural Language Data (Lingua Reproduction) {#lingua-reproduction}\n</code></pre></p>"},{"location":"main/recipes/recipes/vit/vit/","title":"<code>BioNeMo-Vision</code>: Training a <code>VisionTransformer</code> (ViT) with <code>Megatron-FSDP</code> and <code>TransformerEngine</code>","text":"<p>Adapted ViT model code from huggingface/pytorch-image-models (TImM) written by Ross Wightman (@rwightman / Copyright 2020), which you can check out here: https://github.com/huggingface/pytorch-image-models</p>"},{"location":"main/recipes/recipes/vit/vit/#pre-requisites","title":"Pre-Requisites","text":""},{"location":"main/recipes/recipes/vit/vit/#docker-container","title":"Docker Container","text":"<p>To build a Docker image for this recipe, run the following commands:</p> <pre><code>docker build -t &lt;image_repo&gt;:&lt;image_tag&gt; .\n</code></pre> <p>To launch a Docker container from the image, run the following command:</p> <pre><code># Utilize plenty of shared memory (--shm-size) to support loading large batches of image data!\ndocker run -it --rm --gpus=all --shm-size=16G &lt;image_repo&gt;:&lt;image_tag&gt;\n</code></pre>"},{"location":"main/recipes/recipes/vit/vit/#pip-install","title":"PIP Install","text":"<p>If you have a virtual environment and CUDA installed, you can install the recipe's dependencies using <code>pip</code>:</p> <pre><code>cd recipes/vit\n# If this causes problems, you can add PIP_CONSTRAINT= before the `pip install` command to ignore potentially trivial dependency conflicts.\n# We strongly recommend installing into a clean virtual environment or CUDA container, such as the image built from the Dockerfile in this recipe.\npip install -r requirements.txt\n</code></pre>"},{"location":"main/recipes/recipes/vit/vit/#training-a-vision-transformer","title":"Training a Vision Transformer","text":"<p>To train a ViT using FSDP, execute the following command in your Docker container, Python virtual environment, or directly after your <code>docker run</code> command:</p> <pre><code>torchrun --nproc-per-node ${NGPU} train.py --config-name vit_base_patch16_224 distributed.dp_shard=${NGPU} training.checkpoint.path=./ckpts/vit\n</code></pre> <p>This will train on the <code>AI-Lab-Makerere/ibean</code> (HuggingFace: <code>AI-Lab-Makerere/beans</code>) dataset and save auto-resumable Torch DCP checkpoints to the <code>training.checkpoint.path</code> directory.</p> <p><code>train.py</code> is the transparent entrypoint to this script that explains how to modify your own training loop for <code>Megatron-FSDP</code> (PyPI: <code>megatron-fsdp</code> / Source: Megatron-LM) to fully-shard your model across all devices.</p> <p>The TIMM-derived model code for the ViT can be found in <code>vit.py</code>, and data utilities for Beans can be found in <code>beans.py</code>.</p> <p>Various configuration options common in computer vision modeling can be found in config.</p>"},{"location":"main/recipes/recipes/vit/vit/#checkpointing","title":"Checkpointing","text":""},{"location":"main/recipes/recipes/vit/vit/#megatron-fsdp-dcp","title":"Megatron-FSDP DCP","text":"<p>To save Megatron-FSDP distributed checkpoints, refer to the following helper functions in checkpoint.py:</p> <pre><code>import torch\n\n\ndef save_dcp_checkpoint(checkpoint_path, model=None, optimizer=None):\n    \"\"\"Save a Torch DCP checkpoint of the model and optimizer to checkpoint_path.\n\n    Docs: https://docs.pytorch.org/docs/stable/distributed.checkpoint.html\n    \"\"\"\n    # Save model and optimizer checkpoints.\n    state_dict = {}\n    if model is not None:\n        state_dict[\"model\"] = model.state_dict()\n    if optimizer is not None:\n        state_dict[\"optimizer\"] = optimizer.state_dict()\n    torch.distributed.checkpoint.save(state_dict, checkpoint_id=checkpoint_path)\n\n\ndef load_dcp_checkpoint(checkpoint_path, model=None, optimizer=None):\n    \"\"\"Load a Torch DCP checkpoint from checkpoint_path into model and optimizer.\n\n    Docs: https://docs.pytorch.org/docs/stable/distributed.checkpoint.html\n    \"\"\"\n    # Load model and optimizer checkpoints.\n    state_dict = {}\n    if model is not None:\n        state_dict[\"model\"] = model.state_dict()\n    if optimizer is not None:\n        state_dict[\"optimizer\"] = optimizer.state_dict()\n    torch.distributed.checkpoint.load(state_dict, checkpoint_id=checkpoint_path)\n    if model is not None:\n        model.load_state_dict(state_dict[\"model\"], strict=False)\n    if optimizer is not None:\n        optimizer.load_state_dict(state_dict[\"optimizer\"])\n</code></pre> <p>This can be loaded directly into the <code>MegatronFSDP</code> model:</p> <pre><code># Create a MegatronFSDP model and optimizer.\nmodel, optimizer = fully_shard(model, optimizer, ...)\n\n# Load Megatron-FSDP DCP checkpoint into model and/or optimizer.\nload_dcp_checkpoint(CKPT_PATH, model=model, optimizer=optimizer)\n</code></pre>"},{"location":"main/recipes/recipes/vit/vit/#checkpoint-conversion","title":"Checkpoint Conversion","text":"<p>To convert DCP checkpoints to non-distributed Torch checkpoints, and vice-versa, you can run the following command from <code>torch</code>:</p> <pre><code>python -m torch.distributed.checkpoint.format_utils --help\nusage: format_utils.py [-h] {torch_to_dcp,dcp_to_torch} src dst\n\npositional arguments:\n  {torch_to_dcp,dcp_to_torch}\n                        Conversion mode\n  src                   Path to the source model\n  dst                   Path to the destination model\n\noptions:\n  -h, --help            show this help message and exit\n</code></pre> <p>For example:</p> <pre><code>python -m torch.distributed.checkpoint.format_utils dcp_to_torch step_75_loss_1.725 torch_ckpt_test.pt\n</code></pre> <p>or:</p> <pre><code>from torch.distributed.checkpoint.format_utils import (\n    dcp_to_torch_save,\n    torch_save_to_dcp,\n)\n\n# Convert DCP model checkpoint to torch.save format.\ndcp_to_torch_save(CHECKPOINT_DIR, TORCH_SAVE_CHECKPOINT_PATH)\n\n# Convert torch.save model checkpoint back to DCP format.\ntorch_save_to_dcp(TORCH_SAVE_CHECKPOINT_PATH, f\"{CHECKPOINT_DIR}_new\")\n</code></pre>"},{"location":"main/recipes/recipes/vit/vit/#megatron-fsdp-checkpoint-state-caveats","title":"Megatron-FSDP Checkpoint State Caveats","text":"<p>Note that <code>torch.save</code>-converted distributed checkpoints (DCP) cannot be loaded directly into <code>MegatronFSDP</code> module classes, because Megatron-FSDP expects an unevenly-sharded DCP checkpoint with metadata not available in <code>torch.save</code> checkpoints that defines the distributed read and write sharding strategy for DCP load and save respectively. To load a non-distributed checkpoint for training with Megatron-FSDP, simply load the checkpoint into the unsharded model before calling <code>fully_shard</code> as an alternative to loading in a DCP checkpoint after <code>fully_shard</code>!</p> <pre><code>from checkpoint import load_torch_checkpoint\n\n# Initialize model.\nmodel = build_vit_model(cfg, device_mesh)\n\n# Load torch.save model checkpoint. If the checkpoint was converted\n# from a DCP checkpoint produced by Megatron-FSDP, set megatron_fsdp=True,\n# which simply strips the \"module.\" prefix from the state dictionary.\nload_torch_checkpoint(CKPT_PATH, model, megatron_fsdp=True)\n\n# Fully-shard.\nmodel, _ = fully_shard(model, ...)\n</code></pre> <p>TODO(@cspades): For converting DCP directly to HuggingFace SafeTensors checkpoints, you can look into: https://pytorch.org/blog/huggingface-safetensors-support-in-pytorch-distributed-checkpointing/</p>"},{"location":"main/recipes/recipes/vit/vit/#inference","title":"Inference","text":"<p>infer.py is an example inference script that loads in a non-distributed <code>torch.save</code> checkpoint into an un-sharded ViT.</p> <p>For inference with Megatron-FSDP, refer to the <code>fully_shard</code> + <code>load_dcp_checkpoint</code> pattern in train.py / checkpoint.py and described in Megatron-FSDP DCP.</p>"},{"location":"main/references/FAQ/","title":"Frequently Asked Questions","text":""},{"location":"main/references/FAQ/#is-bionemo-framework-free-to-use","title":"Is BioNeMo Framework free to use?","text":"<p>Yes, BioNeMo Framework is free to use. BioNeMo Framework code is licensed under the Apache 2.0 License. The Apache 2.0 License is a permissive open-source license that allows users to freely use, modify, and distribute software. With this license, users have the right to use the software for any purpose, including commercial use, without requiring royalties or attribution. Overall, our choice of the Apache 2.0 License allows for wide adoption and use of BioNeMo Framework, while also providing a high degree of freedom and flexibility for users.</p> <p>For users that would like NVIDIA AI Enterprise support for BioNeMo Framework container usage, refer to the NVAIE Landing Page for more information.</p>"},{"location":"main/references/FAQ/#how-do-i-install-bionemo-framework","title":"How do I install BioNeMo Framework?","text":"<p>BioNeMo Framework is distributed as a Docker container through NVIDIA NGC. To download the pre-built Docker container and data assets, you will need a free NVIDIA NGC account.</p> <p>Alternatively, you can install individual sub-packages from within BioNeMo Framework by following the corresponding README pages the BioNeMo Framework GitHub. Please note that this is a beta feature and may require some additional effort to install seamlessly. We are actively working on testing this functionality and expect it will be a fully supported feature in future releases. You can review our release notes to stay up to date on our releases.</p>"},{"location":"main/references/FAQ/#how-do-i-update-bionemo-framework-to-the-latest-version","title":"How do I update BioNeMo Framework to the latest version?","text":"<p>To update the BioNeMo Framework Docker container, you need to pull the latest version of the Docker image using the command <code>docker pull</code>. For available tags, refer to the BioNeMo Framework page in the NGC Catalog.</p>"},{"location":"main/references/FAQ/#what-are-the-system-requirements-for-bionemo-framework","title":"What are the system requirements for BioNeMo Framework?","text":"<p>Generally, BioNeMo Framework should run on any NVIDIA GPU with Compute Capability \u22658.0. For a full list of supported hardware, refer to the Hardware and Software Prerequisites.</p>"},{"location":"main/references/FAQ/#can-i-contribute-code-or-models-to-bionemo-framework","title":"Can I contribute code or models to BioNeMo Framework?","text":"<p>Yes, BioNeMo Framework is open source and we welcome contributions from organizations and individuals. You can do so either by forking the repository and directly opening a PR against our <code>main</code> branch from your fork or by contacting us fo r further assistance. BioNeMo Framework's mission is to stay extremely light weight and primarily support building blocks required for various AI models. As such, we currently prioritize feature extensions, bug fixes, and new independent modules such as dataloaders, tokenizers, custom architecture blocks, and other reusable features over end-to-end model implementations. We might consider end-to-end model implementations on a case-by-case basis. If you're interested in this contribution of this kind, we recommend reaching out to us first</p> <p>For more information about external contributions, refer to the Contributing and Code Review pages.</p>"},{"location":"main/references/FAQ/#how-do-i-report-bugs-or-suggest-new-features","title":"How do I report bugs or suggest new features?","text":"<p>To report a bug or suggest a new feature, open an issue on the BioNeMo Framework GitHub site. For the fastest turnaround, thoroughly describe your issue, including any steps and/or minimal data sets necessary to reproduce (when possible), as well as the expected behavior.</p>"},{"location":"main/references/FAQ/#can-i-train-models-in-jupyter-notebooks-using-bionemo-framework","title":"Can I train models in Jupyter notebooks using BioNeMo Framework?","text":"<p>At the current time, notebook-based training is not supported due to restrictions imposed by the Megatron framework that underpins the BioNeMo Framework models. However, the user may call training scripts using a subprocess, either through the use of the Python Subprocess module or through Jupyter's Shell Assignment or Bash Cell Magic. For the latter two options, we caution the user to be careful when using Python and shell variables as we have observed unpredictable and unreproducible behavior in certain instances.</p>"},{"location":"main/references/API_reference/","title":"API reference","text":"<p>The API reference contains detailed descriptions of all public functions and objects. It's the best place to look if you need information on a specific function.</p>"},{"location":"main/references/API_reference/bionemo/core/api/","title":"Api","text":""},{"location":"main/references/API_reference/bionemo/core/api/#bionemo.core.api.ModelOutput","title":"<code>ModelOutput = TypeVar('ModelOutput', Tensor, list[Tensor], tuple[Tensor], dict[str, Tensor], covariant=True)</code>  <code>module-attribute</code>","text":"<p>A Model's forward pass may produce a tensor, multiple tensors, or named tensors.</p>"},{"location":"main/references/API_reference/bionemo/core/api/#bionemo.core.api.BionemoModelConfig","title":"<code>BionemoModelConfig</code>","text":"<p>               Bases: <code>Generic[ModelType]</code>, <code>ABC</code></p> <p>An abstract class for model configuration.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>class BionemoModelConfig(Generic[ModelType], ABC):\n    \"\"\"An abstract class for model configuration.\"\"\"\n\n    @abstractmethod\n    def configure_model(self, *args, **kwargs) -&gt; Model:\n        \"\"\"Configures the model.\"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/api/#bionemo.core.api.BionemoModelConfig.configure_model","title":"<code>configure_model(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Configures the model.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>@abstractmethod\ndef configure_model(self, *args, **kwargs) -&gt; Model:\n    \"\"\"Configures the model.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/api/#bionemo.core.api.BionemoTrainableModelConfig","title":"<code>BionemoTrainableModelConfig</code>","text":"<p>               Bases: <code>Generic[ModelType, LossType]</code>, <code>BionemoModelConfig[ModelType]</code></p> <p>An abstract class for trainable model configuration.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>class BionemoTrainableModelConfig(Generic[ModelType, LossType], BionemoModelConfig[ModelType]):\n    \"\"\"An abstract class for trainable model configuration.\"\"\"\n\n    @abstractmethod\n    def get_loss_reduction_class(self) -&gt; Type[LossType]:\n        \"\"\"Returns the loss reduction class.\"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/api/#bionemo.core.api.BionemoTrainableModelConfig.get_loss_reduction_class","title":"<code>get_loss_reduction_class()</code>  <code>abstractmethod</code>","text":"<p>Returns the loss reduction class.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>@abstractmethod\ndef get_loss_reduction_class(self) -&gt; Type[LossType]:\n    \"\"\"Returns the loss reduction class.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/api/#bionemo.core.api.Model","title":"<code>Model</code>","text":"<p>               Bases: <code>Protocol[ModelOutput]</code></p> <p>Lightweight interface for a model: must have a forward method.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>class Model(Protocol[ModelOutput]):\n    \"\"\"Lightweight interface for a model: must have a forward method.\"\"\"\n\n    def forward(self, *args, **kwargs) -&gt; ModelOutput:\n        \"\"\"Prediction / forward-step for a model.\"\"\"\n        ...\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/api/#bionemo.core.api.Model.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Prediction / forward-step for a model.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>def forward(self, *args, **kwargs) -&gt; ModelOutput:\n    \"\"\"Prediction / forward-step for a model.\"\"\"\n    ...\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/","title":"BioNeMo test data management","text":"<p>This library manages the downloading and caching of large or binary data files used in the documentation or test suite. These files should not be committed directly to the repo, and instead should be loaded at test-time when they are needed.</p> <p>We currently support two locations for test data or saved models:</p> SwiftStack <p>SwiftStack or <code>pbss</code> is an NVIDIA-internal, s3-compatible object store that allows for very large data and fast, parallel read/writes. Most critically, <code>pbss</code> can be uploaded to without legal approvals for dataset redistribution. These files will not be accessible by external collaborators.</p> NGC <p>NGC hosts containers, models, and resources, some of which require authentication and others that are generally available. This library uses the model and resource types to save test data and reference model weights. These items are accessible by external collaborators, but require legal approval before re-distributing test data.</p>"},{"location":"main/references/API_reference/bionemo/core/data/#loading-test-or-example-data","title":"Loading test or example data","text":"<p>Test data are specified via yaml files in <code>sub-packages/bionemo-testing/src/bionemo/testing/data/resources</code>. As an example, in <code>esm2.yaml</code>:</p> <pre><code>- tag: nv_650m:1.0\n  ngc: \"nvidia/clara/esm2nv650m:1.0\"\n  ngc_registry: model\n  pbss: \"s3://bionemo-ci/models/esm2nv_650M_converted.nemo\"\n  sha256: 1e38063cafa808306329428dd17ea6df78c9e5d6b3d2caf04237c555a1f131b7\n  owner: Farhad Ramezanghorbani &lt;farhadr@nvidia.com&gt;\n  description: &gt;\n    A pretrained 650M parameter ESM-2 model.\n    See https://ngc.nvidia.com/catalog/models/nvidia:clara:esm2nv650m.\n</code></pre> <p>To load these model weights during a test, use the load function with the filename and tag of the desired asset, which returns a path a the specified file:</p> <pre><code>path_to_my_checkpoint = load(\"esm2/nv_650m:1.0\")\nconfig = ESM2Config(nemo1_ckpt_path=path_to_my_checkpoint)\n</code></pre> <p>If this function is called without the data available on the local machine, it will be fetched from the default source (currently <code>pbss</code>.) Otherwise, it will return the cached directory. To download with NGC, pass <code>source=\"ngc\"</code> to load.</p>"},{"location":"main/references/API_reference/bionemo/core/data/#file-unpacking-andor-decompression","title":"File unpacking and/or decompression","text":"<p>All test artifacts are individual files. If a zip or tar archive is specified, it will be unpacked automatically, and the path to the directory will be returned via load. Compressed files ('gzip', 'bz2', or 'xz') are automatically decompressed before they are returned. The file's compression and/or archive format is determined based on the filename specified in the <code>pbss</code> URL.</p> <p>Files in NGC resources</p> <pre><code>NGC resources are folders, i.e., they may contain multiple files per resource.\n[load][bionemo.core.data.load.load] will _only_ download the filename matching the stem of the `pbss` url. The\nsame NGC resource can therefore be used to host multiple test assets that are used independently.\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/#adding-new-test-assets","title":"Adding new test assets","text":"<p>To add new data, first ensure that the data is available from either NGC or <code>pbss</code>. Next, extend or create a new yaml file in <code>sub-packages/bionemo-testing/src/bionemo/testing/data/resources</code> with the required information. Owner emails must be provided for all assets. The description and <code>ngc</code> fields are currently optional. If the <code>sha256</code> is left unspecified, <code>pooch</code> will report the downloaded file's sha when loaded.</p> <p>Warning</p> <pre><code>SHAs should be provided for all files to ensure the download completes correctly, and to invalidate caches if the\nfiles change.\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/api/","title":"Api","text":""},{"location":"main/references/API_reference/bionemo/core/data/load/","title":"Load","text":""},{"location":"main/references/API_reference/bionemo/core/data/load/#bionemo.core.data.load.NGCDownloader","title":"<code>NGCDownloader</code>  <code>dataclass</code>","text":"<p>A class to download files from NGC in a Pooch-compatible way.</p> <p>NGC downloads are typically structured as directories, while pooch expects a single file. This class downloads a single file from an NGC directory and moves it to the desired location.</p> Source code in <code>bionemo/core/data/load.py</code> <pre><code>@dataclass\nclass NGCDownloader:\n    \"\"\"A class to download files from NGC in a Pooch-compatible way.\n\n    NGC downloads are typically structured as directories, while pooch expects a single file. This class\n    downloads a single file from an NGC directory and moves it to the desired location.\n    \"\"\"\n\n    filename: str\n    ngc_registry: Literal[\"model\", \"resource\"]\n\n    def __call__(self, url: str, output_file: str | Path, _: pooch.Pooch) -&gt; None:\n        \"\"\"Download a file from NGC.\"\"\"\n        client = default_ngc_client()\n        nest_asyncio.apply()\n\n        download_fns = {\n            \"model\": client.registry.model.download_version,\n            \"resource\": client.registry.resource.download_version,\n        }\n\n        output_file = Path(output_file)\n        output_file.parent.mkdir(parents=True, exist_ok=True)\n\n        # NGC seems to always download to a specific directory that we can't specify ourselves.\n        ngc_dirname = Path(url).name.replace(\":\", \"_v\")\n\n        with tempfile.TemporaryDirectory(dir=output_file.parent) as temp_dir:\n            download_fns[self.ngc_registry](url, temp_dir, file_patterns=[self.filename])\n            shutil.move(Path(temp_dir) / ngc_dirname / self.filename, output_file)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/load/#bionemo.core.data.load.NGCDownloader.__call__","title":"<code>__call__(url, output_file, _)</code>","text":"<p>Download a file from NGC.</p> Source code in <code>bionemo/core/data/load.py</code> <pre><code>def __call__(self, url: str, output_file: str | Path, _: pooch.Pooch) -&gt; None:\n    \"\"\"Download a file from NGC.\"\"\"\n    client = default_ngc_client()\n    nest_asyncio.apply()\n\n    download_fns = {\n        \"model\": client.registry.model.download_version,\n        \"resource\": client.registry.resource.download_version,\n    }\n\n    output_file = Path(output_file)\n    output_file.parent.mkdir(parents=True, exist_ok=True)\n\n    # NGC seems to always download to a specific directory that we can't specify ourselves.\n    ngc_dirname = Path(url).name.replace(\":\", \"_v\")\n\n    with tempfile.TemporaryDirectory(dir=output_file.parent) as temp_dir:\n        download_fns[self.ngc_registry](url, temp_dir, file_patterns=[self.filename])\n        shutil.move(Path(temp_dir) / ngc_dirname / self.filename, output_file)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/load/#bionemo.core.data.load.default_ngc_client","title":"<code>default_ngc_client(use_guest_if_api_key_invalid=True)</code>","text":"<p>Create a default NGC client.</p> <p>This should load the NGC API key from ~/.ngc/config, or from environment variables passed to the docker container.</p> Source code in <code>bionemo/core/data/load.py</code> <pre><code>def default_ngc_client(use_guest_if_api_key_invalid: bool = True) -&gt; \"ngcsdk.Client\":\n    \"\"\"Create a default NGC client.\n\n    This should load the NGC API key from ~/.ngc/config, or from environment variables passed to the docker container.\n    \"\"\"\n    import ngcsdk\n\n    client = ngcsdk.Client()\n\n    try:\n        client.configure()\n\n    except ValueError as e:\n        if use_guest_if_api_key_invalid:\n            logger.error(f\"Error configuring NGC client: {e}, signing in as guest.\")\n            client = ngcsdk.Client(\"no-apikey\")\n            client.configure(\n                api_key=\"no-apikey\",  # pragma: allowlist secret\n                org_name=\"no-org\",\n                team_name=\"no-team\",\n                ace_name=\"no-ace\",\n            )\n\n        else:\n            raise\n\n    return client\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/load/#bionemo.core.data.load.default_pbss_client","title":"<code>default_pbss_client()</code>","text":"<p>Create a default S3 client for PBSS.</p> Source code in <code>bionemo/core/data/load.py</code> <pre><code>def default_pbss_client():\n    \"\"\"Create a default S3 client for PBSS.\"\"\"\n    try:\n        import boto3\n    except ImportError:\n        raise ImportError(\"boto3 is required to download from PBSS.\")\n\n    retry_config = Config(retries={\"max_attempts\": 10, \"mode\": \"standard\"})\n    return boto3.client(\"s3\", endpoint_url=\"https://pbss.s8k.io\", config=retry_config)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/load/#bionemo.core.data.load.entrypoint","title":"<code>entrypoint()</code>","text":"<p>Allows a user to get a specific artifact from the command line.</p> Source code in <code>bionemo/core/data/load.py</code> <pre><code>def entrypoint():\n    \"\"\"Allows a user to get a specific artifact from the command line.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Retrieve the local path to the requested artifact name or list resources.\"\n    )\n\n    # Create mutually exclusive group\n    group = parser.add_mutually_exclusive_group(required=True)\n\n    # Add the argument for artifact name, which is required if --list-resources is not used\n    group.add_argument(\"artifact_name\", type=str, nargs=\"?\", help=\"Name of the artifact\")\n\n    # Add the --list-resources option\n    group.add_argument(\n        \"--list-resources\", action=\"store_true\", default=False, help=\"List all available artifacts and then exit.\"\n    )\n\n    # Add the --source option\n    parser.add_argument(\n        \"--source\",\n        type=str,\n        choices=[\"pbss\", \"ngc\"],\n        default=\"ngc\",\n        help='Backend to use, Internal NVIDIA users can set this to \"pbss\".',\n    )\n\n    parser.add_argument(\n        \"--all\",\n        action=\"store_true\",\n        default=False,\n        help=\"Download all resources. Ignores all other options.\",\n    )\n    args = parser.parse_args()\n    maybe_error = main(\n        download_all=args.all,\n        list_resources=args.list_resources,\n        artifact_name=args.artifact_name,\n        source=args.source,\n    )\n    if maybe_error is not None:\n        parser.error(maybe_error)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/load/#bionemo.core.data.load.load","title":"<code>load(model_or_data_tag, source=DEFAULT_SOURCE, resources=None, cache_dir=None)</code>","text":"<p>Download a resource from PBSS or NGC.</p> <p>Parameters:</p> Name Type Description Default <code>model_or_data_tag</code> <code>str</code> <p>A pointer to the desired resource. Must be a key in the resources dictionary.</p> required <code>source</code> <code>SourceOptions</code> <p>Either \"pbss\" (NVIDIA-internal download) or \"ngc\" (NVIDIA GPU Cloud). Defaults to \"pbss\".</p> <code>DEFAULT_SOURCE</code> <code>resources</code> <code>dict[str, Resource] | None</code> <p>A custom dictionary of resources. If None, the default resources will be used. (Mostly for testing.)</p> <code>None</code> <code>cache_dir</code> <code>Path | None</code> <p>The directory to store downloaded files. Defaults to BIONEMO_CACHE_DIR. (Mostly for testing.)</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the desired tag was not found, or if an NGC url was requested but not provided.</p> <p>Returns:</p> Type Description <code>Path</code> <p>A Path object pointing either at the downloaded file, or at a decompressed folder containing the</p> <code>Path</code> <p>file(s).</p> <p>Examples:</p> <p>For a resource specified in 'filename.yaml' with tag 'tag', the following will download the file:</p> <pre><code>&gt;&gt;&gt; load(\"filename/tag\")\nPosixPath(/tmp/bionemo/downloaded-file-name)\n</code></pre> Source code in <code>bionemo/core/data/load.py</code> <pre><code>def load(\n    model_or_data_tag: str,\n    source: SourceOptions = DEFAULT_SOURCE,\n    resources: dict[str, Resource] | None = None,\n    cache_dir: Path | None = None,\n) -&gt; Path:\n    \"\"\"Download a resource from PBSS or NGC.\n\n    Args:\n        model_or_data_tag: A pointer to the desired resource. Must be a key in the resources dictionary.\n        source: Either \"pbss\" (NVIDIA-internal download) or \"ngc\" (NVIDIA GPU Cloud). Defaults to \"pbss\".\n        resources: A custom dictionary of resources. If None, the default resources will be used. (Mostly for testing.)\n        cache_dir: The directory to store downloaded files. Defaults to BIONEMO_CACHE_DIR. (Mostly for testing.)\n\n    Raises:\n        ValueError: If the desired tag was not found, or if an NGC url was requested but not provided.\n\n    Returns:\n        A Path object pointing either at the downloaded file, or at a decompressed folder containing the\n        file(s).\n\n    Examples:\n        For a resource specified in 'filename.yaml' with tag 'tag', the following will download the file:\n        &gt;&gt;&gt; load(\"filename/tag\")\n        PosixPath(/tmp/bionemo/downloaded-file-name)\n    \"\"\"\n    if resources is None:\n        resources = get_all_resources()\n\n    if cache_dir is None:\n        cache_dir = BIONEMO_CACHE_DIR\n\n    if model_or_data_tag not in resources:\n        raise ValueError(f\"Resource '{model_or_data_tag}' not found.\")\n\n    if source == \"ngc\" and resources[model_or_data_tag].ngc is None:\n        raise ValueError(f\"Resource '{model_or_data_tag}' does not have an NGC URL.\")\n\n    resource = resources[model_or_data_tag]\n    filename = str(resource.pbss).split(\"/\")[-1]\n\n    extension = \"\".join(Path(filename).suffixes)\n    processor = _get_processor(extension, resource.unpack, resource.decompress)\n\n    if source == \"pbss\":\n        download_fn = _s3_download\n        url = resource.pbss\n\n    elif source == \"ngc\":\n        assert resource.ngc_registry is not None\n        download_fn = NGCDownloader(filename=filename, ngc_registry=resource.ngc_registry)\n        url = resource.ngc\n\n    else:\n        raise ValueError(f\"Source '{source}' not supported.\")\n\n    # Pooch will keep checking hashes and unpacking archives for each call,\n    # which is very time-consuming for large checkpoints. Instead, we make it\n    # do it only once by marking the resource as fully checked.\n    fname = f\"{resource.sha256}-{filename}\"\n    checked = cache_dir / (fname + \".checked\")\n    if checked.exists():\n        path = checked.read_text()\n        logger.debug(f\"Using cached {path=} from {checked=}\")\n        return Path(path)\n\n    download = pooch.retrieve(\n        url=str(url),\n        fname=fname,\n        known_hash=resource.sha256,\n        path=cache_dir,\n        downloader=download_fn,\n        processor=processor,\n    )\n\n    # Pooch by default returns a list of unpacked files if they unpack a zipped or tarred directory. Instead of that, we\n    # just want the unpacked, parent folder.\n    if isinstance(download, list):\n        path = Path(processor.extract_dir)  # type: ignore\n    else:\n        path = Path(download)\n\n    checked.write_text(str(path))\n    return path\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/load/#bionemo.core.data.load.main","title":"<code>main(download_all, list_resources, artifact_name, source)</code>","text":"<p>Main download script logic: parameters are 1:1 with CLI flags. Returns string describing error on failure.</p> Source code in <code>bionemo/core/data/load.py</code> <pre><code>def main(\n    download_all: bool, list_resources: bool, artifact_name: str, source: Literal[\"pbss\", \"ngc\"]\n) -&gt; Optional[str]:\n    \"\"\"Main download script logic: parameters are 1:1 with CLI flags. Returns string describing error on failure.\"\"\"\n    if download_all:\n        print(\"Downloading all resources:\", file=sys.stderr)\n        print_resources(output_source=sys.stderr)\n        print(\"-\" * 80, file=sys.stderr)\n\n        resource_to_local: dict[str, Path] = {}\n        for resource_name in tqdm(\n            sorted(get_all_resources()),\n            desc=\"Downloading Resources\",\n        ):\n            with contextlib.redirect_stdout(sys.stderr):\n                local_path = load(resource_name, source=source)\n            resource_to_local[resource_name] = local_path\n\n        print(\"-\" * 80, file=sys.stderr)\n        print(\"All resources downloaded:\", file=sys.stderr)\n        for resource_name, local_path in sorted(resource_to_local.items()):\n            print(f\"  {resource_name}: {str(local_path.absolute())}\", file=sys.stderr)\n\n    elif list_resources:\n        print_resources(output_source=sys.stdout)\n\n    elif artifact_name is not None and len(artifact_name) &gt; 0:\n        # Get the local path for the provided artifact name\n        with contextlib.redirect_stdout(sys.stderr):\n            local_path = load(artifact_name, source=source)\n\n        # Print the result =&gt; CLI use assumes that we can get the single downloaded resource's path on STDOUT\n        print(str(local_path.absolute()))\n\n    else:\n        return \"You must provide an artifact name if --list-resources or --all is not set!\"\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/load/#bionemo.core.data.load.print_resources","title":"<code>print_resources(*, output_source=sys.stdout)</code>","text":"<p>Prints all available downloadable resources &amp; their sources to STDOUT.</p> Source code in <code>bionemo/core/data/load.py</code> <pre><code>def print_resources(*, output_source: TextIO = sys.stdout) -&gt; None:\n    \"\"\"Prints all available downloadable resources &amp; their sources to STDOUT.\"\"\"\n    print(\"#resource_name\\tsource_options\", file=output_source)\n    for resource_name, resource in sorted(get_all_resources().items()):\n        sources = []\n        if resource.ngc is not None:\n            sources.append(\"ngc\")\n        if resource.pbss is not None:\n            sources.append(\"pbss\")\n        print(f\"{resource_name}\\t{','.join(sources)}\", file=output_source)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/multi_epoch_dataset/","title":"Multi epoch dataset","text":""},{"location":"main/references/API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.EpochIndex","title":"<code>EpochIndex</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A tuple that contains both the current epoch and index for multi-epoch training.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>class EpochIndex(NamedTuple):\n    \"\"\"A tuple that contains both the current epoch and index for multi-epoch training.\"\"\"\n\n    epoch: int\n    \"\"\"An integer representing the current epoch.\"\"\"\n\n    idx: int\n    \"\"\"An integer representing the index within the current epoch.\"\"\"\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.EpochIndex.epoch","title":"<code>epoch</code>  <code>instance-attribute</code>","text":"<p>An integer representing the current epoch.</p>"},{"location":"main/references/API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.EpochIndex.idx","title":"<code>idx</code>  <code>instance-attribute</code>","text":"<p>An integer representing the index within the current epoch.</p>"},{"location":"main/references/API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.IdentityMultiEpochDatasetWrapper","title":"<code>IdentityMultiEpochDatasetWrapper</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MultiEpochDatasetWrapper[T, T]</code></p> <p>An implementation of the <code>MultiEpochDatasetWrapper</code> that does not apply any transformations.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>class IdentityMultiEpochDatasetWrapper(MultiEpochDatasetWrapper[T, T]):\n    \"\"\"An implementation of the `MultiEpochDatasetWrapper` that does not apply any transformations.\"\"\"\n\n    def apply_transform(self, sample: T, index: EpochIndex) -&gt; T:\n        \"\"\"Return the sample as is.\"\"\"\n        del index  # Unused.\n        return sample\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.IdentityMultiEpochDatasetWrapper.apply_transform","title":"<code>apply_transform(sample, index)</code>","text":"<p>Return the sample as is.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>def apply_transform(self, sample: T, index: EpochIndex) -&gt; T:\n    \"\"\"Return the sample as is.\"\"\"\n    del index  # Unused.\n    return sample\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDataset","title":"<code>MultiEpochDataset</code>","text":"<p>               Bases: <code>Protocol[T_co]</code></p> <p>A protocol for datasets for multi-epoch training in Megatron-LM.</p> <p>Dataset determinism in Megatron-LM</p> <p>In megatron training, the sampler and dataset objects are used to ensure consistent data loading across model-parallel ranks. For datasets to work with megatron training, they must return exactly the same data for every call to <code>__getitem__</code> with the same index.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>class MultiEpochDataset(Protocol[T_co]):\n    \"\"\"A protocol for datasets for multi-epoch training in Megatron-LM.\n\n    !!! important \"Dataset determinism in Megatron-LM\"\n        In megatron training, the sampler and dataset objects are used to ensure consistent data loading across\n        model-parallel ranks. For datasets to work with megatron training, they must return exactly the same data for\n        every call to `__getitem__` with the same index.\n    \"\"\"\n\n    def __getitem__(self, index: EpochIndex) -&gt; T_co:  # noqa: D105\n        ...\n\n    def __len__(self) -&gt; int:  # noqa: D105\n        ...\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler","title":"<code>MultiEpochDatasetResampler</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset[T_co]</code></p> <p>A dataset wrapper class that converts the sequential sampling from Megatron-LM to epoch-based sampling.</p> <p>Either <code>num_epochs</code> or <code>num_samples</code> should be provided. If neither are provided, the dataset will use a single epoch. If <code>num_epochs</code> is given, the resampled dataset will have <code>len(dataset) * num_epochs</code> samples. If <code>num_samples</code> the resampled dataset will have <code>num_samples</code> samples. For <code>num_samples</code>, the dataset will be repeated for multiple epochs until the desired number of samples is reached (with the final epoch being truncated).</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>@dataclass\nclass MultiEpochDatasetResampler(Dataset[T_co]):\n    \"\"\"A dataset wrapper class that converts the sequential sampling from Megatron-LM to epoch-based sampling.\n\n    Either `num_epochs` or `num_samples` should be provided. If neither are provided, the dataset will use a single\n    epoch. If `num_epochs` is given, the resampled dataset will have `len(dataset) * num_epochs` samples. If\n    `num_samples` the resampled dataset will have `num_samples` samples. For `num_samples`, the dataset will be repeated\n    for multiple epochs until the desired number of samples is reached (with the final epoch being truncated).\n    \"\"\"\n\n    dataset: MultiEpochDataset[T_co]\n    \"\"\"The dataset to resample. Must support indexing with an `EpochIndex`.\"\"\"\n\n    num_epochs: int | None = None\n    \"\"\"The total number of epochs. The length of the resampled dataset will be len(dataset) * num_epochs.\"\"\"\n\n    num_samples: int | None = None\n    \"\"\"The total number of samples to draw.\n\n    The number of epochs will be determined by the number of samples and the length of the dataset.\n    \"\"\"\n\n    shuffle: bool = True\n    \"\"\"Whether to shuffle the samples in the dataset each epoch.\"\"\"\n\n    seed: int = 42  # type: ignore\n    \"\"\"A random seed for reproducibility.\"\"\"\n\n    def __post_init__(self):\n        \"\"\"Pre-shuffle each epoch's samples.\"\"\"\n        if self.num_epochs is None and self.num_samples is None:\n            self.num_epochs = 1\n        elif self.num_epochs is not None and self.num_samples is not None:\n            raise ValueError(\"Only one of num_epochs and num_samples should be provided.\")\n\n        if self.num_epochs is None and self.num_samples is not None:\n            self.num_epochs = math.ceil(self.num_samples / len(self.dataset))\n\n        elif self.num_samples is None and self.num_epochs is not None:\n            self.num_samples = len(self.dataset) * self.num_epochs\n\n        # Type guard statements, the above if/elif block should ensure these are not None.\n        assert self.num_epochs is not None\n        assert self.num_samples is not None\n\n        if self.num_epochs &lt; 1:\n            raise ValueError(\"num_epochs must be at least 1.\")\n\n        rng = np.random.default_rng(self.seed)\n\n        # Initialize a vector of random seeds so that each epoch is shuffled differently.\n        self.epoch_seeds = rng.integers(0, np.iinfo(np.int32).max, size=self.num_epochs)\n\n    def __getitem__(self, index: int) -&gt; T_co:\n        \"\"\"Get the sample at the given index.\"\"\"\n        if index not in range(len(self)):\n            raise IndexError(f\"Index {index} out of bounds for dataset of length {len(self)}.\")\n        return self.dataset[self._global_index_to_permuted_local_index(index)]\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the resampled dataset.\"\"\"\n        return self.num_samples  # type: ignore\n\n    def _global_index_to_permuted_local_index(self, index: int) -&gt; EpochIndex:\n        \"\"\"Convert a global index to an epoch index.\"\"\"\n        epoch = index // len(self.dataset)\n        idx = index % len(self.dataset)\n        if self.shuffle:\n            idx = permute(idx, len(self.dataset), self.epoch_seeds[epoch])\n        return EpochIndex(epoch, idx)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler.dataset","title":"<code>dataset</code>  <code>instance-attribute</code>","text":"<p>The dataset to resample. Must support indexing with an <code>EpochIndex</code>.</p>"},{"location":"main/references/API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler.num_epochs","title":"<code>num_epochs = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The total number of epochs. The length of the resampled dataset will be len(dataset) * num_epochs.</p>"},{"location":"main/references/API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler.num_samples","title":"<code>num_samples = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The total number of samples to draw.</p> <p>The number of epochs will be determined by the number of samples and the length of the dataset.</p>"},{"location":"main/references/API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler.seed","title":"<code>seed = 42</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A random seed for reproducibility.</p>"},{"location":"main/references/API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler.shuffle","title":"<code>shuffle = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to shuffle the samples in the dataset each epoch.</p>"},{"location":"main/references/API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get the sample at the given index.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>def __getitem__(self, index: int) -&gt; T_co:\n    \"\"\"Get the sample at the given index.\"\"\"\n    if index not in range(len(self)):\n        raise IndexError(f\"Index {index} out of bounds for dataset of length {len(self)}.\")\n    return self.dataset[self._global_index_to_permuted_local_index(index)]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of the resampled dataset.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the resampled dataset.\"\"\"\n    return self.num_samples  # type: ignore\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Pre-shuffle each epoch's samples.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Pre-shuffle each epoch's samples.\"\"\"\n    if self.num_epochs is None and self.num_samples is None:\n        self.num_epochs = 1\n    elif self.num_epochs is not None and self.num_samples is not None:\n        raise ValueError(\"Only one of num_epochs and num_samples should be provided.\")\n\n    if self.num_epochs is None and self.num_samples is not None:\n        self.num_epochs = math.ceil(self.num_samples / len(self.dataset))\n\n    elif self.num_samples is None and self.num_epochs is not None:\n        self.num_samples = len(self.dataset) * self.num_epochs\n\n    # Type guard statements, the above if/elif block should ensure these are not None.\n    assert self.num_epochs is not None\n    assert self.num_samples is not None\n\n    if self.num_epochs &lt; 1:\n        raise ValueError(\"num_epochs must be at least 1.\")\n\n    rng = np.random.default_rng(self.seed)\n\n    # Initialize a vector of random seeds so that each epoch is shuffled differently.\n    self.epoch_seeds = rng.integers(0, np.iinfo(np.int32).max, size=self.num_epochs)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetWrapper","title":"<code>MultiEpochDatasetWrapper</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset[U_co]</code>, <code>Generic[T, U_co]</code>, <code>ABC</code></p> <p>A wrapper to convert a standard pytorch dataset into one that supports multi-epoch megatron training.</p> <p>The underlying dataset's getitem method must be deterministic, i.e. it must return the same data for the same index every time it is called. If there are any non-deterministic operations, they should be moved to the <code>apply_transform</code> method. This method must also be deterministic for every (epoch, index) pair, but it can use the epoch to implement data augmentation each epoch.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>@dataclass\nclass MultiEpochDatasetWrapper(Dataset[U_co], Generic[T, U_co], ABC):\n    \"\"\"A wrapper to convert a standard pytorch dataset into one that supports multi-epoch megatron training.\n\n    The underlying dataset's __getitem__ method must be deterministic, i.e. it must return the same data for the same\n    index every time it is called. If there are any non-deterministic operations, they should be moved to the\n    `apply_transform` method. This method must also be deterministic for every (epoch, index) pair, but it can use\n    the epoch to implement data augmentation each epoch.\n    \"\"\"\n\n    dataset: SizedDataset[T]\n    \"\"\"A deterministic dataset that supports indexing with an integer index.\"\"\"\n\n    @abstractmethod\n    def apply_transform(self, sample: T, index: EpochIndex) -&gt; U_co:\n        \"\"\"Apply any transformations to the sample for the given epoch.\"\"\"\n        raise NotImplementedError\n\n    def __getitem__(self, index: EpochIndex) -&gt; U_co:\n        \"\"\"Get the sample at the given epoch and index.\"\"\"\n        return self.apply_transform(self.dataset[index.idx], index)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.dataset)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetWrapper.dataset","title":"<code>dataset</code>  <code>instance-attribute</code>","text":"<p>A deterministic dataset that supports indexing with an integer index.</p>"},{"location":"main/references/API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetWrapper.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get the sample at the given epoch and index.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>def __getitem__(self, index: EpochIndex) -&gt; U_co:\n    \"\"\"Get the sample at the given epoch and index.\"\"\"\n    return self.apply_transform(self.dataset[index.idx], index)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetWrapper.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of the dataset.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.dataset)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetWrapper.apply_transform","title":"<code>apply_transform(sample, index)</code>  <code>abstractmethod</code>","text":"<p>Apply any transformations to the sample for the given epoch.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>@abstractmethod\ndef apply_transform(self, sample: T, index: EpochIndex) -&gt; U_co:\n    \"\"\"Apply any transformations to the sample for the given epoch.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.SizedDataset","title":"<code>SizedDataset</code>","text":"<p>               Bases: <code>Protocol[T_co]</code></p> <p>A protocol for integer-indexed datasets that have a fixed length.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>class SizedDataset(Protocol[T_co]):\n    \"\"\"A protocol for integer-indexed datasets that have a fixed length.\"\"\"\n\n    def __getitem__(self, index: int) -&gt; T_co:  # noqa: D105\n        ...\n\n    def __len__(self) -&gt; int:  # noqa: D105\n        ...\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/permute/","title":"Permute","text":""},{"location":"main/references/API_reference/bionemo/core/data/permute/#bionemo.core.data.permute.permute","title":"<code>permute(index, length, seed)</code>","text":"<p>Index into a permuted array with constant space and time complexity.</p> <p>This function permutes an index <code>i</code> into a range <code>[0, l)</code> using a hash function. See https://afnan.io/posts/2019-04-05-explaining-the-hashed-permutation/ for more details and \"Correlated Multi-Jittered Sampling\" by Andrew Kensler for the original algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index to permute.</p> required <code>length</code> <code>int</code> <p>The range of the permuted index.</p> required <code>seed</code> <code>int</code> <p>The permutation seed.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The permuted index in range(0, length).</p> Source code in <code>bionemo/core/data/permute.py</code> <pre><code>def permute(index: int, length: int, seed: int) -&gt; int:\n    \"\"\"Index into a permuted array with constant space and time complexity.\n\n    This function permutes an index `i` into a range `[0, l)` using a hash function. See\n    https://afnan.io/posts/2019-04-05-explaining-the-hashed-permutation/ for more details and\n    \"Correlated Multi-Jittered Sampling\" by Andrew Kensler for the original algorithm.\n\n    Args:\n        index: The index to permute.\n        length: The range of the permuted index.\n        seed: The permutation seed.\n\n    Returns:\n        The permuted index in range(0, length).\n    \"\"\"\n    if length &lt;= 1:\n        raise ValueError(\"The length of the permuted range must be greater than 1.\")\n\n    if index not in range(length):\n        raise ValueError(\"The index to permute must be in the range [0, l).\")\n\n    if seed &lt; 0:\n        raise ValueError(\"The permutation seed must be greater than or equal to 0.\")\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n\n        w = length - 1\n        w |= w &gt;&gt; 1\n        w |= w &gt;&gt; 2\n        w |= w &gt;&gt; 4\n        w |= w &gt;&gt; 8\n        w |= w &gt;&gt; 16\n\n        while True:\n            index ^= seed\n            index *= 0xE170893D\n            index ^= seed &gt;&gt; 16\n            index ^= (index &amp; w) &gt;&gt; 4\n            index ^= seed &gt;&gt; 8\n            index *= 0x0929EB3F\n            index ^= seed &gt;&gt; 23\n            index ^= (index &amp; w) &gt;&gt; 1\n            index *= 1 | seed &gt;&gt; 27\n            index *= 0x6935FA69\n            index ^= (index &amp; w) &gt;&gt; 11\n            index *= 0x74DCB303\n            index ^= (index &amp; w) &gt;&gt; 2\n            index *= 0x9E501CC3\n            index ^= (index &amp; w) &gt;&gt; 2\n            index *= 0xC860A3DF\n            index &amp;= w\n            if index &lt; length:\n                break\n\n    return (index + seed) % length\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/resamplers/","title":"Resamplers","text":""},{"location":"main/references/API_reference/bionemo/core/data/resamplers/#bionemo.core.data.resamplers.PRNGResampleDataset","title":"<code>PRNGResampleDataset</code>","text":"<p>               Bases: <code>Dataset[T_co]</code></p> <p>A thread-safe dataset shuffler that uses a pseudo-random number generator (PRNG) to shuffle the dataset.</p> <p>PRNGResampleDataset shuffles a given dataset using a pseudo-random number generator (PRNG). This allows for reproducible shuffling by controlling the random seed, while not ever storing the list of indices in memory. It works by generating random indices assuming that the requesting function asks for them sequentially. Although random lookups are supported, random lookups will involve recomputing state which is slow, and involves linearly advancing from 0 if the last requested index was greater than or equal to this requested index. This should work well with the megatron sampler which is sequential. It handles skipped lookups as will happen with multiple workers by not generating those numbers.</p> <p>Prefer bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler</p> <p>This class performs sampling with replacement of an underlying dataset. It is recommended to use the epoch-based sampling provided by <code>bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler</code> instead, which ensures that each sample is seen exactly once per epoch. This dataset is useful for cases where the dataset is too large for the shuffled list of indices to fit in memory and exhaustive sampling is not required.</p> Source code in <code>bionemo/core/data/resamplers.py</code> <pre><code>class PRNGResampleDataset(Dataset[T_co]):\n    \"\"\"A thread-safe dataset shuffler that uses a pseudo-random number generator (PRNG) to shuffle the dataset.\n\n    PRNGResampleDataset shuffles a given dataset using a pseudo-random number generator (PRNG). This allows for\n    reproducible shuffling by controlling the random seed, while not ever storing the list of indices in memory. It\n    works by generating random indices assuming that the requesting function asks for them sequentially. Although random\n    lookups are supported, random lookups will involve recomputing state which is slow, and involves linearly advancing\n    from 0 if the last requested index was greater than or equal to this requested index. This should work well with the\n    megatron sampler which is sequential. It handles skipped lookups as will happen with multiple workers by not\n    generating those numbers.\n\n    !!! warning \"Prefer bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler\"\n\n        This class performs sampling with replacement of an underlying dataset. It is recommended to use the epoch-based\n        sampling provided by `bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler` instead, which ensures\n        that each sample is seen exactly once per epoch. This dataset is useful for cases where the dataset is too large\n        for the shuffled list of indices to fit in memory and exhaustive sampling is not required.\n    \"\"\"\n\n    def __init__(self, dataset: Dataset[T_co], seed: int = 42, num_samples: Optional[int] = None):\n        \"\"\"Initializes the PRNGResampleDataset.\n\n        Args:\n            dataset: The dataset to be shuffled.\n            seed: The seed value for the PRNG. Default is 42.\n            num_samples: The number of samples to draw from the dataset.\n                If None, the length of the dataset is used. Default is None.\n        \"\"\"\n        self.initial_seed = seed\n        self.rng = random.Random(seed)\n        self.dataset_len = len(dataset)  # type: ignore\n        self.num_samples = num_samples if num_samples is not None else len(dataset)\n        self.dataset = dataset\n        # Store the last accessed index. On this first pass this is initialized to infinity, which will trigger a reset since\n        #  index - inf &lt; 0 for all values of index. This will lead to `self.advance_state(index)` being called which will advance\n        #  the state to the correct starting index. The last_index will be then be replaced by `index` in that case and the algorithm\n        #  will proceed normally.\n        self.last_index: Union[int, math.inf] = math.inf\n        self.last_rand_index: Optional[int] = None\n\n    def rand_idx(self) -&gt; int:\n        \"\"\"Generates a random index within the range of the dataset size.\"\"\"\n        return self.rng.randint(0, self.dataset_len - 1)\n\n    def advance_state(self, num_to_advance: int):\n        \"\"\"Advances the PRNG state by generating n_to_advance random indices.\n\n        Args:\n            num_to_advance: The number of random state steps to advance.\n        \"\"\"\n        for _ in range(num_to_advance):\n            self.rand_idx()\n\n    def __getitem__(self, index: int) -&gt; T_co:\n        \"\"\"Returns the item from the dataset at the specified index.\n\n        Args:\n            index: The index of the item to retrieve.\n\n        Returns:\n            The item from the dataset at the specified index.\n\n        Note:\n            If the requested index is before the last accessed index, the PRNG state is reset to the initial seed\n            and advanced to the correct state. This is less efficient than advancing forward.\n        \"\"\"\n        idx_diff = index - self.last_index\n        if idx_diff &lt; 0:\n            # We need to go backwards (or it is the first call), which involves resetting to the initial seed and\n            #   then advancing to just before the correct index, which is accomplished with `range(index)`.\n            self.rng = random.Random(self.initial_seed)\n            self.advance_state(index)\n        elif idx_diff == 0:\n            # If the index is the same as the last index, we can just return the last random index that was generated.\n            #  no state needs to be updated in this case so just return.\n            return self.dataset[self.last_rand_index]\n        else:\n            # We need to advance however many steps were skipped since the last call. Since i+1 - i = 1, we need to advance\n            #  by `idx_diff - 1` to accomodate for skipped indices.\n            self.advance_state(idx_diff - 1)\n        self.last_index = index\n        self.last_rand_index = (\n            self.rand_idx()\n        )  # store the last index called incase the user wants to requrest this index again.\n        return self.dataset[self.last_rand_index]  # Advances state by 1\n\n    def __len__(self) -&gt; int:\n        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n        return self.num_samples\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/resamplers/#bionemo.core.data.resamplers.PRNGResampleDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Returns the item from the dataset at the specified index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the item to retrieve.</p> required <p>Returns:</p> Type Description <code>T_co</code> <p>The item from the dataset at the specified index.</p> Note <p>If the requested index is before the last accessed index, the PRNG state is reset to the initial seed and advanced to the correct state. This is less efficient than advancing forward.</p> Source code in <code>bionemo/core/data/resamplers.py</code> <pre><code>def __getitem__(self, index: int) -&gt; T_co:\n    \"\"\"Returns the item from the dataset at the specified index.\n\n    Args:\n        index: The index of the item to retrieve.\n\n    Returns:\n        The item from the dataset at the specified index.\n\n    Note:\n        If the requested index is before the last accessed index, the PRNG state is reset to the initial seed\n        and advanced to the correct state. This is less efficient than advancing forward.\n    \"\"\"\n    idx_diff = index - self.last_index\n    if idx_diff &lt; 0:\n        # We need to go backwards (or it is the first call), which involves resetting to the initial seed and\n        #   then advancing to just before the correct index, which is accomplished with `range(index)`.\n        self.rng = random.Random(self.initial_seed)\n        self.advance_state(index)\n    elif idx_diff == 0:\n        # If the index is the same as the last index, we can just return the last random index that was generated.\n        #  no state needs to be updated in this case so just return.\n        return self.dataset[self.last_rand_index]\n    else:\n        # We need to advance however many steps were skipped since the last call. Since i+1 - i = 1, we need to advance\n        #  by `idx_diff - 1` to accomodate for skipped indices.\n        self.advance_state(idx_diff - 1)\n    self.last_index = index\n    self.last_rand_index = (\n        self.rand_idx()\n    )  # store the last index called incase the user wants to requrest this index again.\n    return self.dataset[self.last_rand_index]  # Advances state by 1\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/resamplers/#bionemo.core.data.resamplers.PRNGResampleDataset.__init__","title":"<code>__init__(dataset, seed=42, num_samples=None)</code>","text":"<p>Initializes the PRNGResampleDataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset[T_co]</code> <p>The dataset to be shuffled.</p> required <code>seed</code> <code>int</code> <p>The seed value for the PRNG. Default is 42.</p> <code>42</code> <code>num_samples</code> <code>Optional[int]</code> <p>The number of samples to draw from the dataset. If None, the length of the dataset is used. Default is None.</p> <code>None</code> Source code in <code>bionemo/core/data/resamplers.py</code> <pre><code>def __init__(self, dataset: Dataset[T_co], seed: int = 42, num_samples: Optional[int] = None):\n    \"\"\"Initializes the PRNGResampleDataset.\n\n    Args:\n        dataset: The dataset to be shuffled.\n        seed: The seed value for the PRNG. Default is 42.\n        num_samples: The number of samples to draw from the dataset.\n            If None, the length of the dataset is used. Default is None.\n    \"\"\"\n    self.initial_seed = seed\n    self.rng = random.Random(seed)\n    self.dataset_len = len(dataset)  # type: ignore\n    self.num_samples = num_samples if num_samples is not None else len(dataset)\n    self.dataset = dataset\n    # Store the last accessed index. On this first pass this is initialized to infinity, which will trigger a reset since\n    #  index - inf &lt; 0 for all values of index. This will lead to `self.advance_state(index)` being called which will advance\n    #  the state to the correct starting index. The last_index will be then be replaced by `index` in that case and the algorithm\n    #  will proceed normally.\n    self.last_index: Union[int, math.inf] = math.inf\n    self.last_rand_index: Optional[int] = None\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/resamplers/#bionemo.core.data.resamplers.PRNGResampleDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the total number of samples in the dataset.</p> Source code in <code>bionemo/core/data/resamplers.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the total number of samples in the dataset.\"\"\"\n    return self.num_samples\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/resamplers/#bionemo.core.data.resamplers.PRNGResampleDataset.advance_state","title":"<code>advance_state(num_to_advance)</code>","text":"<p>Advances the PRNG state by generating n_to_advance random indices.</p> <p>Parameters:</p> Name Type Description Default <code>num_to_advance</code> <code>int</code> <p>The number of random state steps to advance.</p> required Source code in <code>bionemo/core/data/resamplers.py</code> <pre><code>def advance_state(self, num_to_advance: int):\n    \"\"\"Advances the PRNG state by generating n_to_advance random indices.\n\n    Args:\n        num_to_advance: The number of random state steps to advance.\n    \"\"\"\n    for _ in range(num_to_advance):\n        self.rand_idx()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/resamplers/#bionemo.core.data.resamplers.PRNGResampleDataset.rand_idx","title":"<code>rand_idx()</code>","text":"<p>Generates a random index within the range of the dataset size.</p> Source code in <code>bionemo/core/data/resamplers.py</code> <pre><code>def rand_idx(self) -&gt; int:\n    \"\"\"Generates a random index within the range of the dataset size.\"\"\"\n    return self.rng.randint(0, self.dataset_len - 1)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/resource/","title":"Resource","text":""},{"location":"main/references/API_reference/bionemo/core/data/resource/#bionemo.core.data.resource.Resource","title":"<code>Resource</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class that represents a remote resource for downloading and caching test data.</p> Source code in <code>bionemo/core/data/resource.py</code> <pre><code>class Resource(pydantic.BaseModel):\n    \"\"\"Class that represents a remote resource for downloading and caching test data.\"\"\"\n\n    model_config = pydantic.ConfigDict(use_attribute_docstrings=True)\n\n    tag: Annotated[str, pydantic.StringConstraints(pattern=r\"^[^/]*/[^/]*$\")]  # Only slash between filename and tag.\n    \"\"\"A unique identifier for the resource. The file(s) will be accessible via load(\"filename/tag\").\"\"\"\n\n    ngc: Annotated[str, pydantic.AfterValidator(_validate_ngc_resource)] | None = None\n    \"\"\"The NGC URL for the resource.\n\n    Should be in format [org/[team/]]name[:version]. If None, the resource is not available on NGC.\n    \"\"\"\n\n    ngc_registry: Literal[\"model\", \"resource\"] | None = None\n    \"\"\"The NGC resource type (model or resource) for the data. Must be provided if ngc is not None.\"\"\"\n\n    pbss: Annotated[pydantic.AnyUrl, pydantic.UrlConstraints(allowed_schemes=[\"s3\"])]\n    \"\"\"The PBSS (NVIDIA-internal) URL of the resource.\"\"\"\n\n    sha256: str | None\n    \"\"\"The SHA256 checksum of the resource. If None, the SHA will not be checked on download (not recommended).\"\"\"\n\n    owner: pydantic.NameEmail\n    \"\"\"The owner or primary point of contact for the resource, in the format \"Name &lt;email&gt;\".\"\"\"\n\n    description: str | None = None\n    \"\"\"A description of the file(s).\"\"\"\n\n    unpack: Literal[False, None] = None\n    \"\"\"Whether the resource should be unpacked after download. If None, will defer to the file extension.\"\"\"\n\n    decompress: Literal[False, None] = None\n    \"\"\"Whether the resource should be decompressed after download. If None, will defer to the file extension.\"\"\"\n\n    @pydantic.model_validator(mode=\"after\")\n    def _validate_ngc_registry(self):\n        if self.ngc and not self.ngc_registry:\n            raise ValueError(f\"ngc_registry must be provided if ngc is not None: {self.tag}\")\n        return self\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/data/resource/#bionemo.core.data.resource.Resource.decompress","title":"<code>decompress = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the resource should be decompressed after download. If None, will defer to the file extension.</p>"},{"location":"main/references/API_reference/bionemo/core/data/resource/#bionemo.core.data.resource.Resource.description","title":"<code>description = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A description of the file(s).</p>"},{"location":"main/references/API_reference/bionemo/core/data/resource/#bionemo.core.data.resource.Resource.ngc","title":"<code>ngc = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The NGC URL for the resource.</p> <p>Should be in format [org/[team/]]name[:version]. If None, the resource is not available on NGC.</p>"},{"location":"main/references/API_reference/bionemo/core/data/resource/#bionemo.core.data.resource.Resource.ngc_registry","title":"<code>ngc_registry = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The NGC resource type (model or resource) for the data. Must be provided if ngc is not None.</p>"},{"location":"main/references/API_reference/bionemo/core/data/resource/#bionemo.core.data.resource.Resource.owner","title":"<code>owner</code>  <code>instance-attribute</code>","text":"<p>The owner or primary point of contact for the resource, in the format \"Name \"."},{"location":"main/references/API_reference/bionemo/core/data/resource/#bionemo.core.data.resource.Resource.pbss","title":"<code>pbss</code>  <code>instance-attribute</code>","text":"<p>The PBSS (NVIDIA-internal) URL of the resource.</p>"},{"location":"main/references/API_reference/bionemo/core/data/resource/#bionemo.core.data.resource.Resource.sha256","title":"<code>sha256</code>  <code>instance-attribute</code>","text":"<p>The SHA256 checksum of the resource. If None, the SHA will not be checked on download (not recommended).</p>"},{"location":"main/references/API_reference/bionemo/core/data/resource/#bionemo.core.data.resource.Resource.tag","title":"<code>tag</code>  <code>instance-attribute</code>","text":"<p>A unique identifier for the resource. The file(s) will be accessible via load(\"filename/tag\").</p>"},{"location":"main/references/API_reference/bionemo/core/data/resource/#bionemo.core.data.resource.Resource.unpack","title":"<code>unpack = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the resource should be unpacked after download. If None, will defer to the file extension.</p>"},{"location":"main/references/API_reference/bionemo/core/data/resource/#bionemo.core.data.resource.get_all_resources","title":"<code>get_all_resources(resource_path=None)</code>  <code>cached</code>","text":"<p>Return a dictionary of all resources.</p> Source code in <code>bionemo/core/data/resource.py</code> <pre><code>@functools.cache\ndef get_all_resources(resource_path: Path | None = None) -&gt; dict[str, Resource]:\n    \"\"\"Return a dictionary of all resources.\"\"\"\n    if not resource_path:\n        resource_path = Path(files(\"bionemo.core.data\").joinpath(\"resources\"))  # type: ignore\n\n    resources_files = itertools.chain(resource_path.glob(\"*.yaml\"), resource_path.glob(\"*.yml\"))\n\n    all_resources = [resource for file in resources_files for resource in _parse_resource_file(file)]\n\n    resource_list = pydantic.TypeAdapter(list[Resource]).validate_python(all_resources)\n    resource_dict = {resource.tag: resource for resource in resource_list}\n\n    if len(resource_dict) != len(resource_list):\n        # Show the # of and which ones are duplicated so that a user can begin debugging and resolve the issue.\n        tag_counts = Counter([resource.tag for resource in resource_list])\n        raise ValueError(f\"Duplicate resource tags found!: {[tag for tag, count in tag_counts.items() if count &gt; 1]}\")\n\n    return resource_dict\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/model/config/","title":"Config","text":""},{"location":"main/references/API_reference/bionemo/core/model/config/#bionemo.core.model.config.LossType","title":"<code>LossType = TypeVar('LossType')</code>  <code>module-attribute</code>","text":"<p>Stand-in for a loss function; no constraints.</p>"},{"location":"main/references/API_reference/bionemo/core/model/config/#bionemo.core.model.config.ModelOutput","title":"<code>ModelOutput = TypeVar('ModelOutput', Tensor, list[Tensor], tuple[Tensor], dict[str, Tensor], covariant=True)</code>  <code>module-attribute</code>","text":"<p>A Model's forward pass may produce a tensor, multiple tensors, or named tensors.</p>"},{"location":"main/references/API_reference/bionemo/core/model/config/#bionemo.core.model.config.ModelType","title":"<code>ModelType = TypeVar('ModelType', bound=Model)</code>  <code>module-attribute</code>","text":"<p>Generic type for things that have a forward pass.</p>"},{"location":"main/references/API_reference/bionemo/core/model/config/#bionemo.core.model.config.BionemoModelConfig","title":"<code>BionemoModelConfig</code>","text":"<p>               Bases: <code>Generic[ModelType]</code>, <code>ABC</code></p> <p>An abstract class for model configuration.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>class BionemoModelConfig(Generic[ModelType], ABC):\n    \"\"\"An abstract class for model configuration.\"\"\"\n\n    @abstractmethod\n    def configure_model(self, *args, **kwargs) -&gt; Model:\n        \"\"\"Configures the model.\"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/model/config/#bionemo.core.model.config.BionemoModelConfig.configure_model","title":"<code>configure_model(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Configures the model.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>@abstractmethod\ndef configure_model(self, *args, **kwargs) -&gt; Model:\n    \"\"\"Configures the model.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/model/config/#bionemo.core.model.config.BionemoTrainableModelConfig","title":"<code>BionemoTrainableModelConfig</code>","text":"<p>               Bases: <code>Generic[ModelType, LossType]</code>, <code>BionemoModelConfig[ModelType]</code></p> <p>An abstract class for trainable model configuration.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>class BionemoTrainableModelConfig(Generic[ModelType, LossType], BionemoModelConfig[ModelType]):\n    \"\"\"An abstract class for trainable model configuration.\"\"\"\n\n    @abstractmethod\n    def get_loss_reduction_class(self) -&gt; Type[LossType]:\n        \"\"\"Returns the loss reduction class.\"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/model/config/#bionemo.core.model.config.BionemoTrainableModelConfig.get_loss_reduction_class","title":"<code>get_loss_reduction_class()</code>  <code>abstractmethod</code>","text":"<p>Returns the loss reduction class.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>@abstractmethod\ndef get_loss_reduction_class(self) -&gt; Type[LossType]:\n    \"\"\"Returns the loss reduction class.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/model/config/#bionemo.core.model.config.Model","title":"<code>Model</code>","text":"<p>               Bases: <code>Protocol[ModelOutput]</code></p> <p>Lightweight interface for a model: must have a forward method.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>class Model(Protocol[ModelOutput]):\n    \"\"\"Lightweight interface for a model: must have a forward method.\"\"\"\n\n    def forward(self, *args, **kwargs) -&gt; ModelOutput:\n        \"\"\"Prediction / forward-step for a model.\"\"\"\n        ...\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/model/config/#bionemo.core.model.config.Model.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Prediction / forward-step for a model.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>def forward(self, *args, **kwargs) -&gt; ModelOutput:\n    \"\"\"Prediction / forward-step for a model.\"\"\"\n    ...\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/utils/batching_utils/","title":"Batching utils","text":""},{"location":"main/references/API_reference/bionemo/core/utils/batching_utils/#bionemo.core.utils.batching_utils.pad_token_ids","title":"<code>pad_token_ids(token_ids, padding_value=0, padding_len=None, pad_size_divisible_by=1, **convert_to_kwargs)</code>","text":"<p>Pads token ids with padding value, and return the padded tokens and the corresponding mask.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>Union[List[int], List[Tensor]]</code> <p>List of token ids or tensors</p> required <code>padding_value</code> <code>int</code> <p>Value to pad with. Defaults to 0.</p> <code>0</code> <code>padding_len</code> <code>Optional[int]</code> <p>Max length of the padded token ids. Defaults to None.</p> <code>None</code> <code>pad_size_divisible_by</code> <code>int</code> <p>Pad the length of the token ids to be divisible by this number. Defaults to 1.</p> <code>1</code> <code>**convert_to_kwargs</code> <p>Passed directly to tensor.to(**kwargs) if provided</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple[List[int], List[int]]: Padded token ids and mask</p> Source code in <code>bionemo/core/utils/batching_utils.py</code> <pre><code>def pad_token_ids(\n    token_ids: Union[List[int], List[torch.Tensor]],\n    padding_value: int = 0,\n    padding_len: Optional[int] = None,\n    pad_size_divisible_by: int = 1,\n    **convert_to_kwargs,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Pads token ids with padding value, and return the padded tokens and the corresponding mask.\n\n    Args:\n        token_ids: List of token ids or tensors\n        padding_value: Value to pad with. Defaults to 0.\n        padding_len: Max length of the padded token ids. Defaults to None.\n        pad_size_divisible_by: Pad the length of the token ids to be divisible by this number. Defaults to 1.\n        **convert_to_kwargs: Passed directly to tensor.to(**kwargs) if provided\n\n    Returns:\n        Tuple[List[int], List[int]]: Padded token ids and mask\n    \"\"\"\n    lengths = torch.tensor([len(s) for s in token_ids])\n    if padding_len is None:\n        padding_len = lengths.max()\n\n    # make padding divisible by pad_size_divisible_by\n    if pad_size_divisible_by &gt; 1:\n        padding_len = int(math.ceil(padding_len / pad_size_divisible_by) * pad_size_divisible_by)\n\n    # build mask\n    mask = torch.arange(padding_len)[None, :] &lt; lengths[:, None]\n\n    # make sure all sequences are pytorch tensors\n    token_ids = [torch.tensor(s) if not torch.is_tensor(s) else s for s in token_ids]\n    # pad sequences\n    masked_token_ids = torch.nn.utils.rnn.pad_sequence(token_ids, batch_first=True, padding_value=padding_value)\n\n    # convert to desired device\n    if len(convert_to_kwargs):\n        mask = mask.to(**convert_to_kwargs)\n        masked_token_ids = masked_token_ids.to(**convert_to_kwargs)\n\n    # Further pad the sequences to the fixed maximum length, if necessary\n    if masked_token_ids.size(1) &lt; padding_len:\n        padding_size = padding_len - masked_token_ids.size(1)\n        masked_token_ids = torch.nn.functional.pad(masked_token_ids, [0, padding_size], value=padding_value)\n\n    return masked_token_ids, mask\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/utils/dtypes/","title":"Dtypes","text":""},{"location":"main/references/API_reference/bionemo/core/utils/dtypes/#bionemo.core.utils.dtypes.get_autocast_dtype","title":"<code>get_autocast_dtype(precision)</code>","text":"<p>Returns the torch dtype corresponding to the given precision.</p> <p>Parameters:</p> Name Type Description Default <code>precision</code> <code>PrecisionTypes</code> <p>The precision type.</p> required <p>Returns:</p> Type Description <code>dtype</code> <p>torch.dtype: The torch dtype corresponding to the given precision.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the precision is not supported.</p> Source code in <code>bionemo/core/utils/dtypes.py</code> <pre><code>def get_autocast_dtype(precision: PrecisionTypes) -&gt; torch.dtype:\n    \"\"\"Returns the torch dtype corresponding to the given precision.\n\n    Args:\n        precision: The precision type.\n\n    Returns:\n        torch.dtype: The torch dtype corresponding to the given precision.\n\n    Raises:\n        ValueError: If the precision is not supported.\n    \"\"\"\n    # TODO move this to a utilities folder, or find/import the function that does this in NeMo\n    if precision in precision_to_dtype:\n        return precision_to_dtype[precision]\n    else:\n        raise ValueError(f\"Unsupported precision: {precision}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/utils/random_utils/","title":"Random utils","text":""},{"location":"main/references/API_reference/bionemo/core/utils/random_utils/#bionemo.core.utils.random_utils.get_seed_from_rng","title":"<code>get_seed_from_rng(rng, dtype=np.int64)</code>","text":"<p>Generates a deterministic random seed from an existing random generator.</p> <p>This is useful in particular because setting the torch seed doesn't want to accept a tuple of numbers, we we often do in initializing a numpy random generator with epoch, index, and global seeds.</p> <p>Used to seed a torch random generator from a numpy random generator.</p> Source code in <code>bionemo/core/utils/random_utils.py</code> <pre><code>def get_seed_from_rng(rng: np.random.Generator, dtype: Type[np.signedinteger] = np.int64) -&gt; int:\n    \"\"\"Generates a deterministic random seed from an existing random generator.\n\n    This is useful in particular because setting the torch seed doesn't want to accept a tuple of numbers, we we often\n    do in initializing a numpy random generator with epoch, index, and global seeds.\n\n    Used to seed a torch random generator from a numpy random generator.\n    \"\"\"\n    return int(rng.integers(np.iinfo(dtype).max))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/utils/random_utils/#bionemo.core.utils.random_utils.random_numpy_context","title":"<code>random_numpy_context(seed=42)</code>","text":"<p>Context manager for setting numpy random state.</p> <p>The state is saved on entry and restored on exit to what it was. This way you can run code that needs random state in a <code>with</code> context using this function, and get back to whatever state was there before. This is useful for testing where you don't want the random state from one test to impact other tests.</p> Example <p>import numpy as np from bionemo.core.utils.random_utils import random_numpy_context ori_state = np.random.get_state() with random_numpy_context(45):     np.random.randint(5) # this will change the state new_state = np.random.get_state() assert ori_state == new_state</p> Source code in <code>bionemo/core/utils/random_utils.py</code> <pre><code>@contextmanager\ndef random_numpy_context(seed: int = 42) -&gt; Iterator[None]:\n    \"\"\"Context manager for setting numpy random state.\n\n    The state is saved on entry and restored on exit to what it was. This way you can run code that needs random state\n    in a `with` context using this function, and get back to whatever state was there before. This is useful for testing\n    where you don't want the random state from one test to impact other tests.\n\n    Example:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from bionemo.core.utils.random_utils import random_numpy_context\n        &gt;&gt;&gt; ori_state = np.random.get_state()\n        &gt;&gt;&gt; with random_numpy_context(45):\n            np.random.randint(5) # this will change the state\n        &gt;&gt;&gt; new_state = np.random.get_state()\n        &gt;&gt;&gt; assert ori_state == new_state\n    \"\"\"\n    state = np.random.get_state()  # just fail if this fails\n    try:\n        np.random.seed(seed)\n        yield\n    finally:\n        np.random.set_state(state)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/core/utils/subprocess_utils/","title":"Subprocess utils","text":""},{"location":"main/references/API_reference/bionemo/core/utils/subprocess_utils/#bionemo.core.utils.subprocess_utils.run_subprocess_safely","title":"<code>run_subprocess_safely(command, timeout=2000)</code>","text":"<p>Run a subprocess and raise an error if it fails.</p> <p>Parameters:</p> Name Type Description Default <code>command</code> <code>str</code> <p>The command to run.</p> required <code>timeout</code> <code>int</code> <p>The timeout for the command.</p> <code>2000</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The result of the subprocess.</p> Source code in <code>bionemo/core/utils/subprocess_utils.py</code> <pre><code>def run_subprocess_safely(command: str, timeout: int = 2000) -&gt; Dict[str, Any]:\n    \"\"\"Run a subprocess and raise an error if it fails.\n\n    Args:\n        command: The command to run.\n        timeout: The timeout for the command.\n\n    Returns:\n        The result of the subprocess.\n    \"\"\"\n    try:\n        # Use Popen to enable real-time output while still capturing it\n        process = subprocess.Popen(\n            shlex.split(command),\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            bufsize=1,\n            universal_newlines=True,\n        )\n\n        stdout_lines = []\n        stderr_lines = []\n\n        # Read output in real-time\n        import select\n        import sys\n\n        while True:\n            # Use select to check for available output (Unix/Linux/Mac only)\n            if hasattr(select, \"select\"):\n                ready, _, _ = select.select([process.stdout, process.stderr], [], [], 0.1)\n\n                if process.stdout in ready:\n                    line = process.stdout.readline()\n                    if line:\n                        stdout_lines.append(line)\n                        print(line.rstrip(), file=sys.stdout, flush=True)\n\n                if process.stderr in ready:\n                    line = process.stderr.readline()\n                    if line:\n                        stderr_lines.append(line)\n                        print(line.rstrip(), file=sys.stderr, flush=True)\n            else:\n                # Fallback for Windows - read with timeout\n                try:\n                    stdout_data, stderr_data = process.communicate(timeout=0.1)\n                    if stdout_data:\n                        stdout_lines.extend(stdout_data.splitlines(keepends=True))\n                        print(stdout_data.rstrip(), file=sys.stdout, flush=True)\n                    if stderr_data:\n                        stderr_lines.extend(stderr_data.splitlines(keepends=True))\n                        print(stderr_data.rstrip(), file=sys.stderr, flush=True)\n                    break\n                except subprocess.TimeoutExpired:\n                    pass\n\n            # Check if process has finished\n            if process.poll() is not None:\n                # Read any remaining output\n                remaining_stdout, remaining_stderr = process.communicate()\n                if remaining_stdout:\n                    stdout_lines.extend(remaining_stdout.splitlines(keepends=True))\n                    print(remaining_stdout.rstrip(), file=sys.stdout, flush=True)\n                if remaining_stderr:\n                    stderr_lines.extend(remaining_stderr.splitlines(keepends=True))\n                    print(remaining_stderr.rstrip(), file=sys.stderr, flush=True)\n                break\n\n        # Check for timeout\n        try:\n            process.wait(timeout=timeout)\n        except subprocess.TimeoutExpired:\n            process.kill()\n            raise\n\n        # Check return code\n        if process.returncode != 0:\n            raise subprocess.CalledProcessError(\n                process.returncode, command, output=\"\".join(stdout_lines), stderr=\"\".join(stderr_lines)\n            )\n\n        # Create result object similar to subprocess.run\n        class Result:\n            def __init__(self, stdout, stderr, returncode):\n                self.stdout = stdout\n                self.stderr = stderr\n                self.returncode = returncode\n\n        result = Result(\"\".join(stdout_lines), \"\".join(stderr_lines), process.returncode)\n        return {\"stdout\": result.stdout, \"stderr\": result.stderr, \"returncode\": result.returncode}\n    except subprocess.TimeoutExpired as e:\n        logger.error(f\"Command timed out. Command: {command}\\nstdout:\\n{e.stdout}\\nstderr:\\n{e.stderr}\")\n        return {\"error\": \"timeout\", \"stdout\": e.stdout, \"stderr\": e.stderr, \"returncode\": None}\n\n    except subprocess.CalledProcessError as e:\n        logger.error(\n            f\"Command failed. Command: {command}\\nreturncode: {e.returncode}\\nstdout:\\n{e.stdout}\\nstderr:\\n{e.stderr}\"\n        )\n        return {\"error\": \"non-zero exit\", \"stdout\": e.stdout, \"stderr\": e.stderr, \"returncode\": e.returncode}\n\n    except FileNotFoundError as e:\n        logger.error(f\"Command not found. Command: {command}\\nstderr:\\n{str(e)}\")\n        return {\"error\": \"not found\", \"stdout\": \"\", \"stderr\": str(e), \"returncode\": None}\n\n    except Exception as e:\n        # catch-all for other unexpected errors\n        return {\"error\": \"other\", \"message\": str(e), \"stdout\": \"\", \"stderr\": \"\", \"returncode\": None}\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/","title":"Evo2 Data Preparation","text":""},{"location":"main/references/API_reference/bionemo/evo2/data/#data-preprocessing","title":"Data Preprocessing","text":"<p>To streamline the process of preparing and building datasets for training Evo2 on DNA sequences, we provide a configurable preprocessing script (<code>preprocess.py</code>) that can preprocess and tokenize a collection of <code>.fasta</code> files and convert them into Megatron-compatible <code>IndexedDataset</code>.</p> <pre><code>preprocess_evo2 -c &lt;CONFIG_PATH&gt;\n</code></pre> <p>or if you are running the script outside of the BioNeMo container or you haven't pip-installed <code>bionemo-evo2</code>, then you can run the script directly:</p> <pre><code>python sub-packages/bionemo-evo2/src/bionemo/evo2/data/preprocess.py -c &lt;CONFIG_PATH&gt;\n</code></pre> <p>Configuration YAML parameters for the script can be found in <code>utils/config.py</code>:</p> <pre><code>class Evo2PreprocessingConfig(BaseModel):\n    \"\"\"Pydantic model class specifying the configuration schema for a preprocessed IndexedDataset (.bin, .idx).\"\"\"\n\n    # Collection of FASTA files to preprocess and wrap into a single IndexedDataset.\n    datapaths: list[Path] = []\n    # Output directory for the preprocessed dataset .bin/.idx.\n    output_dir: None | Path = None\n    # Output file prefix for identifying your datasets.\n    output_prefix: None | str = None\n    # Random Sequence-Level Datasplit\n    train_split: float = 0.7\n    valid_split: float = 0.2\n    test_split: float = 0.1\n    # Overwrite existing binaries. Otherwise, skip already preprocessed datasets.\n    overwrite: bool = False\n    # Raw Preprocessing Transforms\n    # For every sequence, include a reverse-complemented copy of that sequence in the dataset. Doubles the size of the dataset.\n    embed_reverse_complement: bool = False\n    # For every sequence, randomly reverse complement the sequence with the specified probability instead of using the original sequence.\n    random_reverse_complement: float = 0.0\n    # For sequences associated with taxonomic lineages specified in `taxonomy_data`, randomly drop out nodes of the lineage with the specified probability. For instance: |d__KINGDOM;p__None;c__CLASS;o__None;f__None;g__None;s__None|\n    random_lineage_dropout: float = 0.0\n    # Transcribe (DNA -&gt; RNA) or Back-Transcribe (RNA -&gt; DNA) the sequence before tokenization.\n    transcribe: None | Literal[\"transcribe\", \"back_transcribe\"] = None\n    # Force upper-case alphabetical characters in the `.fasta` sequences.\n    force_uppercase: bool = False\n    # Data type of the IndexedDataset. When using the byte-level tokenizer, uint8 is more than sufficient with a vocabulary size of 255 for ASCII.\n    indexed_dataset_dtype: str = \"uint8\"\n    # Tokenization Transforms\n    # Append end-of-document token to the end of each sequence.\n    append_eod: bool = False\n    # Enforce the length of the sequence, by padding shorter sequences and raising exceptions when the length is exceeded.\n    enforce_sample_length: None | int = None\n    # Run ftfy on the sequence characters prior to tokenization to fix encoding issues.\n    ftfy: bool = False\n    # Tokenizer\n    tokenizer_type: Literal[\n        \"Byte-Level\",\n        \"HuggingFace\",\n        \"SentencePiece\",\n        \"Regex\",\n        \"Megatron\",\n        \"Tiktoken\",\n    ] = \"Byte-Level\"  # Recommended for DNA / RNA sequences. All other tokenizers have not been tested, and only supported here for experimentation!\n    # For more information on the behavior of the following parameters, refer to NeMo:\n    # https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/modules/common/tokenizer_utils.py\n    vocab_file: None | Path = None\n    vocab_size: None | int = 512\n    merges_file: None | Path = None\n    tokenizer_model_name: None | str = None\n    pretrained_tokenizer_model: None | str = None\n    special_tokens: None | dict[str, str] = {}\n    fast_hf_tokenizer: bool = False\n    # Compute Configuration\n    # NOTE: If preprocessing a large amount of short individual sequences (&lt; 1000 bp), do NOT use\n    # multiprocessing (workers &gt; 1) because sequence-level parallel IPC will dominate the preprocessing time!\n    workers: int = 1\n    # Number of sequences to load into memory at any given time during preprocessing.\n    # Prevents OOM while doing sequence-parallel.\n    preproc_concurrency: int = 100000\n    chunksize: int = 1\n    # Data Filters\n    drop_empty_sequences: bool = False\n    # If `NNN` is detected in the sequence, drop it from the preprocessed dataset.\n    nnn_filter: bool = False\n    # RNG\n    seed: None | int = None\n    # Evo2 Taxonomic Lineage Tags\n    # SeqID Sub-String Indexing: \"ABC\" will have taxonomy data from \"A\".\n    taxonomy_data: dict[str, Evo2TaxonomyLineage] = {}\n    # Periodicity of injecting phylogenetic lineage tags in the sequence prior to tokenization.\n    prompt_spacer_length: int = 131072\n</code></pre> <p>Furthermore, the <code>taxonomy_data</code> field contains a map from sequence ID substrings to phylogenetic lineage data of the form:</p> <pre><code>class Evo2TaxonomyLineage(BaseModel):\n    \"\"\"Pydantic model class that defines the source lineage of a DNA sequence.\"\"\"\n\n    kingdom: None | str = None\n    phylum: None | str = None\n    clazz: None | str = None\n    order: None | str = None\n    family: None | str = None\n    genus: None | str = None\n    species: None | str = None\n</code></pre> <p>which gets converted into a lineage string prior to tokenization as a prefix to the sequence:</p> <pre><code># (Example) Escherichia coli\n|d__Bacteria;p__Pseudomonadota;c__Gammaproteobacteria;o__Enterobacterales;f__Enterobacteriaceae;g__Escherichia;s__Escherichia coli|ATCGTACGTACATCTCTA...\n</code></pre> <p>In the Evo2 model, this special \"token\" is masked out in the loss function, so the model will learn to not generate tokens of this form.</p>"},{"location":"main/references/API_reference/bionemo/evo2/data/#testing","title":"Testing","text":"<p>To test equivalence with the reference implementation we first downloaded source-of-truth preprocessed Megatron <code>IndexedDataset</code> containing promoters data:</p> <pre><code>$ ls -lah\n-rwxr-xr-x  1 bionemo bionemo 1.2M Dec  4 00:56 data_promoters_test_text_CharLevelTokenizer_document.bin\n-rwxr-xr-x  1 bionemo bionemo  20K Dec  4 00:56 data_promoters_test_text_CharLevelTokenizer_document.idx\n-rwxr-xr-x  1 bionemo bionemo 392M Dec  4 00:56 data_promoters_train_text_CharLevelTokenizer_document.bin\n-rwxr-xr-x  1 bionemo bionemo 6.6M Dec  4 00:56 data_promoters_train_text_CharLevelTokenizer_document.idx\n-rwxr-xr-x  1 bionemo bionemo 1.2M Dec  4 00:56 data_promoters_valid_text_CharLevelTokenizer_document.bin\n-rwxr-xr-x  1 bionemo bionemo  20K Dec  4 00:56 data_promoters_valid_text_CharLevelTokenizer_document.idx\n</code></pre> <p>Next we acquired the <code>.fasta</code> file that was used to generate this, and configured our scripts to preprocess the sequence data into equivalent Megatron <code>IndexedDataset</code>.</p> <pre><code># mmseqs_promotors_config.yaml\n- datapaths: [\"/workspace/bionemo2/data/mmseqs_results_rep_seq_distinct.fasta\"]\n  output_dir: \"/workspace/bionemo2/data\"\n  output_prefix: promoters_uint8_distinct\n  train_split: 1.0  # We're just going to dump everything into a single file and compare against the union of the 3 splits in the SoT.\n  valid_split: 0.0\n  test_split: 0.0\n  overwrite: True\n  embed_reverse_complement: true\n  random_reverse_complement: 0.0\n  random_lineage_dropout: 0.0\n  include_sequence_id: false\n  transcribe: \"back_transcribe\"\n  force_uppercase: true\n  indexed_dataset_dtype: \"uint8\"\n  tokenizer_type: \"Byte-Level\"\n  vocab_file: null\n  vocab_size: null\n  merges_file: null\n  pretrained_tokenizer_model: null\n  special_tokens: null\n  fast_hf_tokenizer: true\n  append_eod: true\n  enforce_sample_length: null\n  ftfy: false\n  workers: 1\n  preproc_concurrency: 100000\n  chunksize: 25\n  drop_empty_sequences: true\n  nnn_filter: true\n  seed: null  # Not relevant because we are not using random reverse complement or lineage dropout.\n</code></pre> <p>To run the preprocessing script, we ran the following command:</p> <pre><code>$ python preprocess.py -c mmseqs_promotors_config.yaml\n</code></pre> <p>To check equivalence of the two preprocessed datasets, we verify that we get the same elements out of our processed dataset as the original, but do not enforce ordering of the data. (<code>bionemo-noodles</code> does not sequentially read the <code>.fasta</code> file.)</p> <pre><code>&gt;&gt;&gt; from megatron.core.datasets.indexed_dataset import IndexedDataset\n&gt;&gt;&gt; ds_train_ref = IndexedDataset(\"./data_promoters_train_text_CharLevelTokenizer_document\")\n&gt;&gt;&gt; ds_val_ref = IndexedDataset(\"./data_promoters_valid_text_CharLevelTokenizer_document\")\n&gt;&gt;&gt; ds_test_ref = IndexedDataset(\"./data_promoters_test_text_CharLevelTokenizer_document\")\n&gt;&gt;&gt; ds_train_ours = IndexedDataset(\"./promoters_uint8_distinct_byte-level_train\")\n&gt;&gt;&gt; len(ds_train_ours) == len(ds_train_ref) + len(ds_test_ref) + len(ds_val_ref)\nTrue\n&gt;&gt;&gt;  # Example of what one of these set elements looks like, it's just a string representation of the token list for an\n&gt;&gt;&gt;  #  element of the training dataset. We can then compare all of these to make sure that the two datasets have the\n&gt;&gt;&gt;  #  same set of samples.\n&gt;&gt;&gt; ','.join([str(t) for t in ds_train_ref[0]])\n'67,84,71,71,65,71,67,67,84,71,65,67,67,65,84,65,65,71,84,65,71,84,71,71,67,84,65,84,65,65,67,71,65,71,71,65,65,71,65,65,71,65,84,71,65,65,71,65,71,65,84,84,65,71,65,71,65,65,65,65,84,71,65,65,84,71,84,84,67,84,84,71,65,65,71,84,65,71,67,67,65,84,84,71,84,84,71,84,65,71,84,84,71,84,84,71,84,71,84,71,84,71,84,65,84,71,84,84,71,65,71,65,84,71,84,84,84,84,71,71,71,71,84,84,84,71,84,84,65,84,65,84,65,71,65,71,65,71,65,71,65,84,71,84,65,71,84,84,84,71,71,84,71,65,65,71,65,71,84,65,71,71,65,84,84,67,84,67,84,84,65,67,84,65,71,84,71,84,71,65,65,71,65,84,84,65,84,84,65,67,84,65,71,71,84,65,65,67,84,65,65,65,84,71,65,71,65,84,84,67,84,65,84,67,65,65,67,84,65,65,71,84,67,65,84,84,65,71,65,71,65,84,84,71,71,65,65,65,84,71,84,84,84,67,84,84,84,84,65,71,71,84,84,84,65,65,84,65,65,65,71,84,84,84,71,84,84,84,71,65,65,84,84,71,65,71,65,65,65,71,65,71,65,71,65,71,71,65,71,65,71,65,67,65,84,84,71,67,84,84,84,71,65,65,71,71,71,65,71,65,71,84,84,84,71,71,71,84,71,71,71,84,71,65,71,71,65,84,84,71,65,65,65,65,84,71,65,65,65,65,65,84,71,65,65,67,84,71,65,65,65,65,65,71,71,84,71,84,84,65,84,65,71,84,71,65,67,67,84,71,84,67,65,65,65,65,65,65,71,67,84,71,84,71,65,65,71,65,65,71,84,71,84,84,65,84,67,67,65,65,71,65,65,65,84,65,84,71,71,65,84,84,71,67,84,65,65,84,67,65,84,65,67,84,65,67,84,71,84,84,67,65,84,84,65,84,71,65,84,84,84,84,65,84,71,84,71,84,67,65,84,71,84,71,84,71,84,71,67,67,84,65,84,67,65,84,67,65,84,84,67,67,84,84,65,84,65,84,84,84,84,65,71,84,84,71,71,67,65,65,65,65,65,65,65,65,65,65,65,71,65,67,84,84,71,71,65,65,71,84,65,84,84,71,65,65,65,65,67,67,65,65,65,84,67,84,71,65,84,67,84,67,65,65,67,67,84,65,71,65,67,65,65,71,84,67,71,65,84,84,65,65,65,71,67,84,65,65,65,67,67,71,65,65,65,65,67,67,71,65,65,84,67,67,67,71,65,67,67,71,71,84,84,65,65,84,84,71,65,65,65,65,67,67,71,65,84,67,67,65,0'\n&gt;&gt;&gt; # Create a set of all of these elements:\n&gt;&gt;&gt; all_ref_data = {','.join([str(t) for t in rec]) for ds in [ds_train_ref, ds_val_ref, ds_test_ref] for rec in ds}\n&gt;&gt;&gt; # Verify that there is no redundancy so we can do set equality safely\n&gt;&gt;&gt; len(all_ref_data) == len(ds_train_ours)\nTrue\n&gt;&gt;&gt; len(all_ref_data)\n343504\n&gt;&gt;&gt; all_our_data = {','.join([str(t) for t in rec]) for ds in [ds_train_ours] for rec in ds}\n&gt;&gt;&gt; len(all_our_data)\n343504\n&gt;&gt;&gt; # Verify set equality to show that we have processed an identical dataset\n&gt;&gt;&gt; #  (ignoring shuffling order and train/test/val splits)\n&gt;&gt;&gt; all_our_data == all_ref_data\nTrue\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/#sequence-splicing-stitching","title":"Sequence Splicing &amp; Stitching","text":"<p>Evo2 has also been trained on spliced DNA and mRNA sequences, where introns are removed leaving only the concatenated exons of the genome. Moreover, \"stitched\" variants of spliced transcripts have been introduced into Evo2's training dataset, which include 1024 bp of sequence from the promoter and 32 bp around each exon.</p> <p>To perform splicing or \"stitched\" splicing on sequences in a FASTA file given an associated gene transfer format (GTF) file, execute the following command:</p> <pre><code>$ splice_evo2 --help\nusage: splice_evo2 [-h] --fasta-path FASTA_PATH --gtf-path GTF_PATH [--output-path OUTPUT_PATH] [--transcript-type {default,stitched}] [--stitched-promoter STITCHED_PROMOTER] [--stitched-intron STITCHED_INTRON] [--stitched-overlap] [--only-longest-transcript] [-v]\n\nExtract spliced transcripts from a FASTA and GTF.\n\noptions:\n  -h, --help            show this help message and exit\n  --fasta-path FASTA_PATH\n                        Path to FASTA file to extract transcripts from.\n  --gtf-path GTF_PATH   Path to gene transfer format (GTF) file associated with the FASTA.\n  --output-path OUTPUT_PATH\n                        Path to output FASTA file.\n  --transcript-type {default,stitched}\n                        Type of transcript to extract from the GTF and FASTA files for splicing. 'Stitched' transcripts include 1024 bp of sequence from the promoter and 32 bp around each exon.\n  --stitched-promoter STITCHED_PROMOTER\n                        Number of bp to include in the promoter region when --transcript-type=stitched is used. Defaults to 1024.\n  --stitched-intron STITCHED_INTRON\n                        Number of bp to include from neighboring introns when --transcript-type=stitched is used. Defaults to 32.\n  --stitched-overlap    Allow overlap of neighboring intron windows when --transcript-type=stitched is used. Defaults to False, i.e. prevents overlap by shortening the intron windows for a contiguous splice.\n  --only-longest-transcript\n                        Only extract the longest transcript per gene.\n  -v, --verbose         Turn on verbose log messages.\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/fasta_dataset/","title":"Fasta dataset","text":""},{"location":"main/references/API_reference/bionemo/evo2/data/fasta_dataset/#bionemo.evo2.data.fasta_dataset.SimpleFastaDataset","title":"<code>SimpleFastaDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A simple dataset for Evo2 prediction.</p> <p>Currently, this will not work for pre-training or fine-tuning, as that would require: 1) including \"labels\" in the input and 2) offsetting/rolling either the labels or input_ids to handle the off-by-one token prediction alignment.</p> Source code in <code>bionemo/evo2/data/fasta_dataset.py</code> <pre><code>class SimpleFastaDataset(torch.utils.data.Dataset):\n    \"\"\"A simple dataset for Evo2 prediction.\n\n    Currently, this will not work for pre-training or fine-tuning, as that would require:\n    1) including \"labels\" in the input and 2) offsetting/rolling either the labels or\n    input_ids to handle the off-by-one token prediction alignment.\n    \"\"\"\n\n    def __init__(\n        self, fasta_path: Path, tokenizer, prepend_bos: bool = True, custom_loss_masker: Callable | None = None\n    ):\n        \"\"\"Initialize the dataset.\"\"\"\n        super().__init__()\n        self.fasta = NvFaidx(fasta_path)\n        self.seqids = sorted(self.fasta.keys())\n        self.tokenizer = tokenizer\n        self.prepend_bos = prepend_bos  # needed for getting predictions for the requested set of tokens.\n        self.custom_loss_masker = custom_loss_masker\n\n    def write_idx_map(self, output_dir: Path):\n        \"\"\"Write the index map to the output directory.\"\"\"\n        with open(output_dir / \"seq_idx_map.json\", \"w\") as f:\n            json.dump({seqid: idx for idx, seqid in enumerate(self.seqids)}, f)\n\n    def __len__(self):\n        \"\"\"Get the length of the dataset.\"\"\"\n        return len(self.seqids)\n\n    def __getitem__(self, idx: int) -&gt; dict[str, torch.Tensor]:\n        \"\"\"Get an item from the dataset.\"\"\"\n        sequence = self.fasta[self.seqids[idx]].sequence().upper()\n        tokenized_seq = self.tokenizer.text_to_ids(sequence)\n        if self.prepend_bos:  # in pretraining we use EOS to start new sequences.\n            tokens: list[int] = [self.tokenizer.eod] + tokenized_seq\n        else:\n            tokens: list[int] = tokenized_seq\n        loss_mask = torch.ones_like(torch.tensor(tokens, dtype=torch.long), dtype=torch.long)\n        if self.custom_loss_masker is not None:\n            custom_loss_mask = self.custom_loss_masker(tokens)\n            loss_mask &amp;= custom_loss_mask\n        if self.prepend_bos:\n            loss_mask[0] = (\n                0  # mask the eos token which we use for causal offsetting. Later in predict we take the output\n            )\n            #  for the first [:-1] tokens which align with the sequence starting after the EOS.\n        return {\n            \"tokens\": torch.tensor(tokens, dtype=torch.long),\n            \"position_ids\": torch.arange(len(tokens), dtype=torch.long),\n            \"seq_idx\": torch.tensor(idx, dtype=torch.long),\n            \"loss_mask\": loss_mask,\n        }\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/fasta_dataset/#bionemo.evo2.data.fasta_dataset.SimpleFastaDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Get an item from the dataset.</p> Source code in <code>bionemo/evo2/data/fasta_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Get an item from the dataset.\"\"\"\n    sequence = self.fasta[self.seqids[idx]].sequence().upper()\n    tokenized_seq = self.tokenizer.text_to_ids(sequence)\n    if self.prepend_bos:  # in pretraining we use EOS to start new sequences.\n        tokens: list[int] = [self.tokenizer.eod] + tokenized_seq\n    else:\n        tokens: list[int] = tokenized_seq\n    loss_mask = torch.ones_like(torch.tensor(tokens, dtype=torch.long), dtype=torch.long)\n    if self.custom_loss_masker is not None:\n        custom_loss_mask = self.custom_loss_masker(tokens)\n        loss_mask &amp;= custom_loss_mask\n    if self.prepend_bos:\n        loss_mask[0] = (\n            0  # mask the eos token which we use for causal offsetting. Later in predict we take the output\n        )\n        #  for the first [:-1] tokens which align with the sequence starting after the EOS.\n    return {\n        \"tokens\": torch.tensor(tokens, dtype=torch.long),\n        \"position_ids\": torch.arange(len(tokens), dtype=torch.long),\n        \"seq_idx\": torch.tensor(idx, dtype=torch.long),\n        \"loss_mask\": loss_mask,\n    }\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/fasta_dataset/#bionemo.evo2.data.fasta_dataset.SimpleFastaDataset.__init__","title":"<code>__init__(fasta_path, tokenizer, prepend_bos=True, custom_loss_masker=None)</code>","text":"<p>Initialize the dataset.</p> Source code in <code>bionemo/evo2/data/fasta_dataset.py</code> <pre><code>def __init__(\n    self, fasta_path: Path, tokenizer, prepend_bos: bool = True, custom_loss_masker: Callable | None = None\n):\n    \"\"\"Initialize the dataset.\"\"\"\n    super().__init__()\n    self.fasta = NvFaidx(fasta_path)\n    self.seqids = sorted(self.fasta.keys())\n    self.tokenizer = tokenizer\n    self.prepend_bos = prepend_bos  # needed for getting predictions for the requested set of tokens.\n    self.custom_loss_masker = custom_loss_masker\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/fasta_dataset/#bionemo.evo2.data.fasta_dataset.SimpleFastaDataset.__len__","title":"<code>__len__()</code>","text":"<p>Get the length of the dataset.</p> Source code in <code>bionemo/evo2/data/fasta_dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"Get the length of the dataset.\"\"\"\n    return len(self.seqids)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/fasta_dataset/#bionemo.evo2.data.fasta_dataset.SimpleFastaDataset.write_idx_map","title":"<code>write_idx_map(output_dir)</code>","text":"<p>Write the index map to the output directory.</p> Source code in <code>bionemo/evo2/data/fasta_dataset.py</code> <pre><code>def write_idx_map(self, output_dir: Path):\n    \"\"\"Write the index map to the output directory.\"\"\"\n    with open(output_dir / \"seq_idx_map.json\", \"w\") as f:\n        json.dump({seqid: idx for idx, seqid in enumerate(self.seqids)}, f)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/preprocess/","title":"Preprocess","text":"<p>Module containing data preprocessing and splitting functions for Evo2 in BioNeMo.</p> <p>It can also be utilized as a script to dump pre-processed data to JSON.</p>"},{"location":"main/references/API_reference/bionemo/evo2/data/preprocess/#bionemo.evo2.data.preprocess.Evo2Preprocessor","title":"<code>Evo2Preprocessor</code>","text":"<p>Data preprocessing class for Evo2.</p> Source code in <code>bionemo/evo2/data/preprocess.py</code> <pre><code>class Evo2Preprocessor:\n    \"\"\"Data preprocessing class for Evo2.\"\"\"\n\n    BIN = \".bin\"\n    IDX = \".idx\"\n    TRAIN = \"train\"\n    VAL = \"val\"\n    TEST = \"test\"\n\n    def __init__(self, params: Evo2PreprocessingConfig | None = None):\n        \"\"\"Initialize Evo2Preprocessor.\n\n        Args:\n            params (Evo2PreprocessingConfig | None): Configuration parameters for preprocessing.\n        \"\"\"\n        self.tokenizer: Evo2Tokenizer = Evo2Tokenizer(params)\n\n    @staticmethod\n    @contextmanager\n    def preprocessing_context_manager(seed: Optional[int] = None):\n        \"\"\"Context manager for setting and restoring the random number generator state.\n\n        Args:\n            seed (int | None): Seed for the random number generator. Defaults to None.\n        \"\"\"\n        # Track current state.\n        current_state = random.getstate()\n        try:\n            # Set random seed.\n            random.seed(seed)\n            yield seed\n        finally:\n            # Restore random state.\n            random.setstate(current_state)\n\n    @staticmethod\n    def _get_output_filename(\n        config: Evo2PreprocessingConfig, ext: Optional[str] = None, split: Optional[str] = None, temp: bool = False\n    ) -&gt; Path:\n        \"\"\"Generate the output filename for the preprocessed data.\n\n        Args:\n            config (Evo2PreprocessingConfig): Configuration object containing preprocessing settings.\n            ext (Optional[str]): File extension for the output file. Defaults to None.\n            split (Optional[str]): Data split type (e.g., 'train', 'val', 'test'). Defaults to None.\n            temp (bool): Flag indicating whether the file is temporary. Defaults to False.\n\n        Returns:\n            Path: The constructed output file path.\n        \"\"\"\n        # Get output directory. Defaults to CWD.\n        output_dir = config.output_dir\n        if output_dir is None:\n            output_dir = Path.cwd()\n        # Pickup output file prefix.\n        config_prefix = \"{}_{}\".format(config.output_prefix, config.tokenizer_type.lower().replace(\" \", \"\"))\n        output_filepath = Path(output_dir) / (\n            config_prefix\n            + (f\"_{split}\" if split is not None else \"\")\n            + (ext if ext is not None else \"\")\n            + (\".tmp\" if temp else \"\")\n        )\n        return output_filepath\n\n    @staticmethod\n    def _subsequence_generator(sequence: str, subsequence_length: Optional[int] = None, offset: Optional[int] = None):\n        \"\"\"Generate subsequences from a given sequence.\n\n        Args:\n            sequence (str): The input sequence.\n            subsequence_length (int | None): Length of each subsequence. Defaults to the length of the sequence.\n            offset (int | None): Step size for generating subsequences. Defaults to subsequence_length.\n\n        Yields:\n            str: Subsequences of the input sequence.\n        \"\"\"\n        subsequence_length = subsequence_length if subsequence_length is not None else len(sequence)\n        step_size = offset if offset is not None else subsequence_length\n        for i in range(0, len(sequence), step_size):\n            yield sequence[i : i + subsequence_length]\n\n    @staticmethod\n    def _random_reverse_complement(seq: str, prob: float = 0.0, seed: Optional[int] = None):\n        \"\"\"Randomly reverse complements a DNA sequence based on a given probability.\n\n        Args:\n            seq (str): The DNA sequence to potentially reverse complement.\n            prob (float): The probability of reverse complementing the sequence. Defaults to 0.0.\n            seed (Optional[int]): The seed for the random number generator. Defaults to None.\n\n        Returns:\n            str: The original or reverse complemented DNA sequence based on the probability.\n        \"\"\"\n        with Evo2Preprocessor.preprocessing_context_manager(seed):\n            if random.random() &lt; prob:\n                return complement_sequence(reverse_sequence(seq))\n            else:\n                return seq\n\n    @staticmethod\n    def _reverse_complement_expansion(seq: str):\n        \"\"\"Generate a list containing the original and reverse complemented sequence.\n\n        Args:\n            seq (str): The input DNA sequence.\n\n        Returns:\n            list[str]: List containing the original and reverse complemented sequence.\n        \"\"\"\n        return [seq, complement_sequence(reverse_sequence(seq))]\n\n    @staticmethod\n    def _train_val_test_split(train_weight: float, val_weight: float, test_weight: float, seed: Optional[int] = None):\n        \"\"\"Randomly assign a data point to train, validation, or test split based on provided weights.\n\n        Args:\n            train_weight (float): The weight for the training split.\n            val_weight (float): The weight for the validation split.\n            test_weight (float): The weight for the test split.\n            seed (Optional[int]): The seed for the random number generator. Defaults to None.\n\n        Returns:\n            str: The split assignment ('train', 'val', or 'test').\n\n        Raises:\n            ValueError: If the sum of the weights is zero or negative.\n        \"\"\"\n        with Evo2Preprocessor.preprocessing_context_manager(seed if seed is not None else None):\n            # Generate random number.\n            roll = random.random()\n            # Rectify and normalize split ratios.\n            total_weight = abs(train_weight) + abs(val_weight) + abs(test_weight)\n            if total_weight &lt;= 0:\n                raise ValueError(\"Train-validation-test split proportions cannot be zero.\")\n            train_split = abs(train_weight) / total_weight\n            test_split = abs(test_weight) / total_weight\n            split = \"train\"\n            if roll &gt; train_split:\n                if roll &lt; 1 - test_split:\n                    split = \"val\"\n                else:\n                    split = \"test\"\n            return split\n\n    @staticmethod\n    def _construct_taxonomy_token(\n        lineage: Evo2TaxonomyLineage, dropout: float = 0.0, seed: Optional[int] = None\n    ) -&gt; Optional[str]:\n        \"\"\"Construct a special Taxonomy token for natural language prompting of DNA generation models.\n\n        Args:\n            lineage (Evo2TaxonomyLineage): The taxonomy lineage information.\n            dropout (float): The probability of dropping out segments of the lineage. Defaults to 0.0.\n            seed (Optional[int]): The seed for the random number generator. Defaults to None.\n\n        Returns:\n            Optional[str]: The constructed taxonomy token or None if lineage is None.\n        \"\"\"\n        # If dropout &gt; 0, randomly drop out segments of the lineage for training on incomplete lineages.\n        with Evo2Preprocessor.preprocessing_context_manager(seed if seed is not None else None):\n            return (\n                \"|d__{};p__{};c__{};o__{};f__{};g__{};s__{}|\".format(\n                    lineage.domain if random.random() &gt;= dropout else None,\n                    lineage.phylum if random.random() &gt;= dropout else None,\n                    lineage.clazz if random.random() &gt;= dropout else None,\n                    lineage.order if random.random() &gt;= dropout else None,\n                    lineage.family if random.random() &gt;= dropout else None,\n                    lineage.genus if random.random() &gt;= dropout else None,\n                    lineage.species if random.random() &gt;= dropout else None,\n                )\n                if lineage is not None\n                else None\n            )\n\n    def preprocess_data(self, filepath: str, seqid: str, seq: str, seq_idx: int, config: Evo2PreprocessingConfig):\n        \"\"\"Preprocess fasta datapaths.\n\n        Args:\n            filepath (str): Path to the .fasta file.\n            seqid (str): Sequence ID.\n            seq (str): DNA sequence.\n            seq_idx (int): Sequence index.\n            config (Evo2PreprocessingConfig): Configuration object containing preprocessing settings.\n\n        Returns:\n            tuple[list[dict], float]: Preprocessed data and the time taken for preprocessing.\n        \"\"\"\n        # Timing.\n        start = time.time()\n        # Retrieve taxonomy lineage string if SeqID has associated taxonomy data.\n        # Note: Better implemented as a suffix tree substring dictionary, but convenient\n        # for identifying a large amount of sequences with identical lineages.\n        # Slow for extremely large dictionaries of (SeqID Substr, Taxonomy) pairs.\n        lineage = None\n        for id, tax in config.taxonomy_data.items():\n            # Taxonomy ID is a substring of Seq ID.\n            if id in seqid:\n                lineage = tax\n                break\n\n        # Preprocess data.\n        preproc_data = []\n        with self.preprocessing_context_manager(\n            config.seed + hash(filepath) + seq_idx if config.seed is not None else None\n        ):\n            # Randomly reverse complement the sequence.\n            seq = self._random_reverse_complement(seq, prob=config.random_reverse_complement)\n            seqs_to_parse = self._reverse_complement_expansion(seq) if config.embed_reverse_complement else [seq]\n            for seq in seqs_to_parse:\n                # Sequence Modifiers\n                if config.force_uppercase:\n                    seq = seq.upper()\n                if config.transcribe == \"transcribe\":\n                    seq = transcribe_sequence(seq)\n                elif config.transcribe == \"back_transcribe\":\n                    seq = back_transcribe_sequence(seq)\n                if config.drop_empty_sequences and len(seq) == 0:\n                    continue\n                if config.nnn_filter and \"NNN\" in seq.upper():\n                    continue\n\n                # Construct taxonomy token with random dropout on the lineage categories per sequence.\n                taxonomy_token = self._construct_taxonomy_token(lineage, dropout=config.random_lineage_dropout)\n\n                # Inject taxonomy lineage tokens every prompt_spacer_length tokens in the sequence.\n                # If the taxonomy lineage token is not provided, then just take the original sequence.\n                target_length = (\n                    config.prompt_spacer_length - len(taxonomy_token) if taxonomy_token is not None else None\n                )\n                taxonomy_injected_sequence = [\n                    taxonomy_token + str(subseq) if taxonomy_token is not None else str(subseq)\n                    for subseq in self._subsequence_generator(seq, target_length, target_length)\n                ]\n\n                # Wrap and tokenize.\n                preproc_data_record = {\n                    \"text\": \"\".join(taxonomy_injected_sequence),\n                }\n                preproc_data_record[\"tokens\"] = self.tokenizer.tokenize(\n                    preproc_data_record[\"text\"],\n                    use_ftfy=config.ftfy,\n                    enforce_sample_length=config.enforce_sample_length,\n                    append_eod=config.append_eod,\n                    drop_empty_sequences=config.drop_empty_sequences,\n                )\n                preproc_data.append(preproc_data_record)\n        end = time.time()\n        return preproc_data, end - start\n\n    def preprocess_data_task(self, file_sequence_config):\n        \"\"\"Wrapper function to unpack args for preprocess_data.\n\n        Args:\n            file_sequence_config (tuple): Tuple containing arguments for preprocess_data.\n\n        Returns:\n            tuple[list[dict], float]: Preprocessed data and the time taken for preprocessing.\n        \"\"\"\n        return self.preprocess_data(*file_sequence_config)\n\n    @staticmethod\n    def _yield_sequences_from_files(config: Evo2PreprocessingConfig, semaphore: Semaphore):\n        \"\"\"Iterator over sequences within multiple input documents. Arguments for multiprocessing tasks.\n\n        Utilized to limit the amount of sequences streamed into memory.\n\n        Args:\n            config (Evo2PreprocessingConfig): Configuration object containing preprocessing settings.\n            semaphore (Semaphore): Semaphore to limit the number of sequences in memory.\n\n        Yields:\n            tuple: Arguments for preprocess_data.\n        \"\"\"\n\n        def yielder(fname, semaphore):\n            # Read FASTA.\n            index = NvFaidx(fname)\n            for i, (seqid, sequence) in enumerate(index.items()):\n                semaphore.acquire()\n                # Yield filename and sequence within fasta.\n                yield str(fname), seqid, sequence, i, config\n\n        for fname in config.datapaths:\n            semaphore.acquire()\n            yield from yielder(fname, semaphore)\n\n    def preprocess_generator(self, preproc_config: Evo2PreprocessingConfig):\n        \"\"\"Main function to preprocess data for Evo2.\n\n        Args:\n            preproc_config (Evo2PreprocessingConfig): Configuration object containing preprocessing settings.\n\n        Yields:\n            tuple[dict, float]: Preprocessed sequence data and the time taken for preprocessing.\n        \"\"\"\n        # Track which splits have been assigned\n        split_assignments = {\n            \"train\": preproc_config.train_split &gt; 0,\n            \"val\": preproc_config.valid_split &gt; 0,\n            \"test\": preproc_config.test_split &gt; 0,\n        }\n        splits_needed = {k for k, v in split_assignments.items() if v}\n\n        # Instantiate multiprocessing pool. Use semaphore to limit the amount of sequences to read into memory.\n        semaphore = Semaphore(preproc_config.preproc_concurrency + preproc_config.workers)\n        if preproc_config.workers &gt; 1:\n            pool = mp.Pool(preproc_config.workers)\n            # Ordered imap for downstream seeded splitting.\n            preproc_tasks = pool.imap(\n                self.preprocess_data_task,\n                self._yield_sequences_from_files(preproc_config, semaphore),\n                chunksize=preproc_config.chunksize,\n            )\n        else:\n            preproc_tasks = (\n                self.preprocess_data_task(x) for x in self._yield_sequences_from_files(preproc_config, semaphore)\n            )\n\n        # Preprocess data and split results into train, test, and split.\n        with self.preprocessing_context_manager(preproc_config.seed if preproc_config.seed is not None else None):\n            for result, elapsed_time in preproc_tasks:\n                # Release semaphore for the task associated with the result.\n                semaphore.release()\n                # If we still need to ensure splits are assigned\n                if splits_needed:\n                    # Force assign to a needed split\n                    split = splits_needed.pop()\n                else:\n                    # Regular random assignment\n                    split = self._train_val_test_split(\n                        preproc_config.train_split, preproc_config.valid_split, preproc_config.test_split\n                    )\n                for sequence in result:\n                    sequence[\"split\"] = split\n                    yield sequence, elapsed_time\n\n    def preprocess_offline(self, preproc_config: Evo2PreprocessingConfig):\n        \"\"\"Offline data preprocessing script for Evo2.\n\n        Args:\n            preproc_config (Evo2PreprocessingConfig): Configuration object containing preprocessing settings.\n        \"\"\"\n        # Validate if binaries have already been produced for the given config and overwrite is set to False.\n        if any(\n            self._get_output_filename(preproc_config, ext, split).is_file()\n            for ext, split in zip([self.BIN, self.IDX], [self.TRAIN, self.VAL, self.TEST])\n        ):\n            if not preproc_config.overwrite:\n                # Skip this dataset!\n                logging.info(\n                    f\"Skipped overwriting (overwrite: False) existing preprocessed data: {preproc_config.output_prefix}\"\n                )\n                return\n            else:\n                logging.info(\n                    f\"Overwriting (overwrite: True) existing preprocessed data: {preproc_config.output_prefix}\"\n                )\n\n        # Instantiate indexed data builders.\n        dataset_dtype = getattr(np, preproc_config.indexed_dataset_dtype)\n        temp_train_bin = self._get_output_filename(preproc_config, self.BIN, self.TRAIN, temp=True)\n        temp_val_bin = self._get_output_filename(preproc_config, self.BIN, self.VAL, temp=True)\n        temp_test_bin = self._get_output_filename(preproc_config, self.BIN, self.TEST, temp=True)\n        train_builder: IndexedDatasetBuilder = IndexedDatasetBuilder(bin_path=str(temp_train_bin), dtype=dataset_dtype)\n        val_builder: IndexedDatasetBuilder = IndexedDatasetBuilder(bin_path=str(temp_val_bin), dtype=dataset_dtype)\n        test_builder: IndexedDatasetBuilder = IndexedDatasetBuilder(bin_path=str(temp_test_bin), dtype=dataset_dtype)\n        logging.info(f\"Created temporary binary datasets: {temp_train_bin} {temp_val_bin} {temp_test_bin}\")\n\n        # Preprocess data and split results into train, validation, or test.\n        avg_preproc_time = 0.0\n        avg_index_time = 0.0\n        count = 0\n        for sequence, elapsed_time in self.preprocess_generator(preproc_config):\n            index_start_time = time.time()\n            if sequence[\"split\"] == \"train\":\n                train_builder.add_item(torch.Tensor(sequence[\"tokens\"]))\n                train_builder.end_document()\n            elif sequence[\"split\"] == \"val\":\n                val_builder.add_item(torch.Tensor(sequence[\"tokens\"]))\n                val_builder.end_document()\n            elif sequence[\"split\"] == \"test\":\n                test_builder.add_item(torch.Tensor(sequence[\"tokens\"]))\n                test_builder.end_document()\n            index_end_time = time.time()\n            # Update average preprocessing and indexing time.\n            avg_preproc_time = (avg_preproc_time * count + elapsed_time) / (count + 1)\n            avg_index_time = (avg_index_time * count + index_end_time - index_start_time) / (count + 1)\n            count += 1\n\n        # Report timing.\n        logging.info(f\"Average preprocessing time per sequence: {avg_preproc_time}\")\n        logging.info(f\"Average indexing time per sequence: {avg_index_time}\")\n        logging.info(f\"Number of sequences processed: {count}\")\n\n        # Write preprocessed index data to disk. Rename temporary binaries to denote preprocessing completion.\n        train_builder.finalize(idx_path=str(self._get_output_filename(preproc_config, self.IDX, self.TRAIN)))\n        val_builder.finalize(idx_path=str(self._get_output_filename(preproc_config, self.IDX, self.VAL)))\n        test_builder.finalize(idx_path=str(self._get_output_filename(preproc_config, self.IDX, self.TEST)))\n        os.rename(temp_train_bin, self._get_output_filename(preproc_config, self.BIN, self.TRAIN))\n        os.rename(temp_val_bin, self._get_output_filename(preproc_config, self.BIN, self.VAL))\n        os.rename(temp_test_bin, self._get_output_filename(preproc_config, self.BIN, self.TEST))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/preprocess/#bionemo.evo2.data.preprocess.Evo2Preprocessor.__init__","title":"<code>__init__(params=None)</code>","text":"<p>Initialize Evo2Preprocessor.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Evo2PreprocessingConfig | None</code> <p>Configuration parameters for preprocessing.</p> <code>None</code> Source code in <code>bionemo/evo2/data/preprocess.py</code> <pre><code>def __init__(self, params: Evo2PreprocessingConfig | None = None):\n    \"\"\"Initialize Evo2Preprocessor.\n\n    Args:\n        params (Evo2PreprocessingConfig | None): Configuration parameters for preprocessing.\n    \"\"\"\n    self.tokenizer: Evo2Tokenizer = Evo2Tokenizer(params)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/preprocess/#bionemo.evo2.data.preprocess.Evo2Preprocessor.preprocess_data","title":"<code>preprocess_data(filepath, seqid, seq, seq_idx, config)</code>","text":"<p>Preprocess fasta datapaths.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the .fasta file.</p> required <code>seqid</code> <code>str</code> <p>Sequence ID.</p> required <code>seq</code> <code>str</code> <p>DNA sequence.</p> required <code>seq_idx</code> <code>int</code> <p>Sequence index.</p> required <code>config</code> <code>Evo2PreprocessingConfig</code> <p>Configuration object containing preprocessing settings.</p> required <p>Returns:</p> Type Description <p>tuple[list[dict], float]: Preprocessed data and the time taken for preprocessing.</p> Source code in <code>bionemo/evo2/data/preprocess.py</code> <pre><code>def preprocess_data(self, filepath: str, seqid: str, seq: str, seq_idx: int, config: Evo2PreprocessingConfig):\n    \"\"\"Preprocess fasta datapaths.\n\n    Args:\n        filepath (str): Path to the .fasta file.\n        seqid (str): Sequence ID.\n        seq (str): DNA sequence.\n        seq_idx (int): Sequence index.\n        config (Evo2PreprocessingConfig): Configuration object containing preprocessing settings.\n\n    Returns:\n        tuple[list[dict], float]: Preprocessed data and the time taken for preprocessing.\n    \"\"\"\n    # Timing.\n    start = time.time()\n    # Retrieve taxonomy lineage string if SeqID has associated taxonomy data.\n    # Note: Better implemented as a suffix tree substring dictionary, but convenient\n    # for identifying a large amount of sequences with identical lineages.\n    # Slow for extremely large dictionaries of (SeqID Substr, Taxonomy) pairs.\n    lineage = None\n    for id, tax in config.taxonomy_data.items():\n        # Taxonomy ID is a substring of Seq ID.\n        if id in seqid:\n            lineage = tax\n            break\n\n    # Preprocess data.\n    preproc_data = []\n    with self.preprocessing_context_manager(\n        config.seed + hash(filepath) + seq_idx if config.seed is not None else None\n    ):\n        # Randomly reverse complement the sequence.\n        seq = self._random_reverse_complement(seq, prob=config.random_reverse_complement)\n        seqs_to_parse = self._reverse_complement_expansion(seq) if config.embed_reverse_complement else [seq]\n        for seq in seqs_to_parse:\n            # Sequence Modifiers\n            if config.force_uppercase:\n                seq = seq.upper()\n            if config.transcribe == \"transcribe\":\n                seq = transcribe_sequence(seq)\n            elif config.transcribe == \"back_transcribe\":\n                seq = back_transcribe_sequence(seq)\n            if config.drop_empty_sequences and len(seq) == 0:\n                continue\n            if config.nnn_filter and \"NNN\" in seq.upper():\n                continue\n\n            # Construct taxonomy token with random dropout on the lineage categories per sequence.\n            taxonomy_token = self._construct_taxonomy_token(lineage, dropout=config.random_lineage_dropout)\n\n            # Inject taxonomy lineage tokens every prompt_spacer_length tokens in the sequence.\n            # If the taxonomy lineage token is not provided, then just take the original sequence.\n            target_length = (\n                config.prompt_spacer_length - len(taxonomy_token) if taxonomy_token is not None else None\n            )\n            taxonomy_injected_sequence = [\n                taxonomy_token + str(subseq) if taxonomy_token is not None else str(subseq)\n                for subseq in self._subsequence_generator(seq, target_length, target_length)\n            ]\n\n            # Wrap and tokenize.\n            preproc_data_record = {\n                \"text\": \"\".join(taxonomy_injected_sequence),\n            }\n            preproc_data_record[\"tokens\"] = self.tokenizer.tokenize(\n                preproc_data_record[\"text\"],\n                use_ftfy=config.ftfy,\n                enforce_sample_length=config.enforce_sample_length,\n                append_eod=config.append_eod,\n                drop_empty_sequences=config.drop_empty_sequences,\n            )\n            preproc_data.append(preproc_data_record)\n    end = time.time()\n    return preproc_data, end - start\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/preprocess/#bionemo.evo2.data.preprocess.Evo2Preprocessor.preprocess_data_task","title":"<code>preprocess_data_task(file_sequence_config)</code>","text":"<p>Wrapper function to unpack args for preprocess_data.</p> <p>Parameters:</p> Name Type Description Default <code>file_sequence_config</code> <code>tuple</code> <p>Tuple containing arguments for preprocess_data.</p> required <p>Returns:</p> Type Description <p>tuple[list[dict], float]: Preprocessed data and the time taken for preprocessing.</p> Source code in <code>bionemo/evo2/data/preprocess.py</code> <pre><code>def preprocess_data_task(self, file_sequence_config):\n    \"\"\"Wrapper function to unpack args for preprocess_data.\n\n    Args:\n        file_sequence_config (tuple): Tuple containing arguments for preprocess_data.\n\n    Returns:\n        tuple[list[dict], float]: Preprocessed data and the time taken for preprocessing.\n    \"\"\"\n    return self.preprocess_data(*file_sequence_config)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/preprocess/#bionemo.evo2.data.preprocess.Evo2Preprocessor.preprocess_generator","title":"<code>preprocess_generator(preproc_config)</code>","text":"<p>Main function to preprocess data for Evo2.</p> <p>Parameters:</p> Name Type Description Default <code>preproc_config</code> <code>Evo2PreprocessingConfig</code> <p>Configuration object containing preprocessing settings.</p> required <p>Yields:</p> Type Description <p>tuple[dict, float]: Preprocessed sequence data and the time taken for preprocessing.</p> Source code in <code>bionemo/evo2/data/preprocess.py</code> <pre><code>def preprocess_generator(self, preproc_config: Evo2PreprocessingConfig):\n    \"\"\"Main function to preprocess data for Evo2.\n\n    Args:\n        preproc_config (Evo2PreprocessingConfig): Configuration object containing preprocessing settings.\n\n    Yields:\n        tuple[dict, float]: Preprocessed sequence data and the time taken for preprocessing.\n    \"\"\"\n    # Track which splits have been assigned\n    split_assignments = {\n        \"train\": preproc_config.train_split &gt; 0,\n        \"val\": preproc_config.valid_split &gt; 0,\n        \"test\": preproc_config.test_split &gt; 0,\n    }\n    splits_needed = {k for k, v in split_assignments.items() if v}\n\n    # Instantiate multiprocessing pool. Use semaphore to limit the amount of sequences to read into memory.\n    semaphore = Semaphore(preproc_config.preproc_concurrency + preproc_config.workers)\n    if preproc_config.workers &gt; 1:\n        pool = mp.Pool(preproc_config.workers)\n        # Ordered imap for downstream seeded splitting.\n        preproc_tasks = pool.imap(\n            self.preprocess_data_task,\n            self._yield_sequences_from_files(preproc_config, semaphore),\n            chunksize=preproc_config.chunksize,\n        )\n    else:\n        preproc_tasks = (\n            self.preprocess_data_task(x) for x in self._yield_sequences_from_files(preproc_config, semaphore)\n        )\n\n    # Preprocess data and split results into train, test, and split.\n    with self.preprocessing_context_manager(preproc_config.seed if preproc_config.seed is not None else None):\n        for result, elapsed_time in preproc_tasks:\n            # Release semaphore for the task associated with the result.\n            semaphore.release()\n            # If we still need to ensure splits are assigned\n            if splits_needed:\n                # Force assign to a needed split\n                split = splits_needed.pop()\n            else:\n                # Regular random assignment\n                split = self._train_val_test_split(\n                    preproc_config.train_split, preproc_config.valid_split, preproc_config.test_split\n                )\n            for sequence in result:\n                sequence[\"split\"] = split\n                yield sequence, elapsed_time\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/preprocess/#bionemo.evo2.data.preprocess.Evo2Preprocessor.preprocess_offline","title":"<code>preprocess_offline(preproc_config)</code>","text":"<p>Offline data preprocessing script for Evo2.</p> <p>Parameters:</p> Name Type Description Default <code>preproc_config</code> <code>Evo2PreprocessingConfig</code> <p>Configuration object containing preprocessing settings.</p> required Source code in <code>bionemo/evo2/data/preprocess.py</code> <pre><code>def preprocess_offline(self, preproc_config: Evo2PreprocessingConfig):\n    \"\"\"Offline data preprocessing script for Evo2.\n\n    Args:\n        preproc_config (Evo2PreprocessingConfig): Configuration object containing preprocessing settings.\n    \"\"\"\n    # Validate if binaries have already been produced for the given config and overwrite is set to False.\n    if any(\n        self._get_output_filename(preproc_config, ext, split).is_file()\n        for ext, split in zip([self.BIN, self.IDX], [self.TRAIN, self.VAL, self.TEST])\n    ):\n        if not preproc_config.overwrite:\n            # Skip this dataset!\n            logging.info(\n                f\"Skipped overwriting (overwrite: False) existing preprocessed data: {preproc_config.output_prefix}\"\n            )\n            return\n        else:\n            logging.info(\n                f\"Overwriting (overwrite: True) existing preprocessed data: {preproc_config.output_prefix}\"\n            )\n\n    # Instantiate indexed data builders.\n    dataset_dtype = getattr(np, preproc_config.indexed_dataset_dtype)\n    temp_train_bin = self._get_output_filename(preproc_config, self.BIN, self.TRAIN, temp=True)\n    temp_val_bin = self._get_output_filename(preproc_config, self.BIN, self.VAL, temp=True)\n    temp_test_bin = self._get_output_filename(preproc_config, self.BIN, self.TEST, temp=True)\n    train_builder: IndexedDatasetBuilder = IndexedDatasetBuilder(bin_path=str(temp_train_bin), dtype=dataset_dtype)\n    val_builder: IndexedDatasetBuilder = IndexedDatasetBuilder(bin_path=str(temp_val_bin), dtype=dataset_dtype)\n    test_builder: IndexedDatasetBuilder = IndexedDatasetBuilder(bin_path=str(temp_test_bin), dtype=dataset_dtype)\n    logging.info(f\"Created temporary binary datasets: {temp_train_bin} {temp_val_bin} {temp_test_bin}\")\n\n    # Preprocess data and split results into train, validation, or test.\n    avg_preproc_time = 0.0\n    avg_index_time = 0.0\n    count = 0\n    for sequence, elapsed_time in self.preprocess_generator(preproc_config):\n        index_start_time = time.time()\n        if sequence[\"split\"] == \"train\":\n            train_builder.add_item(torch.Tensor(sequence[\"tokens\"]))\n            train_builder.end_document()\n        elif sequence[\"split\"] == \"val\":\n            val_builder.add_item(torch.Tensor(sequence[\"tokens\"]))\n            val_builder.end_document()\n        elif sequence[\"split\"] == \"test\":\n            test_builder.add_item(torch.Tensor(sequence[\"tokens\"]))\n            test_builder.end_document()\n        index_end_time = time.time()\n        # Update average preprocessing and indexing time.\n        avg_preproc_time = (avg_preproc_time * count + elapsed_time) / (count + 1)\n        avg_index_time = (avg_index_time * count + index_end_time - index_start_time) / (count + 1)\n        count += 1\n\n    # Report timing.\n    logging.info(f\"Average preprocessing time per sequence: {avg_preproc_time}\")\n    logging.info(f\"Average indexing time per sequence: {avg_index_time}\")\n    logging.info(f\"Number of sequences processed: {count}\")\n\n    # Write preprocessed index data to disk. Rename temporary binaries to denote preprocessing completion.\n    train_builder.finalize(idx_path=str(self._get_output_filename(preproc_config, self.IDX, self.TRAIN)))\n    val_builder.finalize(idx_path=str(self._get_output_filename(preproc_config, self.IDX, self.VAL)))\n    test_builder.finalize(idx_path=str(self._get_output_filename(preproc_config, self.IDX, self.TEST)))\n    os.rename(temp_train_bin, self._get_output_filename(preproc_config, self.BIN, self.TRAIN))\n    os.rename(temp_val_bin, self._get_output_filename(preproc_config, self.BIN, self.VAL))\n    os.rename(temp_test_bin, self._get_output_filename(preproc_config, self.BIN, self.TEST))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/preprocess/#bionemo.evo2.data.preprocess.Evo2Preprocessor.preprocessing_context_manager","title":"<code>preprocessing_context_manager(seed=None)</code>  <code>staticmethod</code>","text":"<p>Context manager for setting and restoring the random number generator state.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int | None</code> <p>Seed for the random number generator. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/evo2/data/preprocess.py</code> <pre><code>@staticmethod\n@contextmanager\ndef preprocessing_context_manager(seed: Optional[int] = None):\n    \"\"\"Context manager for setting and restoring the random number generator state.\n\n    Args:\n        seed (int | None): Seed for the random number generator. Defaults to None.\n    \"\"\"\n    # Track current state.\n    current_state = random.getstate()\n    try:\n        # Set random seed.\n        random.seed(seed)\n        yield seed\n    finally:\n        # Restore random state.\n        random.setstate(current_state)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/preprocess/#bionemo.evo2.data.preprocess.main","title":"<code>main()</code>","text":"<p>Main function to execute the preprocessing script.</p> <p>This function parses command-line arguments, reads the configuration file, and initiates the preprocessing of data as specified in the configuration.</p> Source code in <code>bionemo/evo2/data/preprocess.py</code> <pre><code>def main():\n    \"\"\"Main function to execute the preprocessing script.\n\n    This function parses command-line arguments, reads the configuration file,\n    and initiates the preprocessing of data as specified in the configuration.\n    \"\"\"\n    # Parse arguments.\n    args = parse_args()\n    # Read config YAML.\n    with open(args.config, \"r\") as yaml_fs:\n        evo2_preproc_config_batch = yaml.safe_load(yaml_fs)\n    for config in evo2_preproc_config_batch:\n        start = time.time()\n        # Convert into Evo2PreprocessingConfig.\n        evo2_preproc_config = Evo2PreprocessingConfig(**config)\n        if evo2_preproc_config.output_dir is not None:\n            evo2_preproc_config.output_dir.mkdir(parents=True, exist_ok=True)\n        # Instantiate Evo2Preprocessor.\n        evo2_preprocessor = Evo2Preprocessor(evo2_preproc_config)\n        # Preprocess data specified in config.\n        evo2_preprocessor.preprocess_offline(evo2_preproc_config)\n        end = time.time()\n        logging.info(\n            f\"Finished preprocessing {evo2_preproc_config.output_prefix} ({evo2_preproc_config.datapaths}) in {end - start:.3f} seconds with {evo2_preproc_config.workers} workers.\"\n        )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/preprocess/#bionemo.evo2.data.preprocess.parse_args","title":"<code>parse_args()</code>","text":"<p>Parse arguments for preprocessing.</p> Source code in <code>bionemo/evo2/data/preprocess.py</code> <pre><code>def parse_args():\n    \"\"\"Parse arguments for preprocessing.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Preprocess FASTA files for training Evo2.\")\n    parser.add_argument(\"-c\", \"--config\", type=str, required=True, help=\"Path to data preprocessing config JSON.\")\n    return parser.parse_args()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/sharded_eden_dataloader/","title":"Sharded Eden DataLoader Implementation","text":""},{"location":"main/references/API_reference/bionemo/evo2/data/sharded_eden_dataloader/#overview","title":"Overview","text":"<p>The <code>sharded_eden_dataloader.py</code> implements a dataloader for genomic sequences that uses pre-computed data structures and SQLite databases for efficient data access. This implementation is designed to significantly reduce the computational overhead during training by moving expensive operations to a pre-processing phase.</p>"},{"location":"main/references/API_reference/bionemo/evo2/data/sharded_eden_dataloader/#key-features","title":"Key Features","text":""},{"location":"main/references/API_reference/bionemo/evo2/data/sharded_eden_dataloader/#1-split-specific-window-databases","title":"1. Split-Specific Window Databases","text":"<ul> <li>Sharded: Uses separate pre-computed window databases for each split:</li> <li><code>train_window_db_path</code>: SQLite database containing window mappings for training data</li> <li><code>val_window_db_path</code>: SQLite database containing window mappings for validation data</li> <li><code>test_window_db_path</code>: SQLite database containing window mappings for test data</li> </ul>"},{"location":"main/references/API_reference/bionemo/evo2/data/sharded_eden_dataloader/#2-sqlite-database-storage","title":"2. SQLite Database Storage","text":"<ul> <li>Sharded: Uses SQLite databases organized by sample:</li> <li>Per-Sample Sequence Databases: Each sample has its own SQLite file at <code>sequence_db_dir/&lt;sample_id&gt;/glm_dataset_&lt;sample_id&gt;.sqlite</code></li> <li>Split-Specific Window Databases: Pre-computed window mappings stored in separate databases for each data split</li> </ul>"},{"location":"main/references/API_reference/bionemo/evo2/data/sharded_eden_dataloader/#3-virtual-window-pre-computation","title":"3. Virtual Window Pre-computation","text":"<ul> <li>Sharded: Window mappings are pre-computed from Parquet files and stored in split-specific databases</li> </ul>"},{"location":"main/references/API_reference/bionemo/evo2/data/sharded_eden_dataloader/#sequence-id-format","title":"Sequence ID Format","text":"<p>Sequence IDs follow a specific format: <code>BCR__ECT-SAMPLE1__CT1-1</code></p> <p>The sample ID can be extracted using: <code>extract_sample_id(sequence_id)</code> which implements <code>\".\".join(sequence_id.split(\"__\")[1].split(\"-\")[1:])</code> (returns <code>SAMPLE1</code>)</p>"},{"location":"main/references/API_reference/bionemo/evo2/data/sharded_eden_dataloader/#database-schema","title":"Database Schema","text":""},{"location":"main/references/API_reference/bionemo/evo2/data/sharded_eden_dataloader/#per-sample-sequence-database","title":"Per-Sample Sequence Database","text":"<p>Each sample has its own SQLite file with the following schema:</p> <pre><code>CREATE TABLE sequences (\n    contig_id TEXT PRIMARY KEY,\n    nt_sequence TEXT NOT NULL\n);\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/sharded_eden_dataloader/#split-specific-window-database","title":"Split-Specific Window Database","text":"<p>Each split (train/validation/test) has its own window database:</p> <pre><code>CREATE TABLE metadata (\n    key TEXT PRIMARY KEY,\n    value INTEGER NOT NULL\n);\n\nCREATE TABLE window_mappings (\n    window_idx INTEGER PRIMARY KEY,\n    sequence_id TEXT NOT NULL,\n    window_in_seq_idx INTEGER NOT NULL\n);\nCREATE INDEX idx_sequence_id ON window_mappings(sequence_id);\n</code></pre> <p>The metadata table stores the <code>window_size</code> and <code>stride</code> parameters used during pre-computation.</p>"},{"location":"main/references/API_reference/bionemo/evo2/data/sharded_eden_dataloader/#directory-structure","title":"Directory Structure","text":"<pre><code>sequence_db_dir/\n\u251c\u2500\u2500 SAMPLE1/\n\u2502   \u2514\u2500\u2500 glm_dataset_SAMPLE1.sqlite\n\u251c\u2500\u2500 SAMPLE2/\n\u2502   \u2514\u2500\u2500 glm_dataset_SAMPLE2.sqlite\n\u251c\u2500\u2500 SAMPLE3/\n\u2502   \u2514\u2500\u2500 glm_dataset_SAMPLE3.sqlite\n\u2514\u2500\u2500 ...\n\nWindow databases (separate files):\n\u251c\u2500\u2500 train_windows.db\n\u251c\u2500\u2500 val_windows.db\n\u2514\u2500\u2500 test_windows.db\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/sharded_eden_dataloader/#usage-example","title":"Usage Example","text":"<pre><code>from bionemo.evo2.run.sharded_eden_dataloader import ShardedEdenDataModule\n\n# Create the data module\ndata_module = ShardedEdenDataModule(\n    sequence_db_dir=\"path/to/sequence_db_dir\",  # Directory containing sample folders\n    train_window_db_path=\"path/to/train_windows.db\",\n    val_window_db_path=\"path/to/val_windows.db\",\n    test_window_db_path=\"path/to/test_windows.db\",\n    seq_length=8192,\n    micro_batch_size=1,\n    global_batch_size=4,\n    num_workers=8,\n    rc_aug=True,\n    use_control_tags=True,\n)\n\n# Use with PyTorch Lightning trainer\ntrainer = pl.Trainer(...)\ntrainer.fit(model, data_module)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/sharded_eden_dataloader/#pre-processing-workflow","title":"Pre-processing Workflow","text":""},{"location":"main/references/API_reference/bionemo/evo2/data/sharded_eden_dataloader/#1-create-sample-sequence-databases","title":"1. Create Sample Sequence Databases","text":"<p>For each sample, create its SQLite database:</p> <pre><code>import sqlite3\nimport os\n\n\ndef create_sample_database(sample_id, sequences, output_dir):\n    \"\"\"Create SQLite database for a single sample.\"\"\"\n    # Create sample directory\n    sample_dir = os.path.join(output_dir, sample_id)\n    os.makedirs(sample_dir, exist_ok=True)\n\n    # Create database\n    db_path = os.path.join(sample_dir, f\"glm_dataset_{sample_id}.sqlite\")\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n\n    # Create table\n    cursor.execute(\n        \"\"\"\n        CREATE TABLE sequences (\n            contig_id TEXT PRIMARY KEY,\n            nt_sequence TEXT NOT NULL\n        )\n    \"\"\"\n    )\n\n    # Insert sequences for this sample\n    for seq_id, sequence in sequences:\n        cursor.execute(\n            \"INSERT INTO sequences (contig_id, nt_sequence) VALUES (?, ?)\",\n            (seq_id, sequence),\n        )\n\n    conn.commit()\n    conn.close()\n\n\n# Example usage\n# Group sequences by sample_id\nfrom collections import defaultdict\n\nsequences_by_sample = defaultdict(list)\nfor seq_id, sequence in all_sequences:  # all_sequences is your data\n    sample_id = extract_sample_id(seq_id)\n    sequences_by_sample[sample_id].append((seq_id, sequence))\n\n# Create database for each sample\nfor sample_id, sequences in sequences_by_sample.items():\n    create_sample_database(sample_id, sequences, \"path/to/sequence_db_dir\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/sharded_eden_dataloader/#2-create-split-data-files","title":"2. Create Split Data Files","text":"<p>Create Parquet files for each split containing sequence metadata:</p> <pre><code>import polars as pl\n\n# Create train split Parquet file\ntrain_data = pl.DataFrame(\n    {\n        \"contig_id\": [\"BCR__ECT-SAMPLE1__CT1-1\", \"BCR__ECT-SAMPLE1__CT1-2\", ...],\n        \"length\": [1500, 2000, ...],  # sequence lengths\n    }\n)\ntrain_data.write_parquet(\"train_split.parquet\")\n\n# Similarly for validation and test splits\nval_data = pl.DataFrame(\n    {\"contig_id\": [\"BCR__ECT-SAMPLE2__CT1-1\", ...], \"length\": [1800, ...]}\n)\nval_data.write_parquet(\"val_split.parquet\")\n\ntest_data = pl.DataFrame(\n    {\"contig_id\": [\"BCR__ECT-SAMPLE3__CT1-1\", ...], \"length\": [1600, ...]}\n)\ntest_data.write_parquet(\"test_split.parquet\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/sharded_eden_dataloader/#3-create-window-mappings-databases-using-cli","title":"3. Create Window Mappings Databases using CLI","text":"<p>The package includes a CLI tool for pre-computing the window databases:</p> <pre><code># Pre-compute window mappings for training split\npython -m bionemo.evo2.run.sharded_eden_dataloader precompute \\\n    train_split.parquet \\\n    train_windows.db \\\n    --window-size 8192 \\\n    --stride 7992\n\n# Pre-compute window mappings for validation split\npython -m bionemo.evo2.run.sharded_eden_dataloader precompute \\\n    val_split.parquet \\\n    val_windows.db \\\n    --window-size 8192 \\\n    --stride 7992\n\n# Pre-compute window mappings for test split\npython -m bionemo.evo2.run.sharded_eden_dataloader precompute \\\n    test_split.parquet \\\n    test_windows.db \\\n    --window-size 8192 \\\n    --stride 7992\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/sharded_eden_dataloader/#implementation-details","title":"Implementation Details","text":""},{"location":"main/references/API_reference/bionemo/evo2/data/sharded_eden_dataloader/#key-components","title":"Key Components","text":"<ol> <li> <p>ShardedEdenDataModule:</p> </li> <li> <p>Uses separate window databases for each split (train/val/test)</p> </li> <li>Manages per-sample SQLite file paths</li> <li>Creates datasets with directory and database paths</li> <li> <p>Handles distributed training setup with Megatron integration</p> </li> <li> <p>ShardedEdenDataset:</p> </li> <li> <p>Automatically discovers sample SQLite files from directory structure</p> </li> <li>Maps sequence IDs to appropriate sample databases using <code>extract_sample_id()</code></li> <li>Pre-opens all database connections for performance</li> <li>Attaches window database to each sequence connection for efficient JOINs</li> <li>Implements sequence caching with connection pooling</li> <li>Maintains compatibility with original tokenization and formatting logic</li> <li> <p>Optional window access logging for performance analysis</p> </li> <li> <p>CLI Tool:</p> </li> <li> <p><code>precompute</code>: Creates window databases from Parquet files</p> </li> </ol>"},{"location":"main/references/API_reference/bionemo/evo2/data/sharded_eden_dataloader/#advanced-features","title":"Advanced Features","text":""},{"location":"main/references/API_reference/bionemo/evo2/data/sharded_eden_dataloader/#window-access-logging","title":"Window Access Logging","text":"<p>Enable detailed logging of window access patterns:</p> <pre><code>dataset = ShardedEdenDataset(\n    # ... other parameters ...\n    log_windows=True,\n    log_dir=\"sequence_logs\",\n)\n</code></pre> <p>This creates CSV logs tracking which windows are accessed, useful for analyzing data loading patterns.</p>"},{"location":"main/references/API_reference/bionemo/evo2/data/sharded_eden_dataloader/#connection-management","title":"Connection Management","text":"<ul> <li>All database connections are pre-opened during initialization for performance</li> <li>Database connections are pooled and reused per sample</li> <li>Sequence data is fetched on-demand using SQL SUBSTR for memory efficiency</li> <li>Position IDs are shared across instances to reduce memory usage</li> <li>Connections are properly closed when dataset is destroyed</li> </ul>"},{"location":"main/references/API_reference/bionemo/evo2/data/sharded_eden_dataloader/#metadata-validation","title":"Metadata Validation","text":"<p>The implementation validates that window databases were created with compatible parameters:</p> <ul> <li>Checks stored <code>window_size</code> matches dataset <code>seq_length</code></li> <li>Checks stored <code>stride</code> matches dataset <code>stride</code></li> <li>Provides clear error messages for mismatches</li> </ul>"},{"location":"main/references/API_reference/bionemo/evo2/data/sharded_eden_dataloader/#error-handling","title":"Error Handling","text":"<ul> <li>Validates sample SQLite files exist during initialization</li> <li>Handles missing sequences gracefully with informative error messages</li> <li>Ensures proper cleanup of database connections</li> <li>Provides detailed debugging information for database issues</li> <li>Validates Parquet file schema during pre-computation</li> </ul>"},{"location":"main/references/API_reference/bionemo/evo2/data/tokenizer/","title":"Tokenizer","text":""},{"location":"main/references/API_reference/bionemo/evo2/data/tokenizer/#bionemo.evo2.data.tokenizer.Evo2Tokenizer","title":"<code>Evo2Tokenizer</code>","text":"<p>Tokenizer for Evo2.</p> Source code in <code>bionemo/evo2/data/tokenizer.py</code> <pre><code>class Evo2Tokenizer:\n    \"\"\"Tokenizer for Evo2.\"\"\"\n\n    def __init__(self, params: Evo2PreprocessingConfig | None = None):\n        \"\"\"Initialize the Evo2Tokenizer.\"\"\"\n        # Pass all NeMo2/Megatron-compliant parameters associated with config.Evo2PreprocessingConfig.\n        self.params: Evo2PreprocessingConfig = params if params is not None else Evo2PreprocessingConfig()\n        self.tokenizer: TokenizerSpec = get_nmt_tokenizer(\n            library=self.params.tokenizer_type.lower(),\n            vocab_file=str(self.params.vocab_file) if self.params.vocab_file is not None else None,\n            merges_file=str(self.params.merges_file) if self.params.merges_file is not None else None,\n            model_name=self.params.tokenizer_model_name,\n            tokenizer_model=self.params.pretrained_tokenizer_model,\n            special_tokens=self.params.special_tokens,\n            use_fast=self.params.fast_hf_tokenizer,\n        )\n\n    def tokenize(\n        self,\n        text: str | list[str],\n        use_ftfy: bool = False,\n        enforce_sample_length: None | int = None,\n        append_eod: bool = False,\n        drop_empty_sequences: bool = False,\n    ):\n        \"\"\"Tokenize the input text data for Evo2.\"\"\"\n        if isinstance(text, str):\n            text = [text]\n        # Tokenize a document or batch of strings.\n        doc_ids = []\n        for l, t in enumerate(text):\n            if use_ftfy:\n                t = ftfy.fix_text(t)\n            # Tokenize the string.\n            text_ids: list = self.tokenizer.text_to_ids(t)\n            if drop_empty_sequences and len(text_ids) == 0:\n                continue\n            # Append EOD token (EOD ID: 0) if appropriate.\n            eod_length = int(append_eod and l == len(text) - 1)\n            token_length = len(text_ids) + eod_length\n            text_ids += [0] * eod_length\n            if enforce_sample_length is not None:\n                # Pad shorter sequences (Pad ID: 1) and except excessive sequences.\n                if token_length &gt; enforce_sample_length:\n                    raise ValueError(\n                        \"Detected input text with a length greater than the maximum \"\n                        f\"possible sample length of {enforce_sample_length}.)\"\n                    )\n                else:\n                    text_ids += [1] * (enforce_sample_length - token_length)\n            # Append to document.\n            doc_ids.append(text_ids)\n        return doc_ids\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/tokenizer/#bionemo.evo2.data.tokenizer.Evo2Tokenizer.__init__","title":"<code>__init__(params=None)</code>","text":"<p>Initialize the Evo2Tokenizer.</p> Source code in <code>bionemo/evo2/data/tokenizer.py</code> <pre><code>def __init__(self, params: Evo2PreprocessingConfig | None = None):\n    \"\"\"Initialize the Evo2Tokenizer.\"\"\"\n    # Pass all NeMo2/Megatron-compliant parameters associated with config.Evo2PreprocessingConfig.\n    self.params: Evo2PreprocessingConfig = params if params is not None else Evo2PreprocessingConfig()\n    self.tokenizer: TokenizerSpec = get_nmt_tokenizer(\n        library=self.params.tokenizer_type.lower(),\n        vocab_file=str(self.params.vocab_file) if self.params.vocab_file is not None else None,\n        merges_file=str(self.params.merges_file) if self.params.merges_file is not None else None,\n        model_name=self.params.tokenizer_model_name,\n        tokenizer_model=self.params.pretrained_tokenizer_model,\n        special_tokens=self.params.special_tokens,\n        use_fast=self.params.fast_hf_tokenizer,\n    )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/tokenizer/#bionemo.evo2.data.tokenizer.Evo2Tokenizer.tokenize","title":"<code>tokenize(text, use_ftfy=False, enforce_sample_length=None, append_eod=False, drop_empty_sequences=False)</code>","text":"<p>Tokenize the input text data for Evo2.</p> Source code in <code>bionemo/evo2/data/tokenizer.py</code> <pre><code>def tokenize(\n    self,\n    text: str | list[str],\n    use_ftfy: bool = False,\n    enforce_sample_length: None | int = None,\n    append_eod: bool = False,\n    drop_empty_sequences: bool = False,\n):\n    \"\"\"Tokenize the input text data for Evo2.\"\"\"\n    if isinstance(text, str):\n        text = [text]\n    # Tokenize a document or batch of strings.\n    doc_ids = []\n    for l, t in enumerate(text):\n        if use_ftfy:\n            t = ftfy.fix_text(t)\n        # Tokenize the string.\n        text_ids: list = self.tokenizer.text_to_ids(t)\n        if drop_empty_sequences and len(text_ids) == 0:\n            continue\n        # Append EOD token (EOD ID: 0) if appropriate.\n        eod_length = int(append_eod and l == len(text) - 1)\n        token_length = len(text_ids) + eod_length\n        text_ids += [0] * eod_length\n        if enforce_sample_length is not None:\n            # Pad shorter sequences (Pad ID: 1) and except excessive sequences.\n            if token_length &gt; enforce_sample_length:\n                raise ValueError(\n                    \"Detected input text with a length greater than the maximum \"\n                    f\"possible sample length of {enforce_sample_length}.)\"\n                )\n            else:\n                text_ids += [1] * (enforce_sample_length - token_length)\n        # Append to document.\n        doc_ids.append(text_ids)\n    return doc_ids\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/transcript_extraction/","title":"Transcript extraction","text":""},{"location":"main/references/API_reference/bionemo/evo2/data/transcript_extraction/#bionemo.evo2.data.transcript_extraction.extract_default_transcript_sequences","title":"<code>extract_default_transcript_sequences(transcript_info, fasta_records, output_file)</code>","text":"<p>Extracts default transcript sequences from the provided transcript information and writes them to an output file.</p> <p>Parameters:</p> Name Type Description Default <code>transcript_info</code> <code>dict</code> <p>Dictionary containing transcript and exon information.</p> required <code>fasta_records</code> <code>NvFaidx</code> <p>Indexed FASTA records.</p> required <code>output_file</code> <code>TextIO</code> <p>File object to write the output sequences.</p> required Source code in <code>bionemo/evo2/data/transcript_extraction.py</code> <pre><code>def extract_default_transcript_sequences(transcript_info, fasta_records, output_file):\n    \"\"\"Extracts default transcript sequences from the provided transcript information and writes them to an output file.\n\n    Args:\n        transcript_info (dict): Dictionary containing transcript and exon information.\n        fasta_records (NvFaidx): Indexed FASTA records.\n        output_file (TextIO): File object to write the output sequences.\n    \"\"\"\n    for transcript_id in transcript_info[\"transcripts\"]:\n        gene_id = transcript_info[\"transcript2gene\"][transcript_id]\n        this_exons = sorted(transcript_info[\"transcript2exon\"][transcript_id], key=lambda x: x[-1])\n\n        seqname = None\n        exon_qc_failed = False\n        if len(this_exons) &gt; 1:\n            for i in range(1, len(this_exons)):\n                this_exon = this_exons[i]\n                prev_exon = this_exons[i - 1]\n                this_coords = transcript_info[\"exons\"][this_exon]\n                prev_coords = transcript_info[\"exons\"][prev_exon]\n                if this_coords[\"strand\"] != prev_coords[\"strand\"]:\n                    exon_qc_failed = True\n                if this_coords[\"strand\"] == \"+\" and this_coords[\"start\"] &lt; prev_coords[\"start\"]:\n                    exon_qc_failed = True\n                if this_coords[\"strand\"] == \"-\" and this_coords[\"start\"] &gt; prev_coords[\"start\"]:\n                    exon_qc_failed = True\n                if this_coords[\"seqname\"] != prev_coords[\"seqname\"]:\n                    exon_qc_failed = True\n\n        if exon_qc_failed:\n            continue\n\n        transcript_seq = \"\"\n        for exon in this_exons:\n            coords = transcript_info[\"exons\"][exon]\n            if seqname is None:\n                seqname = coords[\"seqname\"]\n            exon_seq = str(fasta_records[coords[\"seqname\"]][coords[\"start\"] : coords[\"end\"]])\n            if coords[\"strand\"] == \"-\":\n                exon_seq = reverse_sequence(complement_sequence(exon_seq))\n            transcript_seq += exon_seq\n\n        print(f\"&gt;{seqname}|{gene_id}|{transcript_id}\\n{transcript_seq}\", file=output_file)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/transcript_extraction/#bionemo.evo2.data.transcript_extraction.extract_stitched_transcript_sequences","title":"<code>extract_stitched_transcript_sequences(transcript_info, fasta_records, output_file, stitch_token='@', promoter_size=1024, intron_window=32, overlap=False)</code>","text":"<p>Extracts stitched transcript sequences from the provided transcript information and writes them to an output file.</p> <p>The \"stitched\" word refers to the process of combining sequences from different regions of the genome to form a single, continuous transcript sequence. This includes: Promoter Region: A specified number of base pairs (bp) upstream of the transcript start site. Exons: The coding regions of the transcript. Intron Windows: A specified number of bp from the neighboring introns around each exon.</p> <p>The stitch_token is used to denote the boundaries between these regions in the stitched transcript sequences.</p> <p>Parameters:</p> Name Type Description Default <code>transcript_info</code> <code>dict</code> <p>Dictionary containing transcript and exon information.</p> required <code>fasta_records</code> <code>NvFaidx</code> <p>Indexed FASTA records.</p> required <code>output_file</code> <code>TextIO</code> <p>File object to write the output sequences.</p> required <code>stitch_token</code> <code>str</code> <p>Token to use for stitching sequences. Defaults to \"@\".</p> <code>'@'</code> <code>promoter_size</code> <code>int</code> <p>Number of bp to include in the promoter region. Defaults to 1024.</p> <code>1024</code> <code>intron_window</code> <code>int</code> <p>Number of bp to include from neighboring introns. Defaults to 32.</p> <code>32</code> <code>overlap</code> <code>bool</code> <p>Whether to allow overlap of neighboring intron windows. Defaults to False.</p> <code>False</code> Source code in <code>bionemo/evo2/data/transcript_extraction.py</code> <pre><code>def extract_stitched_transcript_sequences(\n    transcript_info, fasta_records, output_file, stitch_token=\"@\", promoter_size=1024, intron_window=32, overlap=False\n):\n    \"\"\"Extracts stitched transcript sequences from the provided transcript information and writes them to an output file.\n\n    The \"stitched\" word refers to the process of combining sequences from different regions of the genome to form a single,\n    continuous transcript sequence.\n    This includes:\n    Promoter Region: A specified number of base pairs (bp) upstream of the transcript start site.\n    Exons: The coding regions of the transcript.\n    Intron Windows: A specified number of bp from the neighboring introns around each exon.\n\n    The stitch_token is used to denote the boundaries between\n    these regions in the stitched transcript sequences.\n\n    Args:\n        transcript_info (dict): Dictionary containing transcript and exon information.\n        fasta_records (NvFaidx): Indexed FASTA records.\n        output_file (TextIO): File object to write the output sequences.\n        stitch_token (str, optional): Token to use for stitching sequences. Defaults to \"@\".\n        promoter_size (int, optional): Number of bp to include in the promoter region. Defaults to 1024.\n        intron_window (int, optional): Number of bp to include from neighboring introns. Defaults to 32.\n        overlap (bool, optional): Whether to allow overlap of neighboring intron windows. Defaults to False.\n    \"\"\"\n    for transcript_id in transcript_info[\"transcripts\"]:\n        gene_id = transcript_info[\"transcript2gene\"][transcript_id]\n        this_exons = sorted(transcript_info[\"transcript2exon\"][transcript_id], key=lambda x: x[-1])\n\n        exon_qc_failed = False\n        if len(this_exons) &gt; 1:\n            for i in range(1, len(this_exons)):\n                this_exon = this_exons[i]\n                prev_exon = this_exons[i - 1]\n                this_coords = transcript_info[\"exons\"][this_exon]\n                prev_coords = transcript_info[\"exons\"][prev_exon]\n                if this_coords[\"strand\"] != prev_coords[\"strand\"]:\n                    exon_qc_failed = True\n                if this_coords[\"strand\"] == \"+\" and this_coords[\"start\"] &lt; prev_coords[\"start\"]:\n                    exon_qc_failed = True\n                if this_coords[\"strand\"] == \"-\" and this_coords[\"start\"] &gt; prev_coords[\"start\"]:\n                    exon_qc_failed = True\n                if this_coords[\"seqname\"] != prev_coords[\"seqname\"]:\n                    exon_qc_failed = True\n\n        if exon_qc_failed:\n            continue\n\n        transcript_seq = \"\"\n        seqname = None\n        for i in range(len(this_exons)):\n            # Previous Exon\n            prev_exon = this_exons[i - 1] if i &gt; 0 else None\n            prev_coords = transcript_info[\"exons\"].get(prev_exon, None)\n            # Current Exon\n            cur_exon = this_exons[i]\n            cur_coords = transcript_info[\"exons\"].get(cur_exon, None)\n            exon_number = cur_exon[-1]\n            if seqname is None:\n                seqname = cur_coords[\"seqname\"]\n            # Next Exon\n            next_exon = this_exons[i + 1] if i &lt; len(this_exons) - 1 else None\n            next_coords = transcript_info[\"exons\"].get(next_exon, None)\n            # Extract the stitched spliced sequence without overlapping intron windows.\n            intron_window_left = (\n                min(intron_window, math.floor(abs(cur_coords[\"start\"] - prev_coords[\"end\"]) / 2))\n                if not overlap and prev_coords is not None\n                else intron_window\n            )\n            intron_window_right = (\n                min(intron_window, math.ceil(abs(next_coords[\"start\"] - cur_coords[\"end\"]) / 2))\n                if not overlap and next_coords is not None\n                else intron_window\n            )\n            if cur_coords[\"strand\"] == \"+\" and exon_number == 1:\n                exon_start = cur_coords[\"start\"] - promoter_size\n                exon_end = cur_coords[\"end\"] + intron_window_right\n            elif cur_coords[\"strand\"] == \"-\" and exon_number == 1:\n                exon_start = cur_coords[\"start\"] - intron_window_left\n                exon_end = cur_coords[\"end\"] + promoter_size\n            else:\n                exon_start = cur_coords[\"start\"] - intron_window_left\n                exon_end = cur_coords[\"end\"] + intron_window_right\n            exon_seq = str(fasta_records[cur_coords[\"seqname\"]][exon_start:exon_end])\n            if cur_coords[\"strand\"] == \"-\":\n                exon_seq = stitch_token + reverse_sequence(complement_sequence(exon_seq))\n            transcript_seq += exon_seq\n\n        if stitch_token and len(stitch_token) &gt; 0:\n            transcript_seq = transcript_seq[len(stitch_token) :]\n\n        print(f\"&gt;{seqname}|{gene_id}|{transcript_id}\\n{transcript_seq}\", file=output_file)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/transcript_extraction/#bionemo.evo2.data.transcript_extraction.extract_transcript_exons","title":"<code>extract_transcript_exons(gtf_path, only_longest_transcript)</code>","text":"<p>Extracts transcript exons from a GTF file and optionally keeps only the longest transcript per gene.</p> <p>Parameters:</p> Name Type Description Default <code>gtf_path</code> <code>str</code> <p>Path to the GTF file.</p> required <code>only_longest_transcript</code> <code>bool</code> <p>Whether to keep only the longest transcript per gene.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing transcript and exon information.</p> Source code in <code>bionemo/evo2/data/transcript_extraction.py</code> <pre><code>def extract_transcript_exons(gtf_path: str, only_longest_transcript: bool):\n    \"\"\"Extracts transcript exons from a GTF file and optionally keeps only the longest transcript per gene.\n\n    Args:\n        gtf_path (str): Path to the GTF file.\n        only_longest_transcript (bool): Whether to keep only the longest transcript per gene.\n\n    Returns:\n        dict: A dictionary containing transcript and exon information.\n    \"\"\"\n    genes = defaultdict(set)\n    gene2transcripts = defaultdict(set)\n    transcripts = {}\n    exons = {}\n    exon2transcript = {}\n    transcript2gene = {}\n    transcript2exon = defaultdict(set)\n    skip_transcripts = set()\n\n    gtf_fields = [\"seqname\", \"source\", \"feature\", \"start\", \"end\", \"score\", \"strand\", \"frame\", \"attribute\"]\n    with open(gtf_path) as infile:\n        for line in infile:\n            # skip header lines\n            if line.startswith(\"#\"):\n                continue\n            line = line.strip().split(\"\\t\")\n            if len(line) &lt; 9:\n                continue\n\n            # parse the attributes into a dictionary\n            line = dict(zip(gtf_fields, line))\n            attribs = parse_gtf_attributes(line[\"attribute\"])\n\n            if line[\"feature\"] == \"gene\":\n                contig, start, end, strand = line[\"seqname\"], line[\"start\"], line[\"end\"], line[\"strand\"]\n                start, end = int(line[\"start\"]) - 1, int(line[\"end\"])\n                gene_id = attribs.get(\"gene_id\", None)\n                if not gene_id:\n                    continue\n                genes[gene_id].add((contig, start, end, strand))\n\n            elif line[\"feature\"] == \"exon\":\n                contig, start, end, strand = line[\"seqname\"], line[\"start\"], line[\"end\"], line[\"strand\"]\n                start, end = int(line[\"start\"]) - 1, int(line[\"end\"])\n                gene_id = attribs.get(\"gene_id\", None)\n                if not gene_id:\n                    continue\n                transcript_id = attribs[\"transcript_id\"]\n                gene2transcripts[gene_id].add(transcript_id)\n\n                # Skip exons that have already been handled and are likely errors\n                if transcript_id in skip_transcripts:\n                    continue\n                exon_number = int(attribs[\"exon_number\"])\n\n                exon_id = (gene_id, transcript_id, exon_number)\n                if exon_id in exons:\n                    del exons[exon_id]\n                    transcripts.pop(transcript_id, None)\n                    if transcript_id in transcript2exon:\n                        del transcript2exon[transcript_id]\n                    skip_transcripts.add(transcript_id)\n                    continue\n\n                exons[exon_id] = {\"seqname\": contig, \"start\": start, \"end\": end, \"strand\": strand}\n                if exon_id in exon2transcript:\n                    raise Exception(\"Exon Already Exists in exon2transcript\")\n                exon2transcript[exon_id] = transcript_id\n                transcript2exon[transcript_id].add(exon_id)\n\n            elif line[\"feature\"] == \"transcript\":\n                contig, start, end, strand = line[\"seqname\"], line[\"start\"], line[\"end\"], line[\"strand\"]\n                start, end = int(line[\"start\"]) - 1, int(line[\"end\"])\n                gene_id = attribs.get(\"gene_id\", None)\n                if not gene_id:\n                    continue\n                gbkey = attribs[\"gbkey\"]\n                transcript_biotype = attribs[\"transcript_biotype\"]\n                transcript_id = attribs[\"transcript_id\"]\n                if transcript_id in skip_transcripts:\n                    continue\n\n                transcripts[transcript_id] = {\n                    \"seqname\": contig,\n                    \"start\": start,\n                    \"end\": end,\n                    \"strand\": strand,\n                    \"gbkey\": gbkey,\n                    \"transcript_biotype\": transcript_biotype,\n                }\n                transcript2gene[transcript_id] = gene_id\n                gene2transcripts[gene_id].add(transcript_id)\n\n    if only_longest_transcript:\n        transcript_lengths = defaultdict(int)\n        for exon in exons:\n            transcript_lengths[exon[1]] += exons[exon][\"end\"] - exons[exon][\"start\"]\n\n        keep_transcripts = {}\n        keep_exons = {}\n        keep_exon2transcript = {}\n        keep_transcript2gene = {}\n        keep_transcript2exon = defaultdict(set)\n        keep_skip_transcripts = set()\n\n        for gene in gene2transcripts:\n            this_transcripts = gene2transcripts[gene]\n            this_transcript_lengths = [(transcript, transcript_lengths[transcript]) for transcript in this_transcripts]\n            longest_transcript = max(this_transcript_lengths, key=lambda x: x[1])[0]\n            keep_transcripts[longest_transcript] = dict(transcripts[longest_transcript])\n            for exon in transcript2exon[longest_transcript]:\n                keep_exons[exon] = dict(exons[exon])\n                keep_exon2transcript[exon] = longest_transcript\n                keep_transcript2exon[longest_transcript].add(exon)\n                keep_transcript2gene[longest_transcript] = gene\n\n        transcripts = keep_transcripts\n        exons = keep_exons\n        exon2transcript = keep_exon2transcript\n        transcript2gene = keep_transcript2gene\n        transcript2exon = keep_transcript2exon\n        skip_transcripts = keep_skip_transcripts\n\n    return {\n        \"transcripts\": transcripts,\n        \"exons\": exons,\n        \"exon2transcript\": exon2transcript,\n        \"transcript2gene\": transcript2gene,\n        \"transcript2exon\": transcript2exon,\n    }\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/transcript_extraction/#bionemo.evo2.data.transcript_extraction.main","title":"<code>main()</code>","text":"<p>Entry point for the script. Parses arguments and runs the extraction process.</p> Source code in <code>bionemo/evo2/data/transcript_extraction.py</code> <pre><code>def main():\n    \"\"\"Entry point for the script. Parses arguments and runs the extraction process.\"\"\"\n    args = parse_args()\n    if args.verbose:\n        logging.info(args)\n    run(args)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/transcript_extraction/#bionemo.evo2.data.transcript_extraction.parse_args","title":"<code>parse_args()</code>","text":"<p>Parses command line arguments for the transcript extraction script.</p> <p>Returns:</p> Type Description <p>argparse.Namespace: Parsed command line arguments.</p> Source code in <code>bionemo/evo2/data/transcript_extraction.py</code> <pre><code>def parse_args():\n    \"\"\"Parses command line arguments for the transcript extraction script.\n\n    Returns:\n        argparse.Namespace: Parsed command line arguments.\n    \"\"\"\n    ap = argparse.ArgumentParser(description=\"Extract spliced transcripts from a FASTA and GTF.\")\n    ap.add_argument(\"--fasta-path\", type=str, required=True, help=\"Path to FASTA file to extract transcripts from.\")\n    ap.add_argument(\n        \"--gtf-path\",\n        type=str,\n        required=True,\n        help=\"Path to gene transfer format (GTF) file associated with the FASTA.\",\n    )\n    ap.add_argument(\"--output-path\", type=str, default=None, help=\"Path to output FASTA file.\")\n    ap.add_argument(\n        \"--transcript-type\",\n        type=str,\n        default=\"default\",\n        choices=[\"default\", \"stitched\"],\n        help=\"Type of transcript to extract from the GTF and FASTA files for splicing. 'Stitched' transcripts include 1024 bp of sequence from the promoter and 32 bp around each exon.\",\n    )\n    ap.add_argument(\n        \"--stitched-promoter\",\n        type=int,\n        default=1024,\n        help=\"Number of bp to include in the promoter region when --transcript-type=stitched is used. Defaults to 1024.\",\n    )\n    ap.add_argument(\n        \"--stitched-intron\",\n        type=int,\n        default=32,\n        help=\"Number of bp to include from neighboring introns when --transcript-type=stitched is used. Defaults to 32.\",\n    )\n    ap.add_argument(\n        \"--stitched-overlap\",\n        action=\"store_true\",\n        help=\"Allow overlap of neighboring intron windows when --transcript-type=stitched is used. Defaults to False, i.e. prevents overlap by shortening the intron windows for a contiguous splice.\",\n    )\n    ap.add_argument(\n        \"--only-longest-transcript\", action=\"store_true\", help=\"Only extract the longest transcript per gene.\"\n    )\n    ap.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"Turn on verbose log messages.\")\n    return ap.parse_args()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/transcript_extraction/#bionemo.evo2.data.transcript_extraction.parse_gtf_attributes","title":"<code>parse_gtf_attributes(attributes)</code>","text":"<p>Parses the attributes field of a GTF file line into a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>attributes</code> <code>str</code> <p>The attributes field from a GTF file line.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary of attribute key-value pairs.</p> Source code in <code>bionemo/evo2/data/transcript_extraction.py</code> <pre><code>def parse_gtf_attributes(attributes: str):\n    \"\"\"Parses the attributes field of a GTF file line into a dictionary.\n\n    Args:\n        attributes (str): The attributes field from a GTF file line.\n\n    Returns:\n        dict: A dictionary of attribute key-value pairs.\n    \"\"\"\n    # Split on all semicolons that are not inside quotes\n    attributes = re.split(r';(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)', attributes)\n    out = {}\n    for a in attributes:\n        if len(a) == 0:\n            continue\n        key = a.split()[0]\n        value = a.split('\"')[1]\n        out[key] = value\n    return out\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/data/transcript_extraction/#bionemo.evo2.data.transcript_extraction.run","title":"<code>run(args)</code>","text":"<p>Main function to run the transcript extraction process based on command line arguments.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>Parsed command line arguments.</p> required Source code in <code>bionemo/evo2/data/transcript_extraction.py</code> <pre><code>def run(args):\n    \"\"\"Main function to run the transcript extraction process based on command line arguments.\n\n    Args:\n        args (argparse.Namespace): Parsed command line arguments.\n    \"\"\"\n    with open(args.output_path, \"w\") if args.output_path is not None else sys.stdout as output_file:\n        if args.verbose:\n            logging.info(\"Indexing FASTA file...\")\n\n        fasta_index = NvFaidx(args.fasta_path)\n\n        if args.transcript_type == \"default\":\n            if args.verbose:\n                logging.info(\"Extracting default transcripts...\")\n                if args.only_longest_transcript:\n                    logging.info(\"Only extracting the longest transcript per gene.\")\n                else:\n                    logging.info(\"Extracting all transcripts regardless of length.\")\n\n        elif args.transcript_type == \"stitched\":\n            if args.verbose:\n                logging.info(\"Extracting stitched transcripts...\")\n                if args.only_longest_transcript:\n                    logging.info(\"Only extracting the longest transcript per gene.\")\n                else:\n                    logging.info(\"Extracting all transcripts regardless of length.\")\n\n        transcript_info = extract_transcript_exons(args.gtf_path, args.only_longest_transcript)\n\n        if args.transcript_type == \"default\":\n            extract_default_transcript_sequences(transcript_info, fasta_index, output_file)\n        elif args.transcript_type == \"stitched\":\n            extract_stitched_transcript_sequences(\n                transcript_info,\n                fasta_index,\n                output_file,\n                promoter_size=args.stitched_promoter,\n                intron_window=args.stitched_intron,\n                overlap=args.stitched_overlap,\n            )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/llama/","title":"Llama","text":""},{"location":"main/references/API_reference/bionemo/evo2/models/llama/#bionemo.evo2.models.llama.Eden11BConfig","title":"<code>Eden11BConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>EdenConfig</code></p> <p>Eden-flavoured Llama-3.1 ~14B (keeps all Eden behaviors).</p> Source code in <code>bionemo/evo2/models/llama.py</code> <pre><code>@dataclass\nclass Eden11BConfig(EdenConfig):\n    \"\"\"Eden-flavoured Llama-3.1 ~14B (keeps all Eden behaviors).\"\"\"\n\n    # If you want long context like Eden-long, bump this; else inherit 8192.\n    seq_length: int = 8192  # or remove this line to keep 8192\n\n    # ~14B sizing (head_dim \u2248 128)\n    num_layers: int = 36\n    hidden_size: int = 5120\n    ffn_hidden_size: int = 13824\n    num_attention_heads: int = 40\n    num_query_groups: int = 8  # GQA (inherited value is also fine if already 8)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/llama/#bionemo.evo2.models.llama.Eden18BConfig","title":"<code>Eden18BConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>EdenConfig</code></p> <p>Eden-flavoured Llama-3.1 ~18B (keeps all Eden behaviors).</p> Source code in <code>bionemo/evo2/models/llama.py</code> <pre><code>@dataclass\nclass Eden18BConfig(EdenConfig):\n    \"\"\"Eden-flavoured Llama-3.1 ~18B (keeps all Eden behaviors).\"\"\"\n\n    # If you want long context like Eden-long, bump this; else inherit 8192.\n    seq_length: int = 8192  # or remove this line to keep 8192\n\n    # ~18B sizing (head_dim \u2248 128)\n    num_layers: int = 48\n    hidden_size: int = 6144\n    ffn_hidden_size: int = 16384\n    num_attention_heads: int = 48\n    num_query_groups: int = 8  # GQA (inherited value is also fine if already 8)\n    old_context_len: int = 8192  # or remove this line to keep 8192\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/llama/#bionemo.evo2.models.llama.Eden21BConfig","title":"<code>Eden21BConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>EdenConfig</code></p> <p>Eden-flavoured Llama-3.1 ~21B (keeps all Eden behaviors).</p> Source code in <code>bionemo/evo2/models/llama.py</code> <pre><code>@dataclass\nclass Eden21BConfig(EdenConfig):\n    \"\"\"Eden-flavoured Llama-3.1 ~21B (keeps all Eden behaviors).\"\"\"\n\n    seq_length: int = 8192\n\n    # ~21B sizing (head_dim = 128)\n    num_layers: int = 42  # 42 layers for 21B target\n    hidden_size: int = 7168  # 56 * 128 = 7168 for exact head_dim\n    ffn_hidden_size: int = 19456  # ~2.7x hidden_size\n    num_attention_heads: int = 56  # Divisible by 8\n    num_query_groups: int = 8  # GQA\n    old_context_len: int = 8192\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/llama/#bionemo.evo2.models.llama.Eden24BConfig","title":"<code>Eden24BConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>EdenConfig</code></p> <p>Eden-flavoured Llama-3.1 ~8B (keeps all Eden behaviors).</p> Source code in <code>bionemo/evo2/models/llama.py</code> <pre><code>@dataclass\nclass Eden24BConfig(EdenConfig):\n    \"\"\"Eden-flavoured Llama-3.1 ~8B (keeps all Eden behaviors).\"\"\"\n\n    # If you want long context like Eden-long, bump this; else inherit 8192.\n    seq_length: int = 32768  # or remove this line to keep 8192\n\n    # ~8B sizing (head_dim \u2248 128)\n    num_layers: int = 46\n    hidden_size: int = 6144\n    ffn_hidden_size: int = 23296\n    num_attention_heads: int = 48\n    num_query_groups: int = 8  # GQA (inherited value is also fine if already 8)\n    old_context_len: int = 8192\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/llama/#bionemo.evo2.models.llama.Eden27BConfig","title":"<code>Eden27BConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>EdenConfig</code></p> <p>Eden-flavoured Llama-3.1 ~8B (keeps all Eden behaviors).</p> Source code in <code>bionemo/evo2/models/llama.py</code> <pre><code>@dataclass\nclass Eden27BConfig(EdenConfig):\n    \"\"\"Eden-flavoured Llama-3.1 ~8B (keeps all Eden behaviors).\"\"\"\n\n    # If you want long context like Eden-long, bump this; else inherit 8192.\n    seq_length: int = 32768  # or remove this line to keep 8192\n\n    # ~8B sizing (head_dim \u2248 128)\n    num_layers: int = 46\n    hidden_size: int = 6656\n    ffn_hidden_size: int = 23296\n    num_attention_heads: int = 52\n    num_query_groups: int = 8  # GQA (inherited value is also fine if already 8)\n    old_context_len: int = 8192\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/llama/#bionemo.evo2.models.llama.Eden28BConfig","title":"<code>Eden28BConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>EdenConfig</code></p> <p>Eden-flavoured Llama-3.1 ~28B (keeps all Eden behaviors).</p> Source code in <code>bionemo/evo2/models/llama.py</code> <pre><code>@dataclass\nclass Eden28BConfig(EdenConfig):\n    \"\"\"Eden-flavoured Llama-3.1 ~28B (keeps all Eden behaviors).\"\"\"\n\n    # If you want long context like Eden-long, bump this; else inherit 8192.\n    seq_length: int = 8192  # or remove this line to keep 8192\n\n    # ~8B sizing (head_dim \u2248 128)\n    num_layers: int = 48\n    hidden_size: int = 6144\n    ffn_hidden_size: int = 26368\n    num_attention_heads: int = 48\n    num_query_groups: int = 8  # GQA (inherited value is also fine if already 8)\n    old_context_len: int = 8192  # or remove this line to keep 8192\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/llama/#bionemo.evo2.models.llama.Eden35BConfig","title":"<code>Eden35BConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>EdenConfig</code></p> <p>Eden-flavoured Llama-3.1 ~35B (keeps all Eden behaviors).</p> Source code in <code>bionemo/evo2/models/llama.py</code> <pre><code>@dataclass\nclass Eden35BConfig(EdenConfig):\n    \"\"\"Eden-flavoured Llama-3.1 ~35B (keeps all Eden behaviors).\"\"\"\n\n    seq_length: int = 8192\n\n    # ~35B sizing (head_dim \u2248 128)\n    num_layers: int = 64\n    hidden_size: int = 7168\n    ffn_hidden_size: int = 20480\n    num_attention_heads: int = 56\n    num_query_groups: int = 8  # GQA\n    old_context_len: int = 8192\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/llama/#bionemo.evo2.models.llama.EdenConfig","title":"<code>EdenConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Llama31Config8B</code></p> <p>Eden-flavoured Llama-3.1 ~8B (keeps all Eden behaviors). Inherits from the llama 3.1 config for proper handling of RoPE when converting checkpoints.</p> Source code in <code>bionemo/evo2/models/llama.py</code> <pre><code>@dataclass\nclass EdenConfig(llm.Llama31Config8B):\n    \"\"\"Eden-flavoured Llama-3.1 ~8B (keeps all Eden behaviors). Inherits from the llama 3.1 config for proper handling of RoPE when converting checkpoints.\"\"\"\n\n    rotary_base: int = 500_000\n    seq_length: int = 8192\n    num_layers: int = 32\n    hidden_size: int = 4096\n    ffn_hidden_size: int = 14336\n    num_attention_heads: int = 32\n\n    scale_factor: int = 1\n    low_freq_factor: int = 1\n    high_freq_factor: int = 4\n    old_context_len: int = 8192\n    init_method_std: float = 0.02\n    embedding_init_method_std: Optional[float] = None\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/llama/#bionemo.evo2.models.llama.HFEdenLlamaImporter","title":"<code>HFEdenLlamaImporter</code>","text":"<p>               Bases: <code>HFLlamaImporter</code></p> <p>Importer for Eden-flavoured Llama models which just overrides the tokenizer and config classes from NeMo.</p> Source code in <code>bionemo/evo2/models/llama.py</code> <pre><code>@io.model_importer(LlamaModel, \"hf\")\nclass HFEdenLlamaImporter(HFLlamaImporter):\n    \"\"\"Importer for Eden-flavoured Llama models which just overrides the tokenizer and config classes from NeMo.\"\"\"\n\n    @property\n    def config(self) -&gt; EdenConfig:\n        \"\"\"Create a NeMo LlamaConfig from the HF model config.\n\n        Translates the HF configuration parameters to the equivalent NeMo\n        configuration.\n\n        Returns:\n            LlamaConfig: NeMo configuration for Llama models\n        \"\"\"\n        from transformers import AutoConfig, GenerationConfig\n\n        source = AutoConfig.from_pretrained(str(self))\n        try:\n            generation_config = GenerationConfig.from_pretrained(str(self))\n        except Exception:\n            generation_config = None\n\n        def make_vocab_size_divisible_by(vocab_size):\n            base = 128\n            while vocab_size % base != 0:\n                base //= 2\n            return base\n\n        cls = EdenConfig\n        scale_factor = source.rope_scaling.get(\"factor\", 8.0) if source.rope_scaling is not None else 8.0\n\n        args = {}\n\n        output = cls(\n            num_layers=source.num_hidden_layers,\n            hidden_size=source.hidden_size,\n            ffn_hidden_size=(\n                source.intermediate_size\n                if not getattr(source, \"intermediate_size_mlp\", None)\n                else source.intermediate_size_mlp\n            ),\n            num_attention_heads=source.num_attention_heads,\n            init_method_std=source.initializer_range,\n            layernorm_epsilon=source.rms_norm_eps,\n            num_query_groups=source.num_key_value_heads,\n            seq_length=source.max_position_embeddings,\n            rotary_base=source.rope_theta,\n            gated_linear_unit=True,\n            make_vocab_size_divisible_by=make_vocab_size_divisible_by(source.vocab_size),\n            share_embeddings_and_output_weights=getattr(source, \"tie_word_embeddings\", False),\n            fp16=(dtype_from_hf(source) == torch.float16),\n            bf16=(dtype_from_hf(source) == torch.bfloat16),\n            params_dtype=dtype_from_hf(source),\n            generation_config=generation_config,\n            vocab_size=source.vocab_size,\n            kv_channels=getattr(source, \"head_dim\", None),\n            scale_factor=scale_factor,\n            **args,\n        )\n\n        return output\n\n    @property\n    def tokenizer(self):\n        \"\"\"Override the tokenizer to use the Eden-flavoured tokenizer.\"\"\"\n        from bionemo.evo2.run.utils import patch_eden_tokenizer  # avoid circular import\n\n        tokenizer = get_nmt_tokenizer(\"byte-level\")\n        patch_eden_tokenizer(tokenizer)\n        return tokenizer\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/llama/#bionemo.evo2.models.llama.HFEdenLlamaImporter.config","title":"<code>config</code>  <code>property</code>","text":"<p>Create a NeMo LlamaConfig from the HF model config.</p> <p>Translates the HF configuration parameters to the equivalent NeMo configuration.</p> <p>Returns:</p> Name Type Description <code>LlamaConfig</code> <code>EdenConfig</code> <p>NeMo configuration for Llama models</p>"},{"location":"main/references/API_reference/bionemo/evo2/models/llama/#bionemo.evo2.models.llama.HFEdenLlamaImporter.tokenizer","title":"<code>tokenizer</code>  <code>property</code>","text":"<p>Override the tokenizer to use the Eden-flavoured tokenizer.</p>"},{"location":"main/references/API_reference/bionemo/evo2/models/mamba/","title":"Mamba","text":""},{"location":"main/references/API_reference/bionemo/evo2/models/mamba/#bionemo.evo2.models.mamba.Evo2StyleMCoreMambaModel","title":"<code>Evo2StyleMCoreMambaModel</code>","text":"<p>               Bases: <code>MambaModel</code></p> <p>Custom version of MCoreMambaModel that implements reweighted loss calculation.</p> <p>Note that this is similar to the HyenaModel for uppercase/lowercase handling.</p> Source code in <code>bionemo/evo2/models/mamba.py</code> <pre><code>class Evo2StyleMCoreMambaModel(megatron.core.models.mamba.mamba_model.MambaModel):\n    \"\"\"Custom version of MCoreMambaModel that implements reweighted loss calculation.\n\n    Note that this is similar to the HyenaModel for uppercase/lowercase handling.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initializes `Evo2StyleMCoreMambaModel` with unique parameters for the Evo2 variant of `MCoreMambaModel`.\"\"\"\n        super().__init__(*args, **kwargs)\n        if self.config.use_targeted_variance_loss:\n            if not hasattr(self.config, \"embedding_init_method_std\"):\n                logger.warning(\"embedding_init_method_std is not supported in this config, please upgrade Megatron-LM\")\n            # 1.0 is the suggested value for embedding_init_method_std from the\n            # [Spike No More](https://arxiv.org/abs/2312.16903) paper.\n            embedding_init_method_std: float = getattr(self.config, \"embedding_init_method_std\", 1.0)\n            self.targeted_variance_loss = SquaredErrorTargetedVarianceLoss(\n                loss_coeff=self.config.targeted_variance_loss_loss_coeff,\n                var_target=embedding_init_method_std**2,\n            )\n\n    @override\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        position_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        decoder_input: torch.Tensor | None = None,\n        labels: torch.Tensor | None = None,\n        inference_context=None,\n        runtime_gather_output: bool | None = None,\n        *,\n        inference_params=None,\n        loss_mask: torch.Tensor | None = None,\n    ):\n        \"\"\"Forward pass with custom loss calculation for uppercase/lowercase reweighting.\n\n        Note that this mimics the behavior in hyena_model.py lines 273-292.\n\n        Forward function of the Mamba model. This function passes the input tensors\n        through the embedding layer, and then the decoder and finally into the post\n        processing layer (optional).\n\n        It either returns the Loss values if labels are given or the final hidden units\n        \"\"\"\n        # If decoder_input is provided (not None), then input_ids and position_ids are ignored.\n        # Otherwise, apply embedding layer on input_ids and position_ids to get decoder_input.\n\n        inference_context = deprecate_inference_params(inference_context, inference_params)\n\n        # Decoder embedding.\n        if decoder_input is not None:\n            pass\n        elif self.pre_process:\n            decoder_input = self.embedding(input_ids=input_ids, position_ids=position_ids)\n        else:\n            # intermediate stage of pipeline\n            # decoder will get hidden_states from encoder.input_tensor\n            decoder_input = None\n\n        rotary_pos_emb = None\n        if self.position_embedding_type == \"rope\":\n            rotary_seq_len = self.rotary_pos_emb.get_rotary_seq_len(\n                inference_context, self.decoder, decoder_input, self.config\n            )\n            rotary_pos_emb = self.rotary_pos_emb(rotary_seq_len)\n\n        # Wrap decoder_input to allow the decoder (MambaBlock) to delete the\n        # reference held by this caller function, enabling early garbage collection\n        # for inference.\n        if inference_context is not None and not self.training:\n            decoder_input = WrappedTensor(decoder_input)\n\n        # The following assert will currently fail when running inference.\n        # Commented out for now.\n        # TODO (duncan/rwaleffe): (1) confirm that the externally-generated\n        #   attention mask is not needed and is ignored by the model in\n        #   inference mode, (2) reduce the size of the externally-generated\n        #   attention mask to prevent CPU OOM (as we did for training), (3)\n        #   force the attention mask passed to the model in inference mode to\n        #   be None, so this assert will succeed.\n        # assert attention_mask is None, \"The attention mask is ignored and should be set to None\"\n\n        # Run decoder.\n        hidden_states = self.decoder(\n            hidden_states=decoder_input,\n            attention_mask=attention_mask,\n            inference_context=inference_context,\n            rotary_pos_emb=rotary_pos_emb,\n        )\n\n        if not self.post_process:\n            return hidden_states\n\n        # logits and loss\n        output_weight = None\n        if self.share_embeddings_and_output_weights:\n            output_weight = self.shared_embedding_or_output_weight()\n\n        if (\n            not self.training\n            and inference_context is not None\n            and inference_context.materialize_only_last_token_logits\n        ):\n            hidden_states = hidden_states[-1, :, :].unsqueeze(0)\n\n        logits, _ = self.output_layer(hidden_states, weight=output_weight, runtime_gather_output=runtime_gather_output)\n\n        if labels is None:\n            # [s b h] =&gt; [b s h]\n            return logits.transpose(0, 1).contiguous()\n\n        # Apply reweighted loss calculation for uppercase/lowercase handling\n        labels, lowercase_mask = make_upper_case(labels)\n        loss = self.compute_language_model_loss(labels, logits)\n        normalize_per_batch = True if self.config.to_upper == \"normalized_weighted\" else False\n        loss = reweighted_cross_entropy(\n            loss,\n            (labels, loss_mask, lowercase_mask),\n            lowercase_weight=self.config.lowercase_loss_reweighting,\n            normalize_per_batch=normalize_per_batch,\n        )\n        if self.training and self.config.use_targeted_variance_loss:\n            # Only use this in training, not validation etc.\n            var_loss = self.targeted_variance_loss(self.embedding.word_embeddings.weight)\n            loss += var_loss\n        return loss\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/mamba/#bionemo.evo2.models.mamba.Evo2StyleMCoreMambaModel.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initializes <code>Evo2StyleMCoreMambaModel</code> with unique parameters for the Evo2 variant of <code>MCoreMambaModel</code>.</p> Source code in <code>bionemo/evo2/models/mamba.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Initializes `Evo2StyleMCoreMambaModel` with unique parameters for the Evo2 variant of `MCoreMambaModel`.\"\"\"\n    super().__init__(*args, **kwargs)\n    if self.config.use_targeted_variance_loss:\n        if not hasattr(self.config, \"embedding_init_method_std\"):\n            logger.warning(\"embedding_init_method_std is not supported in this config, please upgrade Megatron-LM\")\n        # 1.0 is the suggested value for embedding_init_method_std from the\n        # [Spike No More](https://arxiv.org/abs/2312.16903) paper.\n        embedding_init_method_std: float = getattr(self.config, \"embedding_init_method_std\", 1.0)\n        self.targeted_variance_loss = SquaredErrorTargetedVarianceLoss(\n            loss_coeff=self.config.targeted_variance_loss_loss_coeff,\n            var_target=embedding_init_method_std**2,\n        )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/mamba/#bionemo.evo2.models.mamba.Evo2StyleMCoreMambaModel.forward","title":"<code>forward(input_ids, position_ids, attention_mask, decoder_input=None, labels=None, inference_context=None, runtime_gather_output=None, *, inference_params=None, loss_mask=None)</code>","text":"<p>Forward pass with custom loss calculation for uppercase/lowercase reweighting.</p> <p>Note that this mimics the behavior in hyena_model.py lines 273-292.</p> <p>Forward function of the Mamba model. This function passes the input tensors through the embedding layer, and then the decoder and finally into the post processing layer (optional).</p> <p>It either returns the Loss values if labels are given or the final hidden units</p> Source code in <code>bionemo/evo2/models/mamba.py</code> <pre><code>@override\ndef forward(\n    self,\n    input_ids: torch.Tensor,\n    position_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    decoder_input: torch.Tensor | None = None,\n    labels: torch.Tensor | None = None,\n    inference_context=None,\n    runtime_gather_output: bool | None = None,\n    *,\n    inference_params=None,\n    loss_mask: torch.Tensor | None = None,\n):\n    \"\"\"Forward pass with custom loss calculation for uppercase/lowercase reweighting.\n\n    Note that this mimics the behavior in hyena_model.py lines 273-292.\n\n    Forward function of the Mamba model. This function passes the input tensors\n    through the embedding layer, and then the decoder and finally into the post\n    processing layer (optional).\n\n    It either returns the Loss values if labels are given or the final hidden units\n    \"\"\"\n    # If decoder_input is provided (not None), then input_ids and position_ids are ignored.\n    # Otherwise, apply embedding layer on input_ids and position_ids to get decoder_input.\n\n    inference_context = deprecate_inference_params(inference_context, inference_params)\n\n    # Decoder embedding.\n    if decoder_input is not None:\n        pass\n    elif self.pre_process:\n        decoder_input = self.embedding(input_ids=input_ids, position_ids=position_ids)\n    else:\n        # intermediate stage of pipeline\n        # decoder will get hidden_states from encoder.input_tensor\n        decoder_input = None\n\n    rotary_pos_emb = None\n    if self.position_embedding_type == \"rope\":\n        rotary_seq_len = self.rotary_pos_emb.get_rotary_seq_len(\n            inference_context, self.decoder, decoder_input, self.config\n        )\n        rotary_pos_emb = self.rotary_pos_emb(rotary_seq_len)\n\n    # Wrap decoder_input to allow the decoder (MambaBlock) to delete the\n    # reference held by this caller function, enabling early garbage collection\n    # for inference.\n    if inference_context is not None and not self.training:\n        decoder_input = WrappedTensor(decoder_input)\n\n    # The following assert will currently fail when running inference.\n    # Commented out for now.\n    # TODO (duncan/rwaleffe): (1) confirm that the externally-generated\n    #   attention mask is not needed and is ignored by the model in\n    #   inference mode, (2) reduce the size of the externally-generated\n    #   attention mask to prevent CPU OOM (as we did for training), (3)\n    #   force the attention mask passed to the model in inference mode to\n    #   be None, so this assert will succeed.\n    # assert attention_mask is None, \"The attention mask is ignored and should be set to None\"\n\n    # Run decoder.\n    hidden_states = self.decoder(\n        hidden_states=decoder_input,\n        attention_mask=attention_mask,\n        inference_context=inference_context,\n        rotary_pos_emb=rotary_pos_emb,\n    )\n\n    if not self.post_process:\n        return hidden_states\n\n    # logits and loss\n    output_weight = None\n    if self.share_embeddings_and_output_weights:\n        output_weight = self.shared_embedding_or_output_weight()\n\n    if (\n        not self.training\n        and inference_context is not None\n        and inference_context.materialize_only_last_token_logits\n    ):\n        hidden_states = hidden_states[-1, :, :].unsqueeze(0)\n\n    logits, _ = self.output_layer(hidden_states, weight=output_weight, runtime_gather_output=runtime_gather_output)\n\n    if labels is None:\n        # [s b h] =&gt; [b s h]\n        return logits.transpose(0, 1).contiguous()\n\n    # Apply reweighted loss calculation for uppercase/lowercase handling\n    labels, lowercase_mask = make_upper_case(labels)\n    loss = self.compute_language_model_loss(labels, logits)\n    normalize_per_batch = True if self.config.to_upper == \"normalized_weighted\" else False\n    loss = reweighted_cross_entropy(\n        loss,\n        (labels, loss_mask, lowercase_mask),\n        lowercase_weight=self.config.lowercase_loss_reweighting,\n        normalize_per_batch=normalize_per_batch,\n    )\n    if self.training and self.config.use_targeted_variance_loss:\n        # Only use this in training, not validation etc.\n        var_loss = self.targeted_variance_loss(self.embedding.word_embeddings.weight)\n        loss += var_loss\n    return loss\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/mamba/#bionemo.evo2.models.mamba.HybridMambaConfig8BEvo2Loss","title":"<code>HybridMambaConfig8BEvo2Loss</code>  <code>dataclass</code>","text":"<p>               Bases: <code>NemotronHConfigBase</code></p> <p>Config for 8B hybrid Mamba model.</p> Source code in <code>bionemo/evo2/models/mamba.py</code> <pre><code>@dataclass\nclass HybridMambaConfig8BEvo2Loss(NemotronHConfigBase):\n    \"\"\"Config for 8B hybrid Mamba model.\"\"\"\n\n    hybrid_override_pattern: str = \"M-M-M-M*-M-M-M-M-M*-M-M-M-M-M*-M-M-M-M-M*-M-M-M-M-M-\"\n    num_layers: int = 52\n    seq_length: int = 8192\n    hidden_size: int = 4096\n    mamba_ssm_ngroups: int = 8\n    mamba_state_dim: int = 128\n    mamba_head_dim: int = 64\n    ffn_hidden_size: int = 21504\n    num_attention_heads: int = 32\n    init_method_std: float = 0.014\n    num_query_groups: int = 8\n    make_vocab_size_divisible_by: int = 128\n    tokenizer_library: str = \"byte-level\"  # Use Evo2 tokenizer\n    tokenizer_name: str = None\n    masked_softmax_fusion: bool = True\n    apply_query_key_layer_scaling: bool = False\n    persist_layer_norm: bool = True\n    attention_softmax_in_fp32: bool = False\n    vocab_size: int = 512\n    first_last_layers_bf16: bool = True\n    is_hybrid_model: bool = True\n    forward_step_fn: Callable = mamba_forward_step\n    data_step_fn: Callable = gpt_data_step\n    # Set a reasonable default for to_upper to match HyenaModel behavior\n    to_upper: str = \"normalized_weighted\"\n    # Set lowercase loss reweighting factor\n    lowercase_loss_reweighting: float = 1.0\n    activation_func: Callable = lambda x: torch.square(F.relu(x))  # lambda x: torch.pow(F.relu(x), 2)\n    # The trainer is responsible for using this when initializing the optimizer state:\n    #  opt = MegatronOptimizerModule(opt_config, sched, no_weight_decay_cond=model_config.hyena_no_weight_decay_cond_fn)\n    hyena_no_weight_decay_cond_fn: Callable = mamba_no_weight_decay_cond\n    spike_no_more_embedding_init: bool = False  # TODO: remove this.\n    layernorm_embeddings: bool = False\n    # If set to true, use targeted variance loss which encourages the word embedding weight variances\n    # to be close to a target value (1.0).\n    use_targeted_variance_loss: bool = False\n    targeted_variance_loss_loss_coeff: float = 0.1\n    share_embeddings_and_output_weights: bool = False\n\n    def __post_init__(self):\n        \"\"\"Post-init logic for Evo2 to enable backwards compatibility with old configs.\"\"\"\n        # Specific post_init logic for Evo2 to enable backwards compatibility with old configs.\n        if not hasattr(self, \"embedding_init_method_std\"):\n            raise ValueError(\"embedding_init_method_std is not supported in this config, please upgrade Megatron-LM\")\n        if self.spike_no_more_embedding_init and self.embedding_init_method_std is None:\n            logger.warning(\n                \"spike_no_more_embedding_init is deprecated, please set \"\n                \"embedding_init_method_std=[desired_stdev] in the future. To get the old behavior set to 1.0. \"\n                \"For now setting to 1.0.\"\n            )\n            self.embedding_init_method_std = 1.0\n        # Continue with the remaining post-init logic defined in NemotronHConfigBase and/or TransformerConfig.\n        super().__post_init__()\n\n    @override\n    def configure_model(\n        self, tokenizer, pre_process=None, post_process=None, vp_stage: int | None = None\n    ) -&gt; Evo2StyleMCoreMambaModel:\n        \"\"\"Configures the model for training or inference.\"\"\"\n        mamba_stack_spec = self.mamba_stack_spec\n        if not isinstance(mamba_stack_spec, ModuleSpec):\n            mamba_stack_spec = mamba_stack_spec()\n\n        assert getattr(self, \"virtual_pipeline_model_parallel_size\", None) is None and vp_stage is None, (\n            \"Virtual pipeline model parallelism is temporarily unsupported in SSM/Mamaba \"\n            \"models due to upstream MCore MambaModel API dependency\"\n        )\n        return Evo2StyleMCoreMambaModel(\n            self,\n            mamba_stack_spec=mamba_stack_spec,\n            vocab_size=get_vocab_size(self, tokenizer.vocab_size, self.make_vocab_size_divisible_by),\n            max_sequence_length=self.seq_length,\n            hybrid_attention_ratio=self.hybrid_attention_ratio,\n            hybrid_mlp_ratio=self.hybrid_mlp_ratio,\n            hybrid_override_pattern=self.hybrid_override_pattern,\n            position_embedding_type=self.position_embedding_type,\n            rotary_percent=self.rotary_percent,\n            rotary_base=self.rotary_base,\n            seq_len_interpolation_factor=self.seq_len_interpolation_factor,\n            pre_process=pre_process or parallel_state.is_pipeline_first_stage(),\n            post_process=post_process or parallel_state.is_pipeline_last_stage(),\n            share_embeddings_and_output_weights=self.share_embeddings_and_output_weights,\n        )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/mamba/#bionemo.evo2.models.mamba.HybridMambaConfig8BEvo2Loss.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Post-init logic for Evo2 to enable backwards compatibility with old configs.</p> Source code in <code>bionemo/evo2/models/mamba.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Post-init logic for Evo2 to enable backwards compatibility with old configs.\"\"\"\n    # Specific post_init logic for Evo2 to enable backwards compatibility with old configs.\n    if not hasattr(self, \"embedding_init_method_std\"):\n        raise ValueError(\"embedding_init_method_std is not supported in this config, please upgrade Megatron-LM\")\n    if self.spike_no_more_embedding_init and self.embedding_init_method_std is None:\n        logger.warning(\n            \"spike_no_more_embedding_init is deprecated, please set \"\n            \"embedding_init_method_std=[desired_stdev] in the future. To get the old behavior set to 1.0. \"\n            \"For now setting to 1.0.\"\n        )\n        self.embedding_init_method_std = 1.0\n    # Continue with the remaining post-init logic defined in NemotronHConfigBase and/or TransformerConfig.\n    super().__post_init__()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/mamba/#bionemo.evo2.models.mamba.HybridMambaConfig8BEvo2Loss.configure_model","title":"<code>configure_model(tokenizer, pre_process=None, post_process=None, vp_stage=None)</code>","text":"<p>Configures the model for training or inference.</p> Source code in <code>bionemo/evo2/models/mamba.py</code> <pre><code>@override\ndef configure_model(\n    self, tokenizer, pre_process=None, post_process=None, vp_stage: int | None = None\n) -&gt; Evo2StyleMCoreMambaModel:\n    \"\"\"Configures the model for training or inference.\"\"\"\n    mamba_stack_spec = self.mamba_stack_spec\n    if not isinstance(mamba_stack_spec, ModuleSpec):\n        mamba_stack_spec = mamba_stack_spec()\n\n    assert getattr(self, \"virtual_pipeline_model_parallel_size\", None) is None and vp_stage is None, (\n        \"Virtual pipeline model parallelism is temporarily unsupported in SSM/Mamaba \"\n        \"models due to upstream MCore MambaModel API dependency\"\n    )\n    return Evo2StyleMCoreMambaModel(\n        self,\n        mamba_stack_spec=mamba_stack_spec,\n        vocab_size=get_vocab_size(self, tokenizer.vocab_size, self.make_vocab_size_divisible_by),\n        max_sequence_length=self.seq_length,\n        hybrid_attention_ratio=self.hybrid_attention_ratio,\n        hybrid_mlp_ratio=self.hybrid_mlp_ratio,\n        hybrid_override_pattern=self.hybrid_override_pattern,\n        position_embedding_type=self.position_embedding_type,\n        rotary_percent=self.rotary_percent,\n        rotary_base=self.rotary_base,\n        seq_len_interpolation_factor=self.seq_len_interpolation_factor,\n        pre_process=pre_process or parallel_state.is_pipeline_first_stage(),\n        post_process=post_process or parallel_state.is_pipeline_last_stage(),\n        share_embeddings_and_output_weights=self.share_embeddings_and_output_weights,\n    )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/mamba/#bionemo.evo2.models.mamba.MambaModel","title":"<code>MambaModel</code>","text":"<p>               Bases: <code>GPTModel</code></p> <p>Mamba model that extends GPTModel for integration with NeMo.</p> <p>Note that the loss calculation is handled by CustomMCoreMambaModel instead.</p> Source code in <code>bionemo/evo2/models/mamba.py</code> <pre><code>class MambaModel(GPTModel):\n    \"\"\"Mamba model that extends GPTModel for integration with NeMo.\n\n    Note that the loss calculation is handled by CustomMCoreMambaModel instead.\n    \"\"\"\n\n    @override\n    def get_inference_wrapper(\n        self, params_dtype, inference_batch_times_seqlen_threshold, inference_max_seq_length=8192\n    ) -&gt; GPTInferenceWrapper:\n        \"\"\"Gets the inference wrapper for the Mamba model.\"\"\"\n        from megatron.core.models.mamba import MambaModel as MCoreMambaModel\n\n        # Find MCoreMambaModel instance\n        mcore_model = self.module\n        while mcore_model:\n            if isinstance(mcore_model, (MCoreMambaModel, Evo2StyleMCoreMambaModel)):\n                break\n            mcore_model = getattr(mcore_model, \"module\", None)\n        if mcore_model is None or not isinstance(mcore_model, (MCoreMambaModel, Evo2StyleMCoreMambaModel)):\n            raise ValueError(\"Mamba model instance not found in the model structure.\")\n\n        vocab_size = None\n        if self.tokenizer is not None:\n            vocab_size = self.tokenizer.vocab_size\n        elif hasattr(self.config, \"vocab_size\"):\n            vocab_size = self.config.vocab_size\n        else:\n            raise ValueError(\"Unable to find vocab size.\")\n\n        inference_wrapper_config = InferenceWrapperConfig(\n            hidden_size=mcore_model.config.hidden_size,\n            params_dtype=params_dtype,\n            inference_batch_times_seqlen_threshold=inference_batch_times_seqlen_threshold,\n            padded_vocab_size=vocab_size,\n            inference_max_seq_length=inference_max_seq_length,\n        )\n\n        model_inference_wrapper = GPTInferenceWrapper(mcore_model, inference_wrapper_config)\n        return model_inference_wrapper\n\n    @override\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        position_ids: torch.Tensor,\n        attention_mask: torch.Tensor | None = None,\n        labels: torch.Tensor | None = None,\n        decoder_input: torch.Tensor | None = None,\n        inference_context=None,\n        packed_seq_params=None,\n        inference_params=None,\n        runtime_gather_output: bool | None = None,\n        loss_mask: torch.Tensor | None = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Forward pass that delegates to CustomMCoreMambaModel, which handles loss calculation.\"\"\"\n        extra_kwargs = {\"packed_seq_params\": packed_seq_params} if packed_seq_params is not None else {}\n        output_tensor = self.module(\n            input_ids,\n            position_ids,\n            attention_mask,\n            decoder_input=decoder_input,\n            labels=labels,  # Pass labels to the Megatron module\n            inference_params=inference_params,\n            inference_context=inference_context,\n            runtime_gather_output=runtime_gather_output,\n            loss_mask=loss_mask,  # Pass loss_mask to the Megatron module\n            **extra_kwargs,\n        )\n\n        # Return whatever CustomMCoreMambaModel.forward returns\n        # (logits during inference, loss during training)\n        return output_tensor\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/mamba/#bionemo.evo2.models.mamba.MambaModel.forward","title":"<code>forward(input_ids, position_ids, attention_mask=None, labels=None, decoder_input=None, inference_context=None, packed_seq_params=None, inference_params=None, runtime_gather_output=None, loss_mask=None)</code>","text":"<p>Forward pass that delegates to CustomMCoreMambaModel, which handles loss calculation.</p> Source code in <code>bionemo/evo2/models/mamba.py</code> <pre><code>@override\ndef forward(\n    self,\n    input_ids: torch.Tensor,\n    position_ids: torch.Tensor,\n    attention_mask: torch.Tensor | None = None,\n    labels: torch.Tensor | None = None,\n    decoder_input: torch.Tensor | None = None,\n    inference_context=None,\n    packed_seq_params=None,\n    inference_params=None,\n    runtime_gather_output: bool | None = None,\n    loss_mask: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Forward pass that delegates to CustomMCoreMambaModel, which handles loss calculation.\"\"\"\n    extra_kwargs = {\"packed_seq_params\": packed_seq_params} if packed_seq_params is not None else {}\n    output_tensor = self.module(\n        input_ids,\n        position_ids,\n        attention_mask,\n        decoder_input=decoder_input,\n        labels=labels,  # Pass labels to the Megatron module\n        inference_params=inference_params,\n        inference_context=inference_context,\n        runtime_gather_output=runtime_gather_output,\n        loss_mask=loss_mask,  # Pass loss_mask to the Megatron module\n        **extra_kwargs,\n    )\n\n    # Return whatever CustomMCoreMambaModel.forward returns\n    # (logits during inference, loss during training)\n    return output_tensor\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/mamba/#bionemo.evo2.models.mamba.MambaModel.get_inference_wrapper","title":"<code>get_inference_wrapper(params_dtype, inference_batch_times_seqlen_threshold, inference_max_seq_length=8192)</code>","text":"<p>Gets the inference wrapper for the Mamba model.</p> Source code in <code>bionemo/evo2/models/mamba.py</code> <pre><code>@override\ndef get_inference_wrapper(\n    self, params_dtype, inference_batch_times_seqlen_threshold, inference_max_seq_length=8192\n) -&gt; GPTInferenceWrapper:\n    \"\"\"Gets the inference wrapper for the Mamba model.\"\"\"\n    from megatron.core.models.mamba import MambaModel as MCoreMambaModel\n\n    # Find MCoreMambaModel instance\n    mcore_model = self.module\n    while mcore_model:\n        if isinstance(mcore_model, (MCoreMambaModel, Evo2StyleMCoreMambaModel)):\n            break\n        mcore_model = getattr(mcore_model, \"module\", None)\n    if mcore_model is None or not isinstance(mcore_model, (MCoreMambaModel, Evo2StyleMCoreMambaModel)):\n        raise ValueError(\"Mamba model instance not found in the model structure.\")\n\n    vocab_size = None\n    if self.tokenizer is not None:\n        vocab_size = self.tokenizer.vocab_size\n    elif hasattr(self.config, \"vocab_size\"):\n        vocab_size = self.config.vocab_size\n    else:\n        raise ValueError(\"Unable to find vocab size.\")\n\n    inference_wrapper_config = InferenceWrapperConfig(\n        hidden_size=mcore_model.config.hidden_size,\n        params_dtype=params_dtype,\n        inference_batch_times_seqlen_threshold=inference_batch_times_seqlen_threshold,\n        padded_vocab_size=vocab_size,\n        inference_max_seq_length=inference_max_seq_length,\n    )\n\n    model_inference_wrapper = GPTInferenceWrapper(mcore_model, inference_wrapper_config)\n    return model_inference_wrapper\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/mamba/#bionemo.evo2.models.mamba.mamba_forward_step","title":"<code>mamba_forward_step(model, batch)</code>","text":"<p>Forward step function for Mamba models, similar to hyena_forward_step.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The Mamba model</p> required <code>batch</code> <p>Dictionary containing input batch data</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output from the model forward pass</p> Source code in <code>bionemo/evo2/models/mamba.py</code> <pre><code>def mamba_forward_step(model, batch) -&gt; torch.Tensor:\n    \"\"\"Forward step function for Mamba models, similar to hyena_forward_step.\n\n    Args:\n        model: The Mamba model\n        batch: Dictionary containing input batch data\n\n    Returns:\n        torch.Tensor: Output from the model forward pass\n    \"\"\"\n    forward_args = {\n        \"input_ids\": batch[\"tokens\"],\n        \"position_ids\": batch[\"position_ids\"],\n        \"labels\": batch[\"labels\"],\n        \"loss_mask\": batch[\"loss_mask\"],\n    }\n    forward_args[\"attention_mask\"] = None\n    return model(**forward_args)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/mamba/#bionemo.evo2.models.mamba.mamba_no_weight_decay_cond","title":"<code>mamba_no_weight_decay_cond(name, param, exclude_embeddings=False)</code>","text":"<p>Condition for no weight decay for Mamba parameters.</p> <p>Note that this follows the same pattern as in the original Mamba implementation.</p> Source code in <code>bionemo/evo2/models/mamba.py</code> <pre><code>def mamba_no_weight_decay_cond(name, param, exclude_embeddings: bool = False):\n    \"\"\"Condition for no weight decay for Mamba parameters.\n\n    Note that this follows the same pattern as in the original Mamba implementation.\n    \"\"\"\n    # Mamba-specific parameters that should not have weight decay\n    if (\n        name.endswith(\"dt_bias\")\n        or name.endswith(\"A_log\")\n        or name.endswith(\"D\")\n        or (\"embedding\" in name and exclude_embeddings)\n        or getattr(param, \"_no_weight_decay\", False)\n    ):\n        no_wd = True\n    # All other parameters - use default MCore behavior:\n    # Do not regularize biases and norm parameters\n    # (See megatron.core.optimizer._get_pram_groups)\n    # TODO exclude embeddings\n    else:\n        no_wd = name.endswith(\".bias\") or len(param.shape) == 1\n    return no_wd\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/mamba/#bionemo.evo2.models.mamba.mamba_no_weight_decay_cond_with_embeddings","title":"<code>mamba_no_weight_decay_cond_with_embeddings(name, param)</code>","text":"<p>Condition for no weight decay for Mamba parameters with embeddings.</p> <p>Note that this follows the same pattern as in the original Mamba implementation but also skips WD on embeddings.</p> Source code in <code>bionemo/evo2/models/mamba.py</code> <pre><code>def mamba_no_weight_decay_cond_with_embeddings(name, param):\n    \"\"\"Condition for no weight decay for Mamba parameters with embeddings.\n\n    Note that this follows the same pattern as in the original Mamba implementation but also skips WD on embeddings.\n    \"\"\"\n    return mamba_no_weight_decay_cond(name, param, exclude_embeddings=True)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/peft/","title":"Peft","text":""},{"location":"main/references/API_reference/bionemo/evo2/models/peft/#bionemo.evo2.models.peft.Evo2LoRA","title":"<code>Evo2LoRA</code>","text":"<p>               Bases: <code>LoRA</code></p> <p>LoRA adapter specifically for Evo2/Hyena models.</p> Source code in <code>bionemo/evo2/models/peft.py</code> <pre><code>class Evo2LoRA(LoRA):\n    \"\"\"LoRA adapter specifically for Evo2/Hyena models.\"\"\"\n\n    def __init__(\n        self,\n        peft_ckpt_path: Optional[str] = None,\n        freeze_modules: List[str] = [\"encoder\", \"embedding\"],\n        target_modules: List[str] = [\n            \"linear_qkv\",\n            \"linear_proj\",\n            \"linear_fc1\",\n            \"linear_fc2\",\n            \"short_filter\",  # Short convolution filters\n            \"hyena_filter\",  # Hyena layer filters\n            \"positional_encoding\",  # ROPE or other position encodings\n        ],\n        *args,\n        **kwargs,\n    ):\n        \"\"\"Initialize the LoRA Adapter for Evo2.\n\n        Args:\n            peft_ckpt_path: Path to pre-trained LoRA checkpoint.\n            freeze_modules: List of module names to freeze (Evo2-specific defaults).\n            target_modules: Modules to apply LoRA to (uses Evo2 defaults if None).\n            *args: placeholder.\n            **kwargs:\n                dim: LoRA rank dimension.\n                alpha: LoRA scaling parameter.\n                dropout: Dropout rate for LoRA layers.\n                dropout_position: Where to apply dropout ('pre' or 'post').\n                lora_A_init_method: Initialization for A matrix ('xavier', 'uniform', 'normal').\n                lora_B_init_method: Initialization for B matrix ('zero', 'normal').\n        \"\"\"\n        \"\"\"Initialize the LoRA Adapter for Evo2.\"\"\"\n        super().__init__(target_modules=target_modules, *args, **kwargs)\n        self.freeze_modules = freeze_modules\n        self.peft_ckpt_path = peft_ckpt_path\n\n        # CRITICAL: Set model_transform to self\n        # The callback system expects this attribute\n        self.model_transform = self\n\n    def setup(self, trainer, pl_module, stage):\n        \"\"\"Setup callback - properly initialize transform.\"\"\"\n        super().setup(trainer, pl_module, stage)\n\n        logging.info(f\"Will attempt to apply to model if matches: \\n{self.target_modules}\")\n\n        # Ensure model_transform is set\n        if not hasattr(self, \"model_transform\") or self.model_transform is None:\n            self.model_transform = self\n\n        # Pass checkpoint path to wrapped IO if available\n        if hasattr(self, \"wrapped_io\") and self.peft_ckpt_path:\n            self.wrapped_io.adapter_ckpt_path = self.peft_ckpt_path\n\n    def on_predict_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -&gt; None:\n        \"\"\"Event hook.\n\n        Apply transformations for prediction if needed.\n\n        Args:\n            trainer: The trainer object.\n            pl_module: The LightningModule object.\n        \"\"\"\n        self._maybe_apply_transform(trainer)\n\n    def adapter_key_filter(self, key: str) -&gt; bool:\n        \"\"\"Filter state dict keys to identify adapter parameters.\n\n        Args:\n            key: State dict key to check\n\n        Returns:\n            bool: True if key corresponds to an adapter parameter\n        \"\"\"\n        if isinstance(key, tuple):\n            return key[1].requires_grad\n\n        if \"_extra_state\" in key:\n            return False\n\n        # Check if it's an adapter parameter or not in freeze list\n        return (\n            (not any(substring in key for substring in self.freeze_modules))\n            or \".adapter.\" in key\n            or key.endswith(\".adapters\")\n            or \"lora_A\" in key\n            or \"lora_B\" in key\n        )\n\n    def __call__(self, model: nn.Module) -&gt; nn.Module:\n        \"\"\"Apply LoRA transformations to the model.\n\n        Override to avoid fn.walk compatibility issues.\n        \"\"\"\n        # First, manually freeze specified modules\n        self._apply_selective_freeze(model)\n\n        # Then apply LoRA transformations\n        self._apply_lora_transform(model)\n\n        # THEN freeze ALL base model parameters\n        # This must happen AFTER LoRA is applied\n        self._freeze_base_model_parameters(model)\n\n        # Log summary\n        self._log_lora_summary(model)\n\n        return model\n\n    def _apply_selective_freeze(self, model: nn.Module, prefix=\"\"):\n        \"\"\"Manually walk model and freeze specified modules.\"\"\"\n        for name, child in model.named_children():\n            full_name = f\"{prefix}.{name}\" if prefix else name\n\n            # Check if this module should be frozen\n            if name in self.freeze_modules:\n                logging.info(f\"Freezing module: {full_name}\")\n                for param in child.parameters():\n                    param.requires_grad = False\n\n            # Recursively apply to children\n            self._apply_selective_freeze(child, full_name)\n\n    def _freeze_base_model_parameters(self, model: nn.Module):\n        \"\"\"Freeze all parameters except LoRA adapters and critical layers.\"\"\"\n        logging.info(\"\\nFreezing base model parameters...\")\n        frozen_count = 0\n        kept_trainable = []\n\n        for name, param in model.named_parameters():\n            # Keep LoRA/adapter parameters trainable\n            if any(adapter_term in name for adapter_term in [\"adapter\", \"lora_A\", \"lora_B\", \"lora\"]):\n                param.requires_grad = True\n                kept_trainable.append(name)\n            # CRITICAL: Keep output layer trainable to maintain gradient flow\n            elif \"output_layer\" in name or \"lm_head\" in name:\n                param.requires_grad = True\n                kept_trainable.append(name)\n                logging.info(f\"  Keeping output layer trainable: {name}\")\n            # CRITICAL: Keep final layer norm trainable\n            elif \"final_norm\" in name or (\"decoder\" in name and \"norm\" in name and \"24\" in name):\n                param.requires_grad = True\n                kept_trainable.append(name)\n                logging.info(f\"  Keeping final norm trainable: {name}\")\n            else:\n                param.requires_grad = False\n                frozen_count += 1\n\n        logging.info(f\"Froze {frozen_count} parameter tensors\")\n        logging.info(f\"Kept {len(kept_trainable)} parameters trainable\")\n\n    def _apply_lora_transform(self, model: nn.Module, prefix=\"\"):\n        \"\"\"Apply LoRA with better tracking.\"\"\"\n        # Get all modules in a flat list first\n        modules_to_transform = []\n\n        for name, module in model.named_modules():\n            # Skip if has children (not a leaf module)\n            if list(module.children()):\n                continue\n\n            # Check if this matches our target modules\n            module_type = name.split(\".\")[-1] if \".\" in name else name\n            if module_type in self.target_modules:\n                modules_to_transform.append((name, module))\n\n        logging.info(f\"\\nFound {len(modules_to_transform)} modules to apply LoRA to\")\n\n        # Apply transformations\n        for full_name, module in modules_to_transform:\n            # Get parent and attribute name\n            parts = full_name.split(\".\")\n            parent = model\n            for part in parts[:-1]:\n                parent = getattr(parent, part)\n\n            # Apply transform\n            attr_name = parts[-1]\n            transformed = self.transform(module, name=attr_name, prefix=\"\")\n\n            if transformed is not module:\n                setattr(parent, attr_name, transformed)\n                logging.info(f\"Applied LoRA to: {full_name}\")\n\n                # Verify LoRA was applied\n                if hasattr(transformed, \"adapter\") or hasattr(transformed, \"lora_A\"):\n                    logging.info(f\"  \u2713 LoRA adapter confirmed on {full_name}\")\n\n    def selective_freeze(self, m: nn.Module, name=None, prefix=None):\n        \"\"\"Selectively freeze modules based on freeze_modules list.\n\n        Args:\n            m: Module to potentially freeze.\n            name: Name of the module.\n            prefix: Prefix for the module name.\n\n        Returns:\n            nn.Module: The module (frozen or not).\n        \"\"\"\n        if name in self.freeze_modules:\n            FNMixin.freeze(m)\n            logging.info(f\"Freezing module: {prefix}.{name}\" if prefix else f\"Freezing module: {name}\")\n\n        return m\n\n    # Deepcopy compatibility\n    def __deepcopy__(self, memo):\n        \"\"\"Custom deepcopy to handle unpickleable objects.\"\"\"\n        # Create a new instance with the same parameters\n        cls = self.__class__\n        result = cls.__new__(cls)\n\n        # Copy all attributes except problematic ones\n        memo[id(self)] = result\n        for k, v in self.__dict__.items():\n            if k not in [\"_metadata\", \"_fields\"]:  # Skip dataclass internals\n                try:\n                    setattr(result, k, deepcopy(v, memo))\n                except Exception:\n                    # If deepcopy fails, just use the original reference\n                    setattr(result, k, v)\n\n        return result\n\n    def __getstate__(self):\n        \"\"\"Prepare object for pickling.\"\"\"\n        state = self.__dict__.copy()\n        # Remove unpickleable entries\n        state.pop(\"_metadata\", None)\n        state.pop(\"_fields\", None)\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Restore object from pickle.\"\"\"\n        self.__dict__.update(state)\n\n    # Debug module\n    def _log_lora_summary(self, model: nn.Module):\n        \"\"\"Log a summary of LoRA modifications.\"\"\"\n        total_params = sum(p.numel() for p in model.parameters())\n        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n        adapter_params = sum(p.numel() for n, p in model.named_parameters() if p.requires_grad and \"adapter\" in n)\n\n        logging.info(f\"\\n{'=' * 50}\")\n        logging.info(\"LoRA Summary:\")\n        logging.info(f\"  Total parameters: {total_params:,}\")\n        logging.info(f\"  Trainable parameters: {trainable_params:,}\")\n        logging.info(f\"  Adapter parameters: {adapter_params:,}\")  # Changed from \"LoRA parameters\"\n        logging.info(f\"  Percentage trainable: {100 * trainable_params / total_params:.2f}%\")\n        logging.info(f\"  Percentage adapters: {100 * adapter_params / total_params:.2f}%\")\n        logging.info(f\"{'=' * 50}\\n\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/peft/#bionemo.evo2.models.peft.Evo2LoRA.__call__","title":"<code>__call__(model)</code>","text":"<p>Apply LoRA transformations to the model.</p> <p>Override to avoid fn.walk compatibility issues.</p> Source code in <code>bionemo/evo2/models/peft.py</code> <pre><code>def __call__(self, model: nn.Module) -&gt; nn.Module:\n    \"\"\"Apply LoRA transformations to the model.\n\n    Override to avoid fn.walk compatibility issues.\n    \"\"\"\n    # First, manually freeze specified modules\n    self._apply_selective_freeze(model)\n\n    # Then apply LoRA transformations\n    self._apply_lora_transform(model)\n\n    # THEN freeze ALL base model parameters\n    # This must happen AFTER LoRA is applied\n    self._freeze_base_model_parameters(model)\n\n    # Log summary\n    self._log_lora_summary(model)\n\n    return model\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/peft/#bionemo.evo2.models.peft.Evo2LoRA.__deepcopy__","title":"<code>__deepcopy__(memo)</code>","text":"<p>Custom deepcopy to handle unpickleable objects.</p> Source code in <code>bionemo/evo2/models/peft.py</code> <pre><code>def __deepcopy__(self, memo):\n    \"\"\"Custom deepcopy to handle unpickleable objects.\"\"\"\n    # Create a new instance with the same parameters\n    cls = self.__class__\n    result = cls.__new__(cls)\n\n    # Copy all attributes except problematic ones\n    memo[id(self)] = result\n    for k, v in self.__dict__.items():\n        if k not in [\"_metadata\", \"_fields\"]:  # Skip dataclass internals\n            try:\n                setattr(result, k, deepcopy(v, memo))\n            except Exception:\n                # If deepcopy fails, just use the original reference\n                setattr(result, k, v)\n\n    return result\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/peft/#bionemo.evo2.models.peft.Evo2LoRA.__getstate__","title":"<code>__getstate__()</code>","text":"<p>Prepare object for pickling.</p> Source code in <code>bionemo/evo2/models/peft.py</code> <pre><code>def __getstate__(self):\n    \"\"\"Prepare object for pickling.\"\"\"\n    state = self.__dict__.copy()\n    # Remove unpickleable entries\n    state.pop(\"_metadata\", None)\n    state.pop(\"_fields\", None)\n    return state\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/peft/#bionemo.evo2.models.peft.Evo2LoRA.__init__","title":"<code>__init__(peft_ckpt_path=None, freeze_modules=['encoder', 'embedding'], target_modules=['linear_qkv', 'linear_proj', 'linear_fc1', 'linear_fc2', 'short_filter', 'hyena_filter', 'positional_encoding'], *args, **kwargs)</code>","text":"<p>Initialize the LoRA Adapter for Evo2.</p> <p>Parameters:</p> Name Type Description Default <code>peft_ckpt_path</code> <code>Optional[str]</code> <p>Path to pre-trained LoRA checkpoint.</p> <code>None</code> <code>freeze_modules</code> <code>List[str]</code> <p>List of module names to freeze (Evo2-specific defaults).</p> <code>['encoder', 'embedding']</code> <code>target_modules</code> <code>List[str]</code> <p>Modules to apply LoRA to (uses Evo2 defaults if None).</p> <code>['linear_qkv', 'linear_proj', 'linear_fc1', 'linear_fc2', 'short_filter', 'hyena_filter', 'positional_encoding']</code> <code>*args</code> <p>placeholder.</p> <code>()</code> <code>**kwargs</code> <p>dim: LoRA rank dimension. alpha: LoRA scaling parameter. dropout: Dropout rate for LoRA layers. dropout_position: Where to apply dropout ('pre' or 'post'). lora_A_init_method: Initialization for A matrix ('xavier', 'uniform', 'normal'). lora_B_init_method: Initialization for B matrix ('zero', 'normal').</p> <code>{}</code> Source code in <code>bionemo/evo2/models/peft.py</code> <pre><code>def __init__(\n    self,\n    peft_ckpt_path: Optional[str] = None,\n    freeze_modules: List[str] = [\"encoder\", \"embedding\"],\n    target_modules: List[str] = [\n        \"linear_qkv\",\n        \"linear_proj\",\n        \"linear_fc1\",\n        \"linear_fc2\",\n        \"short_filter\",  # Short convolution filters\n        \"hyena_filter\",  # Hyena layer filters\n        \"positional_encoding\",  # ROPE or other position encodings\n    ],\n    *args,\n    **kwargs,\n):\n    \"\"\"Initialize the LoRA Adapter for Evo2.\n\n    Args:\n        peft_ckpt_path: Path to pre-trained LoRA checkpoint.\n        freeze_modules: List of module names to freeze (Evo2-specific defaults).\n        target_modules: Modules to apply LoRA to (uses Evo2 defaults if None).\n        *args: placeholder.\n        **kwargs:\n            dim: LoRA rank dimension.\n            alpha: LoRA scaling parameter.\n            dropout: Dropout rate for LoRA layers.\n            dropout_position: Where to apply dropout ('pre' or 'post').\n            lora_A_init_method: Initialization for A matrix ('xavier', 'uniform', 'normal').\n            lora_B_init_method: Initialization for B matrix ('zero', 'normal').\n    \"\"\"\n    \"\"\"Initialize the LoRA Adapter for Evo2.\"\"\"\n    super().__init__(target_modules=target_modules, *args, **kwargs)\n    self.freeze_modules = freeze_modules\n    self.peft_ckpt_path = peft_ckpt_path\n\n    # CRITICAL: Set model_transform to self\n    # The callback system expects this attribute\n    self.model_transform = self\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/peft/#bionemo.evo2.models.peft.Evo2LoRA.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Restore object from pickle.</p> Source code in <code>bionemo/evo2/models/peft.py</code> <pre><code>def __setstate__(self, state):\n    \"\"\"Restore object from pickle.\"\"\"\n    self.__dict__.update(state)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/peft/#bionemo.evo2.models.peft.Evo2LoRA.adapter_key_filter","title":"<code>adapter_key_filter(key)</code>","text":"<p>Filter state dict keys to identify adapter parameters.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>State dict key to check</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if key corresponds to an adapter parameter</p> Source code in <code>bionemo/evo2/models/peft.py</code> <pre><code>def adapter_key_filter(self, key: str) -&gt; bool:\n    \"\"\"Filter state dict keys to identify adapter parameters.\n\n    Args:\n        key: State dict key to check\n\n    Returns:\n        bool: True if key corresponds to an adapter parameter\n    \"\"\"\n    if isinstance(key, tuple):\n        return key[1].requires_grad\n\n    if \"_extra_state\" in key:\n        return False\n\n    # Check if it's an adapter parameter or not in freeze list\n    return (\n        (not any(substring in key for substring in self.freeze_modules))\n        or \".adapter.\" in key\n        or key.endswith(\".adapters\")\n        or \"lora_A\" in key\n        or \"lora_B\" in key\n    )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/peft/#bionemo.evo2.models.peft.Evo2LoRA.on_predict_epoch_start","title":"<code>on_predict_epoch_start(trainer, pl_module)</code>","text":"<p>Event hook.</p> <p>Apply transformations for prediction if needed.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The trainer object.</p> required <code>pl_module</code> <code>LightningModule</code> <p>The LightningModule object.</p> required Source code in <code>bionemo/evo2/models/peft.py</code> <pre><code>def on_predict_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -&gt; None:\n    \"\"\"Event hook.\n\n    Apply transformations for prediction if needed.\n\n    Args:\n        trainer: The trainer object.\n        pl_module: The LightningModule object.\n    \"\"\"\n    self._maybe_apply_transform(trainer)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/peft/#bionemo.evo2.models.peft.Evo2LoRA.selective_freeze","title":"<code>selective_freeze(m, name=None, prefix=None)</code>","text":"<p>Selectively freeze modules based on freeze_modules list.</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>Module</code> <p>Module to potentially freeze.</p> required <code>name</code> <p>Name of the module.</p> <code>None</code> <code>prefix</code> <p>Prefix for the module name.</p> <code>None</code> <p>Returns:</p> Type Description <p>nn.Module: The module (frozen or not).</p> Source code in <code>bionemo/evo2/models/peft.py</code> <pre><code>def selective_freeze(self, m: nn.Module, name=None, prefix=None):\n    \"\"\"Selectively freeze modules based on freeze_modules list.\n\n    Args:\n        m: Module to potentially freeze.\n        name: Name of the module.\n        prefix: Prefix for the module name.\n\n    Returns:\n        nn.Module: The module (frozen or not).\n    \"\"\"\n    if name in self.freeze_modules:\n        FNMixin.freeze(m)\n        logging.info(f\"Freezing module: {prefix}.{name}\" if prefix else f\"Freezing module: {name}\")\n\n    return m\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/models/peft/#bionemo.evo2.models.peft.Evo2LoRA.setup","title":"<code>setup(trainer, pl_module, stage)</code>","text":"<p>Setup callback - properly initialize transform.</p> Source code in <code>bionemo/evo2/models/peft.py</code> <pre><code>def setup(self, trainer, pl_module, stage):\n    \"\"\"Setup callback - properly initialize transform.\"\"\"\n    super().setup(trainer, pl_module, stage)\n\n    logging.info(f\"Will attempt to apply to model if matches: \\n{self.target_modules}\")\n\n    # Ensure model_transform is set\n    if not hasattr(self, \"model_transform\") or self.model_transform is None:\n        self.model_transform = self\n\n    # Pass checkpoint path to wrapped IO if available\n    if hasattr(self, \"wrapped_io\") and self.peft_ckpt_path:\n        self.wrapped_io.adapter_ckpt_path = self.peft_ckpt_path\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/run/infer/","title":"Infer","text":""},{"location":"main/references/API_reference/bionemo/evo2/run/infer/#bionemo.evo2.run.infer.infer","title":"<code>infer(prompt, ckpt_dir, temperature, top_k, top_p, max_new_tokens, tensor_parallel_size, pipeline_model_parallel_size, context_parallel_size, output_file=None, ckpt_format='torch_dist', seed=None, vortex_style_fp8=False, flash_decode=False, return_log_probs=False)</code>","text":"<p>Inference workflow for Evo2.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Prompt to generate text from Evo2.</p> required <code>ckpt_dir</code> <code>str</code> <p>Path to checkpoint directory containing pre-trained Evo2 model.</p> required <code>temperature</code> <code>float</code> <p>Temperature during sampling for generation.</p> required <code>top_k</code> <code>int</code> <p>Top K during sampling for generation.</p> required <code>top_p</code> <code>float</code> <p>Top P during sampling for generation.</p> required <code>max_new_tokens</code> <code>int</code> <p>Maximum number of tokens to generate.</p> required <code>tensor_parallel_size</code> <code>int</code> <p>Order of tensor parallelism.</p> required <code>pipeline_model_parallel_size</code> <code>int</code> <p>Order of pipeline parallelism.</p> required <code>context_parallel_size</code> <code>int</code> <p>Order of context parallelism.</p> required <code>output_file</code> <code>str</code> <p>Output file containing the generated text produced by the Evo2 model.</p> <code>None</code> <code>ckpt_format</code> <code>CheckpointFormats</code> <p>Checkpoint format to use.</p> <code>'torch_dist'</code> <code>seed</code> <code>int</code> <p>Random seed for generation.</p> <code>None</code> <code>vortex_style_fp8</code> <code>bool</code> <p>Whether to use vortex style FP8.</p> <code>False</code> <code>flash_decode</code> <code>bool</code> <p>Whether to use flash decode.</p> <code>False</code> <code>return_log_probs</code> <code>bool</code> <p>Whether to return log probabilities.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[InferenceRequest]</code> <p>None</p> Source code in <code>bionemo/evo2/run/infer.py</code> <pre><code>def infer(\n    prompt: str,\n    ckpt_dir: str,\n    temperature: float,\n    top_k: int,\n    top_p: float,\n    max_new_tokens: int,\n    tensor_parallel_size: int,\n    pipeline_model_parallel_size: int,\n    context_parallel_size: int,\n    output_file: Optional[str] = None,\n    ckpt_format: CheckpointFormats = \"torch_dist\",\n    seed: Optional[int] = None,\n    vortex_style_fp8: bool = False,\n    flash_decode: bool = False,\n    return_log_probs: bool = False,\n) -&gt; list[InferenceRequest]:\n    \"\"\"Inference workflow for Evo2.\n\n    Args:\n        prompt (str): Prompt to generate text from Evo2.\n        ckpt_dir (str): Path to checkpoint directory containing pre-trained Evo2 model.\n        temperature (float): Temperature during sampling for generation.\n        top_k (int): Top K during sampling for generation.\n        top_p (float): Top P during sampling for generation.\n        max_new_tokens (int): Maximum number of tokens to generate.\n        tensor_parallel_size (int): Order of tensor parallelism.\n        pipeline_model_parallel_size (int): Order of pipeline parallelism.\n        context_parallel_size (int): Order of context parallelism.\n        output_file (str): Output file containing the generated text produced by the Evo2 model.\n        ckpt_format (CheckpointFormats): Checkpoint format to use.\n        seed (int): Random seed for generation.\n        vortex_style_fp8 (bool): Whether to use vortex style FP8.\n        flash_decode (bool): Whether to use flash decode.\n        return_log_probs (bool): Whether to return log probabilities.\n\n    Returns:\n        None\n    \"\"\"\n    model_parallel_size = tensor_parallel_size * pipeline_model_parallel_size * context_parallel_size\n    if model_parallel_size &gt; torch.cuda.device_count():\n        raise ValueError(\n            f\"Requested model parallel size {model_parallel_size} is greater than the \"\n            f\"number of available CUDA devices {torch.cuda.device_count()}\"\n        )\n    # Create PTL trainer.\n    trainer = nl.Trainer(\n        accelerator=\"gpu\",\n        devices=model_parallel_size,\n        strategy=nl.MegatronStrategy(\n            tensor_model_parallel_size=tensor_parallel_size,\n            pipeline_model_parallel_size=pipeline_model_parallel_size,\n            context_parallel_size=context_parallel_size,\n            pipeline_dtype=torch.bfloat16,\n            ckpt_load_optimizer=False,  # Needs to be false for a normal model checkpoint.\n            ckpt_save_optimizer=False,\n            ckpt_async_save=False,\n            save_ckpt_format=ckpt_format,\n            ckpt_load_strictness=\"log_all\",\n        ),\n        log_every_n_steps=1,\n        limit_val_batches=10,\n        num_sanity_val_steps=0,\n        plugins=nl.MegatronMixedPrecision(\n            precision=\"bf16-mixed\",\n            params_dtype=torch.bfloat16,\n        ),\n    )\n    inference_wrapped_model, mcore_tokenizer = inference.setup_model_and_tokenizer(\n        path=ckpt_dir,\n        trainer=trainer,\n        params_dtype=torch.bfloat16,\n        inference_batch_times_seqlen_threshold=8192,  # TODO\n        inference_max_seq_length=8192,  # TODO\n        recompute_granularity=None,\n        recompute_num_layers=None,\n        recompute_method=None,\n        vortex_style_fp8=vortex_style_fp8,\n        flash_decode=flash_decode,\n        enable_flash_decode=flash_decode,\n    )\n    t0 = time.perf_counter_ns()\n    # TODO: fix return type in NeMo inference.generate (it is a list[InferenceRequest] not a dict)\n    results: list[InferenceRequest] = inference.generate(\n        model=inference_wrapped_model,\n        max_batch_size=1,  # vortex only supports batch size 1\n        tokenizer=mcore_tokenizer,\n        prompts=[prompt],\n        random_seed=seed,\n        inference_params=CommonInferenceParams(\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            return_log_probs=return_log_probs,\n            num_tokens_to_generate=max_new_tokens,\n        ),\n    )\n    dt = (time.perf_counter_ns() - t0) / 1e9  # seconds\n    tokens_per_sec = (len(results[0].generated_text) + 1) / dt  # +1 for the prompt\n\n    print(f\"Inference time: {dt} seconds, {tokens_per_sec} tokens/sec\", file=sys.stderr)\n    if torch.distributed.get_rank() == 0:\n        if output_file is None:\n            logging.info(results)\n        else:\n            with open(output_file, \"w\") as f:\n                f.write(f\"{results[0]}\\n\")\n\n    return results\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/run/infer/#bionemo.evo2.run.infer.main","title":"<code>main()</code>","text":"<p>Main function for Evo2 inference.</p> Source code in <code>bionemo/evo2/run/infer.py</code> <pre><code>def main():\n    \"\"\"Main function for Evo2 inference.\"\"\"\n    # Parse args.\n    args = parse_args()\n    infer(\n        prompt=args.prompt,\n        ckpt_dir=args.ckpt_dir,\n        temperature=args.temperature,\n        top_k=args.top_k,\n        top_p=args.top_p,\n        max_new_tokens=args.max_new_tokens,\n        tensor_parallel_size=args.tensor_parallel_size,\n        pipeline_model_parallel_size=args.pipeline_model_parallel_size,\n        context_parallel_size=args.context_parallel_size,\n        output_file=args.output_file,\n        ckpt_format=args.ckpt_format,\n        seed=args.seed,\n        vortex_style_fp8=args.fp8,  # Vortex only applied FP8 to some layers.\n        flash_decode=args.flash_decode,\n    )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/run/infer/#bionemo.evo2.run.infer.parse_args","title":"<code>parse_args()</code>","text":"<p>Parse arguments for Evo2 inference.</p> Source code in <code>bionemo/evo2/run/infer.py</code> <pre><code>def parse_args():\n    \"\"\"Parse arguments for Evo2 inference.\"\"\"\n    ap = argparse.ArgumentParser()\n\n    # generation args:\n    default_prompt = (\n        \"|d__Bacteria;\"\n        + \"p__Pseudomonadota;\"\n        + \"c__Gammaproteobacteria;\"\n        + \"o__Enterobacterales;\"\n        + \"f__Enterobacteriaceae;\"\n        + \"g__Escherichia;\"\n        + \"s__Escherichia|\"\n    )\n    ap.add_argument(\n        \"--prompt\",\n        type=str,\n        default=default_prompt,\n        help=\"Prompt to generate text from Evo2. Defaults to a phylogenetic lineage tag for E coli.\",\n    )\n    ap.add_argument(\n        \"--ckpt-dir\", type=str, required=True, help=\"Path to checkpoint directory containing pre-trained Evo2 model.\"\n    )\n    ap.add_argument(\"--temperature\", type=float, default=1.0, help=\"Temperature during sampling for generation.\")\n    ap.add_argument(\"--top-k\", type=int, default=0, help=\"Top K during sampling for generation.\")\n    ap.add_argument(\"--top-p\", type=float, default=0.0, help=\"Top P during sampling for generation.\")\n    ap.add_argument(\"--max-new-tokens\", type=int, default=1024, help=\"Maximum number of tokens to generate.\")\n    ap.add_argument(\"--seed\", type=int, default=None, help=\"Random seed for generation.\")\n    # compute args:\n    ap.add_argument(\"--tensor-parallel-size\", type=int, default=1, help=\"Order of tensor parallelism. Defaults to 1.\")\n    ap.add_argument(\n        \"--pipeline-model-parallel-size\", type=int, default=1, help=\"Order of pipeline parallelism. Defaults to 1.\"\n    )\n    ap.add_argument(\n        \"--context-parallel-size\", type=int, default=1, help=\"Order of context parallelism. Defaults to 1.\"\n    )\n    # output args:\n    ap.add_argument(\n        \"--output-file\",\n        type=str,\n        default=None,\n        help=\"Output file containing the generated text produced by the Evo2 model. If not provided, the output will be logged.\",\n    )\n    # extra:\n    ap.add_argument(\n        \"--ckpt-format\",\n        type=str,\n        choices=[\"torch_dist\", \"zarr\"],\n        default=\"torch_dist\",\n        help=\"Specify checkpoint format to use. Defaults to 'torch_dist', as 'zarr' is deprecated.\",\n    )\n    ap.add_argument(\n        \"--fp8\",\n        action=\"store_true\",\n        default=False,\n        help=\"Whether to use vortex style FP8. Defaults to False.\",\n    )\n    ap.add_argument(\n        \"--flash-decode\",\n        action=\"store_true\",\n        default=False,\n        help=\"Whether to use flash decode. Defaults to True.\",\n    )\n    return ap.parse_args()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/run/predict/","title":"Predict","text":""},{"location":"main/references/API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.BasePredictor","title":"<code>BasePredictor</code>","text":"<p>               Bases: <code>LightningPassthroughPredictionMixin</code></p> <p>Base predictor for GPT-style models.</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>class BasePredictor(LightningPassthroughPredictionMixin):\n    \"\"\"Base predictor for GPT-style models.\"\"\"\n\n    def __init__(\n        self,\n        *args,\n        output_log_prob_seqs: bool = False,\n        include_tokens_with_logprob_seqs: bool = False,\n        log_prob_collapse_option: Literal[\"sum\", \"mean\", \"per_token\"] = \"mean\",\n        **kwargs,\n    ):\n        \"\"\"Initialize the base predictor with arguments needed for writing predictions.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.output_log_prob_seqs = output_log_prob_seqs\n        self.log_prob_collapse_option = log_prob_collapse_option\n        self.include_tokens_with_logprob_seqs = include_tokens_with_logprob_seqs\n        self.shuffle_warning_raised = False\n\n    def predict_step(\n        self, batch, batch_idx: int | None = None, to_cpu: bool = True\n    ) -&gt; Tensor | dict[str, Tensor] | None:\n        \"\"\"Alias for forward_step, also log the pad mask since sequences may not all have the same length.\"\"\"\n        if len(batch) == 0:\n            return\n        assert self.training is False, \"predict_step should be called in eval mode\"\n        with torch.no_grad():\n            forward_out = self.forward_step(batch)\n        if not parallel_state.is_pipeline_last_stage():\n            return None\n        # Reminder: the model's predictions for input i land at output i+1. To get everything to align, we prepend the\n        # EOS token to the input sequences and take the outputs for all but the first token.\n        forward_out_tp_gathered = _gather_along_last_dim(\n            forward_out, group=parallel_state.get_tensor_model_parallel_group()\n        )\n\n        forward_out_gathered = _gather_along_cp_dim(forward_out_tp_gathered)\n        loss_mask_gathered = _gather_along_cp_dim(batch[\"loss_mask\"])\n        tokens_gathered = _gather_along_cp_dim(batch[\"tokens\"])\n        cp_group_size = max(parallel_state.get_context_parallel_world_size(), 1)\n        assert self.tokenizer.vocab_size == forward_out_gathered.shape[-1]\n        to_cpu_fn = _to_cpu if to_cpu else _identity\n        if self.output_log_prob_seqs:\n            if self.log_prob_collapse_option == \"per_token\" and cp_group_size &gt; 1 and not self.shuffle_warning_raised:\n                logger.warning(SHUFFLE_MESSAGE)\n                self.shuffle_warning_raised = True\n            softmax_logprobs = torch.log_softmax(forward_out_gathered, dim=-1)\n            softmax_logprobs = softmax_logprobs[:, :-1]\n            input_ids = tokens_gathered[:, 1:]\n            if softmax_logprobs.shape[1] != input_ids.shape[1]:\n                raise RuntimeError(\n                    f\"Softmax logprobs shape {softmax_logprobs.shape} does not match input ids shape {input_ids.shape}\"\n                )\n\n            logprobs = torch.gather(\n                softmax_logprobs,  # Gather likelihoods...\n                2,  # along the vocab dimension...\n                input_ids.unsqueeze(-1),  # using the token ids to index.\n            ).squeeze(-1)\n            log_prob_per_token = logprobs * loss_mask_gathered[:, 1:].float()\n            if self.log_prob_collapse_option == \"per_token\":\n                return to_cpu_fn(\n                    {\n                        \"log_probs_seqs\": log_prob_per_token,\n                        \"seq_idx\": batch[\"seq_idx\"],\n                        \"loss_mask\": loss_mask_gathered[:, 1:],\n                    }\n                )\n            else:\n                log_prob_seqs = torch.sum(log_prob_per_token, dim=1)\n                if self.log_prob_collapse_option == \"mean\":\n                    log_prob_seqs = log_prob_seqs / torch.clamp(loss_mask_gathered[:, 1:].float().sum(dim=-1), min=1.0)\n                return to_cpu_fn({\"log_probs_seqs\": log_prob_seqs, \"seq_idx\": batch[\"seq_idx\"]})\n        else:\n            # If the user wants to match back to logits, then they will need to do the offsetting logic themselves.\n            if cp_group_size &gt; 1 and not self.shuffle_warning_raised:\n                logger.warning(SHUFFLE_MESSAGE)\n                self.shuffle_warning_raised = True\n            logprob_seqs_result = {\n                \"token_logits\": forward_out_gathered,\n                \"pad_mask\": loss_mask_gathered,\n                \"seq_idx\": batch[\"seq_idx\"],\n            }\n            if self.include_tokens_with_logprob_seqs:\n                logprob_seqs_result[\"tokens\"] = tokens_gathered\n            # Note, to match up tokens with logprobs, you need to offset by 1. Eg something like this:\n            #  shifted_token_logits = token_logits[:, :-1]\n            #  shifted_pad_mask = pad_mask[:, 1:]\n            #  shifted_tokens = tokens[:, 1:]\n            return to_cpu_fn(logprob_seqs_result)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.BasePredictor.__init__","title":"<code>__init__(*args, output_log_prob_seqs=False, include_tokens_with_logprob_seqs=False, log_prob_collapse_option='mean', **kwargs)</code>","text":"<p>Initialize the base predictor with arguments needed for writing predictions.</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    output_log_prob_seqs: bool = False,\n    include_tokens_with_logprob_seqs: bool = False,\n    log_prob_collapse_option: Literal[\"sum\", \"mean\", \"per_token\"] = \"mean\",\n    **kwargs,\n):\n    \"\"\"Initialize the base predictor with arguments needed for writing predictions.\"\"\"\n    super().__init__(*args, **kwargs)\n    self.output_log_prob_seqs = output_log_prob_seqs\n    self.log_prob_collapse_option = log_prob_collapse_option\n    self.include_tokens_with_logprob_seqs = include_tokens_with_logprob_seqs\n    self.shuffle_warning_raised = False\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.BasePredictor.predict_step","title":"<code>predict_step(batch, batch_idx=None, to_cpu=True)</code>","text":"<p>Alias for forward_step, also log the pad mask since sequences may not all have the same length.</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>def predict_step(\n    self, batch, batch_idx: int | None = None, to_cpu: bool = True\n) -&gt; Tensor | dict[str, Tensor] | None:\n    \"\"\"Alias for forward_step, also log the pad mask since sequences may not all have the same length.\"\"\"\n    if len(batch) == 0:\n        return\n    assert self.training is False, \"predict_step should be called in eval mode\"\n    with torch.no_grad():\n        forward_out = self.forward_step(batch)\n    if not parallel_state.is_pipeline_last_stage():\n        return None\n    # Reminder: the model's predictions for input i land at output i+1. To get everything to align, we prepend the\n    # EOS token to the input sequences and take the outputs for all but the first token.\n    forward_out_tp_gathered = _gather_along_last_dim(\n        forward_out, group=parallel_state.get_tensor_model_parallel_group()\n    )\n\n    forward_out_gathered = _gather_along_cp_dim(forward_out_tp_gathered)\n    loss_mask_gathered = _gather_along_cp_dim(batch[\"loss_mask\"])\n    tokens_gathered = _gather_along_cp_dim(batch[\"tokens\"])\n    cp_group_size = max(parallel_state.get_context_parallel_world_size(), 1)\n    assert self.tokenizer.vocab_size == forward_out_gathered.shape[-1]\n    to_cpu_fn = _to_cpu if to_cpu else _identity\n    if self.output_log_prob_seqs:\n        if self.log_prob_collapse_option == \"per_token\" and cp_group_size &gt; 1 and not self.shuffle_warning_raised:\n            logger.warning(SHUFFLE_MESSAGE)\n            self.shuffle_warning_raised = True\n        softmax_logprobs = torch.log_softmax(forward_out_gathered, dim=-1)\n        softmax_logprobs = softmax_logprobs[:, :-1]\n        input_ids = tokens_gathered[:, 1:]\n        if softmax_logprobs.shape[1] != input_ids.shape[1]:\n            raise RuntimeError(\n                f\"Softmax logprobs shape {softmax_logprobs.shape} does not match input ids shape {input_ids.shape}\"\n            )\n\n        logprobs = torch.gather(\n            softmax_logprobs,  # Gather likelihoods...\n            2,  # along the vocab dimension...\n            input_ids.unsqueeze(-1),  # using the token ids to index.\n        ).squeeze(-1)\n        log_prob_per_token = logprobs * loss_mask_gathered[:, 1:].float()\n        if self.log_prob_collapse_option == \"per_token\":\n            return to_cpu_fn(\n                {\n                    \"log_probs_seqs\": log_prob_per_token,\n                    \"seq_idx\": batch[\"seq_idx\"],\n                    \"loss_mask\": loss_mask_gathered[:, 1:],\n                }\n            )\n        else:\n            log_prob_seqs = torch.sum(log_prob_per_token, dim=1)\n            if self.log_prob_collapse_option == \"mean\":\n                log_prob_seqs = log_prob_seqs / torch.clamp(loss_mask_gathered[:, 1:].float().sum(dim=-1), min=1.0)\n            return to_cpu_fn({\"log_probs_seqs\": log_prob_seqs, \"seq_idx\": batch[\"seq_idx\"]})\n    else:\n        # If the user wants to match back to logits, then they will need to do the offsetting logic themselves.\n        if cp_group_size &gt; 1 and not self.shuffle_warning_raised:\n            logger.warning(SHUFFLE_MESSAGE)\n            self.shuffle_warning_raised = True\n        logprob_seqs_result = {\n            \"token_logits\": forward_out_gathered,\n            \"pad_mask\": loss_mask_gathered,\n            \"seq_idx\": batch[\"seq_idx\"],\n        }\n        if self.include_tokens_with_logprob_seqs:\n            logprob_seqs_result[\"tokens\"] = tokens_gathered\n        # Note, to match up tokens with logprobs, you need to offset by 1. Eg something like this:\n        #  shifted_token_logits = token_logits[:, :-1]\n        #  shifted_pad_mask = pad_mask[:, 1:]\n        #  shifted_tokens = tokens[:, 1:]\n        return to_cpu_fn(logprob_seqs_result)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.HyenaPredictor","title":"<code>HyenaPredictor</code>","text":"<p>               Bases: <code>BasePredictor</code>, <code>HyenaModel</code></p> <p>A predictor for the Hyena model. This adds in the predict step and the passthrough method.</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>class HyenaPredictor(BasePredictor, HyenaModel):\n    \"\"\"A predictor for the Hyena model. This adds in the predict step and the passthrough method.\"\"\"\n\n    def configure_model(self, *args, **kwargs) -&gt; None:\n        \"\"\"Configure the model.\"\"\"\n        super().configure_model(*args, **kwargs)\n        self.trainer.strategy._init_model_parallel = True\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.HyenaPredictor.configure_model","title":"<code>configure_model(*args, **kwargs)</code>","text":"<p>Configure the model.</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>def configure_model(self, *args, **kwargs) -&gt; None:\n    \"\"\"Configure the model.\"\"\"\n    super().configure_model(*args, **kwargs)\n    self.trainer.strategy._init_model_parallel = True\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.LlamaPredictor","title":"<code>LlamaPredictor</code>","text":"<p>               Bases: <code>BasePredictor</code>, <code>GPTModel</code></p> <p>Llama model for prediction with additional metrics.</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>class LlamaPredictor(BasePredictor, GPTModel):\n    \"\"\"Llama model for prediction with additional metrics.\"\"\"\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.MambaPredictor","title":"<code>MambaPredictor</code>","text":"<p>               Bases: <code>BasePredictor</code>, <code>MambaModel</code></p> <p>Mamba model for prediction with additional metrics.</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>class MambaPredictor(BasePredictor, MambaModel):\n    \"\"\"Mamba model for prediction with additional metrics.\"\"\"\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.PredictDataModule","title":"<code>PredictDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>Create a dataloader for prediction.</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>class PredictDataModule(LightningDataModule):\n    \"\"\"Create a dataloader for prediction.\"\"\"\n\n    def __init__(\n        self,\n        dataset: torch.utils.data.Dataset,\n        batch_size: int = 1,\n        tokenizer=None,\n        min_length: int | None = None,\n    ):\n        \"\"\"Create a dataloader for prediction.\"\"\"\n        super().__init__()\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.tokenizer = tokenizer\n        self.min_length = min_length\n        default_pad_id = 0\n        self.pad_token_id = getattr(tokenizer, \"pad_id\", default_pad_id) if tokenizer is not None else default_pad_id\n\n    def setup(self, stage: str | None = None) -&gt; None:\n        \"\"\"Set up the dataloader.\"\"\"\n        pass\n\n    def predict_dataloader(self):\n        \"\"\"Create a dataloader for prediction.\"\"\"\n        # need to use this to communicate that we are in predict mode and safe to not drop last batch\n        return WrappedDataLoader(\n            mode=\"predict\",\n            dataset=self.dataset,\n            batch_size=self.batch_size,\n            num_workers=8,\n            shuffle=False,\n            drop_last=False,\n            collate_fn=functools.partial(\n                collate.padding_collate_fn,\n                padding_values={\"tokens\": self.pad_token_id, \"position_ids\": self.pad_token_id, \"loss_mask\": False},\n                min_length=self.min_length,\n                max_length=None,\n            ),\n        )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.PredictDataModule.__init__","title":"<code>__init__(dataset, batch_size=1, tokenizer=None, min_length=None)</code>","text":"<p>Create a dataloader for prediction.</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>def __init__(\n    self,\n    dataset: torch.utils.data.Dataset,\n    batch_size: int = 1,\n    tokenizer=None,\n    min_length: int | None = None,\n):\n    \"\"\"Create a dataloader for prediction.\"\"\"\n    super().__init__()\n    self.dataset = dataset\n    self.batch_size = batch_size\n    self.tokenizer = tokenizer\n    self.min_length = min_length\n    default_pad_id = 0\n    self.pad_token_id = getattr(tokenizer, \"pad_id\", default_pad_id) if tokenizer is not None else default_pad_id\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.PredictDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Create a dataloader for prediction.</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>def predict_dataloader(self):\n    \"\"\"Create a dataloader for prediction.\"\"\"\n    # need to use this to communicate that we are in predict mode and safe to not drop last batch\n    return WrappedDataLoader(\n        mode=\"predict\",\n        dataset=self.dataset,\n        batch_size=self.batch_size,\n        num_workers=8,\n        shuffle=False,\n        drop_last=False,\n        collate_fn=functools.partial(\n            collate.padding_collate_fn,\n            padding_values={\"tokens\": self.pad_token_id, \"position_ids\": self.pad_token_id, \"loss_mask\": False},\n            min_length=self.min_length,\n            max_length=None,\n        ),\n    )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.PredictDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Set up the dataloader.</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>def setup(self, stage: str | None = None) -&gt; None:\n    \"\"\"Set up the dataloader.\"\"\"\n    pass\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.hyena_predict_data_step","title":"<code>hyena_predict_data_step(dataloader_iter)</code>","text":"<p>Data step for the Hyena model prediction. Modified from the original gpt data step to include the seq_idx.</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>def hyena_predict_data_step(dataloader_iter) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Data step for the Hyena model prediction. Modified from the original gpt data step to include the seq_idx.\"\"\"\n    from megatron.core import parallel_state\n\n    # Based on: https://github.com/NVIDIA/Megatron-LM/blob/main/pretrain_gpt.py#L87\n    # https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L828-L842\n\n    batch = next(dataloader_iter)\n\n    _batch: dict\n    if isinstance(batch, tuple) and len(batch) == 3:\n        _batch = batch[0]\n    else:\n        _batch = batch\n\n    required_device_keys = set()\n    required_host_keys = set()\n\n    required_device_keys.add(\"attention_mask\")\n    if \"cu_seqlens\" in _batch:\n        required_device_keys.add(\"cu_seqlens\")\n        required_host_keys.add(\"cu_seqlens_argmin\")\n        required_host_keys.add(\"max_seqlen\")\n\n    if parallel_state.is_pipeline_first_stage():\n        required_device_keys.update((\"tokens\", \"position_ids\"))\n    include_seq_idx = False\n    if parallel_state.is_pipeline_last_stage():\n        include_seq_idx = True\n        required_device_keys.update((\"labels\", \"tokens\", \"loss_mask\"))\n\n    _batch_required_keys = {}\n    for key, val in _batch.items():\n        if key in required_device_keys:\n            _batch_required_keys[key] = val.cuda(non_blocking=True)\n        elif key in required_host_keys:\n            _batch_required_keys[key] = val.cpu()\n        else:\n            _batch_required_keys[key] = None\n\n    # slice batch along sequence dimension for context parallelism\n    output = get_batch_on_this_cp_rank(_batch_required_keys)\n    if include_seq_idx:\n        output[\"seq_idx\"] = _batch[\"seq_idx\"].cuda(non_blocking=True)\n    return output\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.hyena_predict_forward_step","title":"<code>hyena_predict_forward_step(model, batch)</code>","text":"<p>Performs a forward step for the Hyena model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The Hyena model</p> required <code>batch</code> <p>Dictionary containing input batch data with keys: - tokens: Input token IDs - position_ids: Position IDs - labels: Labels for loss computation - loss_mask: Mask for loss computation</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output from the model forward pass</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>def hyena_predict_forward_step(model, batch) -&gt; torch.Tensor:\n    \"\"\"Performs a forward step for the Hyena model.\n\n    Args:\n        model: The Hyena model\n        batch: Dictionary containing input batch data with keys:\n            - tokens: Input token IDs\n            - position_ids: Position IDs\n            - labels: Labels for loss computation\n            - loss_mask: Mask for loss computation\n\n    Returns:\n        torch.Tensor: Output from the model forward pass\n    \"\"\"\n    forward_args = {\n        \"input_ids\": batch[\"tokens\"],\n        \"position_ids\": batch[\"position_ids\"],\n        # \"labels\": batch[\"labels\"],\n        # \"loss_mask\": batch[\"loss_mask\"],\n    }\n\n    forward_args[\"attention_mask\"] = None\n    if \"cu_seqlens\" in batch:\n        forward_args[\"packed_seq_params\"] = get_packed_seq_params(batch)\n    return model(**forward_args)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.main","title":"<code>main()</code>","text":"<p>Entrypoint for Evo2 prediction (single inference step, no new tokens).</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>def main():\n    \"\"\"Entrypoint for Evo2 prediction (single inference step, no new tokens).\"\"\"\n    args = parse_args()\n    predict(\n        num_nodes=args.num_nodes,\n        devices=args.devices,\n        fasta_path=args.fasta,\n        ckpt_dir=args.ckpt_dir,\n        tensor_parallel_size=args.tensor_parallel_size,\n        pipeline_model_parallel_size=args.pipeline_model_parallel_size,\n        context_parallel_size=args.context_parallel_size,\n        output_dir=args.output_dir,\n        model_size=args.model_size,\n        ckpt_format=args.ckpt_format,\n        fp8=args.fp8,\n        full_fp8=args.full_fp8,\n        fp8_recipe=args.fp8_recipe,\n        micro_batch_size=args.micro_batch_size,\n        output_log_prob_seqs=args.output_log_prob_seqs,\n        log_prob_collapse_option=args.log_prob_collapse_option,\n        prepend_bos=args.prepend_bos,\n        no_sequence_parallel=args.no_sequence_parallel,\n        hybrid_override_pattern=args.hybrid_override_pattern,\n        seq_len_interpolation_factor=args.seq_len_interpolation_factor,\n        num_layers=args.num_layers,\n        files_per_subdir=args.files_per_subdir,\n        write_interval=args.write_interval,\n        lora_checkpoint_path=args.lora_checkpoint_path,\n        mask_phylogenetic_tags=args.mask_phylogenetic_tags,\n        min_length=args.min_length,\n        eden_tokenizer=args.eden_tokenizer,\n    )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.parse_args","title":"<code>parse_args()</code>","text":"<p>Parse arguments for Evo2 inference.</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>def parse_args():\n    \"\"\"Parse arguments for Evo2 inference.\"\"\"\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--num-nodes\", type=int, default=1, help=\"Number of nodes to use for prediction, defaults to 1.\")\n    ap.add_argument(\n        \"--devices\",\n        type=int,\n        help=\"Number of devices to use for prediction, defaults to tensor_model_parallel_size * pipeline_model_parallel_size * context_parallel_size.\",\n    )\n    ap.add_argument(\n        \"--eden-tokenizer\",\n        action=\"store_true\",\n        help=\"Patch the tokenizer to work with the one used in training the Eden model.\",\n    )\n    ap.add_argument(\"--fasta\", type=Path, required=True, help=\"Fasta path from which to generate logit predictions.\")\n    ap.add_argument(\"--ckpt-dir\", type=Path, required=True, help=\"NeMo2 checkpoint directory for inference.\")\n    ap.add_argument(\"--min-length\", type=int, required=False, help=\"Minimum sequence length for padding.\")\n    ap.add_argument(\"--prepend-bos\", action=\"store_true\", help=\"Prepend BOS token to sequences. Defaults to False.\")\n    ap.add_argument(\n        \"--mask-phylogenetic-tags\",\n        action=\"store_true\",\n        help=\"Mask phylogenetic tags in loss computation. Defaults to False.\",\n    )\n    ap.add_argument(\"--tensor-parallel-size\", type=int, default=1, help=\"Order of tensor parallelism. Defaults to 1.\")\n    ap.add_argument(\n        \"--pipeline-model-parallel-size\",\n        type=int,\n        choices=[1],\n        default=1,\n        help=\"Order of pipeline parallelism. Defaults to 1 and currently only 1 is supported.\",\n    )\n    ap.add_argument(\n        \"--context-parallel-size\", type=int, default=1, help=\"Order of context parallelism. Defaults to 1.\"\n    )\n    ap.add_argument(\n        \"--fp8-recipe\",\n        type=str,\n        default=\"delayed\",\n        choices=list(Fp8Recipe.__members__.keys()),\n        help=\"FP8 recipe to use for FP8 tensors in the forward and backward pass. Note that some recipes are only \"\n        \"supported by certain architectures. For example 'mxfp8' requires at least blackwell, and 'blockwise' is only \"\n        \"implemented for hopper (but not blackwell). 'tensorwise' and 'delayed' are currently supported by all \"\n        \"architectures, but 'tensorwise' is preferred over 'delayed' which is the default for historical reasons.\",\n    )\n    ap.add_argument(\n        \"--no-sequence-parallel\",\n        action=\"store_true\",\n        help=\"When using TP, skip sequence parallelism. Otherwise sequence parallelism is used whenever tensor \"\n        \"parallelism is used. sequence parallelism should save a small amount of GPU memory so it's on\"\n        \" by default.\",\n    )\n    ap.add_argument(\"--micro-batch-size\", type=int, default=1, help=\"Batch size for prediction. Defaults to 1.\")\n    ap.add_argument(\n        \"--write-interval\",\n        type=str,\n        default=\"epoch\",\n        choices=[\"epoch\", \"batch\"],\n        help=\"Interval to write predictions to disk. If doing very large predictions, you may want to set this to 'batch'.\",\n    )\n    ap.add_argument(\n        \"--model-size\",\n        type=str,\n        default=\"7b_arc_longcontext\",\n        choices=sorted(\n            list(HYENA_MODEL_OPTIONS.keys()) + list(MAMBA_MODEL_OPTIONS.keys()) + list(LLAMA_MODEL_OPTIONS.keys())\n        ),\n        help=\"Model size to use. Defaults to '7b_arc_longcontext'.\",\n    )\n    # output args:\n    ap.add_argument(\n        \"--output-dir\",\n        type=Path,\n        default=None,\n        help=\"Output dir that will contain the generated text produced by the Evo2 model. If not provided, the output will be logged.\",\n    )\n    ap.add_argument(\n        \"--files-per-subdir\",\n        type=int,\n        help=\"Number of files to write to each subdirectory. If provided, subdirectories with N files each will be created. Ignored unless --write-interval is 'batch'.\",\n    )\n    ap.add_argument(\n        \"--full-fp8\",\n        action=\"store_true\",\n        help=\"Use full FP8 precision (faster but less accurate) rather than vortex style which \"\n        \"only applies FP8 to the projection layer of the hyena mixer, when using FP8.\",\n    )\n    ap.add_argument(\"--fp8\", action=\"store_true\", help=\"Use FP8 precision. Defaults to BF16.\")\n    # extra:\n    ap.add_argument(\n        \"--ckpt-format\",\n        type=str,\n        choices=[\"torch_dist\", \"zarr\"],\n        default=\"torch_dist\",\n        help=\"Specify checkpoint format to use. Defaults to 'torch_dist', as 'zarr' is deprecated.\",\n    )\n    ap.add_argument(\n        \"--output-log-prob-seqs\", action=\"store_true\", help=\"Output log probability of sequences. Defaults to False.\"\n    )\n    ap.add_argument(\n        \"--log-prob-collapse-option\",\n        choices=[\"sum\", \"mean\", \"per_token\"],\n        default=\"mean\",\n        help=\"How to collapse the log probabilities across the sequence dimension.\",\n    )\n    ap.add_argument(\n        \"--hybrid-override-pattern\",\n        type=str,\n        help=\"Override the hybrid override pattern in the config (specifies hyena layer ordering and type).\",\n    )\n    ap.add_argument(\n        \"--num-layers\", type=int, help=\"If set, override the number of layers specified in the requested config.\"\n    )\n    ap.add_argument(\n        \"--seq-len-interpolation-factor\",\n        type=int,\n        help=\"If set, override the sequence length interpolation factor specified in the requested config. If you \"\n        \"know a model was trained with a specific interpolation factor for ROPE, provide it here, it can make a big \"\n        \"difference in accuracy.\",\n    )\n    ap.add_argument(\n        \"--lora-checkpoint-path\",\n        type=Path,\n        required=False,\n        default=None,\n        help=\"Path to the lora states to restore from.\",\n    )\n    return ap.parse_args()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.predict","title":"<code>predict(fasta_path, ckpt_dir, output_dir, tensor_parallel_size, pipeline_model_parallel_size, context_parallel_size, num_nodes=1, devices=None, eden_tokenizer=False, model_size='7b', ckpt_format='torch_dist', fp8=False, full_fp8=False, fp8_recipe='delayed', work_dir=None, micro_batch_size=1, output_log_prob_seqs=False, log_prob_collapse_option='mean', write_interval='epoch', prepend_bos=False, no_sequence_parallel=False, hybrid_override_pattern=None, num_layers=None, seq_len_interpolation_factor=None, files_per_subdir=None, lora_checkpoint_path=None, mask_phylogenetic_tags=False, min_length=None, extra_callbacks=None)</code>","text":"<p>Inference workflow for Evo2.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>def predict(\n    fasta_path: Path,\n    ckpt_dir: str,\n    output_dir: Path,\n    tensor_parallel_size: int,\n    pipeline_model_parallel_size: int,\n    context_parallel_size: int,\n    num_nodes: int = 1,\n    devices: int | None = None,\n    eden_tokenizer: bool = False,\n    model_size: str = \"7b\",\n    ckpt_format: CheckpointFormats = \"torch_dist\",\n    fp8: bool = False,\n    full_fp8: bool = False,\n    fp8_recipe: str = \"delayed\",\n    work_dir: Path | None = None,\n    micro_batch_size: int = 1,\n    output_log_prob_seqs: bool = False,\n    log_prob_collapse_option: Literal[\"sum\", \"mean\", \"per_token\"] = \"mean\",\n    write_interval: Literal[\"epoch\", \"batch\"] = \"epoch\",\n    prepend_bos: bool = False,\n    no_sequence_parallel: bool = False,\n    hybrid_override_pattern: str | None = None,\n    num_layers: int | None = None,\n    seq_len_interpolation_factor: int | None = None,\n    files_per_subdir: int | None = None,\n    lora_checkpoint_path: Path | None = None,\n    mask_phylogenetic_tags: bool = False,\n    min_length: int | None = None,\n    extra_callbacks: list | None = None,  # use this for making testing the predict loop easier.\n):\n    \"\"\"Inference workflow for Evo2.\n\n    Returns:\n        None\n    \"\"\"\n    if fp8 and not full_fp8 and fp8_recipe != \"delayed\":\n        logger.warning(\n            \"fp8_recipe is ignored when using fp8 and not full_fp8 since it is set inside of the layer \"\n            \"config to match vortex style FP8.\"\n        )\n    if work_dir is None:\n        work_dir = Path(tempfile.mkdtemp())\n    if files_per_subdir is None and write_interval == \"batch\":\n        logger.warning(\n            \"--files-per-subdir is not set with --write-interval batch, will write all predictions to a \"\n            \"single directory. This may cause problems if you are predicting on a very large dataset.\"\n        )\n    sequence_parallel = tensor_parallel_size &gt; 1 and not no_sequence_parallel\n    output_dir.mkdir(parents=True, exist_ok=True)  # Make sure the output directory exists, files will be written here.\n    model_parallel_size = tensor_parallel_size * pipeline_model_parallel_size * context_parallel_size\n    if devices is None:\n        devices = model_parallel_size\n    world_size = num_nodes * devices\n    if world_size % model_parallel_size != 0:\n        raise ValueError(\n            f\"world_size must be divisible by model_parallel_size, got {world_size} and\"\n            f\" {model_parallel_size}. Please set --num-nodes and --devices such that num_nodes * devices is divisible \"\n            \"by model_parallel_size, which is TP * CP * PP.\"\n        )\n    global_batch_size = micro_batch_size * world_size // model_parallel_size\n\n    callbacks = [\n        PredictionWriter(\n            output_dir=output_dir,\n            write_interval=write_interval,\n            batch_dim_key_defaults={\"token_logits\": 0},\n            seq_dim_key_defaults={\"token_logits\": 1},\n            files_per_subdir=files_per_subdir,\n            save_all_model_parallel_ranks=False,  # only write one copy of predictions.\n        )\n    ]\n    if extra_callbacks is not None:\n        callbacks.extend(extra_callbacks)\n\n    # The following two config options are really only used for testing, but may also be useful for getting output from\n    #   specific layers of the model.\n    config_modifiers_init: dict[str, Any] = {\n        \"distribute_saved_activations\": False if sequence_parallel and tensor_parallel_size &gt; 1 else True,\n    }\n    if hybrid_override_pattern is not None:\n        config_modifiers_init[\"hybrid_override_pattern\"] = hybrid_override_pattern\n    if num_layers is not None:\n        config_modifiers_init[\"num_layers\"] = num_layers\n    if seq_len_interpolation_factor is not None:\n        config_modifiers_init[\"seq_len_interpolation_factor\"] = seq_len_interpolation_factor\n\n    tokenizer = get_nmt_tokenizer(\"byte-level\")\n    if eden_tokenizer:\n        patch_eden_tokenizer(tokenizer)\n\n    model_type = infer_model_type(model_size)\n\n    # Select model config based on model type\n    if model_type == \"hyena\":\n        if model_size not in HYENA_MODEL_OPTIONS:\n            raise ValueError(f\"Invalid model size for Hyena: {model_size}\")\n        config = HYENA_MODEL_OPTIONS[model_size](\n            forward_step_fn=hyena_predict_forward_step,\n            data_step_fn=hyena_predict_data_step,\n            # Only use vortex style FP8 in the model config if using FP8 and not full FP8. This will only apply FP8 to\n            #   the projection layer of the hyena mixer.\n            vortex_style_fp8=fp8 and not full_fp8,\n            **config_modifiers_init,\n        )\n\n        if lora_checkpoint_path:\n            model_transform = Evo2LoRA(peft_ckpt_path=str(lora_checkpoint_path))\n            callbacks.append(model_transform)\n        else:\n            model_transform = None\n\n        model = HyenaPredictor(\n            config,\n            tokenizer=tokenizer,\n            output_log_prob_seqs=output_log_prob_seqs,\n            log_prob_collapse_option=log_prob_collapse_option,\n            model_transform=model_transform,\n        )\n    elif model_type == \"mamba\":  # mamba\n        if model_size not in MAMBA_MODEL_OPTIONS:\n            raise ValueError(f\"Invalid model size for Mamba: {model_size}\")\n        config = MAMBA_MODEL_OPTIONS[model_size](\n            forward_step_fn=hyena_predict_forward_step,  # Can reuse the same forward steps\n            data_step_fn=hyena_predict_data_step,\n            **config_modifiers_init,\n        )\n\n        model = MambaPredictor(\n            config,\n            tokenizer=tokenizer,\n            output_log_prob_seqs=output_log_prob_seqs,\n            log_prob_collapse_option=log_prob_collapse_option,\n        )\n    elif model_type == \"llama\":\n        if model_size not in LLAMA_MODEL_OPTIONS:\n            raise ValueError(f\"Invalid model size for Llama: {model_size}\")\n        config = LLAMA_MODEL_OPTIONS[model_size](\n            forward_step_fn=hyena_predict_forward_step,\n            data_step_fn=hyena_predict_data_step,\n            **config_modifiers_init,\n        )\n        model = LlamaPredictor(\n            config,\n            tokenizer=tokenizer,\n            output_log_prob_seqs=output_log_prob_seqs,\n            log_prob_collapse_option=log_prob_collapse_option,\n        )\n    else:\n        # This shouldn't be possible to reach.\n        raise ValueError(f\"Invalid model type: {model_type}.\")\n\n    # Create PTL trainer.\n    trainer = nl.Trainer(\n        accelerator=\"gpu\",\n        num_nodes=num_nodes,\n        devices=devices,\n        strategy=nl.MegatronStrategy(\n            drop_last_batch=False,\n            tensor_model_parallel_size=tensor_parallel_size,\n            pipeline_model_parallel_size=pipeline_model_parallel_size,\n            context_parallel_size=context_parallel_size,\n            pipeline_dtype=torch.bfloat16,\n            ckpt_load_optimizer=False,  # Needs to be false for a normal model checkpoint.\n            ckpt_save_optimizer=False,\n            ckpt_async_save=False,\n            sequence_parallel=sequence_parallel,\n            save_ckpt_format=ckpt_format,\n            ckpt_load_strictness=\"log_all\",\n            setup_optimizers=False,\n            store_optimizer_states=False,\n            configure_optimizers=False,\n            data_sampler=nl.MegatronDataSampler(\n                micro_batch_size=micro_batch_size,\n                global_batch_size=global_batch_size,\n                seq_len=8192,\n                output_log=False,  # this is needed for predict step to work\n            ),\n        ),\n        log_every_n_steps=1,\n        limit_val_batches=10,\n        num_sanity_val_steps=0,\n        callbacks=callbacks,\n        plugins=nl.MegatronMixedPrecision(\n            precision=\"bf16-mixed\",\n            params_dtype=torch.bfloat16,\n            # Only use FP8 in this plugin when using full FP8 precision and FP8.\n            #   Otherwise use vortex_style_fp8 in the model config.\n            fp8_recipe=fp8_recipe,\n            fp8=\"hybrid\" if fp8 and full_fp8 else None,\n            fp8_amax_history_len=16 if fp8 and full_fp8 else 1,\n            fp8_amax_compute_algo=\"max\" if fp8 and full_fp8 else \"most_recent\",\n        ),\n    )\n\n    nemo_logger = NeMoLogger(log_dir=str(work_dir))\n    nemo_logger.setup(trainer, resume_if_exists=True)\n    resume = nl.AutoResume(\n        resume_if_exists=True,\n        resume_ignore_no_checkpoint=False,\n        resume_past_end=False,\n        resume_from_path=str(ckpt_dir),\n        restore_config=None,\n    )\n\n    resume.setup(trainer, model)  # this pulls weights from the starting checkpoint.\n\n    if mask_phylogenetic_tags:\n\n        def custom_loss_masker(tokens):\n            # Run the evo2 dataset mask_phylogenetic_tags function\n            return Evo2Dataset.mask_phylogenetic_tags(\n                tokens,\n                Evo2Dataset.TAG_BOUNDS,\n                Evo2Dataset.TAG_CHARS,\n                tokenizer.eod if tokenizer is not None else Evo2Dataset.DEFAULT_EOD,\n                Evo2Dataset.MAX_TAG_LEN,\n            )\n    else:\n        custom_loss_masker = None\n\n    dataset = SimpleFastaDataset(fasta_path, tokenizer, prepend_bos=prepend_bos, custom_loss_masker=custom_loss_masker)\n    datamodule = PredictDataModule(dataset, batch_size=micro_batch_size, tokenizer=tokenizer, min_length=min_length)\n    trainer.predict(model, datamodule=datamodule)  # TODO return_predictions=False\n    dataset.write_idx_map(\n        output_dir\n    )  # Finally write out the index map so we can match the predictions to the original sequences.\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/run/train/","title":"Train","text":""},{"location":"main/references/API_reference/bionemo/evo2/run/train/#bionemo.evo2.run.train.main","title":"<code>main()</code>","text":"<p>Parsing args and running evo2 training.</p> Source code in <code>bionemo/evo2/run/train.py</code> <pre><code>def main():\n    \"\"\"Parsing args and running evo2 training.\"\"\"\n    args = parse_args()\n    train(args=args)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/run/train/#bionemo.evo2.run.train.parse_args","title":"<code>parse_args(args=None)</code>","text":"<p>Parse arguments for Evo2 model training.</p> Source code in <code>bionemo/evo2/run/train.py</code> <pre><code>def parse_args(args: Optional[List[str]] = None) -&gt; argparse.Namespace:\n    \"\"\"Parse arguments for Evo2 model training.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=(\n            \"Train an Evo2/Hyena-family model using NeMo 2.0.\\n\\n\"\n            \"Choose exactly one data source:\\n\"\n            \"  - --dataset-config: blended/weighted dataset YAML.\\n\"\n            \"  - --mock-data: synthetic mock data for testing/debugging.\\n\"\n            \"  - --fasta-data: single FASTA file input (requires --fasta-file).\\n\"\n            \"  - --sharded-eden-data: pre-sharded SQLite sequence DBs + precomputed windows per split\\n\"\n            \"      (requires --sequence-db-dir, --train-window-db, --val-window-db, --test-window-db).\"\n        ),\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    data_group = parser.add_mutually_exclusive_group(required=True)\n\n    data_group.add_argument(\n        \"-d\",\n        \"--dataset-config\",\n        type=str,\n        help=\"Path to the blended / weighted training dataset configuration YAML. Mutually exclusive with \"\n        \"--mock-data, --fasta-data and --sharded-eden-data.\",\n    )\n    data_group.add_argument(\n        \"--mock-data\",\n        action=\"store_true\",\n        help=\"Use synthetic mock data for quick testing/debugging. Mutually exclusive with --dataset-config, --fasta-data and --sharded-eden-data.\",\n    )\n\n    data_group.add_argument(\n        \"--fasta-data\",\n        action=\"store_true\",\n        help=(\n            \"Train on a single FASTA file (EdenDataModule). Requires --fasta-file. Mutually exclusive with \"\n            \"--dataset-config, --mock-data and --sharded-eden-data.\"\n        ),\n    )\n\n    data_group.add_argument(\n        \"--sharded-eden-data\",\n        action=\"store_true\",\n        help=(\n            \"Train on pre-sharded SQLite sequence databases with precomputed windows per split \"\n            \"(ShardedEdenDataModule). Requires: --sequence-db-dir, --train-window-db, --val-window-db, --test-window-db. \"\n            \"Mutually exclusive with --dataset-config, --mock-data and --fasta-data.\"\n        ),\n    )\n\n    # Dataset configuration (unified)\n    parser.add_argument(\n        \"--fasta-file\",\n        type=str,\n        help=(\n            \"Absolute path to FASTA file containing training data. Required when using --fasta-data; \"\n            \"ignored otherwise.\"\n        ),\n    )\n    parser.add_argument(\n        \"--sequence-db-dir\",\n        type=str,\n        help=(\n            \"Directory containing per-sample SQLite databases with sequences. Required with --sharded-eden-data; \"\n            \"ignored otherwise.\"\n        ),\n    )\n    parser.add_argument(\n        \"--train-window-db\",\n        type=str,\n        help=(\n            \"Path to the precomputed training split windows SQLite database. Required with --sharded-eden-data; \"\n            \"ignored otherwise.\"\n        ),\n    )\n    parser.add_argument(\n        \"--val-window-db\",\n        type=str,\n        help=(\n            \"Path to the precomputed validation split windows SQLite database. Required with --sharded-eden-data; \"\n            \"ignored otherwise.\"\n        ),\n    )\n    parser.add_argument(\n        \"--test-window-db\",\n        type=str,\n        help=(\n            \"Path to the precomputed test split windows SQLite database. Required with --sharded-eden-data; \"\n            \"ignored otherwise.\"\n        ),\n    )\n    parser.add_argument(\n        \"--dataset-num-epochs\",\n        type=int,\n        default=1,\n        help=(\n            \"When using --sharded-eden-data, wrap each split with a MultiEpochDatasetResampler over this many epochs. \"\n            \"Default 1 means each split length equals its base dataset length.\"\n        ),\n    )\n    parser.add_argument(\n        \"--stride\",\n        type=int,\n        default=7992,\n        help=(\n            \"Stride between adjacent windows used by ShardedEdenDataModule. Must match the stride used when \"\n            \"precomputing the windows databases. Ignored for other data modes.\"\n        ),\n    )\n    parser.add_argument(\n        \"--window-min-length-threshold\",\n        type=int,\n        default=0,\n        help=(\n            \"If &gt; 0, prune windows shorter than this effective length during precomputation and require matching \"\n            \"value in the window DB metadata. Defaults to 0 (disabled).\"\n        ),\n    )\n    parser.add_argument(\n        \"--log-windows\",\n        action=\"store_true\",\n        default=False,\n        help=(\"Enable window access logging for ShardedEdenDataset (applies only to --sharded-eden-data).\"),\n    )\n    parser.add_argument(\n        \"--window-log-dir\",\n        type=str,\n        default=None,\n        help=(\"Directory for window-access logging SQLite files (applies only to --sharded-eden-data).\"),\n    )\n    parser.add_argument(\n        \"--rc-aug\",\n        action=\"store_true\",\n        default=False,\n        help=(\"Enable reverse-complement augmentation (applies only to --sharded-eden-data).\"),\n    )\n    parser.add_argument(\n        \"--dataset-dir\",\n        type=str,\n        help=\"Absolute path to the dataset directory. Defaults to using the absolute or relative paths (dataset_prefix) specified in the dataset config YAML. Only used with --dataset-config.\",\n    )\n\n    parser.add_argument(\"--num-nodes\", type=int, default=1, help=\"Number of nodes to use for training, defaults to 1.\")\n    parser.add_argument(\"--devices\", type=int, default=1, help=\"Number of devices to use for training, defaults to 1.\")\n    parser.add_argument(\"--seq-length\", type=int, default=8192, help=\"Training sequence length\")\n    parser.add_argument(\n        \"--tensor-parallel-size\", type=int, default=1, help=\"Order of tensor parallelism. Defaults to 1.\"\n    )\n    parser.add_argument(\n        \"--pipeline-model-parallel-size\", type=int, default=1, help=\"Order of pipeline parallelism. Defaults to 1.\"\n    )\n    parser.add_argument(\n        \"--context-parallel-size\", type=int, default=1, help=\"Order of context parallelism. Defaults to 1.\"\n    )\n    parser.add_argument(\n        \"--create-tensorboard-logger\", action=\"store_true\", default=False, help=\"Create a tensorboard logger.\"\n    )\n    parser.add_argument(\"--wandb-entity\", type=str, default=None, help=\"The team posting this run\")\n    parser.add_argument(\"--wandb-project\", type=str, default=None, help=\"Wandb project name \")\n    parser.add_argument(\"--wandb-tags\", nargs=\"+\", type=str, default=None, help=\"Tags associated with this run\")\n    parser.add_argument(\n        \"--wandb-group\", type=str, default=None, help=\"A unique string shared by all runs in a given group\"\n    )\n    parser.add_argument(\n        \"--wandb-job-type\",\n        type=str,\n        default=None,\n        help=\"A unique string representing a type of run, which is useful when you're grouping runs together into larger experiments using group.\",\n    )\n    parser.add_argument(\n        \"--wandb-run-name\",\n        type=str,\n        default=None,\n        help=\"A unique string representing the name of the wandb run. If not provided, the name will be generated from the model and training specifications.\",\n    )\n\n    parser.add_argument(\n        \"--wandb-id\", type=str, default=None, help=\"Sets the version, mainly used to resume a previous run\"\n    )\n    parser.add_argument(\n        \"--wandb-anonymous\", action=\"store_true\", help=\"Enable or explicitly disable anonymous logging\"\n    )\n    parser.add_argument(\n        \"--wandb-log-model\", action=\"store_true\", help=\"Save checkpoints in wandb dir to upload on W&amp;B servers\"\n    )\n    parser.add_argument(\"--wandb-offline\", action=\"store_true\", help=\"Use wandb in offline mode\")\n    parser.add_argument(\"--sequence-parallel\", action=\"store_true\", help=\"Set to enable sequence parallelism.\")\n    parser.add_argument(\"--fp8\", action=\"store_true\", help=\"Set to enable FP8\")\n    parser.add_argument(\"--micro-batch-size\", type=int, default=1, help=\"Micro-batch size for data-parallel training.\")\n    parser.add_argument(\n        \"--global-batch-size\",\n        type=int,\n        default=None,\n        help=\"Global batch size for training. If set to None, infer it from the TP, CP, and PP parameters.\",\n    )\n    parser.add_argument(\n        \"--grad-acc-batches\", type=int, default=1, help=\"Number of batches to accumulate gradients over.\"\n    )\n    parser.add_argument(\n        \"--max-steps\",\n        type=int,\n        help=\"Number of training optimizer update steps. This controls the total number of steps as well as the \"\n        \"shape of the learning rate curve.\",\n        default=500000,\n    )\n    parser.add_argument(\n        \"--constant-steps\",\n        type=int,\n        help=\"Number of steps to keep the learning rate constant at minimum after annealing. This controls the \"\n        \"shape of the learning rate curve.\",\n        default=80000,\n    )\n    parser.add_argument(\n        \"--early-stop-on-step\",\n        type=int,\n        help=\"Stop training on this step, if set. This may be useful for testing or debugging purposes.\",\n    )\n    parser.add_argument(\n        \"--val-check-interval\", type=int, help=\"Number of steps between validation measurements and model checkpoints.\"\n    )\n    parser.add_argument(\"--grad-reduce-in-fp32\", action=\"store_true\", default=False, help=\"Gradient reduce in FP32.\")\n    parser.add_argument(\n        \"--fp8-wgrad\",\n        action=\"store_true\",\n        default=False,\n        help=\"Faster option that is maybe less accurate (TBD) when using fp8.\",\n    )\n    parser.add_argument(\"--use-megatron-comm-overlap-llama3-8k\", action=\"store_true\", default=False)\n    parser.add_argument(\n        \"--tp-comm-overlap-backend\",\n        type=str,\n        choices=[\"nccl\", \"mpi\", \"gloo\"],\n        default=\"nccl\",\n        help=\"TP communication backend to use. Defaults to 'nccl'.\",\n    )\n    parser.add_argument(\"--align-param-gather\", action=\"store_true\", default=False)\n    parser.add_argument(\n        \"--model-size\",\n        type=str,\n        choices=sorted(\n            list(HYENA_MODEL_OPTIONS.keys()) + list(MAMBA_MODEL_OPTIONS.keys()) + list(LLAMA_MODEL_OPTIONS.keys())\n        ),\n        default=\"7b\",\n        help=\"Model size/configuration to use. Options depend on the selected model-type.\",\n    )\n    parser.add_argument(\n        \"--add-bias-output\",\n        action=\"store_true\",\n        default=False,\n        help=\"Add bias to the output layer to enable learning a simple prior.\",\n    )\n    parser.add_argument(\n        \"--result-dir\", type=Path, required=False, default=Path(\"./results\"), help=\"Path to the result directory.\"\n    )\n    parser.add_argument(\"--experiment-name\", type=str, required=False, default=\"evo2\", help=\"Name of the experiment.\")\n\n    parser.add_argument(\n        \"--limit-val-batches\",\n        type=int,\n        default=20,\n        help=\"Number of validation steps\",\n    )\n    parser.add_argument(\n        \"--limit-test-batches\",\n        type=int,\n        help=\"Number of test steps (sometimes useful for getting around megatron errors of too few samples). Defaults \"\n        \"to the same as limit_val_batches.\",\n    )\n    parser.add_argument(\n        \"--log-every-n-steps\",\n        type=int,\n        default=1,\n        required=False,\n        help=\"Number of steps between logging.\",\n    )\n    parser.add_argument(\n        \"--ckpt-dir\",\n        type=str,\n        default=None,\n        help=\"Directory to restore an initial checkpoint from. Use this for supervised fine-tuning.\",\n    )\n    parser.add_argument(\n        \"--use-precision-aware-optimizer\",\n        action=\"store_true\",\n        default=False,\n        help=\"Use precision aware optimizer that stores main weights in FP32 when doing mixed precision training.\",\n    )\n    parser.add_argument(\n        \"--bf16-main-grads\",\n        action=\"store_true\",\n        default=False,\n        help=\"Use bf16 for main gradients, only use this with --use-precision-aware-optimizer.\",\n    )\n    parser.add_argument(\"--wd\", type=float, default=0.01, help=\"Weight decay for optimizer.\")\n    parser.add_argument(\n        \"--adam-beta1\",\n        type=float,\n        default=0.9,\n        help=\"Adam optimizer beta1 parameter.\",\n    )\n    parser.add_argument(\n        \"--adam-beta2\",\n        type=float,\n        default=0.95,\n        help=\"Adam optimizer beta2 parameter.\",\n    )\n    parser.add_argument(\n        \"--adam-eps\",\n        type=float,\n        default=1e-8,\n        help=\"Adam optimizer epsilon parameter. The inverse of this value (1/eps) represents the maximum adaptive learning rate per parameter.\",\n    )\n    parser.add_argument(\n        \"--restore-optimizer-from-ckpt\",\n        action=\"store_true\",\n        help=\"Restore optimizer state from initial checkpoint. Defaults to False.\",\n    )\n    parser.add_argument(\n        \"--average-in-collective\",\n        action=\"store_true\",\n        default=False,\n        help=\"Avaerage optimizer state in collective rather than dividing by dp size and summing.\",\n    )\n    parser.add_argument(\"--seed\", type=int, default=1234, help=\"Set random seed for training.\")\n    parser.add_argument(\"--workers\", type=int, default=8, help=\"Number of workers to use for data loading.\")\n    parser.add_argument(\n        \"--gc-interval\",\n        type=int,\n        default=0,\n        help=\"Set to a value &gt; 0 if you want to synchronize garbage collection, will do gc every gc-interval steps.\",\n    )\n    parser.add_argument(\n        \"--enable-preemption\",\n        action=\"store_true\",\n        default=False,\n        help=\"Enable preemption hooks. If enabled this will save a checkpoint whenever slurm exits.\",\n    )\n    parser.add_argument(\n        \"--ckpt-async-save\",\n        action=\"store_true\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--ckpt-format\",\n        type=str,\n        choices=[\"torch_dist\", \"zarr\"],\n        default=\"torch_dist\",\n        help=\"Specify checkpoint format to use. Defaults to 'torch_dist', as 'zarr' is deprecated. Only use if \"\n        \"resuming training from a zarr checkpoint.\",\n    )\n    parser.add_argument(\n        \"--eod-pad-in-loss-mask\",\n        action=\"store_true\",\n        default=False,\n        help=\"Do not predict EOD/Pad tokens (typical default, but not default in original evo2).\",\n    )\n    parser.add_argument(\n        \"--cross-entropy-loss-fusion\",\n        action=\"store_true\",\n        default=False,\n        help=\"Use the faster, but maybe less accurate fused form of cross entropy, \"\n        \"which also has bf16 grads internally.\",\n    )\n    parser.add_argument(\n        \"--no-fp32-residual-connection\",\n        action=\"store_true\",\n        default=False,\n        help=\"If set, turn off fp32 residual connections which may be faster but may impact accuracy.\",\n    )\n    parser.add_argument(\n        \"--debug-ddp-parity-freq\",\n        type=int,\n        default=0,\n        help=\"Set to value &gt; 0 to debug DDP weight parity between ranks.\",\n    )\n    parser.add_argument(\n        \"--hybrid-override-pattern\",\n        type=str,\n        help=\"Override the hybrid override pattern in the config (specifies hyena layer ordering and type).\",\n    )\n    parser.add_argument(\n        \"--num-layers\", type=int, help=\"If set, override the number of layers specified in the requested config.\"\n    )\n    parser.add_argument(\n        \"--create-tflops-callback\",\n        action=\"store_true\",\n        default=False,\n        help=\"Enable tflops calculation callback for Hyena / Evo2. Defaults to False.\",\n    )\n    parser.add_argument(\n        \"--log-parameters-and-shapes\",\n        action=\"store_true\",\n        default=False,\n        help=\"Log training parameters shapes and dtypes for debugging.\",\n    )\n    parser.add_argument(\"--lr\", type=float, default=3e-4, help=\"Learning rate.\")\n    parser.add_argument(\"--min-lr\", type=float, default=3e-5, help=\"Min learning rate in cosine annealing.\")\n    parser.add_argument(\"--warmup-steps\", type=int, default=2500, help=\"Number of warmup steps in cosine annealing\")\n    parser.add_argument(\n        \"--fp8-recipe\",\n        type=str,\n        default=\"delayed\",\n        choices=list(Fp8Recipe.__members__.keys()),\n        help=\"FP8 recipe to use for FP8 tensors in the forward and backward pass. Note that some recipes are only \"\n        \"supported by certain architectures. For example 'mxfp8' requires at least blackwell, and 'blockwise' is only \"\n        \"implemented for hopper (but not blackwell). 'tensorwise' and 'delayed' are currently supported by all \"\n        \"architectures, but 'tensorwise' is preferred over 'delayed' which is the default for historical reasons.\",\n    )\n    # NSYS profiling/tooling arguments\n    parser.add_argument(\n        \"--nsys-profiling\",\n        action=\"store_true\",\n        default=False,\n        help=\"Enable targeted `nsys` profiling on the training loop for a defined step range. To actually get profiling\"\n        \" output you must run the whole program with `nsys`. For example: \"\n        \" `nsys profile -s none -o output_report_name -t cuda,nvtx --force-overwrite true \"\n        \"--capture-range=cudaProfilerApi --capture-range-end=stop  [regular python command here]`\",\n    )\n    # start, end, rank\n    parser.add_argument(\n        \"--nsys-start-step\",\n        type=int,\n        required=False,\n        default=0,\n        help=\"Start nsys profiling after this step.\",\n    )\n    parser.add_argument(\n        \"--spike-no-more-embedding-init\",\n        action=\"store_true\",\n        default=False,\n        help=\"If set, the embeddings are initialized with a Normal(0, 1.0) distribution rather \"\n        \"than the default Normal(0, 0.02). This may help avoid loss spiking during training. Consider using this with \"\n        \"--no-weight-decay-embeddings to avoid shrinking the embeddings to 0 by skipping weight decay on these layers, \"\n        \"or with --use-targeted-variance-loss to maintain a 1.0 variance during training even with weight decay. This \"\n        \"also turns off shared weights between embeddings and outputs.\",\n    )\n    parser.add_argument(\n        \"--no-weight-decay-embeddings\",\n        action=\"store_true\",\n        default=False,\n        help=\"If set, do not apply weight decay to the embeddings.\",\n    )\n    parser.add_argument(\n        \"--use-targeted-variance-loss\",\n        action=\"store_true\",\n        default=False,\n        help=\"Use targeted variance loss.\",\n    )\n    parser.add_argument(\n        \"--nsys-end-step\",\n        type=int,\n        required=False,\n        help=\"End nsys profiling after this step.\",\n    )\n    parser.add_argument(\n        \"--no-renormalize-loss\",\n        action=\"store_true\",\n        default=False,\n        help=\"Do not renormalize the loss weights.\",\n    )\n    parser.add_argument(\n        \"--mamba-lowercase-loss-weight\",\n        type=float,\n        default=0.1,\n        help=\"Loss weight for the Mamba model for lowercase bases, if you are using a Mamba model. \"\n        \"Default is 0.1 like the Evo2 paper. Set to 1.0 to disable differential loss weighting.\",\n    )\n    # rank as list of integers\n    parser.add_argument(\n        \"--nsys-ranks\",\n        type=int,\n        nargs=\"+\",\n        required=False,\n        default=[0],\n        help=\"Enable nsys profiling for these ranks.\",\n    )\n    parser.add_argument(\n        \"--activation-checkpoint-recompute-num-layers\",\n        type=int,\n        help=\"If set, override the default value set in the config.\",\n    )\n    parser.add_argument(\n        \"--disable-checkpointing\",\n        action=\"store_false\",\n        default=True,\n        dest=\"create_checkpoint_callback\",\n        help=\"Disable creating a ModelCheckpoint callback.\",\n    )\n    parser.add_argument(\n        \"--clip-grad\",\n        type=float,\n        default=1.0,\n        help=\"Grad clip value. Note that when using DDP this may need to be inflated.\",\n    )\n    parser.add_argument(\n        \"--seq-len-interpolation-factor\",\n        type=float,\n        help=\"Adjusts the linear scaling of ROPE (Rotary Position Embedding) for context extension. \"\n        \"Set this factor relative to your base context length e.g., for an original context length of 8192 and \"\n        \"an extended context length of 524288, use 524288/8192 = 64.\",\n    )\n    parser.add_argument(\n        \"--overlap-param-gather\",\n        action=\"store_true\",\n        default=False,\n        help=\"Overlap the parameter gather with the optimizer step. This is currently disabled due to a NeMo bug \"\n        \"when using DDP. Making this an option defaulting to False is a temporary solution until the bug is fixed.\",\n    )\n    parser.add_argument(\n        \"--overlap-grad-reduce\",\n        action=\"store_true\",\n        default=False,\n        help=\"Overlap the gradient reduce with the optimizer step.\",\n    )\n    parser.add_argument(\n        \"--hidden-dropout\",\n        type=float,\n        default=0.0,\n        help=\"Dropout probability for the hyena layers\",\n    )\n    parser.add_argument(\n        \"--ffn-hidden-size\",\n        type=int,\n        default=None,\n        help=\"FFN hidden size for the hyena layers\",\n    )\n    parser.add_argument(\n        \"--log-num-zeros-in-grad\",\n        action=\"store_true\",\n        default=False,\n        help=\"Log the number of zeros in the gradient.\",\n    )\n    parser.add_argument(\n        \"--attention-dropout\",\n        type=float,\n        default=0.0,\n        help=\"Dropout probability for the attention layers.\",\n    )\n    parser.add_argument(\n        \"--use-subquadratic_ops\",\n        action=\"store_true\",\n        help=\"Use subquadratic_ops for improved performance.\",\n    )\n    parser.add_argument(\n        \"--save-top-k\",\n        type=int,\n        default=5,\n        help=\"Number of best checkpoints to keep. Set to -1 to save all checkpoints.\",\n    )\n    parser.add_argument(\n        \"--metric-to-monitor-for-checkpoints\",\n        type=str,\n        default=\"val_loss\",\n        help=\"Metric to monitor for checkpoints.\",\n    )\n    parser.add_argument(\n        \"--save-last-checkpoint\",\n        action=\"store_true\",\n        default=True,\n        help=\"Save the last checkpoint.\",\n    )\n    parser.add_argument(\n        \"--no-save-last-checkpoint\",\n        action=\"store_false\",\n        dest=\"save_last_checkpoint\",\n        default=True,\n        help=\"Disable saving the last checkpoint.\",\n    )\n    parser.add_argument(\"--lora-finetune\", action=\"store_true\", help=\"Use LoRA fine-tuning\", default=False)\n    parser.add_argument(\"--lora-checkpoint-path\", type=str, default=None, help=\"LoRA checkpoint path\")\n    parser.add_argument(\n        \"--no-calculate-per-token-loss\",\n        action=\"store_true\",\n        default=False,\n        help=\"Calculate a simpler mean across the microbatch of the loss prior to DDP reduction rather than the global\"\n        \" per-token mean loss. Use this if speed is critical and if you do not need token masking in your loss.\",\n    )\n    parser.add_argument(\n        \"--no-check-for-nan-in-grad\",\n        action=\"store_true\",\n        default=False,\n        help=\"Skip checking for NaNs in gradients. Only use this for debugging purposes.\",\n    )\n    parser.add_argument(\n        \"--garbage-collect-at-inference\",\n        action=\"store_true\",\n        default=False,\n        help=\"Enable CUDA memory cleanup before validation to prevent initialization errors.\",\n    )\n    parser.add_argument(\n        \"--lora-alpha\",\n        type=int,\n        default=None,\n        help=\"Alpha parameter for LoRA fine-tuning.\",\n    )\n    parser.add_argument(\n        \"--lora-dim\",\n        type=int,\n        default=None,\n        help=\"Dim parameter for LoRA fine-tuning.\",\n    )\n\n    recompute_group = parser.add_mutually_exclusive_group(required=False)\n    recompute_group.add_argument(\"--no-activation-checkpointing\", action=\"store_true\", default=False)\n    recompute_group.add_argument(\"--selective-activation-checkpointing\", action=\"store_true\", default=False)\n    return parser.parse_args(args=args)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/run/train/#bionemo.evo2.run.train.train","title":"<code>train(args)</code>","text":"<p>Main function to run Evo2 training.</p> Source code in <code>bionemo/evo2/run/train.py</code> <pre><code>def train(args: argparse.Namespace) -&gt; nl.Trainer:\n    \"\"\"Main function to run Evo2 training.\"\"\"\n    tokenizer = get_nmt_tokenizer(\n        \"byte-level\",\n    )\n\n    # Infer global batch size.\n    global_batch_size = args.global_batch_size\n    if global_batch_size is None:\n        global_batch_size = infer_global_batch_size(\n            micro_batch_size=args.micro_batch_size,\n            num_nodes=args.num_nodes,\n            devices=args.devices,\n            accumulate_grad_batches=args.grad_acc_batches,\n            tensor_model_parallel_size=args.tensor_parallel_size,\n            pipeline_model_parallel_size=args.pipeline_model_parallel_size,\n            context_model_parallel_size=args.context_parallel_size,\n        )\n    if args.mock_data:\n        data_module = MockDataModule(\n            seq_length=args.seq_length,\n            micro_batch_size=args.micro_batch_size,\n            global_batch_size=global_batch_size,\n            num_train_samples=args.max_steps * global_batch_size,\n            num_val_samples=args.limit_val_batches * global_batch_size,\n            num_test_samples=1,\n            num_workers=args.workers,\n            tokenizer=tokenizer,\n        )\n    elif args.fasta_data:\n        raise NotImplementedError(\"Fasta data is not supported yet. Need to add EdenDataModule\")\n        # data_module = EdenDataModule(\n        #     fasta_file=args.fasta_file,\n        #     seq_length=args.seq_length,\n        #     micro_batch_size=args.micro_batch_size,\n        #     global_batch_size=global_batch_size,\n        #     num_workers=args.workers,\n        #     tokenizer=tokenizer,\n        #     seed=args.seed,\n        # )\n    elif args.sharded_eden_data:\n        # Validate required arguments for sharded data\n        if not args.sequence_db_dir or not args.train_window_db or not args.val_window_db or not args.test_window_db:\n            raise ValueError(\n                \"--sequence-db-dir, --train-window-db, --val-window-db, and --test-window-db are required when using --sharded-eden-data.\"\n            )\n        logger.info(f\"Patching the tokenizer for compatibility with Eden model training: {tokenizer}\")\n        patch_eden_tokenizer(tokenizer)  # Eden tokenizer uses different IDs for BOS, EOS, SEP, and PAD than default.\n        data_module = ShardedEdenDataModule(\n            sequence_db_dir=args.sequence_db_dir,\n            train_window_db_path=args.train_window_db,\n            val_window_db_path=args.val_window_db,\n            test_window_db_path=args.test_window_db,\n            seq_length=args.seq_length,\n            tokenizer=tokenizer,\n            micro_batch_size=args.micro_batch_size,\n            global_batch_size=global_batch_size,\n            num_workers=args.workers,\n            rc_aug=args.rc_aug,\n            stride=args.stride,\n            window_min_length_threshold=args.window_min_length_threshold,\n            seed=args.seed,\n            num_epochs=args.dataset_num_epochs,\n            log_windows=args.log_windows,\n            log_dir=args.window_log_dir,\n        )\n    else:\n        blended_dataset_config = parse_dataset_config(\n            dataset_config_path=args.dataset_config, dataset_path=args.dataset_dir\n        )\n        dataset_cls = Evo2DatasetPadEodLossMask if args.eod_pad_in_loss_mask else Evo2Dataset\n        # Instantiate pre-training module.\n        data_module = PreTrainingDataModule(\n            paths=blended_dataset_config,\n            dataset_cls=dataset_cls,\n            seq_length=args.seq_length,\n            micro_batch_size=args.micro_batch_size,\n            global_batch_size=global_batch_size,\n            seed=args.seed,\n            num_workers=args.workers,\n            tokenizer=tokenizer,\n            eod_mask_loss=args.eod_pad_in_loss_mask,\n        )\n    if args.no_activation_checkpointing:\n        activation_checkpointing_args = {\n            \"recompute_granularity\": None,\n            \"recompute_method\": None,\n            \"recompute_num_layers\": None,\n        }\n    elif args.selective_activation_checkpointing:\n        activation_checkpointing_args = {\n            \"recompute_granularity\": \"selective\",\n            \"recompute_method\": None,\n            \"recompute_num_layers\": None,\n        }\n    else:\n        if args.activation_checkpoint_recompute_num_layers is not None:\n            activation_checkpointing_args = {\n                \"recompute_num_layers\": args.activation_checkpoint_recompute_num_layers,\n            }\n        else:\n            activation_checkpointing_args = {}\n    # Retrieve model config.\n    config_modifiers_init = {\n        \"calculate_per_token_loss\": not args.no_calculate_per_token_loss,  # override megatron internal behavior.\n        \"tp_comm_overlap\": args.use_megatron_comm_overlap_llama3_8k,\n        \"seq_length\": args.seq_length,\n        \"hidden_dropout\": args.hidden_dropout,\n        \"attention_dropout\": args.attention_dropout,\n        \"to_upper\": \"weighted\" if args.no_renormalize_loss else \"normalized_weighted\",\n        \"distribute_saved_activations\": False if args.sequence_parallel else True,\n        \"cross_entropy_loss_fusion\": args.cross_entropy_loss_fusion,\n        \"fp32_residual_connection\": not args.no_fp32_residual_connection,\n        **activation_checkpointing_args,\n    }\n    if args.add_bias_output:\n        config_modifiers_init[\"add_bias_output\"] = args.add_bias_output\n    if args.spike_no_more_embedding_init:\n        config_modifiers_init[\"embedding_init_method_std\"] = 1.0\n        # When using spike_no_more_embedding_init, we don't want to share embeddings and outputs.\n        config_modifiers_init[\"share_embeddings_and_output_weights\"] = False\n    if args.ffn_hidden_size:\n        config_modifiers_init[\"ffn_hidden_size\"] = args.ffn_hidden_size\n    if args.use_targeted_variance_loss:\n        config_modifiers_init[\"use_targeted_variance_loss\"] = True\n    if args.use_subquadratic_ops:\n        config_modifiers_init[\"use_subquadratic_ops\"] = True\n    if args.hybrid_override_pattern:\n        config_modifiers_init[\"hybrid_override_pattern\"] = args.hybrid_override_pattern\n    if args.num_layers:\n        config_modifiers_init[\"num_layers\"] = args.num_layers\n\n    model_type = infer_model_type(args.model_size)\n\n    # Create model based on selected model type\n    if model_type == \"hyena\":\n        if args.model_size not in HYENA_MODEL_OPTIONS:\n            raise ValueError(f\"Invalid model size for Hyena: {args.model_size}\")\n        model_config = HYENA_MODEL_OPTIONS[args.model_size](**config_modifiers_init)\n        if args.no_weight_decay_embeddings:\n            # Override the default weight decay condition for Hyena with our bionemo version that also excludes\n            #  embeddings\n            model_config.hyena_no_weight_decay_cond_fn = hyena_no_weight_decay_cond_with_embeddings\n        # Lora adaptors configuration\n        lora_transform = None\n        if args.lora_finetune:\n            lora_kwargs = {\n                k: v\n                for k, v in {\n                    \"alpha\": args.lora_alpha,\n                    \"dim\": args.lora_dim,\n                }.items()\n                if v is not None\n            }\n\n            lora_transform = Evo2LoRA(peft_ckpt_path=args.lora_checkpoint_path, **lora_kwargs)\n\n        model = llm.HyenaModel(model_config, tokenizer=data_module.tokenizer, model_transform=lora_transform)\n    elif model_type == \"mamba\":  # mamba\n        if args.no_weight_decay_embeddings:\n            config_modifiers_init[\"hyena_no_weight_decay_cond_fn\"] = mamba_no_weight_decay_cond_with_embeddings\n        config_modifiers_init[\"lowercase_loss_reweighting\"] = args.mamba_lowercase_loss_weight\n        if args.model_size not in MAMBA_MODEL_OPTIONS:\n            raise ValueError(f\"Invalid model size for Mamba: {args.model_size}\")\n        model_config = MAMBA_MODEL_OPTIONS[args.model_size](**config_modifiers_init)\n        model = MambaModel(model_config, tokenizer=data_module.tokenizer)\n    elif model_type == \"llama\":\n        config_modifiers_init.pop(\"to_upper\")  # llama model does not handle custom loss renormalization settings.\n        model_config = LLAMA_MODEL_OPTIONS[args.model_size](**config_modifiers_init)\n        model = llm.LlamaModel(model_config, tokenizer=data_module.tokenizer)\n\n    # Setup callbacks.\n    callbacks = [\n        RichModelSummary(max_depth=4),\n        LearningRateMonitor(),\n        TimingCallback(),\n        TEVCallback(),\n    ]\n\n    callbacks.append(_FirstBatchCudaSync())\n\n    if args.garbage_collect_at_inference:\n        callbacks.append(GarbageCollectAtInferenceTime())\n\n    if args.lora_finetune:\n        callbacks.append(lora_transform)\n    if args.enable_preemption:\n        callbacks.append(nl_callbacks.PreemptionCallback())\n    if args.debug_ddp_parity_freq &gt; 0:\n        callbacks.append(nl_callbacks.DdpParityChecker(interval=args.debug_ddp_parity_freq))\n    if args.log_parameters_and_shapes:\n        callbacks.append(nl_callbacks.ParameterDebugger())\n    if args.create_tflops_callback:\n        # Add callback that logs the tera-FLOPS per second per GPU during training.\n        flop_meas_callback = FLOPsMeasurementCallback(\n            model_config,\n            data_module,\n            \"hyena\",\n        )\n        callbacks.append(flop_meas_callback)\n\n    # TODO(@cye): Add this back when it works with 24.12.\n    # if args.straggler_detection:\n    #     callbacks.append(\n    #         res_module.StragglerDetectionCallback(\n    #             report_time_interval=300,\n    #             calc_relative_gpu_perf=True,\n    #             calc_individual_gpu_perf=True,\n    #             num_gpu_perf_scores_to_print=5,\n    #             gpu_relative_perf_threshold=0.7,\n    #             gpu_individual_perf_threshold=0.7,\n    #             stop_if_detected=True,\n    #             enable_ptl_logging=True,\n    #         )\n    #     )\n    if args.use_megatron_comm_overlap_llama3_8k:\n        # Pick the floating point appropriate config.\n        if args.fp8:\n            tp_comm_overlap_cfg = userbuffers_fp8_h100_h8192_tp4_mbs1_seqlen8192\n        else:\n            tp_comm_overlap_cfg = userbuffers_bf16_h100_h8192_tp4_mbs1_seqlen8192\n        callbacks.append(\n            MegatronCommOverlapCallback(\n                tp_comm_overlap=model_config.tp_comm_overlap,\n                tp_comm_overlap_cfg=tp_comm_overlap_cfg,\n                tp_comm_bootstrap_backend=args.tp_comm_overlap_backend,\n                wgrad_deferral_limit=22,  # default from NeMo\n                overlap_param_gather_with_optimizer_step=False,  # Currently disabled due to an issue with checkpointing.\n                align_param_gather=args.align_param_gather,\n            )\n        )\n\n    if args.gc_interval &gt; 0:\n        callbacks.append(\n            nl_callbacks.GarbageCollectionCallback(\n                gc_interval_train=args.gc_interval, gc_interval_val=args.gc_interval\n            )\n        )\n    if args.nsys_profiling:\n        if args.nsys_end_step is None:\n            nsys_end_step = args.max_steps\n        else:\n            nsys_end_step = args.nsys_end_step\n        callbacks.append(\n            nl_callbacks.NsysCallback(\n                start_step=args.nsys_start_step, end_step=nsys_end_step, ranks=args.nsys_ranks, gen_shape=True\n            )\n        )\n    # Average in collective is only supported when per-token loss is not calculated.\n    average_in_collective = args.average_in_collective and args.no_calculate_per_token_loss\n    wandb_run_name = (\n        f\"evo2-size-{args.model_size}-TP{args.tensor_parallel_size}-\"\n        f\"PP{args.pipeline_model_parallel_size}-CP{args.context_parallel_size}\"\n        f\"-GBS{global_batch_size}-MBS{args.micro_batch_size}-SkipLossRenorm{args.no_renormalize_loss}\"\n        f\"-NOAC{args.no_activation_checkpointing}-SELAC{args.selective_activation_checkpointing}\"\n        f\"-ACRNL{model_config.recompute_num_layers}\"\n        f\"-PAT{getattr(model_config, 'hybrid_override_pattern', 'None')}\"\n        f\"-F32R{model_config.fp32_residual_connection}\"\n        f\"-FCE{model_config.cross_entropy_loss_fusion}\"\n        f\"-AIC{average_in_collective}\"\n        f\"-PTL{not args.no_calculate_per_token_loss}\"\n        f\"-PEOD{args.eod_pad_in_loss_mask}\"\n        f\"-BO{args.add_bias_output}\"\n        f\"-GCLP{args.clip_grad}\"\n        f\"-HDO{args.hidden_dropout}\"\n        f\"-ADO{args.attention_dropout}\"\n        f\"-LR{args.lr}-MINLR{args.min_lr}-WUSTEPS{args.warmup_steps}-CONSTSTEPS{args.constant_steps}-WD{args.wd}\"\n        f\"-GRFP32{args.grad_reduce_in_fp32}-FP8WG{args.fp8_wgrad and args.fp8}\"\n        f\"-B1{args.adam_beta1}-B2{args.adam_beta2}-EPS{args.adam_eps}\"\n        f\"-PAO{args.use_precision_aware_optimizer}\"\n        f\"-B16MG{args.bf16_main_grads}\"\n        f\"-EWD{args.no_weight_decay_embeddings}-SNI{args.spike_no_more_embedding_init}\"\n        f\"-OGR{args.overlap_grad_reduce}-OPG{args.overlap_param_gather}\"\n        f\"-TVL{args.use_targeted_variance_loss}\"\n        f\"-NODES{args.num_nodes}-FP8{args.fp8}\"\n    )\n    if model_type == \"mamba\":\n        # Include this setting for mamba models.\n        wandb_run_name += f\"-LLW{args.mamba_lowercase_loss_weight}\"\n    elif model_type == \"llama\":\n        wandb_run_name += f\"-LLAMA{args.model_size}\"\n\n    wandb_config: Optional[WandbConfig] = (\n        None\n        if args.wandb_project is None\n        else WandbConfig(\n            offline=args.wandb_offline,\n            project=args.wandb_project,\n            name=args.wandb_run_name if args.wandb_run_name is not None else wandb_run_name,\n            entity=args.wandb_entity,\n            tags=args.wandb_tags,\n            group=args.wandb_group,\n            job_type=args.wandb_job_type,\n            id=args.wandb_id,\n            anonymous=args.wandb_anonymous,\n            log_model=args.wandb_log_model,\n        )\n    )\n    nemo_logger = setup_nemo_lightning_logger(\n        root_dir=args.result_dir,\n        name=args.experiment_name,\n        initialize_tensorboard_logger=args.create_tensorboard_logger,\n        wandb_config=wandb_config,\n    )\n\n    # Ensure window logging directory lives under the run directory\n    if args.sharded_eden_data and args.log_windows:\n        window_log_leaf = Path(args.window_log_dir).name if args.window_log_dir else \"window_logs\"\n        window_log_dir = Path(nemo_logger.save_dir) / window_log_leaf\n        try:\n            window_log_dir.mkdir(parents=True, exist_ok=True)\n        except Exception:\n            pass\n        # Propagate to data module (datasets are built later during setup)\n        if isinstance(data_module, ShardedEdenDataModule):\n            data_module.log_dir = str(window_log_dir)\n\n    if args.create_checkpoint_callback:\n        checkpoint_path = str(Path(nemo_logger.save_dir) / \"checkpoints\")\n        checkpoint_callback = nl_callbacks.ModelCheckpoint(\n            dirpath=checkpoint_path,\n            save_last=args.save_last_checkpoint,\n            monitor=args.metric_to_monitor_for_checkpoints,\n            save_top_k=args.save_top_k,\n            every_n_train_steps=args.val_check_interval,\n            always_save_context=True,\n            filename=\"{epoch}-{step}-{consumed_samples}\",\n            save_weights_only=False,\n            save_optim_on_train_end=True,\n            save_context_on_train_end=True,\n        )\n        callbacks.append(checkpoint_callback)\n\n        # Note: `nl.AutoResume` is only created if a `ModelCheckpoint` exists, because `nl.AutoResume.setup()`\n        # expects the trainer to have a `checkpoint_callback` set. See: https://github.com/NVIDIA/NeMo/blob/29c230b8a3352bef2128ba2d226a327d52d05be3/nemo/lightning/resume.py#L128\n        #\n        # In principle, this shouldn't be a constraint \u2014 it should be possible to create `nl.AutoResume` even if\n        # checkpointing is not enabled.\n        auto_resume = nl.AutoResume(\n            resume_if_exists=True,\n            resume_ignore_no_checkpoint=True,\n            resume_past_end=False,\n            resume_from_directory=checkpoint_path,\n            restore_config=(\n                RestoreConfig(\n                    path=args.ckpt_dir,\n                    load_model_state=True,\n                    load_optim_state=args.restore_optimizer_from_ckpt,\n                )\n                if args.ckpt_dir\n                else None\n            ),\n        )\n    else:\n        auto_resume = None\n\n    ddp: DistributedDataParallelConfig = DistributedDataParallelConfig(\n        check_for_nan_in_grad=not args.no_check_for_nan_in_grad,\n        overlap_grad_reduce=args.overlap_grad_reduce,\n        overlap_param_gather=args.overlap_param_gather,  # Verify that this works using\n        grad_reduce_in_fp32=args.grad_reduce_in_fp32,\n        align_param_gather=args.align_param_gather,\n        average_in_collective=average_in_collective,\n    )\n    # Initialize Megatron Strategy and Trainer.\n    strategy = nl.MegatronStrategy(\n        ddp=ddp,\n        tensor_model_parallel_size=args.tensor_parallel_size,\n        pipeline_model_parallel_size=args.pipeline_model_parallel_size,\n        context_parallel_size=args.context_parallel_size,\n        pipeline_dtype=torch.bfloat16,\n        sequence_parallel=args.sequence_parallel,\n        ckpt_load_optimizer=True,\n        ckpt_save_optimizer=True,\n        ckpt_async_save=args.ckpt_async_save,\n        save_ckpt_format=args.ckpt_format,\n        ckpt_load_strictness=\"log_all\",  # or rebasing to https://github.com/NVIDIA/NeMo/pull/11988/files#diff-7667eae242a8ef776bff78cd08e79bc81df4896a450f0a781f6ed317a3dfb7ffR139\n        fp8_recipe=None,\n    )\n    trainer = nl.Trainer(\n        devices=args.devices,\n        num_nodes=args.num_nodes,\n        max_steps=args.max_steps if args.early_stop_on_step is None else args.early_stop_on_step,\n        accelerator=\"gpu\",\n        strategy=strategy,\n        callbacks=callbacks,\n        log_every_n_steps=args.log_every_n_steps,\n        limit_val_batches=args.limit_val_batches,\n        limit_test_batches=args.limit_test_batches if args.limit_test_batches is not None else args.limit_val_batches,\n        num_sanity_val_steps=0,\n        use_distributed_sampler=False,\n        plugins=nl.MegatronMixedPrecision(\n            precision=\"bf16-mixed\",\n            fp8_recipe=args.fp8_recipe,\n            params_dtype=torch.bfloat16,\n            grad_reduce_in_fp32=args.grad_reduce_in_fp32,\n            fp8=\"hybrid\" if args.fp8 else None,\n            fp8_amax_history_len=16 if args.fp8 else 1,\n            fp8_amax_compute_algo=\"max\" if args.fp8 else \"most_recent\",\n            fp8_wgrad=args.fp8\n            and (\n                args.fp8_wgrad or args.use_megatron_comm_overlap_llama3_8k\n            ),  # faster and less accurate when set to True, and MUST be True if using TP communication overlap\n        ),\n        val_check_interval=args.val_check_interval,\n        enable_checkpointing=args.create_checkpoint_callback,\n    )\n\n    # Optimizer and scheduler setup\n    opt_config = OptimizerConfig(\n        optimizer=\"adam\",\n        lr=args.lr,\n        adam_beta1=args.adam_beta1,\n        adam_beta2=args.adam_beta2,\n        weight_decay=args.wd,\n        clip_grad=args.clip_grad,\n        adam_eps=args.adam_eps,\n        use_distributed_optimizer=True,\n        log_num_zeros_in_grad=args.log_num_zeros_in_grad,\n        use_precision_aware_optimizer=args.use_precision_aware_optimizer,\n        main_grads_dtype=torch.bfloat16 if args.bf16_main_grads else torch.float32,\n        bf16=True,\n        fp8_recipe=None,\n    )\n\n    sched = CosineAnnealingScheduler(\n        max_steps=trainer.max_steps,\n        warmup_steps=args.warmup_steps,\n        min_lr=args.min_lr,\n        constant_steps=args.constant_steps,\n    )\n    # This is where the no weight decay condition is applied to the optimizer state.\n    opt = MegatronOptimizerModule(\n        opt_config, sched, no_weight_decay_cond=getattr(model_config, \"hyena_no_weight_decay_cond_fn\", None)\n    )\n    llm.train(model, data_module, trainer, log=nemo_logger, resume=auto_resume, optim=opt, tokenizer=\"data\")\n\n    return trainer\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/run/utils/","title":"Utils","text":"<p>Utility functions for Evo2 run functions.</p>"},{"location":"main/references/API_reference/bionemo/evo2/run/utils/#bionemo.evo2.run.utils.infer_model_type","title":"<code>infer_model_type(model_size)</code>","text":"<p>Infer the model type from the model size.</p> Source code in <code>bionemo/evo2/run/utils.py</code> <pre><code>def infer_model_type(model_size: str) -&gt; Literal[\"hyena\", \"mamba\", \"llama\"]:\n    \"\"\"Infer the model type from the model size.\"\"\"\n    all_keys = set(HYENA_MODEL_OPTIONS.keys()) | set(MAMBA_MODEL_OPTIONS.keys()) | set(LLAMA_MODEL_OPTIONS.keys())\n    if len(all_keys) != len(HYENA_MODEL_OPTIONS.keys()) + len(MAMBA_MODEL_OPTIONS.keys()) + len(\n        LLAMA_MODEL_OPTIONS.keys()\n    ):\n        raise ValueError(\n            \"Duplicate model sizes found in HYENA_MODEL_OPTIONS, MAMBA_MODEL_OPTIONS, and LLAMA_MODEL_OPTIONS.\"\n        )\n    if model_size in HYENA_MODEL_OPTIONS:\n        return \"hyena\"\n    elif model_size in MAMBA_MODEL_OPTIONS:\n        return \"mamba\"\n    elif model_size in LLAMA_MODEL_OPTIONS:\n        return \"llama\"\n    else:\n        raise ValueError(f\"Invalid model size: {model_size}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/run/utils/#bionemo.evo2.run.utils.patch_eden_tokenizer","title":"<code>patch_eden_tokenizer(tokenizer)</code>","text":"<p>Patch the Eden tokenizer to work with the Evo2 tokenizer.</p> Source code in <code>bionemo/evo2/run/utils.py</code> <pre><code>def patch_eden_tokenizer(tokenizer):\n    \"\"\"Patch the Eden tokenizer to work with the Evo2 tokenizer.\"\"\"\n    bos_id, eos_id, sep_id, pad_id = 1, 2, 3, 0\n\n    # Patch the private attrs so tokenizer.bos_id/.eos_id/.pad_id work\n    tokenizer._bos_id = bos_id\n    tokenizer._eos_id = eos_id\n    tokenizer._sep_id = sep_id\n    tokenizer._pad_id = pad_id\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/callbacks/","title":"Callbacks","text":""},{"location":"main/references/API_reference/bionemo/evo2/utils/callbacks/#bionemo.evo2.utils.callbacks.GarbageCollectAtInferenceTime","title":"<code>GarbageCollectAtInferenceTime</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to clean up CUDA memory before validation to prevent initialization errors.</p> Source code in <code>bionemo/evo2/utils/callbacks.py</code> <pre><code>class GarbageCollectAtInferenceTime(Callback):\n    \"\"\"Callback to clean up CUDA memory before validation to prevent initialization errors.\"\"\"\n\n    def on_validation_start(self, trainer, pl_module) -&gt; None:\n        \"\"\"Clean up CUDA memory before validation to prevent initialization errors.\"\"\"\n        if torch.cuda.is_available():\n            try:\n                torch.cuda.empty_cache()\n                torch.cuda.synchronize()\n                current_device = torch.cuda.current_device()\n                torch.cuda.set_device(current_device)\n                torch.cuda.synchronize()\n                gc.collect()\n            except Exception as e:\n                print(f\"Warning: CUDA cleanup failed: {e}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/callbacks/#bionemo.evo2.utils.callbacks.GarbageCollectAtInferenceTime.on_validation_start","title":"<code>on_validation_start(trainer, pl_module)</code>","text":"<p>Clean up CUDA memory before validation to prevent initialization errors.</p> Source code in <code>bionemo/evo2/utils/callbacks.py</code> <pre><code>def on_validation_start(self, trainer, pl_module) -&gt; None:\n    \"\"\"Clean up CUDA memory before validation to prevent initialization errors.\"\"\"\n    if torch.cuda.is_available():\n        try:\n            torch.cuda.empty_cache()\n            torch.cuda.synchronize()\n            current_device = torch.cuda.current_device()\n            torch.cuda.set_device(current_device)\n            torch.cuda.synchronize()\n            gc.collect()\n        except Exception as e:\n            print(f\"Warning: CUDA cleanup failed: {e}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/config/","title":"Config","text":""},{"location":"main/references/API_reference/bionemo/evo2/utils/config/#bionemo.evo2.utils.config.Evo2PreprocessingConfig","title":"<code>Evo2PreprocessingConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic model class specifying the configuration schema for a preprocessed IndexedDataset (.bin, .idx).</p> Source code in <code>bionemo/evo2/utils/config.py</code> <pre><code>class Evo2PreprocessingConfig(BaseModel):\n    \"\"\"Pydantic model class specifying the configuration schema for a preprocessed IndexedDataset (.bin, .idx).\"\"\"\n\n    # Paths\n    datapaths: list[Path] = []\n    output_dir: None | Path = None\n    output_prefix: None | str = None\n    # Random Datasplit\n    train_split: float = 0.7\n    valid_split: float = 0.2\n    test_split: float = 0.1\n    # Overwrite existing binaries. Otherwise, skip already preprocessed datasets.\n    overwrite: bool = False\n    # Raw Preprocessing Transforms\n    embed_reverse_complement: bool = False\n    random_reverse_complement: float = 0.0\n    random_lineage_dropout: float = 0.0\n    transcribe: None | Literal[\"transcribe\", \"back_transcribe\"] = None\n    force_uppercase: bool = False\n    indexed_dataset_dtype: str = \"uint8\"\n    # Tokenization Transforms\n    append_eod: bool = True\n    enforce_sample_length: None | int = None\n    ftfy: bool = False\n    # NeMo Tokenizer Configuration\n    tokenizer_type: Literal[\n        \"Byte-Level\",\n        \"HuggingFace\",\n        \"SentencePiece\",\n        \"Regex\",\n        \"Megatron\",\n        \"Tiktoken\",\n    ] = \"Byte-Level\"\n    vocab_file: None | Path = None\n    vocab_size: None | int = 512\n    merges_file: None | Path = None\n    tokenizer_model_name: None | str = None\n    pretrained_tokenizer_model: None | str = None\n    special_tokens: None | dict[str, str] = {}\n    fast_hf_tokenizer: bool = False\n    # Compute Configuration\n    # NOTE: If preprocessing a large amount of short individual sequences (&lt; 1000 bp), do NOT use\n    # multiprocessing (workers &gt; 1) because sequence-level parallel IPC will dominate the preprocessing time!\n    workers: int = 1\n    preproc_concurrency: int = 100000\n    chunksize: int = 1\n    # Filters\n    drop_empty_sequences: bool = False\n    nnn_filter: bool = False\n    # RNG\n    seed: None | int = None\n    # Evo2 Taxonomic Lineage Tags\n    # SeqID Sub-String Indexing: \"ABC\" will have taxonomy data from \"A\".\n    taxonomy_data: dict[str, Evo2TaxonomyLineage] = {}\n    # Periodicity of injecting phylogenetic lineage tags in the sequence prior to tokenization.\n    prompt_spacer_length: int = 131072\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/config/#bionemo.evo2.utils.config.Evo2TaxonomyLineage","title":"<code>Evo2TaxonomyLineage</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic model class that defines the source lineage of a DNA sequence.</p> Source code in <code>bionemo/evo2/utils/config.py</code> <pre><code>class Evo2TaxonomyLineage(BaseModel):\n    \"\"\"Pydantic model class that defines the source lineage of a DNA sequence.\"\"\"\n\n    domain: None | str = None\n    phylum: None | str = None\n    clazz: None | str = None\n    order: None | str = None\n    family: None | str = None\n    genus: None | str = None\n    species: None | str = None\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/config/#bionemo.evo2.utils.config.hyena_no_weight_decay_cond_with_embeddings","title":"<code>hyena_no_weight_decay_cond_with_embeddings(name, param)</code>","text":"<p>Condition for no weight decay for Hyena parameters with embeddings.</p> Source code in <code>bionemo/evo2/utils/config.py</code> <pre><code>def hyena_no_weight_decay_cond_with_embeddings(name, param):\n    \"\"\"Condition for no weight decay for Hyena parameters with embeddings.\"\"\"\n    if \"embedding\" in name:\n        return True\n    return hyena_no_weight_decay_cond(name, param)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/","title":"Evo2 Checkpoint Conversion Library","text":"<p>This library contains helper scripts for converting checkpoint formats for Evo2.</p>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/#converting-zero-1-pytorch-checkpoints-to-nemo2-checkpoints","title":"Converting ZeRO-1 / PyTorch Checkpoints to NeMo2 Checkpoints","text":"<p>To convert a single PyTorch or ZeRO-1 checkpoints (<code>.pt</code>) into NeMo2 format, run the following command:</p> <pre><code>python sub-packages/bionemo-evo2/src/bionemo/evo2/utils/checkpoint/convert_to_nemo.py --model-path &lt;CKPT_FILE&gt; --output-dir &lt;OUTPUT_DIR&gt; --model-size &lt;MODEL_SIZE&gt; --ckpt-format &lt;CONVERTED_CKPT_FORMAT&gt;\n</code></pre> <p>where <code>--model-size</code> can be set to <code>7b</code> or <code>40b</code> (or their <code>_arc_1m</code> variants with modified GLU dimensions) and <code>--ckpt-format</code> can be set to <code>torch_dist</code> or <code>zarr</code>.</p> <p>The NeMo2 checkpoint should have the following structure for <code>torch_dist</code>:</p> <pre><code>default--val_loss=2.3738-epoch=0-consumed_samples=800.0-last\n\u251c\u2500\u2500 context\n\u2502   \u251c\u2500\u2500 io.json\n\u2502   \u2514\u2500\u2500 model.yaml\n\u2514\u2500\u2500 weights\n    \u251c\u2500\u2500 __*_*.distcp\n    \u251c\u2500\u2500 common.pt\n    \u2514\u2500\u2500 metadata.json\n</code></pre> <p>and the following structure for <code>zarr</code>:</p> <pre><code>interleaved_hyena_7b_fix_shape\n\u251c\u2500\u2500 context\n\u2502   \u251c\u2500\u2500 io.json\n\u2502   \u2514\u2500\u2500 model.yaml\n\u2514\u2500\u2500 weights\n    \u251c\u2500\u2500 common.pt\n    \u251c\u2500\u2500 metadata.json\n    \u2514\u2500\u2500 &lt;MODEL_LAYER_NAME&gt;  # Example: module.decoder.layers.0.mixer.dense\n        \u2514\u2500\u2500 shard_*_*.pt\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/#converting-zero-1-mpn-to-zero-1-mp1","title":"Converting ZeRO-1 MP{N} to ZeRO-1 MP1","text":"<p>To convert sharded (MP&gt;1) ZeRO-1 checkpoints to un-sharded (MP1) checkpoints (or any order of model parallelism) compatible with the <code>convert_to_nemo.py</code> conversion script, you can run the following command:</p> <pre><code>python sub-packages/bionemo-evo2/src/bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2.py --source_dir &lt;CKPT_DIR&gt; --output_dir &lt;OUTPUT_DIR&gt; --mp_size &lt;TARGET_MODEL_PARALLEL_SIZE&gt;\n</code></pre> <p>ZeRO-1 checkpoints should have the following structure:</p> <pre><code>arc_7b_tp8_pretrained_ckpt/global_step199400\n\u2514\u2500\u2500 mp_rank_*_model_states.pt\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/#converting-zero-3-to-zero-1","title":"Converting ZeRO-3 to ZeRO-1","text":"<p>To convert ZeRO-3 checkpoints into ZeRO-1 checkpoints, run the following command:</p> <pre><code>python sub-packages/bionemo-evo2/src/bionemo/evo2/utils/checkpoint/convert_zero3_to_zero1.py &lt;INPUT_DIR&gt; &lt;OUTPUT_DIR&gt; --overwrite --mp_size &lt;MODEL_PARALLEL_SIZE&gt;\n</code></pre> <p>ZeRO-3 checkpoints should have the following structure:</p> <pre><code>arc_40b_zero3_w32_mp8_test_notfinal_ckpt/global_step1\n\u251c\u2500\u2500 bf16_zero_pp_rank_*_mp_rank_*_optim_states.pt\n\u251c\u2500\u2500 configs\n\u2502   \u251c\u2500\u2500 40b_test_chkpt.yml\n\u2502   \u2514\u2500\u2500 opengenome.yml\n\u2514\u2500\u2500 zero_pp_rank_*_mp_rank_*_model_states.pt\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2/","title":"Convert checkpoint model parallel evo2","text":"<p>This script converts (potentially sharded) ZeRo1 checkpoint parameters to the desired level of model tensor parallelism for the Evo 2 architecture.</p> <p>It only supports Zero-1 checkpoints and does not convert any optimizer state, only the parameters.</p> Usage <p>python convert_checkpoint_model_parallel_evo2.py         --input-checkpoint-dir /path/to/input/checkpoint/global_step1000         --output-checkpoint-dir /path/to/output/checkpoint_mp2/global_step1000         --output-model-parallelism 2</p>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2/#bionemo.evo2.utils.checkpoint.convert_checkpoint_model_parallel_evo2.check_params","title":"<code>check_params(detected, expected, buffers, param_pattern=DEFAULT_PARAM_PATTERN, verbose=False)</code>","text":"<p>Check that all model parameters are expected.</p> <p>Parameters:</p> Name Type Description Default <code>detected</code> <code>List[str]</code> <p>Detected model parameters names.</p> required <code>expected</code> <code>Set[str]</code> <p>Expected model parameters names.</p> required <code>buffers</code> <code>Set[str]</code> <p>Set of buffer names.</p> required <code>param_pattern</code> <code>str</code> <p>Regex pattern to match parameter names. Defaults to DEFAULT_PARAM_PATTERN.</p> <code>DEFAULT_PARAM_PATTERN</code> <code>verbose</code> <code>bool</code> <p>Whether to print detailed information. Defaults to False.</p> <code>False</code> Source code in <code>bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2.py</code> <pre><code>def check_params(\n    detected: List[str],\n    expected: Union[Set[str], List[str]],\n    buffers: Set[str],\n    param_pattern: str = DEFAULT_PARAM_PATTERN,\n    verbose: bool = False,\n):\n    \"\"\"Check that all model parameters are expected.\n\n    Args:\n        detected (List[str]): Detected model parameters names.\n        expected (Set[str]): Expected model parameters names.\n        buffers (Set[str]): Set of buffer names.\n        param_pattern (str, optional): Regex pattern to match parameter names. Defaults to DEFAULT_PARAM_PATTERN.\n        verbose (bool, optional): Whether to print detailed information. Defaults to False.\n    \"\"\"\n    # Expected model parameters.\n    expected = set(expected) if not isinstance(expected, set) else expected\n    # Detected model parameters.\n    model_param_names = []\n    for k in detected:\n        match = re.search(param_pattern, k)\n        if match is not None:\n            model_param_names.append(match.group(1))\n        else:\n            logging.info(f\"Could not match {k}\")\n    detected_param_set = set(model_param_names)\n    if verbose:\n        logging.info(\"Detected Params:\\n  {detected_params}\".format(detected_params=\"\\n  \".join(detected_param_set)))\n\n    # Log unexpected model parameters.\n    missing_params = expected - detected_param_set\n    extra_params = detected_param_set - expected\n    extra_params = [param for param in extra_params if param not in buffers]\n    extra_params = [param for param in extra_params if not param.endswith(\"._extra_state\")]\n    if len(extra_params) &gt; 0:\n        logging.info(f\"WARNING: detected extra params: {extra_params}\")\n    if len(missing_params) &gt; 0:\n        logging.info(f\"WARNING: missing params: {missing_params}\")\n    if not (extra_params or missing_params):\n        logging.info(\"No missing or extra params detected!\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2/#bionemo.evo2.utils.checkpoint.convert_checkpoint_model_parallel_evo2.concatenate_tensors_across_shards","title":"<code>concatenate_tensors_across_shards(tensor_name, data_shards, partition_dim, hidden_dim=None, verbose=False)</code>","text":"<p>Concatenate tensor shards across multiple shards.</p> <p>Parameters:</p> Name Type Description Default <code>tensor_name</code> <code>str</code> <p>Name of the tensor to concatenate.</p> required <code>data_shards</code> <code>List[OrderedDict[str, Tensor]]</code> <p>List of data shards containing tensors.</p> required <code>partition_dim</code> <code>int</code> <p>Dimension along which to partition the tensor.</p> required <code>hidden_dim</code> <code>int</code> <p>Hidden dimension of the tensor. Defaults to None.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print detailed information. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Concatenated tensor.</p> Source code in <code>bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2.py</code> <pre><code>def concatenate_tensors_across_shards(\n    tensor_name: str,\n    data_shards: List[OrderedDict[str, torch.Tensor]],\n    partition_dim: int,\n    hidden_dim: Optional[int] = None,\n    verbose: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Concatenate tensor shards across multiple shards.\n\n    Args:\n        tensor_name (str): Name of the tensor to concatenate.\n        data_shards (List[OrderedDict[str, torch.Tensor]]): List of data shards containing tensors.\n        partition_dim (int): Dimension along which to partition the tensor.\n        hidden_dim (int, optional): Hidden dimension of the tensor. Defaults to None.\n        verbose (bool, optional): Whether to print detailed information. Defaults to False.\n\n    Returns:\n        torch.Tensor: Concatenated tensor.\n    \"\"\"\n    # Retrieve tensor shards.\n    tensors = [shard[\"module\"][tensor_name] for shard in data_shards]\n\n    # Check shape of tensors without tensor parallelism, i.e. stored in all shards of the checkpoint.\n    if partition_dim is None:\n        for i, tensor in enumerate(tensors):\n            if not torch.allclose(tensors[0], tensor):\n                logging.info(\n                    f\"WARNING: Synchronized params differ for param {tensor_name}: abs max diff = {(tensors[0] - tensor).abs().max()}.\"\n                )\n                # Get the distribution of tensors[0] and tensor.\n                if verbose:\n                    ref_tensor = tensors[0].flatten().to(torch.float32)\n                    ref_min, ref_max = ref_tensor.min(), ref_tensor.max()\n\n                    q = torch.tensor([0.25, 0.5, 0.75], device=ref_tensor.device)\n                    ref_quantiles = ref_tensor.quantile(q)\n                    logging.info(f\"rank0 tensor: min={ref_min}, max={ref_max} quantiles={ref_quantiles}\")\n\n                    target_tensor = tensor.flatten().to(torch.float32)\n                    target_min, target_max = target_tensor.min(), target_tensor.max()\n                    target_quantiles = target_tensor.quantile(q)\n                    logging.info(f\"rank{i} tensor: min={target_min}, max={target_max} quantiles={target_quantiles}\")\n\n                    logging.info(f\"rank0 tensor distribution:\\n {ref_tensor.histc(100, min=ref_min, max=ref_max)}\")\n                    logging.info(f\"rank{i} distribution:\\n {target_tensor.histc(100, min=ref_min, max=ref_max)}\")\n\n        logging.info(f\"tensor {tensor_name} not partitioned, returning rank0 tensor {tensors[0].shape}\")\n        return tensors[0]\n    # Check for sharding across the hidden dimension.\n    elif partition_dim == hidden_dim:\n        raise ValueError(f\"Detected sharding for {tensor_name} across hidden dimension at index {hidden_dim}.\")\n\n    # Check that the tensors have a consistent hidden dimension.\n    expected_dim = None\n    if hidden_dim is not None:\n        for tensor in tensors:\n            if expected_dim is None:\n                # Store expected hidden dimension for all tensors.\n                expected_dim = tensor.shape[hidden_dim]\n            if not tensor.shape[hidden_dim] == expected_dim:\n                raise ValueError(f\"Tensor {tensor_name} has invalid hidden shape {tensor.shape}.\")\n\n    # Concatenate shards.\n    return torch.cat(tensors, dim=partition_dim)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2/#bionemo.evo2.utils.checkpoint.convert_checkpoint_model_parallel_evo2.convert_model_weights","title":"<code>convert_model_weights(input_data_shards, output_data_shards, model_parameter_names, param_list, verbose=False, exclude_extra=False)</code>","text":"<p>Convert model weights from input model parallelism to output model parallelism.</p> <p>Parameters:</p> Name Type Description Default <code>input_data_shards</code> <code>List[OrderedDict]</code> <p>List of input data shards.</p> required <code>output_data_shards</code> <code>List[OrderedDict]</code> <p>List of output data shards.</p> required <code>model_parameter_names</code> <code>List[str]</code> <p>List of model parameter names.</p> required <code>param_list</code> <code>List[Param]</code> <p>List of parameter information.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print detailed information. Defaults to False.</p> <code>False</code> <code>exclude_extra</code> <code>bool</code> <p>Whether to exclude extra states in the conversion. Defaults to False.</p> <code>False</code> Source code in <code>bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2.py</code> <pre><code>def convert_model_weights(\n    input_data_shards: List[OrderedDict],\n    output_data_shards: List[OrderedDict],\n    model_parameter_names: List[str],\n    param_list: List[Param],\n    verbose: bool = False,\n    exclude_extra: bool = False,\n):\n    \"\"\"Convert model weights from input model parallelism to output model parallelism.\n\n    Args:\n        input_data_shards (List[OrderedDict]): List of input data shards.\n        output_data_shards (List[OrderedDict]): List of output data shards.\n        model_parameter_names (List[str]): List of model parameter names.\n        param_list (List[Param]): List of parameter information.\n        verbose (bool, optional): Whether to print detailed information. Defaults to False.\n        exclude_extra (bool, optional): Whether to exclude extra states in the conversion. Defaults to False.\n    \"\"\"\n    logging.info(\n        f\"Converting {len(model_parameter_names)} parameters from {len(input_data_shards)} input shards to {len(output_data_shards)} output shards...\"\n    )\n    converted = 0\n    skipped = 0\n    for model_parameter in model_parameter_names:\n        if args.verbose:\n            logging.info(f\"Processing {model_parameter}...\")\n\n        # Ignore FP8 extra state.\n        if model_parameter.endswith(\"._extra_state\"):\n            if \"extra_state\" in model_parameter:\n                logging.info(f\"Ignoring {model_parameter} -&gt; contains extra state.\")\n            skipped += 1\n            continue\n\n        # Get the partition dimension and hidden dimension of each parameter.\n        param_info = None\n        for param in param_list:\n            if \".\".join(model_parameter.split(\".\")[2:]) == param.name:\n                if param_info is None:\n                    param_info = param\n                else:\n                    raise ValueError(\n                        f\"Found more than one matching model parallelism parameter for {model_parameter}: {param_info}, {param}\"\n                    )\n        if param_info is None:\n            raise ValueError(f\"Could not find {model_parameter} among known parameters.\")\n\n        # Concatenate shards.\n        concatenated_tensor = concatenate_tensors_across_shards(\n            model_parameter, input_data_shards, param_info.partition_dim, param_info.hidden_dim, verbose=verbose\n        )\n        # Split into shards.\n        split_tensor_across_shards(\n            output_data_shards,\n            concatenated_tensor,\n            model_parameter,\n            param_info.partition_dim,\n        )\n        converted += 1\n    logging.info(f\"Converted {converted} of {len(model_parameter_names)} parameters (skipped {skipped} params).\")\n    num_params = len(output_data_shards[0][\"module\"])\n    logging.info(f\"Total Params: {num_params}\")\n    if not all(num_params == len(shard[\"module\"]) for shard in output_data_shards):\n        raise ValueError(\"Shards have different number of parameters, which is not permitted in model parallelism.\")\n\n    if not exclude_extra:\n        logging.info(\"Adding extra states from rank0 input shard...\")\n        rank0_model = input_data_shards[0][\"module\"]\n        for k in rank0_model.keys():\n            for i, output_shard in enumerate(output_data_shards):\n                if k not in output_shard[\"module\"]:\n                    if i == 0:\n                        logging.info(f\"Adding {k} to output shards.\")\n                    output_shard[\"module\"][k] = rank0_model[k]\n        new_params = len(output_data_shards[0][\"module\"]) - num_params\n        logging.info(f\"Added {new_params} extra states, total params: {num_params + new_params}\")\n        if not all(num_params + new_params == len(shard[\"module\"]) for shard in output_data_shards):\n            raise ValueError(\"Shards have different number of parameters after adding extra states.\")\n\n    for shard_idx, output_data_shard in enumerate(output_data_shards):\n        output_path = Path(output_data_shard[\"output_dir\"]) / format_output_filename(shard_idx)\n        torch.save(\n            output_data_shard,\n            output_path,\n        )\n        logging.info(f\"Converted checkpoint saved to: {output_path}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2/#bionemo.evo2.utils.checkpoint.convert_checkpoint_model_parallel_evo2.convert_zero1_model_parallel_checkpoint","title":"<code>convert_zero1_model_parallel_checkpoint(source_dir, output_dir, glob_pattern='mp_rank_*_model_states.pt', model_parallel=8, param_list=EVO2_PARAMS, exclude_extra_params=False, verbose=False)</code>","text":"<p>Convert sharded ZeRo1 checkpoint to desired model parallelism.</p> <p>Parameters:</p> Name Type Description Default <code>source_dir</code> <code>str</code> <p>Path to the input checkpoint directory.</p> required <code>output_dir</code> <code>str</code> <p>Path to the output checkpoint directory.</p> required <code>glob_pattern</code> <code>str</code> <p>Filename pattern to glob for ZeRo1 checkpoint shards. Defaults to \"mp_rank_*_model_states.pt\".</p> <code>'mp_rank_*_model_states.pt'</code> <code>model_parallel</code> <code>int</code> <p>Desired output model parallelism. Defaults to 8.</p> <code>8</code> <code>param_list</code> <code>List[Param]</code> <p>List of parameter information. Defaults to EVO2_PARAMS.</p> <code>EVO2_PARAMS</code> <code>exclude_extra_params</code> <code>bool</code> <p>Whether to exclude extra states in the conversion. Defaults to False.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to print detailed information. Defaults to False.</p> <code>False</code> Source code in <code>bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2.py</code> <pre><code>def convert_zero1_model_parallel_checkpoint(\n    source_dir: str,\n    output_dir: str,\n    glob_pattern: str = \"mp_rank_*_model_states.pt\",\n    model_parallel: int = 8,\n    param_list: List[Param] = EVO2_PARAMS,\n    exclude_extra_params: bool = False,\n    verbose: bool = False,\n):\n    \"\"\"Convert sharded ZeRo1 checkpoint to desired model parallelism.\n\n    Args:\n        source_dir (str): Path to the input checkpoint directory.\n        output_dir (str): Path to the output checkpoint directory.\n        glob_pattern (str): Filename pattern to glob for ZeRo1 checkpoint shards. Defaults to \"mp_rank_*_model_states.pt\".\n        model_parallel (int): Desired output model parallelism. Defaults to 8.\n        param_list (List[Param]): List of parameter information. Defaults to EVO2_PARAMS.\n        exclude_extra_params (bool): Whether to exclude extra states in the conversion. Defaults to False.\n        verbose (bool): Whether to print detailed information. Defaults to False.\n    \"\"\"\n    # Argument validation.\n    if not os.path.exists(source_dir):\n        raise ValueError(f\"Input checkpoint dir ({source_dir}) not found.\")\n    os.makedirs(output_dir, exist_ok=True)\n    logging.info(f\"Converting checkpoint from {source_dir} to {output_dir}\")\n\n    # Identify all checkpoint model path files.\n    parameter_paths = sorted(glob(f\"{source_dir}/{glob_pattern}\"))\n    if len(parameter_paths) == 0:\n        raise ValueError(f\"No parameter files found in {source_dir}\")\n\n    # Load all shards from the ZeRo1 checkpoint.\n    input_data_shards = [torch.load(path, map_location=DEVICE) for path in parameter_paths]\n    buffers = {buf for x in input_data_shards for buf in x.get(\"buffer_names\", [])}\n\n    # Initialize output MP shards.\n    output_data_shards = [\n        {\n            \"module\": OrderedDict(),\n            \"param_shapes\": OrderedDict(),\n            \"dp_world_size\": input_data_shards[0][\"dp_world_size\"],\n            \"output_dir\": output_dir,\n        }\n        for _ in range(model_parallel)\n    ]\n    model_parameter_names = input_data_shards[0][\"module\"].keys()\n\n    # Check no missing or extra params\n    check_params(\n        detected=list(model_parameter_names),\n        expected={param.name for param in param_list},\n        buffers=buffers,\n        verbose=verbose,\n    )\n    # Convert the checkpoint\n    convert_model_weights(\n        input_data_shards,\n        output_data_shards,\n        model_parameter_names,\n        param_list,\n        verbose=verbose,\n        exclude_extra=exclude_extra_params,\n    )\n    logging.info(\"Done!\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2/#bionemo.evo2.utils.checkpoint.convert_checkpoint_model_parallel_evo2.format_output_filename","title":"<code>format_output_filename(shard)</code>","text":"<p>Format the output filename for a given shard index.</p> <p>Parameters:</p> Name Type Description Default <code>shard</code> <code>int</code> <p>Shard index.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Formatted output filename.</p> Source code in <code>bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2.py</code> <pre><code>def format_output_filename(shard: int) -&gt; str:\n    \"\"\"Format the output filename for a given shard index.\n\n    Args:\n        shard (int): Shard index.\n\n    Returns:\n        str: Formatted output filename.\n    \"\"\"\n    return f\"mp_rank_{str(shard).zfill(2)}_model_states.pt\"\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2/#bionemo.evo2.utils.checkpoint.convert_checkpoint_model_parallel_evo2.get_args","title":"<code>get_args()</code>","text":"<p>Parse command-line arguments.</p> Source code in <code>bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2.py</code> <pre><code>def get_args():\n    \"\"\"Parse command-line arguments.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Convert checkpoint parameters to desired model parallelism.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\n        \"--source_dir\",\n        type=str,\n        required=True,\n        help=\"Path to the input checkpoint directory containing ZeRo1 checkpoint shards, i.e. mp_rank_*_model_states.pt.\",\n    )\n    parser.add_argument(\n        \"--glob-pattern\",\n        type=str,\n        default=\"mp_rank_*_model_states.pt\",\n        required=False,\n        help=\"Filename pattern to glob for ZeRo1 checkpoint shards.\",\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        required=True,\n        help=\"Path to the output checkpoint directory to dump the --mp_size converted model checkpoint (ZeRo1).\",\n    )\n    parser.add_argument(\"--mp_size\", type=int, required=True, help=\"Desired output model parallelism to convert to.\")\n    parser.add_argument(\n        \"--exclude-extra\",\n        action=\"store_true\",\n        help=\"Exclude extra states in the conversion. Default to False, i.e. include extra states.\",\n    )\n    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Print more information about the conversion.\")\n    args = parser.parse_args()\n    return args\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2/#bionemo.evo2.utils.checkpoint.convert_checkpoint_model_parallel_evo2.split_tensor_across_shards","title":"<code>split_tensor_across_shards(data_shards, tensor, tensor_name, partition_dim)</code>","text":"<p>Split a tensor across multiple shards.</p> <p>Parameters:</p> Name Type Description Default <code>data_shards</code> <code>List[OrderedDict]</code> <p>List of data shards to store the split tensors.</p> required <code>tensor</code> <code>Tensor</code> <p>Tensor to split.</p> required <code>tensor_name</code> <code>str</code> <p>Name of the tensor.</p> required <code>partition_dim</code> <code>int</code> <p>Dimension along which to partition the tensor.</p> required Source code in <code>bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2.py</code> <pre><code>def split_tensor_across_shards(\n    data_shards: List[OrderedDict],\n    tensor: torch.Tensor,\n    tensor_name: str,\n    partition_dim: int,\n) -&gt; None:\n    \"\"\"Split a tensor across multiple shards.\n\n    Args:\n        data_shards (List[OrderedDict]): List of data shards to store the split tensors.\n        tensor (torch.Tensor): Tensor to split.\n        tensor_name (str): Name of the tensor.\n        partition_dim (int): Dimension along which to partition the tensor.\n    \"\"\"\n    if partition_dim is None:\n        # No sharding. Synchronize weights across all shards.\n        for data_shard in data_shards:\n            data_shard[\"module\"][tensor_name] = tensor\n            data_shard[\"param_shapes\"][tensor_name] = tensor.shape\n    else:\n        # Split the tensor along the partition dimension across shards.\n        n_shards = len(data_shards)\n        if tensor.shape[partition_dim] % n_shards != 0:\n            raise ValueError(\n                f\"Cannot shard {tensor_name} of dimension {tensor.shape[partition_dim]} across {n_shards} evenly.\"\n            )\n        for chunk, data_shard in zip(\n            torch.chunk(tensor, chunks=n_shards, dim=partition_dim),\n            data_shards,\n        ):\n            data_shard[\"module\"][tensor_name] = chunk.clone()\n            data_shard[\"param_shapes\"][tensor_name] = chunk.shape\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/convert_to_nemo/","title":"Convert to nemo","text":""},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/convert_to_nemo/#bionemo.evo2.utils.checkpoint.convert_to_nemo.main","title":"<code>main()</code>","text":"<p>Convert a PyTorch Evo2 model checkpoint to a NeMo model checkpoint.</p> Source code in <code>bionemo/evo2/utils/checkpoint/convert_to_nemo.py</code> <pre><code>def main():\n    \"\"\"Convert a PyTorch Evo2 model checkpoint to a NeMo model checkpoint.\"\"\"\n    args = parse_args()\n    model_type = infer_model_type(args.model_size)\n    if model_type == \"hyena\":\n        config_modifiers_init = {}\n        if args.use_subquadratic_ops:\n            config_modifiers_init[\"use_subquadratic_ops\"] = True\n        evo2_config = HYENA_MODEL_OPTIONS[args.model_size](**config_modifiers_init)\n        if args.model_path.startswith(\"hf://\"):\n            importer = HuggingFaceSavannaHyenaImporter(args.model_path.lstrip(\"hf://\"), model_config=evo2_config)\n        else:\n            importer = PyTorchHyenaImporter(args.model_path, model_config=evo2_config)\n    elif model_type == \"llama\":\n        importer = HFEdenLlamaImporter(args.model_path)\n    else:\n        raise ValueError(f\"Importer model type: {model_type}.\")\n    importer.apply(args.output_dir)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/convert_to_nemo/#bionemo.evo2.utils.checkpoint.convert_to_nemo.parse_args","title":"<code>parse_args()</code>","text":"<p>Parse command-line arguments.</p> Source code in <code>bionemo/evo2/utils/checkpoint/convert_to_nemo.py</code> <pre><code>def parse_args():\n    \"\"\"Parse command-line arguments.\"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model-path\",\n        type=str,\n        required=True,\n        help=\"Path to the Evo2 un-sharded (MP1) model checkpoint file, or a Hugging Face model name. Any model \"\n        \"from the Savanna Evo2 family is supported such as 'hf://arcinstitute/savanna_evo2_1b_base'.\",\n    )\n    parser.add_argument(\"--output-dir\", type=str, required=True, help=\"Output directory path for the converted model.\")\n    parser.add_argument(\n        \"--use-subquadratic_ops\",\n        action=\"store_true\",\n        help=\"The checkpoint being converted should use subquadratic_ops.\",\n    )\n    parser.add_argument(\n        \"--model-size\",\n        type=str,\n        choices=sorted(set(HYENA_MODEL_OPTIONS.keys()) | set(LLAMA_MODEL_OPTIONS.keys())),\n        required=True,\n        help=\"Model architecture to use, choose between 1b, 7b, 40b, or test (a sub-model of 4 layers, \"\n        \"less than 1B parameters). '*_arc_longcontext' models have GLU / FFN dimensions that support 1M \"\n        \"context length when trained with TP&gt;&gt;8. Note that Mamba models are not supported for conversion yet.\",\n    )\n    return parser.parse_args()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/convert_zero3_to_zero1/","title":"Convert zero3 to zero1","text":""},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/convert_zero3_to_zero1/#bionemo.evo2.utils.checkpoint.convert_zero3_to_zero1.convert_zero_checkpoint_to_fp32_state_dict","title":"<code>convert_zero_checkpoint_to_fp32_state_dict(checkpoint_dir, output_dir, tag=None, exclude_frozen_parameters=False, mp_size=8, overwrite=False, num_workers=1, ranks_to_process=None)</code>","text":"<p>Converts a DeepSpeed Zero-3 checkpoint to a PyTorch FP32 state_dict.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>str</code> <p>Path to the desired checkpoint folder.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save the PyTorch FP32 state_dict output files.</p> required <code>tag</code> <code>Optional[str]</code> <p>Checkpoint tag used as a unique identifier or sub-directory that contains the checkpoint.</p> <code>None</code> <code>exclude_frozen_parameters</code> <code>bool</code> <p>Whether to exclude frozen parameters.</p> <code>False</code> <code>mp_size</code> <code>int</code> <p>Model parallel size of the source checkpoint.</p> <code>8</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing MP shards.</p> <code>False</code> <code>num_workers</code> <code>int</code> <p>Number of workers to use for processing.</p> <code>1</code> <code>ranks_to_process</code> <code>Optional[List[int]]</code> <p>List of ranks to process.</p> <code>None</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the checkpoint directory does not exist.</p> Source code in <code>bionemo/evo2/utils/checkpoint/convert_zero3_to_zero1.py</code> <pre><code>def convert_zero_checkpoint_to_fp32_state_dict(\n    checkpoint_dir: str,\n    output_dir: str,\n    tag: Optional[str] = None,\n    exclude_frozen_parameters: bool = False,\n    mp_size: int = 8,\n    overwrite: bool = False,\n    num_workers: int = 1,\n    ranks_to_process: Optional[List[int]] = None,\n):\n    \"\"\"Converts a DeepSpeed Zero-3 checkpoint to a PyTorch FP32 state_dict.\n\n    Args:\n        checkpoint_dir (str): Path to the desired checkpoint folder.\n        output_dir (str): Directory to save the PyTorch FP32 state_dict output files.\n        tag (Optional[str]): Checkpoint tag used as a unique identifier or sub-directory that contains the checkpoint.\n        exclude_frozen_parameters (bool): Whether to exclude frozen parameters.\n        mp_size (int): Model parallel size of the source checkpoint.\n        overwrite (bool): Whether to overwrite existing MP shards.\n        num_workers (int): Number of workers to use for processing.\n        ranks_to_process (Optional[List[int]]): List of ranks to process.\n\n    Raises:\n        FileNotFoundError: If the checkpoint directory does not exist.\n    \"\"\"\n    ds_checkpoint_dir = os.path.join(checkpoint_dir, tag) if tag is not None else checkpoint_dir\n\n    if not os.path.isdir(ds_checkpoint_dir):\n        raise FileNotFoundError(f\"Directory '{ds_checkpoint_dir}' doesn't exist\")\n\n    output_dir = os.path.join(output_dir, tag) if tag is not None else output_dir\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir, exist_ok=True)\n\n    num_workers = min(num_workers, mp_size)\n\n    if ranks_to_process is not None:\n        ranks_to_process = list(ranks_to_process)\n        assert len(ranks_to_process) &lt;= mp_size, f\"Expected {mp_size} ranks to process, got {len(ranks_to_process)}\"\n        assert all(0 &lt;= r &lt; mp_size for r in ranks_to_process), (\n            f\"Expected ranks to be in range [0, {mp_size}), got {ranks_to_process}\"\n        )\n    else:\n        ranks_to_process = list(range(mp_size))\n\n    print(f\"Processing ranks: {ranks_to_process}\", flush=True)\n\n    start = time.time()\n    if num_workers &gt; 1:\n        with Pool(num_workers) as p:\n            p.starmap(\n                process_single_rank,\n                [(i, ds_checkpoint_dir, output_dir, overwrite, exclude_frozen_parameters) for i in ranks_to_process],\n            )\n    else:\n        for i in ranks_to_process:\n            process_single_rank(i, ds_checkpoint_dir, output_dir, overwrite, exclude_frozen_parameters)\n\n    total_time = get_elapsed(time.time() - start)\n    print(f\"All done!\\n-&gt; Total time: {total_time}\\n-&gt; All outputs written to {os.path.abspath(output_dir)}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/evo2_remove_optimizer/","title":"Evo2 remove optimizer","text":""},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/evo2_remove_optimizer/#bionemo.evo2.utils.checkpoint.evo2_remove_optimizer.HyenaOptimizerRemover","title":"<code>HyenaOptimizerRemover</code>","text":"<p>               Bases: <code>_OptimizerRemoverBase</code>, <code>ModelConnector['HyenaModel', HyenaModel]</code></p> <p>Removes the optimizer state from a nemo2 format model checkpoint.</p> Source code in <code>bionemo/evo2/utils/checkpoint/evo2_remove_optimizer.py</code> <pre><code>@io.model_importer(HyenaModel, \"pytorch\")\nclass HyenaOptimizerRemover(_OptimizerRemoverBase, io.ModelConnector[\"HyenaModel\", HyenaModel]):\n    \"\"\"Removes the optimizer state from a nemo2 format model checkpoint.\"\"\"\n\n    MODEL_CLS = HyenaModel\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/evo2_remove_optimizer/#bionemo.evo2.utils.checkpoint.evo2_remove_optimizer.LlamaOptimizerRemover","title":"<code>LlamaOptimizerRemover</code>","text":"<p>               Bases: <code>_OptimizerRemoverBase</code>, <code>ModelConnector['GPTModel', GPTModel]</code></p> <p>Removes the optimizer state from a nemo2 format model checkpoint.</p> Source code in <code>bionemo/evo2/utils/checkpoint/evo2_remove_optimizer.py</code> <pre><code>@io.model_importer(GPTModel, \"pytorch\")\nclass LlamaOptimizerRemover(_OptimizerRemoverBase, io.ModelConnector[\"GPTModel\", GPTModel]):\n    \"\"\"Removes the optimizer state from a nemo2 format model checkpoint.\"\"\"\n\n    MODEL_CLS = GPTModel\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/evo2_remove_optimizer/#bionemo.evo2.utils.checkpoint.evo2_remove_optimizer.MambaOptimizerRemover","title":"<code>MambaOptimizerRemover</code>","text":"<p>               Bases: <code>_OptimizerRemoverBase</code>, <code>ModelConnector['MambaModel', MambaModel]</code></p> <p>Removes the optimizer state from a nemo2 format model checkpoint.</p> Source code in <code>bionemo/evo2/utils/checkpoint/evo2_remove_optimizer.py</code> <pre><code>@io.model_importer(MambaModel, \"pytorch\")\nclass MambaOptimizerRemover(_OptimizerRemoverBase, io.ModelConnector[\"MambaModel\", MambaModel]):\n    \"\"\"Removes the optimizer state from a nemo2 format model checkpoint.\"\"\"\n\n    MODEL_CLS = MambaModel\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/evo2_remove_optimizer/#bionemo.evo2.utils.checkpoint.evo2_remove_optimizer.main","title":"<code>main()</code>","text":"<p>Convert a PyTorch Evo2 model checkpoint to a NeMo model checkpoint.</p> Source code in <code>bionemo/evo2/utils/checkpoint/evo2_remove_optimizer.py</code> <pre><code>def main():\n    \"\"\"Convert a PyTorch Evo2 model checkpoint to a NeMo model checkpoint.\"\"\"\n    args = parse_args()\n    if args.model_type == \"hyena\":\n        optimizer_remover = HyenaOptimizerRemover(args.model_path)\n    elif args.model_type == \"mamba\":\n        optimizer_remover = MambaOptimizerRemover(args.model_path)\n    elif args.model_type == \"llama\":\n        optimizer_remover = LlamaOptimizerRemover(args.model_path)\n    else:\n        raise ValueError(f\"Invalid model type: {args.model_type}.\")\n    optimizer_remover.apply(args.output_dir)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/evo2_remove_optimizer/#bionemo.evo2.utils.checkpoint.evo2_remove_optimizer.parse_args","title":"<code>parse_args()</code>","text":"<p>Parse command-line arguments.</p> Source code in <code>bionemo/evo2/utils/checkpoint/evo2_remove_optimizer.py</code> <pre><code>def parse_args():\n    \"\"\"Parse command-line arguments.\"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model-path\",\n        type=str,\n        required=True,\n        help=\"Path to the Evo2 un-sharded (MP1) model checkpoint file, or a Hugging Face model name. Any model \"\n        \"from the Savanna Evo2 family is supported such as 'hf://arcinstitute/savanna_evo2_1b_base'.\",\n    )\n    parser.add_argument(\"--output-dir\", type=str, required=True, help=\"Output directory path for the converted model.\")\n    parser.add_argument(\n        \"--model-type\",\n        type=str,\n        choices=[\"hyena\", \"mamba\", \"llama\"],\n        default=\"hyena\",\n        help=\"Model architecture to use, choose between 'hyena', 'mamba', or 'llama'.\",\n    )\n    return parser.parse_args()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/nemo2_to_hf/","title":"Nemo2 to hf","text":""},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/nemo2_to_hf/#bionemo.evo2.utils.checkpoint.nemo2_to_hf.main","title":"<code>main()</code>","text":"<p>Convert a NeMo2 Evo2 model checkpoint to a Hugging Face model checkpoint.</p> Source code in <code>bionemo/evo2/utils/checkpoint/nemo2_to_hf.py</code> <pre><code>def main():\n    \"\"\"Convert a NeMo2 Evo2 model checkpoint to a Hugging Face model checkpoint.\"\"\"\n    args = parse_args()\n    model_type = args.model_type\n    if model_type == \"hyena\":\n        raise ValueError(\"Hyena models are not supported for conversion to Hugging Face yet.\")\n    elif model_type == \"mamba\":\n        exporter = HFNemotronExporter(args.model_path)\n    elif model_type == \"llama\":\n        exporter = HFLlamaExporter(args.model_path)\n    else:\n        raise ValueError(f\"Invalid model type: {model_type}.\")\n    exporter.apply(args.output_dir)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/nemo2_to_hf/#bionemo.evo2.utils.checkpoint.nemo2_to_hf.parse_args","title":"<code>parse_args()</code>","text":"<p>Parse command-line arguments.</p> Source code in <code>bionemo/evo2/utils/checkpoint/nemo2_to_hf.py</code> <pre><code>def parse_args():\n    \"\"\"Parse command-line arguments.\"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model-type\", type=str, required=True, help=\"Model type to convert.\", choices=[\"hyena\", \"mamba\", \"llama\"]\n    )\n    parser.add_argument(\"--model-path\", type=str, required=True, help=\"Model path to convert.\")\n    parser.add_argument(\"--output-dir\", type=str, required=True, help=\"Output directory path for the converted model.\")\n    return parser.parse_args()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/params/","title":"Params","text":""},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/params/#bionemo.evo2.utils.checkpoint.params.Param","title":"<code>Param</code>  <code>dataclass</code>","text":"<p>A dataclass representing a parameter in a checkpoint.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the parameter in the checkpoint.</p> <code>partition_dim</code> <code>int</code> <p>The dimension index that gets sharded. <code>None</code> for no sharding.</p> <code>hidden_dim</code> <code>int</code> <p>The hidden dimension index. <code>None</code> for no hidden dimension.</p> Source code in <code>bionemo/evo2/utils/checkpoint/params.py</code> <pre><code>@dataclass\nclass Param:\n    \"\"\"A dataclass representing a parameter in a checkpoint.\n\n    Attributes:\n        name (str): The name of the parameter in the checkpoint.\n        partition_dim (int): The dimension index that gets sharded. `None` for no sharding.\n        hidden_dim (int): The hidden dimension index. `None` for no hidden dimension.\n    \"\"\"\n\n    name: str  # Name of the parameter in the checkpoint.\n    partition_dim: int  # The dimension index that gets sharded. `None` for no sharding.\n    hidden_dim: int  # The hidden dimension index. `None` for no hidden dimension.\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/","title":"Zero3 conversion lib","text":"<p>Helper utility for converting ZeRO3 and ZeRO2 checkpoints to PyTorch.</p>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.ZeroModelState","title":"<code>ZeroModelState</code>  <code>dataclass</code>","text":"<p>A dataclass representing the state of a ZeRO model.</p> <p>Attributes:</p> Name Type Description <code>buffers</code> <code>Dict</code> <p>Buffers in the model state.</p> <code>extra_states</code> <code>Dict</code> <p>Extra states in the model state.</p> <code>param_shapes</code> <code>List</code> <p>Shapes of the parameters.</p> <code>shared_params</code> <code>List</code> <p>Shared parameters in the model state.</p> <code>ds_version</code> <code>int</code> <p>Version of the DeepSpeed checkpoint.</p> <code>frozen_param_shapes</code> <code>Dict</code> <p>Shapes of the frozen parameters.</p> <code>frozen_param_fragments</code> <code>Dict</code> <p>Fragments of the frozen parameters.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>@dataclass\nclass ZeroModelState:\n    \"\"\"A dataclass representing the state of a ZeRO model.\n\n    Attributes:\n        buffers (Dict): Buffers in the model state.\n        extra_states (Dict): Extra states in the model state.\n        param_shapes (List): Shapes of the parameters.\n        shared_params (List): Shared parameters in the model state.\n        ds_version (int): Version of the DeepSpeed checkpoint.\n        frozen_param_shapes (Dict): Shapes of the frozen parameters.\n        frozen_param_fragments (Dict): Fragments of the frozen parameters.\n    \"\"\"\n\n    buffers: Dict\n    extra_states: Dict\n    param_shapes: List\n    shared_params: List\n    ds_version: int\n    frozen_param_shapes: Dict\n    frozen_param_fragments: Dict\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.atoi","title":"<code>atoi(text)</code>","text":"<p>Converts a string to an integer if it is a digit, otherwise returns the string.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to be converted.</p> required <p>Returns:</p> Type Description <p>int or str: The converted integer or the original string.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def atoi(text: str):\n    \"\"\"Converts a string to an integer if it is a digit, otherwise returns the string.\n\n    Args:\n        text (str): The text to be converted.\n\n    Returns:\n        int or str: The converted integer or the original string.\n    \"\"\"\n    return int(text) if text.isdigit() else text\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.create_ds_output_path","title":"<code>create_ds_output_path(rank)</code>","text":"<p>Creates the output path for a DeepSpeed checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>int</code> <p>The rank to create the output path for.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The output path for the DeepSpeed checkpoint.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def create_ds_output_path(rank: int):\n    \"\"\"Creates the output path for a DeepSpeed checkpoint.\n\n    Args:\n        rank (int): The rank to create the output path for.\n\n    Returns:\n        str: The output path for the DeepSpeed checkpoint.\n    \"\"\"\n    return f\"mp_rank_{rank:02}_model_states.pt\"\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.create_zero3_model_state_path","title":"<code>create_zero3_model_state_path(dp_rank, mp_rank)</code>","text":"<p>Creates the path for a ZeRO3 model state file.</p> <p>Parameters:</p> Name Type Description Default <code>dp_rank</code> <code>int</code> <p>The data parallel rank.</p> required <code>mp_rank</code> <code>int</code> <p>The model parallel rank.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The path for the ZeRO3 model state file.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def create_zero3_model_state_path(dp_rank: int, mp_rank: int):\n    \"\"\"Creates the path for a ZeRO3 model state file.\n\n    Args:\n        dp_rank (int): The data parallel rank.\n        mp_rank (int): The model parallel rank.\n\n    Returns:\n        str: The path for the ZeRO3 model state file.\n    \"\"\"\n    return f\"zero_pp_rank_{dp_rank}_mp_rank_{mp_rank:02}_model_states.pt\"\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.create_zero3_optim_state_path","title":"<code>create_zero3_optim_state_path(dp_rank, mp_rank)</code>","text":"<p>Creates the path for a ZeRO3 optimizer state file.</p> <p>Parameters:</p> Name Type Description Default <code>dp_rank</code> <code>int</code> <p>The data parallel rank.</p> required <code>mp_rank</code> <code>int</code> <p>The model parallel rank.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The path for the ZeRO3 optimizer state file.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def create_zero3_optim_state_path(dp_rank: int, mp_rank: int):\n    \"\"\"Creates the path for a ZeRO3 optimizer state file.\n\n    Args:\n        dp_rank (int): The data parallel rank.\n        mp_rank (int): The model parallel rank.\n\n    Returns:\n        str: The path for the ZeRO3 optimizer state file.\n    \"\"\"\n    return f\"bf16_zero_pp_rank_{dp_rank}_mp_rank_{mp_rank:02}_optim_states.pt\"\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.get_checkpoint_files","title":"<code>get_checkpoint_files(checkpoint_dir, glob_pattern)</code>","text":"<p>Retrieves checkpoint files from a directory based on a glob pattern.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>str</code> <p>The directory to search for checkpoint files.</p> required <code>glob_pattern</code> <code>str</code> <p>The glob pattern to match files.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A sorted list of checkpoint files.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If no files matching the glob pattern are found.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def get_checkpoint_files(checkpoint_dir: str, glob_pattern: str):\n    \"\"\"Retrieves checkpoint files from a directory based on a glob pattern.\n\n    Args:\n        checkpoint_dir (str): The directory to search for checkpoint files.\n        glob_pattern (str): The glob pattern to match files.\n\n    Returns:\n        list: A sorted list of checkpoint files.\n\n    Raises:\n        FileNotFoundError: If no files matching the glob pattern are found.\n    \"\"\"\n    # XXX: need to test that this simple glob rule works for multi-node setup too\n    ckpt_files = sorted(glob.glob(os.path.join(checkpoint_dir, glob_pattern)), key=natural_keys)\n\n    if len(ckpt_files) == 0:\n        raise FileNotFoundError(f\"can't find {glob_pattern} files in directory '{checkpoint_dir}'\")\n\n    return ckpt_files\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.get_elapsed","title":"<code>get_elapsed(t)</code>","text":"<p>Converts elapsed time in seconds to a formatted string.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>float</code> <p>The elapsed time in seconds.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The formatted elapsed time as a string.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def get_elapsed(t: float):\n    \"\"\"Converts elapsed time in seconds to a formatted string.\n\n    Args:\n        t (float): The elapsed time in seconds.\n\n    Returns:\n        str: The formatted elapsed time as a string.\n    \"\"\"\n    minutes = t // 60\n    seconds = t % 60\n    if minutes &gt; 0:\n        total_time = f\"{minutes:.0f}min{seconds:.0f}s\"\n    else:\n        total_time = f\"{seconds:.1f}s\"\n    return total_time\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.get_model_files_by_rank","title":"<code>get_model_files_by_rank(checkpoint_dir, rank)</code>","text":"<p>Retrieves model files for a specific rank from a checkpoint directory.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>str</code> <p>The directory to search for model files.</p> required <code>rank</code> <code>int</code> <p>The rank to search for.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A list of model files for the specified rank.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def get_model_files_by_rank(checkpoint_dir: str, rank: int):\n    \"\"\"Retrieves model files for a specific rank from a checkpoint directory.\n\n    Args:\n        checkpoint_dir (str): The directory to search for model files.\n        rank (int): The rank to search for.\n\n    Returns:\n        list: A list of model files for the specified rank.\n    \"\"\"\n    return get_checkpoint_files(checkpoint_dir, f\"*mp_rank_{rank:02}_model_states.pt\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.get_model_state_file","title":"<code>get_model_state_file(checkpoint_dir, zero_stage)</code>","text":"<p>Retrieves the model state file from a checkpoint directory based on the ZeRO stage.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>str</code> <p>The directory to search for the model state file.</p> required <code>zero_stage</code> <code>int</code> <p>The ZeRO stage to search for.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The path to the model state file.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the directory or model state file is not found.</p> <code>ValueError</code> <p>If the ZeRO stage is not supported.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def get_model_state_file(checkpoint_dir: str, zero_stage: int):\n    \"\"\"Retrieves the model state file from a checkpoint directory based on the ZeRO stage.\n\n    Args:\n        checkpoint_dir (str): The directory to search for the model state file.\n        zero_stage (int): The ZeRO stage to search for.\n\n    Returns:\n        str: The path to the model state file.\n\n    Raises:\n        FileNotFoundError: If the directory or model state file is not found.\n        ValueError: If the ZeRO stage is not supported.\n    \"\"\"\n    if not os.path.isdir(checkpoint_dir):\n        raise FileNotFoundError(f\"Directory '{checkpoint_dir}' doesn't exist\")\n\n    # there should be only one file\n    if zero_stage &lt;= 2:\n        file = os.path.join(checkpoint_dir, \"mp_rank_00_model_states.pt\")\n    elif zero_stage == 3:\n        file = os.path.join(checkpoint_dir, \"zero_pp_rank_0_mp_rank_00_model_states.pt\")\n    else:\n        raise ValueError(f\"Unsupported zero stage {zero_stage}. Expected 1, 2, or 3\")\n\n    if not os.path.exists(file):\n        raise FileNotFoundError(f\"can't find model states file at '{file}'\")\n\n    return file\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.get_optim_files_by_rank","title":"<code>get_optim_files_by_rank(checkpoint_dir, rank)</code>","text":"<p>Retrieves optimizer files for a specific rank from a checkpoint directory.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>str</code> <p>The directory to search for optimizer files.</p> required <code>rank</code> <code>int</code> <p>The rank to search for.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A list of optimizer files for the specified rank.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def get_optim_files_by_rank(checkpoint_dir: str, rank: int):\n    \"\"\"Retrieves optimizer files for a specific rank from a checkpoint directory.\n\n    Args:\n        checkpoint_dir (str): The directory to search for optimizer files.\n        rank (int): The rank to search for.\n\n    Returns:\n        list: A list of optimizer files for the specified rank.\n    \"\"\"\n    return get_checkpoint_files(checkpoint_dir, f\"*mp_rank_{rank:02}_optim_states.pt\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.natural_keys","title":"<code>natural_keys(text)</code>","text":"<p>Sorts a list in human order.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to be sorted.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>The sorted list.</p> Note <p>alist.sort(key=natural_keys) sorts in human order. http://nedbatchelder.com/blog/200712/human_sorting.html (See Toothy's implementation in the comments)</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def natural_keys(text: str):\n    \"\"\"Sorts a list in human order.\n\n    Args:\n        text (str): The text to be sorted.\n\n    Returns:\n        list: The sorted list.\n\n    Note:\n        alist.sort(key=natural_keys) sorts in human order.\n        http://nedbatchelder.com/blog/200712/human_sorting.html\n        (See Toothy's implementation in the comments)\n    \"\"\"\n    return [atoi(c) for c in re.split(r\"(\\d+)\", text)]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.parse_model_states","title":"<code>parse_model_states(files)</code>","text":"<p>Parses model state files and returns a list of ZeroModelState objects.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>Set[str]</code> <p>A set of file paths to parse.</p> required <p>Returns:</p> Type Description <p>List[ZeroModelState]: A list of parsed ZeroModelState objects.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a file is not a model state checkpoint.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def parse_model_states(files: Set[str]):\n    \"\"\"Parses model state files and returns a list of ZeroModelState objects.\n\n    Args:\n        files (Set[str]): A set of file paths to parse.\n\n    Returns:\n        List[ZeroModelState]: A list of parsed ZeroModelState objects.\n\n    Raises:\n        ValueError: If a file is not a model state checkpoint.\n    \"\"\"\n    zero_model_states = []\n    for file in files:\n        state_dict = torch.load(file, map_location=device)\n\n        if BUFFER_NAMES not in state_dict:\n            raise ValueError(f\"{file} is not a model state checkpoint\")\n        buffer_names = state_dict[BUFFER_NAMES]\n        if debug:\n            print_pid(\"Found buffers:\", buffer_names)\n\n        # recover just the buffers while restoring them to fp32 if they were saved in fp16\n        buffers = {k: v.float() for k, v in state_dict[\"module\"].items() if k in buffer_names}\n\n        extra_states = {k: v for k, v in state_dict[\"module\"].items() if k.endswith(EXTRA_STATE)}\n\n        # collect parameters that are included in param_shapes\n        param_shapes = state_dict[PARAM_SHAPES]\n        param_names = []\n        for s in param_shapes:\n            for name in s.keys():\n                param_names.append(name)\n\n        # update with frozen parameters\n        frozen_param_shapes = state_dict.get(FROZEN_PARAM_SHAPES, None)\n        if frozen_param_shapes is not None:\n            if debug:\n                print_pid(f\"Found frozen_param_shapes: {frozen_param_shapes}\")\n            param_names += list(frozen_param_shapes.keys())\n\n        # handle shared params\n        shared_params = [[k, v] for k, v in state_dict[\"shared_params\"].items()]\n\n        ds_version = state_dict.get(DS_VERSION, None)\n\n        frozen_param_fragments = state_dict.get(FROZEN_PARAM_FRAGMENTS, None)\n\n        z_model_state = ZeroModelState(\n            buffers=buffers,\n            extra_states=extra_states,\n            param_shapes=param_shapes,\n            shared_params=shared_params,\n            ds_version=ds_version,\n            frozen_param_shapes=frozen_param_shapes,\n            frozen_param_fragments=frozen_param_fragments,\n        )\n        zero_model_states.append(z_model_state)\n\n    return zero_model_states\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.parse_optim_states","title":"<code>parse_optim_states(files, ds_checkpoint_dir)</code>","text":"<p>Parses optimizer state files and returns the ZeRO stage, world size, and fp32 flat groups.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>Set[str]</code> <p>A set of file paths to parse.</p> required <code>ds_checkpoint_dir</code> <code>str</code> <p>The directory containing the DeepSpeed checkpoint.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the ZeRO stage, world size, and fp32 flat groups.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a file is not a ZeRO checkpoint or if the number of files does not match the expected world size.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def parse_optim_states(files: Set[str], ds_checkpoint_dir: str):\n    \"\"\"Parses optimizer state files and returns the ZeRO stage, world size, and fp32 flat groups.\n\n    Args:\n        files (Set[str]): A set of file paths to parse.\n        ds_checkpoint_dir (str): The directory containing the DeepSpeed checkpoint.\n\n    Returns:\n        tuple: A tuple containing the ZeRO stage, world size, and fp32 flat groups.\n\n    Raises:\n        ValueError: If a file is not a ZeRO checkpoint or if the number of files does not match the expected world size.\n    \"\"\"\n    total_files = len(files)\n    state_dicts = []\n    for f in files:\n        state_dict = torch.load(f, map_location=device)\n        # immediately discard the potentially huge 2 optimizer states as we only care for fp32 master weights\n        # and also handle the case where it was already removed by another helper script\n        state_dict[\"optimizer_state_dict\"].pop(\"optimizer_state_dict\", None)\n        state_dict[OPTIMIZER_STATE_DICT] = {\n            FP32_FLAT_GROUPS: state_dict[OPTIMIZER_STATE_DICT][FP32_FLAT_GROUPS],\n            ZERO_STAGE: state_dict[OPTIMIZER_STATE_DICT][ZERO_STAGE],\n            PARTITION_COUNT: state_dict[OPTIMIZER_STATE_DICT][PARTITION_COUNT],\n        }\n        state_dicts.append(state_dict)\n\n    if ZERO_STAGE not in state_dicts[0][OPTIMIZER_STATE_DICT]:\n        raise ValueError(f\"{files[0]} is not a zero checkpoint\")\n    zero_stage = state_dicts[0][OPTIMIZER_STATE_DICT][ZERO_STAGE]\n    world_size = state_dicts[0][OPTIMIZER_STATE_DICT][PARTITION_COUNT]\n\n    # For ZeRO-2 each param group can have different partition_count as data parallelism for expert\n    # parameters can be different from data parallelism for non-expert parameters. So we can just\n    # use the max of the partition_count to get the dp world_size.\n\n    if type(world_size) is list:\n        world_size = max(world_size)\n\n    if world_size != total_files:\n        raise ValueError(\n            f\"Expected {world_size} of '*_optim_states.pt' under '{ds_checkpoint_dir}' but found {total_files} files. \"\n            \"Possibly due to an overwrite of an old checkpoint, or a checkpoint didn't get saved by one or more processes.\"\n        )\n\n    # the groups are named differently in each stage\n    if zero_stage &lt;= 2:\n        fp32_groups_key = SINGLE_PARTITION_OF_FP32_GROUPS\n    elif zero_stage == 3:\n        fp32_groups_key = FP32_FLAT_GROUPS\n    else:\n        raise ValueError(f\"unknown zero stage {zero_stage}\")\n\n    if zero_stage &lt;= 2:\n        fp32_flat_groups = [state_dicts[i][OPTIMIZER_STATE_DICT][fp32_groups_key] for i in range(len(state_dicts))]\n    elif zero_stage == 3:\n        # if there is more than one param group, there will be multiple flattened tensors - one\n        # flattened tensor per group - for simplicity merge them into a single tensor\n        #\n        # XXX: could make the script more memory efficient for when there are multiple groups - it\n        # will require matching the sub-lists of param_shapes for each param group flattened tensor\n\n        fp32_flat_groups = [\n            torch.cat(state_dicts[i][OPTIMIZER_STATE_DICT][fp32_groups_key], 0) for i in range(len(state_dicts))\n        ]\n\n    return zero_stage, world_size, fp32_flat_groups\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.print_pid","title":"<code>print_pid(msg)</code>","text":"<p>Prints the process ID along with a message.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>The message to be printed.</p> required Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def print_pid(msg: str):\n    \"\"\"Prints the process ID along with a message.\n\n    Args:\n        msg (str): The message to be printed.\n    \"\"\"\n    pid = os.getpid()\n    print(f\"{pid=}:{msg}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.process_single_rank","title":"<code>process_single_rank(rank, ds_checkpoint_dir, output_dir, overwrite=False, exclude_frozen_parameters=False)</code>","text":"<p>Processes a single rank to gather and save the state dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>int</code> <p>The rank to process.</p> required <code>ds_checkpoint_dir</code> <code>str</code> <p>Path to the DeepSpeed checkpoint folder.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save the output.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing files. Default is False.</p> <code>False</code> <code>exclude_frozen_parameters</code> <code>bool</code> <p>Whether to exclude frozen parameters. Default is False.</p> <code>False</code> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def process_single_rank(\n    rank: int,\n    ds_checkpoint_dir: str,\n    output_dir: str,\n    overwrite: bool = False,\n    exclude_frozen_parameters: bool = False,\n):\n    \"\"\"Processes a single rank to gather and save the state dictionary.\n\n    Args:\n        rank (int): The rank to process.\n        ds_checkpoint_dir (str): Path to the DeepSpeed checkpoint folder.\n        output_dir (str): Directory to save the output.\n        overwrite (bool): Whether to overwrite existing files. Default is False.\n        exclude_frozen_parameters (bool): Whether to exclude frozen parameters. Default is False.\n    \"\"\"\n    print_pid(f\"Gathering rank {rank} state_dict...\")\n\n    start = time.time()\n    output_path = os.path.join(output_dir, create_ds_output_path(rank))\n    if os.path.exists(output_path) and not overwrite:\n        print_pid(f\"Output path {output_path} exists, skipping\")\n        return\n\n    print_pid(f\" -&gt; Gathering data parallel partitions for mp rank {rank}...\")\n\n    if os.environ.get(\"ZERO3_CONVERSION_DEBUG\", \"0\") == \"1\":\n        breakpoint()\n\n    state_dict = _get_fp32_state_dict_from_zero_checkpoint(\n        ds_checkpoint_dir=ds_checkpoint_dir, rank=rank, exclude_frozen_parameters=exclude_frozen_parameters\n    )\n    print_pid(f\" -&gt; Done processing rank {rank} state_dict, gathered {len(state_dict)} params\")\n\n    checkpoint = {\n        \"module\": state_dict,\n        \"param_shapes\": OrderedDict(),\n        \"dp_world_size\": 1,\n    }\n\n    for param, value in state_dict.items():\n        if isinstance(value, torch.Tensor):\n            checkpoint[\"param_shapes\"][param] = value.shape\n\n    print_pid(f\" -&gt; Saving mp rank {rank} checkpoint to {output_path}\")\n    torch.save(checkpoint, f\"{output_path}\")\n\n    total_time = get_elapsed(time.time() - start)\n    print_pid(f\" -&gt; rank {rank} took {total_time}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.profile_memory_decorator","title":"<code>profile_memory_decorator(func)</code>","text":"<p>A decorator to profile memory usage of a function.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Iterable</code> <p>The function to be decorated.</p> required <p>Returns:</p> Name Type Description <code>wrapper</code> <p>The decorated function with memory profiling.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def profile_memory_decorator(func: Iterable):\n    \"\"\"A decorator to profile memory usage of a function.\n\n    Args:\n        func (Iterable): The function to be decorated.\n\n    Returns:\n        wrapper: The decorated function with memory profiling.\n    \"\"\"\n\n    def profile_memory():\n        pid = os.getpid()\n        process = psutil.Process(pid)\n        memory_info = process.memory_info()\n        print_pid(f\"{pid}: RSS = {memory_info.rss / 1024**2:.2f} MB\")\n\n    def wrapper(*args, **kwargs):\n        profile_memory()\n        func(*args, **kwargs)\n        profile_memory()\n\n    return wrapper\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.zero3_partitioned_param_info","title":"<code>zero3_partitioned_param_info(unpartitioned_numel, world_size)</code>","text":"<p>Returns the partitioned and padding number of elements for a parameter.</p> <p>Parameters:</p> Name Type Description Default <code>unpartitioned_numel</code> <code>int</code> <p>The number of elements in the unpartitioned parameter.</p> required <code>world_size</code> <code>int</code> <p>The world size.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the partitioned number of elements and the padding number of elements.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def zero3_partitioned_param_info(unpartitioned_numel: int, world_size: int):\n    \"\"\"Returns the partitioned and padding number of elements for a parameter.\n\n    Args:\n        unpartitioned_numel (int): The number of elements in the unpartitioned parameter.\n        world_size (int): The world size.\n\n    Returns:\n        tuple: A tuple containing the partitioned number of elements and the padding number of elements.\n    \"\"\"\n    remainder = unpartitioned_numel % world_size\n    padding_numel = (world_size - remainder) if remainder else 0\n    partitioned_numel = math.ceil(unpartitioned_numel / world_size)\n    return partitioned_numel, padding_numel\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/logging/callbacks/","title":"Callbacks","text":""},{"location":"main/references/API_reference/bionemo/evo2/utils/logging/callbacks/#bionemo.evo2.utils.logging.callbacks.TEVCallback","title":"<code>TEVCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback for logging TEV statistics before each optimizer step.</p> <p>This callback handles different parallelism strategies: - Pipeline Parallelism: Only computes on first pipeline stage - Tensor Parallelism: Gathers embedding shards across TP ranks - Context Parallelism: Gathers across CP ranks - Data Parallelism: Only logs on rank 0 of each model parallel group</p> Source code in <code>bionemo/evo2/utils/logging/callbacks.py</code> <pre><code>class TEVCallback(Callback):\n    \"\"\"Callback for logging TEV statistics before each optimizer step.\n\n    This callback handles different parallelism strategies:\n    - Pipeline Parallelism: Only computes on first pipeline stage\n    - Tensor Parallelism: Gathers embedding shards across TP ranks\n    - Context Parallelism: Gathers across CP ranks\n    - Data Parallelism: Only logs on rank 0 of each model parallel group\n    \"\"\"\n\n    @torch.no_grad()\n    def on_before_optimizer_step(self, trainer, pl_module, optimizer) -&gt; None:\n        \"\"\"Called before each optimizer step during training.\n\n        This method calculates and logs Token Embedding Variance (TEV) statistics:\n        1. Gets embedding parameter only on pipeline rank 0 (where embeddings live)\n        2. Gathers embedding shards across tensor and context parallel ranks\n        3. Calculates the token embedding variance (TEV)\n        4. Logs the mean and standard deviation of TEV values only on data parallel rank 0\n\n        Args:\n            trainer: The Lightning trainer instance\n            pl_module: The current Lightning module being trained\n            optimizer: The optimizer being used\n\n        Note:\n            The callback assumes embeddings live on pipeline rank 0, which is the standard\n            configuration in Megatron-LM.\n        \"\"\"\n        # Only compute on pipeline rank 0 where embeddings live\n        if not parallel_state.is_pipeline_first_stage():\n            return\n\n        # Get all named parameters from the model\n        named_params = dict(pl_module.named_parameters())\n\n        # Find all parameter keys containing 'embed'\n        embed_keys = [key for key in named_params.keys() if \"embed\" in key]\n\n        # Validate we have exactly one embedding layer\n        if len(embed_keys) == 0:\n            raise ValueError(\"No embed keys found.\")\n        if len(embed_keys) &gt; 1:\n            raise ValueError(\"Multiple embed keys found.\")\n\n        # Get the embedding parameter\n        embed = named_params[embed_keys[0]]\n\n        # If using tensor parallelism, gather embedding shards\n        if parallel_state.get_tensor_model_parallel_world_size() &gt; 1:\n            embed = _gather_along_last_dim(embed, group=parallel_state.get_tensor_model_parallel_group())\n\n        # If using context parallelism, gather across context parallel ranks\n        if parallel_state.get_context_parallel_world_size() &gt; 1:\n            world_size = parallel_state.get_context_parallel_world_size()\n            dim_size = list(embed.size())\n            dim_size[0] = dim_size[0] * world_size\n\n            output = torch.empty(dim_size, dtype=embed.dtype, device=torch.cuda.current_device())\n            torch.distributed.all_gather_into_tensor(\n                output, embed.contiguous(), group=parallel_state.get_context_parallel_group()\n            )\n            embed = output\n\n        # Calculate token embedding variance (TEV)\n        # First center the embeddings by subtracting the mean\n        # Then calculate the mean squared deviation (variance)\n        # Finally take the square root to get standard deviation\n        tev = torch.sqrt(torch.mean(torch.pow(embed - embed.mean(dim=0), 2), dim=0))\n\n        # Calculate statistics of the TEV values\n        tev_mean = torch.mean(tev).item()\n        tev_sd = torch.std(tev).item()\n\n        # Only log on data parallel rank 0 to avoid duplicate logging\n        if parallel_state.get_data_parallel_rank() == 0:\n            # Log the TEV statistics\n            pl_module.log(\"tev_mean\", tev_mean, on_step=True, on_epoch=False, sync_dist=False)\n            pl_module.log(\"tev_sd\", tev_sd, on_step=True, on_epoch=False, sync_dist=False)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/logging/callbacks/#bionemo.evo2.utils.logging.callbacks.TEVCallback.on_before_optimizer_step","title":"<code>on_before_optimizer_step(trainer, pl_module, optimizer)</code>","text":"<p>Called before each optimizer step during training.</p> <p>This method calculates and logs Token Embedding Variance (TEV) statistics: 1. Gets embedding parameter only on pipeline rank 0 (where embeddings live) 2. Gathers embedding shards across tensor and context parallel ranks 3. Calculates the token embedding variance (TEV) 4. Logs the mean and standard deviation of TEV values only on data parallel rank 0</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <p>The Lightning trainer instance</p> required <code>pl_module</code> <p>The current Lightning module being trained</p> required <code>optimizer</code> <p>The optimizer being used</p> required Note <p>The callback assumes embeddings live on pipeline rank 0, which is the standard configuration in Megatron-LM.</p> Source code in <code>bionemo/evo2/utils/logging/callbacks.py</code> <pre><code>@torch.no_grad()\ndef on_before_optimizer_step(self, trainer, pl_module, optimizer) -&gt; None:\n    \"\"\"Called before each optimizer step during training.\n\n    This method calculates and logs Token Embedding Variance (TEV) statistics:\n    1. Gets embedding parameter only on pipeline rank 0 (where embeddings live)\n    2. Gathers embedding shards across tensor and context parallel ranks\n    3. Calculates the token embedding variance (TEV)\n    4. Logs the mean and standard deviation of TEV values only on data parallel rank 0\n\n    Args:\n        trainer: The Lightning trainer instance\n        pl_module: The current Lightning module being trained\n        optimizer: The optimizer being used\n\n    Note:\n        The callback assumes embeddings live on pipeline rank 0, which is the standard\n        configuration in Megatron-LM.\n    \"\"\"\n    # Only compute on pipeline rank 0 where embeddings live\n    if not parallel_state.is_pipeline_first_stage():\n        return\n\n    # Get all named parameters from the model\n    named_params = dict(pl_module.named_parameters())\n\n    # Find all parameter keys containing 'embed'\n    embed_keys = [key for key in named_params.keys() if \"embed\" in key]\n\n    # Validate we have exactly one embedding layer\n    if len(embed_keys) == 0:\n        raise ValueError(\"No embed keys found.\")\n    if len(embed_keys) &gt; 1:\n        raise ValueError(\"Multiple embed keys found.\")\n\n    # Get the embedding parameter\n    embed = named_params[embed_keys[0]]\n\n    # If using tensor parallelism, gather embedding shards\n    if parallel_state.get_tensor_model_parallel_world_size() &gt; 1:\n        embed = _gather_along_last_dim(embed, group=parallel_state.get_tensor_model_parallel_group())\n\n    # If using context parallelism, gather across context parallel ranks\n    if parallel_state.get_context_parallel_world_size() &gt; 1:\n        world_size = parallel_state.get_context_parallel_world_size()\n        dim_size = list(embed.size())\n        dim_size[0] = dim_size[0] * world_size\n\n        output = torch.empty(dim_size, dtype=embed.dtype, device=torch.cuda.current_device())\n        torch.distributed.all_gather_into_tensor(\n            output, embed.contiguous(), group=parallel_state.get_context_parallel_group()\n        )\n        embed = output\n\n    # Calculate token embedding variance (TEV)\n    # First center the embeddings by subtracting the mean\n    # Then calculate the mean squared deviation (variance)\n    # Finally take the square root to get standard deviation\n    tev = torch.sqrt(torch.mean(torch.pow(embed - embed.mean(dim=0), 2), dim=0))\n\n    # Calculate statistics of the TEV values\n    tev_mean = torch.mean(tev).item()\n    tev_sd = torch.std(tev).item()\n\n    # Only log on data parallel rank 0 to avoid duplicate logging\n    if parallel_state.get_data_parallel_rank() == 0:\n        # Log the TEV statistics\n        pl_module.log(\"tev_mean\", tev_mean, on_step=True, on_epoch=False, sync_dist=False)\n        pl_module.log(\"tev_sd\", tev_sd, on_step=True, on_epoch=False, sync_dist=False)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/loss/embedding_variance/","title":"Embedding variance","text":""},{"location":"main/references/API_reference/bionemo/evo2/utils/loss/embedding_variance/#bionemo.evo2.utils.loss.embedding_variance.SquaredErrorTargetedVarianceLoss","title":"<code>SquaredErrorTargetedVarianceLoss</code>","text":"<p>               Bases: <code>Module</code></p> <p>Applies a loss that will encourage variance of some parameter to be close to var_target.</p> Source code in <code>bionemo/evo2/utils/loss/embedding_variance.py</code> <pre><code>class SquaredErrorTargetedVarianceLoss(torch.nn.Module):\n    \"\"\"Applies a loss that will encourage variance of some parameter to be close to var_target.\"\"\"\n\n    def __init__(self, loss_coeff: float = 0.1, var_target: float = 1.0):\n        \"\"\"Applies a loss that will encourage variance of some parameter to be close to var_target.\n\n        Args:\n            loss_coeff: Loss coefficient. Defaults to 0.1.\n            var_target: targetted variance for the embedding weights. Defaults to 1.0.\n        \"\"\"\n        super().__init__()\n        self.loss_coeff = loss_coeff\n        self.var_target = var_target\n\n    def forward(self, we_weight: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Applies the loss to the embedding weights with the user requested loss coefficient and targeted variance.\n\n        Args:\n            we_weight: Embedding weights.\n\n        Returns:\n            torch.Tensor: Loss value.\n        \"\"\"\n        return SquaredErrorTargetedVarianceLossFunction.apply(we_weight, self.loss_coeff, self.var_target)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/loss/embedding_variance/#bionemo.evo2.utils.loss.embedding_variance.SquaredErrorTargetedVarianceLoss.__init__","title":"<code>__init__(loss_coeff=0.1, var_target=1.0)</code>","text":"<p>Applies a loss that will encourage variance of some parameter to be close to var_target.</p> <p>Parameters:</p> Name Type Description Default <code>loss_coeff</code> <code>float</code> <p>Loss coefficient. Defaults to 0.1.</p> <code>0.1</code> <code>var_target</code> <code>float</code> <p>targetted variance for the embedding weights. Defaults to 1.0.</p> <code>1.0</code> Source code in <code>bionemo/evo2/utils/loss/embedding_variance.py</code> <pre><code>def __init__(self, loss_coeff: float = 0.1, var_target: float = 1.0):\n    \"\"\"Applies a loss that will encourage variance of some parameter to be close to var_target.\n\n    Args:\n        loss_coeff: Loss coefficient. Defaults to 0.1.\n        var_target: targetted variance for the embedding weights. Defaults to 1.0.\n    \"\"\"\n    super().__init__()\n    self.loss_coeff = loss_coeff\n    self.var_target = var_target\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/loss/embedding_variance/#bionemo.evo2.utils.loss.embedding_variance.SquaredErrorTargetedVarianceLoss.forward","title":"<code>forward(we_weight)</code>","text":"<p>Applies the loss to the embedding weights with the user requested loss coefficient and targeted variance.</p> <p>Parameters:</p> Name Type Description Default <code>we_weight</code> <code>Tensor</code> <p>Embedding weights.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Loss value.</p> Source code in <code>bionemo/evo2/utils/loss/embedding_variance.py</code> <pre><code>def forward(self, we_weight: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Applies the loss to the embedding weights with the user requested loss coefficient and targeted variance.\n\n    Args:\n        we_weight: Embedding weights.\n\n    Returns:\n        torch.Tensor: Loss value.\n    \"\"\"\n    return SquaredErrorTargetedVarianceLossFunction.apply(we_weight, self.loss_coeff, self.var_target)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/loss/embedding_variance/#bionemo.evo2.utils.loss.embedding_variance.SquaredErrorTargetedVarianceLossFunction","title":"<code>SquaredErrorTargetedVarianceLossFunction</code>","text":"<p>               Bases: <code>Function</code></p> <p>This loss function is used to calculate the loss based on the squared difference between the global mean of per-word variances and target.</p> Source code in <code>bionemo/evo2/utils/loss/embedding_variance.py</code> <pre><code>class SquaredErrorTargetedVarianceLossFunction(Function):\n    \"\"\"This loss function is used to calculate the loss based on the squared difference between the global mean of per-word variances and target.\"\"\"\n\n    @staticmethod\n    def forward(ctx, we_weight: torch.Tensor, loss_coeff: float, var_target: float) -&gt; torch.Tensor:\n        \"\"\"Calculates a loss based on the squared difference between the global mean of per-word variances and target.\n\n        Assumes vocab-parallel sharding for we_weight (dim 0 is sharded).\n\n        Args:\n            ctx (torch.autograd.FunctionContext): Context object for backward pass.\n            we_weight (torch.Tensor): Local shard of embedding weights (V_local, H).\n            loss_coeff (float): Loss coefficient.\n            var_target (float): Targeted variance for the embedding weights.\n\n        Returns:\n            torch.Tensor: Scalar loss value.\n\n            weights\n        \"\"\"\n        if not we_weight.is_floating_point():\n            we_weight = we_weight.float()\n\n        V_local, H = we_weight.shape  # V_local: words on this rank, H: embedding dim\n\n        # Save dimensions for backward pass\n        ctx.H_embedding_dim = H\n        ctx.V_local_word_count = V_local\n        ctx.loss_coeff = loss_coeff\n        ctx.var_target = var_target\n\n        # Handle H=0 edge case (embedding dimension is zero)\n        if H == 0:\n            ctx.is_H_dim_zero = True\n            # Mean variance is 0 if H=0. Loss is based on (0 - VAR_TARGET)^2.\n            loss_value = loss_coeff * (0.0 - var_target) ** 2\n            final_loss_tensor = torch.tensor(loss_value, device=we_weight.device, dtype=we_weight.dtype)\n            # Save we_weight for shape, None for we_mean_per_word and V_final (as they are not well-defined or zero)\n            ctx.save_for_backward(we_weight, None, None)\n            return final_loss_tensor\n        ctx.is_H_dim_zero = False\n\n        # Get TP info (assuming parallel_state is globally accessible)\n        # Ensure parallel_state is imported and available in the execution scope.\n        # from some_module import parallel_state # Make sure this is accessible\n        tp_world_size = parallel_state.get_tensor_model_parallel_world_size() or 1\n        tp_group = parallel_state.get_tensor_model_parallel_group()  # Can be None\n        ctx.tp_world_size_val = tp_world_size\n\n        # 1. Per-word mean (across embedding dimension H)\n        # Shape: (V_local, 1)\n        we_mean_per_word = we_weight.mean(dim=1, keepdim=True)\n\n        # 2. Per-word variance (across embedding dimension H)\n        # we_sq_diffs_per_word shape: (V_local, H)\n        we_sq_diffs_per_word = (we_weight - we_mean_per_word) ** 2\n        # we_var_per_word_local shape: (V_local,) (biased variance)\n        we_var_per_word_local = we_sq_diffs_per_word.mean(dim=1, keepdim=False)\n\n        # 3. Mean of these per-word variances *on this local rank*\n        # v_local_mean_of_vars shape: scalar tensor\n        v_local_mean_of_vars = torch.tensor(0.0, device=we_weight.device, dtype=we_weight.dtype)\n        if V_local &gt; 0:  # Avoid NaN from mean of empty tensor if V_local is 0\n            v_local_mean_of_vars = we_var_per_word_local.mean(dim=0, keepdim=False)\n\n        # 4. Globally average these local mean variances\n        # V_final_globally_avg_var is the V in the loss formula L = alpha*(V-T)^2\n        V_final_globally_avg_var = v_local_mean_of_vars.clone()\n        if tp_world_size &gt; 1:\n            # Computes V_final = (1/tp_world_size) * sum(v_local_mean_of_vars from each rank)\n            V_final_globally_avg_var /= tp_world_size\n            torch.distributed.all_reduce(V_final_globally_avg_var, group=tp_group, op=torch.distributed.ReduceOp.SUM)\n\n        # 5. Calculate final loss: LOSS_COEFF * (V_final - VAR_TARGET)^2\n        final_loss = loss_coeff * (V_final_globally_avg_var - var_target) ** 2\n\n        # Save tensors needed for gradient computation in backward\n        ctx.save_for_backward(we_weight, we_mean_per_word, V_final_globally_avg_var)\n        # Other necessary scalars (H, V_local, tp_world_size) are already on ctx.\n\n        return final_loss\n\n    @staticmethod\n    def backward(ctx, grad_output: torch.Tensor) -&gt; tuple[torch.Tensor, None, None]:\n        \"\"\"Backward pass for the SquaredErrorTargetedVarianceLossFunction.\"\"\"\n        we_weight, we_mean_per_word, V_final_saved = ctx.saved_tensors\n\n        # Handle H=0 edge case (gradient is zero)\n        if getattr(ctx, \"is_H_dim_zero\", False):\n            return torch.zeros_like(we_weight), None, None  # Grad for we_weight only\n\n        H = ctx.H_embedding_dim\n        V_local = ctx.V_local_word_count\n        tp_world_size = ctx.tp_world_size_val\n        loss_coeff = ctx.loss_coeff\n        var_target = ctx.var_target\n\n        # Handle V_local=0 edge case (no words on this rank, so no gradient)\n        if V_local == 0:\n            return torch.zeros_like(we_weight), None, None  # Grad for we_weight only\n\n        # Chain rule: d(TotalLoss)/dw = d(TotalLoss)/d(final_loss) * d(final_loss)/dw\n        # grad_output is d(TotalLoss)/d(final_loss)\n\n        # 1. Calculate d(final_loss) / d(V_final_saved)\n        # final_loss = LOSS_COEFF * (V_final_saved - VAR_TARGET)**2\n        # dL_dV_final is d(final_loss) / d(V_final_saved)\n        dL_dV_final = loss_coeff * 2.0 * (V_final_saved - var_target)\n\n        # grad_V_final is d(TotalLoss) / d(V_final_saved)\n        grad_V_final = grad_output * dL_dV_final  # Scalar\n\n        # 2. Propagate gradient from V_final_saved to v_local_mean_of_vars (on current rank)\n        # V_final_saved = (1/tp_world_size) * sum_k(v_local_mean_of_vars_k)\n        # So, d(V_final_saved) / d(v_local_mean_of_vars_current_rank) = 1 / tp_world_size\n        # grad_v_local_mean is d(TotalLoss) / d(v_local_mean_of_vars_current_rank)\n        grad_v_local_mean = grad_V_final * (1.0 / tp_world_size)  # Scalar\n\n        # 3. Propagate gradient from v_local_mean_of_vars to we_var_per_word_local_i\n        # v_local_mean_of_vars = mean(we_var_per_word_local) = (1/V_local) * sum_i(we_var_per_word_local_i)\n        # So, d(v_local_mean_of_vars) / d(we_var_per_word_local_i) = 1 / V_local\n        # The coefficient to apply for the next step of chain rule:\n        # This is grad_v_local_mean scaled by (1/V_local)\n        # This represents d(TotalLoss)/d(we_var_per_word_local_i), assuming it's uniform.\n        coeff_for_per_word_var_grad = grad_v_local_mean * (1.0 / V_local)  # Scalar\n\n        # 4. Propagate gradient from we_var_per_word_local_i to we_weight_ik\n        # we_var_per_word_local_i = (1/H) * sum_k (we_weight_ik - we_mean_per_word_i[0])^2\n        # d(we_var_per_word_local_i) / d(we_weight_ik) = (2/H) * (we_weight_ik - we_mean_per_word_i[0])\n        # The term (we_weight_ik - we_mean_per_word_i[0]) is (we_weight - we_mean_per_word)\n\n        # Combine coefficients for the (we_weight - we_mean_per_word) term:\n        # This is coeff_for_per_word_var_grad * (2/H)\n        final_scalar_coefficient = coeff_for_per_word_var_grad * (2.0 / H)\n\n        grad_we_weight = final_scalar_coefficient * (we_weight - we_mean_per_word)\n\n        # The forward function only takes we_weight as a tensor input requiring grad, the other two inputs\n        # are floats and do not get gradients.\n        return grad_we_weight, None, None\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/loss/embedding_variance/#bionemo.evo2.utils.loss.embedding_variance.SquaredErrorTargetedVarianceLossFunction.backward","title":"<code>backward(ctx, grad_output)</code>  <code>staticmethod</code>","text":"<p>Backward pass for the SquaredErrorTargetedVarianceLossFunction.</p> Source code in <code>bionemo/evo2/utils/loss/embedding_variance.py</code> <pre><code>@staticmethod\ndef backward(ctx, grad_output: torch.Tensor) -&gt; tuple[torch.Tensor, None, None]:\n    \"\"\"Backward pass for the SquaredErrorTargetedVarianceLossFunction.\"\"\"\n    we_weight, we_mean_per_word, V_final_saved = ctx.saved_tensors\n\n    # Handle H=0 edge case (gradient is zero)\n    if getattr(ctx, \"is_H_dim_zero\", False):\n        return torch.zeros_like(we_weight), None, None  # Grad for we_weight only\n\n    H = ctx.H_embedding_dim\n    V_local = ctx.V_local_word_count\n    tp_world_size = ctx.tp_world_size_val\n    loss_coeff = ctx.loss_coeff\n    var_target = ctx.var_target\n\n    # Handle V_local=0 edge case (no words on this rank, so no gradient)\n    if V_local == 0:\n        return torch.zeros_like(we_weight), None, None  # Grad for we_weight only\n\n    # Chain rule: d(TotalLoss)/dw = d(TotalLoss)/d(final_loss) * d(final_loss)/dw\n    # grad_output is d(TotalLoss)/d(final_loss)\n\n    # 1. Calculate d(final_loss) / d(V_final_saved)\n    # final_loss = LOSS_COEFF * (V_final_saved - VAR_TARGET)**2\n    # dL_dV_final is d(final_loss) / d(V_final_saved)\n    dL_dV_final = loss_coeff * 2.0 * (V_final_saved - var_target)\n\n    # grad_V_final is d(TotalLoss) / d(V_final_saved)\n    grad_V_final = grad_output * dL_dV_final  # Scalar\n\n    # 2. Propagate gradient from V_final_saved to v_local_mean_of_vars (on current rank)\n    # V_final_saved = (1/tp_world_size) * sum_k(v_local_mean_of_vars_k)\n    # So, d(V_final_saved) / d(v_local_mean_of_vars_current_rank) = 1 / tp_world_size\n    # grad_v_local_mean is d(TotalLoss) / d(v_local_mean_of_vars_current_rank)\n    grad_v_local_mean = grad_V_final * (1.0 / tp_world_size)  # Scalar\n\n    # 3. Propagate gradient from v_local_mean_of_vars to we_var_per_word_local_i\n    # v_local_mean_of_vars = mean(we_var_per_word_local) = (1/V_local) * sum_i(we_var_per_word_local_i)\n    # So, d(v_local_mean_of_vars) / d(we_var_per_word_local_i) = 1 / V_local\n    # The coefficient to apply for the next step of chain rule:\n    # This is grad_v_local_mean scaled by (1/V_local)\n    # This represents d(TotalLoss)/d(we_var_per_word_local_i), assuming it's uniform.\n    coeff_for_per_word_var_grad = grad_v_local_mean * (1.0 / V_local)  # Scalar\n\n    # 4. Propagate gradient from we_var_per_word_local_i to we_weight_ik\n    # we_var_per_word_local_i = (1/H) * sum_k (we_weight_ik - we_mean_per_word_i[0])^2\n    # d(we_var_per_word_local_i) / d(we_weight_ik) = (2/H) * (we_weight_ik - we_mean_per_word_i[0])\n    # The term (we_weight_ik - we_mean_per_word_i[0]) is (we_weight - we_mean_per_word)\n\n    # Combine coefficients for the (we_weight - we_mean_per_word) term:\n    # This is coeff_for_per_word_var_grad * (2/H)\n    final_scalar_coefficient = coeff_for_per_word_var_grad * (2.0 / H)\n\n    grad_we_weight = final_scalar_coefficient * (we_weight - we_mean_per_word)\n\n    # The forward function only takes we_weight as a tensor input requiring grad, the other two inputs\n    # are floats and do not get gradients.\n    return grad_we_weight, None, None\n</code></pre>"},{"location":"main/references/API_reference/bionemo/evo2/utils/loss/embedding_variance/#bionemo.evo2.utils.loss.embedding_variance.SquaredErrorTargetedVarianceLossFunction.forward","title":"<code>forward(ctx, we_weight, loss_coeff, var_target)</code>  <code>staticmethod</code>","text":"<p>Calculates a loss based on the squared difference between the global mean of per-word variances and target.</p> <p>Assumes vocab-parallel sharding for we_weight (dim 0 is sharded).</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>FunctionContext</code> <p>Context object for backward pass.</p> required <code>we_weight</code> <code>Tensor</code> <p>Local shard of embedding weights (V_local, H).</p> required <code>loss_coeff</code> <code>float</code> <p>Loss coefficient.</p> required <code>var_target</code> <code>float</code> <p>Targeted variance for the embedding weights.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Scalar loss value.</p> <code>Tensor</code> <p>weights</p> Source code in <code>bionemo/evo2/utils/loss/embedding_variance.py</code> <pre><code>@staticmethod\ndef forward(ctx, we_weight: torch.Tensor, loss_coeff: float, var_target: float) -&gt; torch.Tensor:\n    \"\"\"Calculates a loss based on the squared difference between the global mean of per-word variances and target.\n\n    Assumes vocab-parallel sharding for we_weight (dim 0 is sharded).\n\n    Args:\n        ctx (torch.autograd.FunctionContext): Context object for backward pass.\n        we_weight (torch.Tensor): Local shard of embedding weights (V_local, H).\n        loss_coeff (float): Loss coefficient.\n        var_target (float): Targeted variance for the embedding weights.\n\n    Returns:\n        torch.Tensor: Scalar loss value.\n\n        weights\n    \"\"\"\n    if not we_weight.is_floating_point():\n        we_weight = we_weight.float()\n\n    V_local, H = we_weight.shape  # V_local: words on this rank, H: embedding dim\n\n    # Save dimensions for backward pass\n    ctx.H_embedding_dim = H\n    ctx.V_local_word_count = V_local\n    ctx.loss_coeff = loss_coeff\n    ctx.var_target = var_target\n\n    # Handle H=0 edge case (embedding dimension is zero)\n    if H == 0:\n        ctx.is_H_dim_zero = True\n        # Mean variance is 0 if H=0. Loss is based on (0 - VAR_TARGET)^2.\n        loss_value = loss_coeff * (0.0 - var_target) ** 2\n        final_loss_tensor = torch.tensor(loss_value, device=we_weight.device, dtype=we_weight.dtype)\n        # Save we_weight for shape, None for we_mean_per_word and V_final (as they are not well-defined or zero)\n        ctx.save_for_backward(we_weight, None, None)\n        return final_loss_tensor\n    ctx.is_H_dim_zero = False\n\n    # Get TP info (assuming parallel_state is globally accessible)\n    # Ensure parallel_state is imported and available in the execution scope.\n    # from some_module import parallel_state # Make sure this is accessible\n    tp_world_size = parallel_state.get_tensor_model_parallel_world_size() or 1\n    tp_group = parallel_state.get_tensor_model_parallel_group()  # Can be None\n    ctx.tp_world_size_val = tp_world_size\n\n    # 1. Per-word mean (across embedding dimension H)\n    # Shape: (V_local, 1)\n    we_mean_per_word = we_weight.mean(dim=1, keepdim=True)\n\n    # 2. Per-word variance (across embedding dimension H)\n    # we_sq_diffs_per_word shape: (V_local, H)\n    we_sq_diffs_per_word = (we_weight - we_mean_per_word) ** 2\n    # we_var_per_word_local shape: (V_local,) (biased variance)\n    we_var_per_word_local = we_sq_diffs_per_word.mean(dim=1, keepdim=False)\n\n    # 3. Mean of these per-word variances *on this local rank*\n    # v_local_mean_of_vars shape: scalar tensor\n    v_local_mean_of_vars = torch.tensor(0.0, device=we_weight.device, dtype=we_weight.dtype)\n    if V_local &gt; 0:  # Avoid NaN from mean of empty tensor if V_local is 0\n        v_local_mean_of_vars = we_var_per_word_local.mean(dim=0, keepdim=False)\n\n    # 4. Globally average these local mean variances\n    # V_final_globally_avg_var is the V in the loss formula L = alpha*(V-T)^2\n    V_final_globally_avg_var = v_local_mean_of_vars.clone()\n    if tp_world_size &gt; 1:\n        # Computes V_final = (1/tp_world_size) * sum(v_local_mean_of_vars from each rank)\n        V_final_globally_avg_var /= tp_world_size\n        torch.distributed.all_reduce(V_final_globally_avg_var, group=tp_group, op=torch.distributed.ReduceOp.SUM)\n\n    # 5. Calculate final loss: LOSS_COEFF * (V_final - VAR_TARGET)^2\n    final_loss = loss_coeff * (V_final_globally_avg_var - var_target) ** 2\n\n    # Save tensors needed for gradient computation in backward\n    ctx.save_for_backward(we_weight, we_mean_per_word, V_final_globally_avg_var)\n    # Other necessary scalars (H, V_local, tp_world_size) are already on ctx.\n\n    return final_loss\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/","title":"Lightning basic","text":"<p>This is intended to be a minimal self-container NeMo2 example.</p>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule","title":"<code>BionemoLightningModule</code>","text":"<p>               Bases: <code>LightningModule</code>, <code>IOMixin</code>, <code>LightningPassthroughPredictionMixin</code></p> <p>A very basic lightning module for testing the megatron strategy and the megatron-nemo2-bionemo contract.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class BionemoLightningModule(pl.LightningModule, io.IOMixin, LightningPassthroughPredictionMixin):\n    \"\"\"A very basic lightning module for testing the megatron strategy and the megatron-nemo2-bionemo contract.\"\"\"\n\n    def __init__(self, config: MegatronBioNeMoTrainableModelConfig):\n        \"\"\"Initializes the model.\n\n        Args:\n            config: a Config object necessary to construct the actual nn.Module (the thing that has the parameters).\n        \"\"\"\n        super().__init__()\n        self.config = config\n        self.vp_stage = None  # Add vp_stage attribute for compatibility with virtual pipeline\n        self.optim = MegatronOptimizerModule(\n            config=OptimizerConfig(\n                lr=1e-4,\n                optimizer=\"adam\",\n                use_distributed_optimizer=True,\n                bf16=config.bf16,\n                fp16=config.fp16,\n                params_dtype=config.params_dtype,\n            ),\n        )\n        # Bind the configure_optimizers method to the model\n        self.optim.connect(self)\n\n    def forward(self, batch: Dict, batch_idx: int) -&gt; Any:\n        \"\"\"This forward will be called by the megatron scheduler and it will be wrapped.\n\n        !!! note\n\n            The `training_step` defines the training loop and is independent of the `forward` method here.\n\n        Args:\n            batch: A dictionary of data.\n            batch_idx: The index of the batch.\n\n        Returns:\n            The output of the model.\n        \"\"\"\n        x = batch[\"data\"]\n        return self.module(x)\n\n    def training_step(self, batch, batch_idx: Optional[int] = None):\n        \"\"\"The training step is where the loss is calculated and the backpropagation is done.\n\n        Background:\n        - NeMo's Strategy overrides this method.\n        - The strategies' training step will call the forward method of the model.\n        - That forward method then calls the wrapped forward step of MegatronParallel which wraps the forward method of the model.\n        - That wrapped forward step is then executed inside the Mcore scheduler, which calls the `_forward_step` method from the\n            MegatronParallel class.\n        - Which then calls the training_step function here.\n\n        In this particular use case, we simply call the forward method of this class, the lightning module.\n\n        Args:\n            batch: A dictionary of data. requires `batch_idx` as default None.\n            batch_idx: The index of the batch.\n        \"\"\"\n        # Forward pass\n        predictions = self(batch, batch_idx)\n\n        # Calculate loss using the training loss reduction function\n        loss_reduction = self.training_loss_reduction()\n        loss_reduction.setup(batch)\n        loss = loss_reduction(predictions)\n\n        # Log the training loss\n        self.log(\"train_loss\", loss[1][\"avg\"], on_step=True, on_epoch=True, prog_bar=True, logger=True)\n\n        return predictions\n\n    def validation_step(self, batch, batch_idx: Optional[int] = None):\n        \"\"\"Alias for forward step at validation.\"\"\"\n        predictions = self(batch, batch_idx)\n\n        # Calculate loss using the validation loss reduction function\n        loss_reduction = self.validation_loss_reduction()\n        loss_reduction.setup(batch)\n        loss = loss_reduction(predictions)\n        # Log the validation loss\n        self.log(\n            \"val_loss\",\n            loss[1][\"avg\"],\n            on_step=False,\n            on_epoch=True,\n            prog_bar=True,\n            logger=True,\n        )\n\n        return predictions\n\n    def predict_step(self, batch, batch_idx: Optional[int] = None):\n        \"\"\"Alias for forward step at prediction.\"\"\"\n        return self(batch, batch_idx)\n\n    def training_loss_reduction(self) -&gt; MegatronLossReduction:\n        \"\"\"This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.\n\n        Returns:\n        A MegatronLossReduction\n        \"\"\"\n        return self.loss_reduction_class()()\n\n    def validation_loss_reduction(self) -&gt; MegatronLossReduction:\n        \"\"\"This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.\n\n        Returns:\n        A MegatronLossReduction\n        \"\"\"\n        return self.loss_reduction_class()()\n\n    def test_loss_reduction(self) -&gt; MegatronLossReduction:\n        \"\"\"This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.\n\n        Returns:\n        A MegatronLossReduction\n        \"\"\"\n        return self.loss_reduction_class()()\n\n    def configure_model(self) -&gt; None:\n        \"\"\"This configures the model. It is called lazily by the megatron strategy.\"\"\"\n        self.module = self.config.configure_model()\n        # Ensure vp_stage attribute is set for compatibility with virtual pipeline\n        if not hasattr(self.module, \"vp_stage\"):\n            self.module.vp_stage = None\n\n    def loss_reduction_class(self) -&gt; Type[MegatronLossReduction]:\n        \"\"\"Get the loss reduction class the user has specified in their config.\"\"\"\n        return self.config.get_loss_reduction_class()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.__init__","title":"<code>__init__(config)</code>","text":"<p>Initializes the model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>MegatronBioNeMoTrainableModelConfig</code> <p>a Config object necessary to construct the actual nn.Module (the thing that has the parameters).</p> required Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def __init__(self, config: MegatronBioNeMoTrainableModelConfig):\n    \"\"\"Initializes the model.\n\n    Args:\n        config: a Config object necessary to construct the actual nn.Module (the thing that has the parameters).\n    \"\"\"\n    super().__init__()\n    self.config = config\n    self.vp_stage = None  # Add vp_stage attribute for compatibility with virtual pipeline\n    self.optim = MegatronOptimizerModule(\n        config=OptimizerConfig(\n            lr=1e-4,\n            optimizer=\"adam\",\n            use_distributed_optimizer=True,\n            bf16=config.bf16,\n            fp16=config.fp16,\n            params_dtype=config.params_dtype,\n        ),\n    )\n    # Bind the configure_optimizers method to the model\n    self.optim.connect(self)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.configure_model","title":"<code>configure_model()</code>","text":"<p>This configures the model. It is called lazily by the megatron strategy.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def configure_model(self) -&gt; None:\n    \"\"\"This configures the model. It is called lazily by the megatron strategy.\"\"\"\n    self.module = self.config.configure_model()\n    # Ensure vp_stage attribute is set for compatibility with virtual pipeline\n    if not hasattr(self.module, \"vp_stage\"):\n        self.module.vp_stage = None\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.forward","title":"<code>forward(batch, batch_idx)</code>","text":"<p>This forward will be called by the megatron scheduler and it will be wrapped.</p> <p>Note</p> <p>The <code>training_step</code> defines the training loop and is independent of the <code>forward</code> method here.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict</code> <p>A dictionary of data.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The output of the model.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def forward(self, batch: Dict, batch_idx: int) -&gt; Any:\n    \"\"\"This forward will be called by the megatron scheduler and it will be wrapped.\n\n    !!! note\n\n        The `training_step` defines the training loop and is independent of the `forward` method here.\n\n    Args:\n        batch: A dictionary of data.\n        batch_idx: The index of the batch.\n\n    Returns:\n        The output of the model.\n    \"\"\"\n    x = batch[\"data\"]\n    return self.module(x)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.loss_reduction_class","title":"<code>loss_reduction_class()</code>","text":"<p>Get the loss reduction class the user has specified in their config.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def loss_reduction_class(self) -&gt; Type[MegatronLossReduction]:\n    \"\"\"Get the loss reduction class the user has specified in their config.\"\"\"\n    return self.config.get_loss_reduction_class()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.predict_step","title":"<code>predict_step(batch, batch_idx=None)</code>","text":"<p>Alias for forward step at prediction.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def predict_step(self, batch, batch_idx: Optional[int] = None):\n    \"\"\"Alias for forward step at prediction.\"\"\"\n    return self(batch, batch_idx)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.test_loss_reduction","title":"<code>test_loss_reduction()</code>","text":"<p>This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.</p> <p>Returns: A MegatronLossReduction</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def test_loss_reduction(self) -&gt; MegatronLossReduction:\n    \"\"\"This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.\n\n    Returns:\n    A MegatronLossReduction\n    \"\"\"\n    return self.loss_reduction_class()()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.training_loss_reduction","title":"<code>training_loss_reduction()</code>","text":"<p>This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.</p> <p>Returns: A MegatronLossReduction</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def training_loss_reduction(self) -&gt; MegatronLossReduction:\n    \"\"\"This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.\n\n    Returns:\n    A MegatronLossReduction\n    \"\"\"\n    return self.loss_reduction_class()()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.training_step","title":"<code>training_step(batch, batch_idx=None)</code>","text":"<p>The training step is where the loss is calculated and the backpropagation is done.</p> <p>Background: - NeMo's Strategy overrides this method. - The strategies' training step will call the forward method of the model. - That forward method then calls the wrapped forward step of MegatronParallel which wraps the forward method of the model. - That wrapped forward step is then executed inside the Mcore scheduler, which calls the <code>_forward_step</code> method from the     MegatronParallel class. - Which then calls the training_step function here.</p> <p>In this particular use case, we simply call the forward method of this class, the lightning module.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <p>A dictionary of data. requires <code>batch_idx</code> as default None.</p> required <code>batch_idx</code> <code>Optional[int]</code> <p>The index of the batch.</p> <code>None</code> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def training_step(self, batch, batch_idx: Optional[int] = None):\n    \"\"\"The training step is where the loss is calculated and the backpropagation is done.\n\n    Background:\n    - NeMo's Strategy overrides this method.\n    - The strategies' training step will call the forward method of the model.\n    - That forward method then calls the wrapped forward step of MegatronParallel which wraps the forward method of the model.\n    - That wrapped forward step is then executed inside the Mcore scheduler, which calls the `_forward_step` method from the\n        MegatronParallel class.\n    - Which then calls the training_step function here.\n\n    In this particular use case, we simply call the forward method of this class, the lightning module.\n\n    Args:\n        batch: A dictionary of data. requires `batch_idx` as default None.\n        batch_idx: The index of the batch.\n    \"\"\"\n    # Forward pass\n    predictions = self(batch, batch_idx)\n\n    # Calculate loss using the training loss reduction function\n    loss_reduction = self.training_loss_reduction()\n    loss_reduction.setup(batch)\n    loss = loss_reduction(predictions)\n\n    # Log the training loss\n    self.log(\"train_loss\", loss[1][\"avg\"], on_step=True, on_epoch=True, prog_bar=True, logger=True)\n\n    return predictions\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.validation_loss_reduction","title":"<code>validation_loss_reduction()</code>","text":"<p>This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.</p> <p>Returns: A MegatronLossReduction</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def validation_loss_reduction(self) -&gt; MegatronLossReduction:\n    \"\"\"This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.\n\n    Returns:\n    A MegatronLossReduction\n    \"\"\"\n    return self.loss_reduction_class()()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.validation_step","title":"<code>validation_step(batch, batch_idx=None)</code>","text":"<p>Alias for forward step at validation.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def validation_step(self, batch, batch_idx: Optional[int] = None):\n    \"\"\"Alias for forward step at validation.\"\"\"\n    predictions = self(batch, batch_idx)\n\n    # Calculate loss using the validation loss reduction function\n    loss_reduction = self.validation_loss_reduction()\n    loss_reduction.setup(batch)\n    loss = loss_reduction(predictions)\n    # Log the validation loss\n    self.log(\n        \"val_loss\",\n        loss[1][\"avg\"],\n        on_step=False,\n        on_epoch=True,\n        prog_bar=True,\n        logger=True,\n    )\n\n    return predictions\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ClassifierLossReduction","title":"<code>ClassifierLossReduction</code>","text":"<p>               Bases: <code>MegatronLossReduction</code></p> <p>A class used for calculating the loss, and for logging the reduced loss across micro batches.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class ClassifierLossReduction(MegatronLossReduction):\n    \"\"\"A class used for calculating the loss, and for logging the reduced loss across micro batches.\"\"\"\n\n    def forward(self, batch: MnistItem, forward_out: Tensor) -&gt; Tuple[Tensor, SameSizeLossDict]:\n        \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n\n        Args:\n            batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n            forward_out: the output of the forward method inside LitAutoEncoder.\n\n        Returns:\n            A tuple containing [&lt;loss_tensor&gt;, ReductionT] where the loss tensor will be used for\n                backpropagation and the ReductionT will be passed to the reduce method\n                (which currently only works for logging.).\n        \"\"\"\n        digits = batch[\"label\"]\n        digit_logits = forward_out\n        loss = nn.functional.cross_entropy(digit_logits, digits)\n        return loss, {\"avg\": loss}\n\n    def reduce(self, losses_reduced_per_micro_batch: Sequence[SameSizeLossDict]) -&gt; Tensor:\n        \"\"\"Works across micro-batches. (data on single gpu).\n\n        Note: This currently only works for logging and this loss will not be used for backpropagation.\n\n        Args:\n            losses_reduced_per_micro_batch: a list of the outputs of forward\n\n        Returns:\n            A tensor that is the mean of the losses. (used for logging).\n        \"\"\"\n        mse_losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n        return mse_losses.mean()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ClassifierLossReduction.forward","title":"<code>forward(batch, forward_out)</code>","text":"<p>Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>MnistItem</code> <p>A batch of data that gets passed to the original forward inside LitAutoEncoder.</p> required <code>forward_out</code> <code>Tensor</code> <p>the output of the forward method inside LitAutoEncoder.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, SameSizeLossDict]</code> <p>A tuple containing [, ReductionT] where the loss tensor will be used for backpropagation and the ReductionT will be passed to the reduce method (which currently only works for logging.). Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def forward(self, batch: MnistItem, forward_out: Tensor) -&gt; Tuple[Tensor, SameSizeLossDict]:\n    \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n\n    Args:\n        batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n        forward_out: the output of the forward method inside LitAutoEncoder.\n\n    Returns:\n        A tuple containing [&lt;loss_tensor&gt;, ReductionT] where the loss tensor will be used for\n            backpropagation and the ReductionT will be passed to the reduce method\n            (which currently only works for logging.).\n    \"\"\"\n    digits = batch[\"label\"]\n    digit_logits = forward_out\n    loss = nn.functional.cross_entropy(digit_logits, digits)\n    return loss, {\"avg\": loss}\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ClassifierLossReduction.reduce","title":"<code>reduce(losses_reduced_per_micro_batch)</code>","text":"<p>Works across micro-batches. (data on single gpu).</p> <p>Note: This currently only works for logging and this loss will not be used for backpropagation.</p> <p>Parameters:</p> Name Type Description Default <code>losses_reduced_per_micro_batch</code> <code>Sequence[SameSizeLossDict]</code> <p>a list of the outputs of forward</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor that is the mean of the losses. (used for logging).</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def reduce(self, losses_reduced_per_micro_batch: Sequence[SameSizeLossDict]) -&gt; Tensor:\n    \"\"\"Works across micro-batches. (data on single gpu).\n\n    Note: This currently only works for logging and this loss will not be used for backpropagation.\n\n    Args:\n        losses_reduced_per_micro_batch: a list of the outputs of forward\n\n    Returns:\n        A tensor that is the mean of the losses. (used for logging).\n    \"\"\"\n    mse_losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n    return mse_losses.mean()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleFineTuneBothConfig","title":"<code>ExampleFineTuneBothConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ExampleGenericConfig['ExampleFineTuneBothModel', 'MSEPlusClassifierLossReduction']</code>, <code>IOMixinWithGettersSetters</code></p> <p>ExampleConfig is a dataclass that is used to configure the model.</p> <p>Timers from ModelParallelConfig are required for megatron forward compatibility.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>@dataclass\nclass ExampleFineTuneBothConfig(\n    ExampleGenericConfig[\"ExampleFineTuneBothModel\", \"MSEPlusClassifierLossReduction\"], iom.IOMixinWithGettersSetters\n):\n    \"\"\"ExampleConfig is a dataclass that is used to configure the model.\n\n    Timers from ModelParallelConfig are required for megatron forward compatibility.\n    \"\"\"\n\n    model_cls: Type[ExampleFineTuneBothModel] = ExampleFineTuneBothModel\n    loss_cls: Type[MSEPlusClassifierLossReduction] = MSEPlusClassifierLossReduction\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleFineTuneBothModel","title":"<code>ExampleFineTuneBothModel</code>","text":"<p>               Bases: <code>ExampleModel</code></p> <p>Example of taking the example model and adding an output task.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class ExampleFineTuneBothModel(ExampleModel):\n    \"\"\"Example of taking the example model and adding an output task.\"\"\"\n\n    def __init__(self, config: ModelParallelConfig):\n        super().__init__(config)\n        # 10 output digits, and use the latent output layer (z) for making predictions\n        self.digit_classifier = nn.Linear(self.linear2.out_features, 10)\n\n    def forward(self, x: Tensor) -&gt; ExampleFineTuneOutput:\n        parent_out: ExampleModelOutput = super().forward(x)\n        digit_logits = self.digit_classifier(parent_out[\"z\"])\n        return {\n            \"x_hat\": parent_out[\"x_hat\"],\n            \"z\": parent_out[\"z\"],\n            \"digit_logits\": digit_logits,\n        }\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleFineTuneConfig","title":"<code>ExampleFineTuneConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ExampleGenericConfig['ExampleFineTuneConfig', 'ClassifierLossReduction']</code>, <code>IOMixinWithGettersSetters</code></p> <p>ExampleConfig is a dataclass that is used to configure the model.</p> <p>Timers from ModelParallelConfig are required for megatron forward compatibility.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>@dataclass\nclass ExampleFineTuneConfig(\n    ExampleGenericConfig[\"ExampleFineTuneConfig\", \"ClassifierLossReduction\"], iom.IOMixinWithGettersSetters\n):\n    \"\"\"ExampleConfig is a dataclass that is used to configure the model.\n\n    Timers from ModelParallelConfig are required for megatron forward compatibility.\n    \"\"\"\n\n    model_cls: Type[ExampleFineTuneModel] = ExampleFineTuneModel\n    loss_cls: Type[ClassifierLossReduction] = ClassifierLossReduction\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleFineTuneModel","title":"<code>ExampleFineTuneModel</code>","text":"<p>               Bases: <code>ExampleModelTrunk</code></p> <p>Example of taking the example model and replacing output task.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class ExampleFineTuneModel(ExampleModelTrunk):\n    \"\"\"Example of taking the example model and replacing output task.\"\"\"\n\n    def __init__(self, config: ModelParallelConfig):\n        super().__init__(config)\n        # 10 output digits, and use the latent output layer (z) for making predictions\n        self.digit_classifier = nn.Linear(self.linear2.out_features, 10)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        z: Tensor = super().forward(x)\n        digit_logits = self.digit_classifier(z)  # to demonstrate flexibility, in this case we return a tensor\n        return digit_logits\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleFineTuneOutput","title":"<code>ExampleFineTuneOutput</code>","text":"<p>               Bases: <code>ExampleModelOutput</code></p> <p>Output for the fine-tuned example model implementation.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class ExampleFineTuneOutput(ExampleModelOutput):\n    \"\"\"Output for the fine-tuned example model implementation.\"\"\"\n\n    digit_logits: Tensor\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleGenericConfig","title":"<code>ExampleGenericConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[ExampleModelT, MegatronLossType]</code>, <code>MegatronBioNeMoTrainableModelConfig[ExampleModelT, MegatronLossType]</code></p> <p>ExampleGenericConfig is a dataclass that is used to configure the model.</p> <p>Timers from ModelParallelConfig are required for megatron forward compatibility.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>@dataclass\nclass ExampleGenericConfig(\n    Generic[ExampleModelT, MegatronLossType], MegatronBioNeMoTrainableModelConfig[ExampleModelT, MegatronLossType]\n):\n    \"\"\"ExampleGenericConfig is a dataclass that is used to configure the model.\n\n    Timers from ModelParallelConfig are required for megatron forward compatibility.\n    \"\"\"\n\n    loss_cls: Type[MegatronLossType] = MSELossReduction  # type: ignore  # this will get overriden by children\n    hidden_size: int = 64  # Needs to be set to avoid zero division error in megatron :(\n    num_attention_heads: int = 1  # Needs to be set to avoid zero division error in megatron :(\n    num_layers: int = 1  # Needs to be set to avoid zero division error in megatron :(\n    # IMPORTANT: Since we're adding/overriding the loss_cls, and that's not how we generally track this, we need to\n    #   add this into the list of config settings that we do not draw from the loaded checkpoint when restoring.\n    override_parent_fields: List[str] = field(default_factory=lambda: OVERRIDE_BIONEMO_CONFIG_DEFAULTS + [\"loss_cls\"])\n\n    def configure_model(self) -&gt; ExampleModelT:\n        \"\"\"Uses model_cls and loss_cls to configure the model.\n\n        Note: Must pass self into Model since model requires having a config object.\n\n        Returns:\n            The model object.\n        \"\"\"\n        # 1. first load any settings that may exist in the checkpoint related to the model.\n        if self.initial_ckpt_path:\n            self.load_settings_from_checkpoint(self.initial_ckpt_path)\n        # 2. then initialize the model\n        model = self.model_cls(self)\n        # 3. Load weights from the checkpoint into the model\n        if self.initial_ckpt_path:\n            self.update_model_from_checkpoint(model, self.initial_ckpt_path)\n        return model\n\n    def get_loss_reduction_class(self) -&gt; Type[MegatronLossType]:\n        \"\"\"Use loss_cls to configure the loss, since we do not change the settings of the loss based on the config.\"\"\"\n        return self.loss_cls\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleGenericConfig.configure_model","title":"<code>configure_model()</code>","text":"<p>Uses model_cls and loss_cls to configure the model.</p> <p>Note: Must pass self into Model since model requires having a config object.</p> <p>Returns:</p> Type Description <code>ExampleModelT</code> <p>The model object.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def configure_model(self) -&gt; ExampleModelT:\n    \"\"\"Uses model_cls and loss_cls to configure the model.\n\n    Note: Must pass self into Model since model requires having a config object.\n\n    Returns:\n        The model object.\n    \"\"\"\n    # 1. first load any settings that may exist in the checkpoint related to the model.\n    if self.initial_ckpt_path:\n        self.load_settings_from_checkpoint(self.initial_ckpt_path)\n    # 2. then initialize the model\n    model = self.model_cls(self)\n    # 3. Load weights from the checkpoint into the model\n    if self.initial_ckpt_path:\n        self.update_model_from_checkpoint(model, self.initial_ckpt_path)\n    return model\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleGenericConfig.get_loss_reduction_class","title":"<code>get_loss_reduction_class()</code>","text":"<p>Use loss_cls to configure the loss, since we do not change the settings of the loss based on the config.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def get_loss_reduction_class(self) -&gt; Type[MegatronLossType]:\n    \"\"\"Use loss_cls to configure the loss, since we do not change the settings of the loss based on the config.\"\"\"\n    return self.loss_cls\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleModel","title":"<code>ExampleModel</code>","text":"<p>               Bases: <code>ExampleModelTrunk</code></p> <p>An example model.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class ExampleModel(ExampleModelTrunk):\n    \"\"\"An example model.\"\"\"\n\n    def __init__(self, config: ModelParallelConfig) -&gt; None:\n        \"\"\"Constructor of the model.\n\n        Args:\n            config: The config object is responsible for telling the strategy what model to create.\n        \"\"\"\n        super().__init__(config)\n        self.linear3 = nn.Linear(3, 64)\n        self.relu2 = nn.ReLU()\n        self.linear4 = nn.Linear(64, 28 * 28)\n\n    def forward(self, x: Tensor) -&gt; ExampleModelOutput:\n        \"\"\"Forward pass of the model.\n\n        Args:\n            x: The input data.\n\n        Returns:\n            x_hat: The result of the last linear layer of the network.\n        \"\"\"\n        z: Tensor = super().forward(x)\n        x_hat = self.linear3(z)\n        x_hat = self.relu2(x_hat)\n        x_hat = self.linear4(x_hat)\n        return {\"x_hat\": x_hat, \"z\": z}\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleModel.__init__","title":"<code>__init__(config)</code>","text":"<p>Constructor of the model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ModelParallelConfig</code> <p>The config object is responsible for telling the strategy what model to create.</p> required Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def __init__(self, config: ModelParallelConfig) -&gt; None:\n    \"\"\"Constructor of the model.\n\n    Args:\n        config: The config object is responsible for telling the strategy what model to create.\n    \"\"\"\n    super().__init__(config)\n    self.linear3 = nn.Linear(3, 64)\n    self.relu2 = nn.ReLU()\n    self.linear4 = nn.Linear(64, 28 * 28)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleModel.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input data.</p> required <p>Returns:</p> Name Type Description <code>x_hat</code> <code>ExampleModelOutput</code> <p>The result of the last linear layer of the network.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def forward(self, x: Tensor) -&gt; ExampleModelOutput:\n    \"\"\"Forward pass of the model.\n\n    Args:\n        x: The input data.\n\n    Returns:\n        x_hat: The result of the last linear layer of the network.\n    \"\"\"\n    z: Tensor = super().forward(x)\n    x_hat = self.linear3(z)\n    x_hat = self.relu2(x_hat)\n    x_hat = self.linear4(x_hat)\n    return {\"x_hat\": x_hat, \"z\": z}\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleModelOutput","title":"<code>ExampleModelOutput</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Output for the example model implementation.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class ExampleModelOutput(TypedDict):\n    \"\"\"Output for the example model implementation.\"\"\"\n\n    x_hat: Tensor\n    z: Tensor\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleModelTrunk","title":"<code>ExampleModelTrunk</code>","text":"<p>               Bases: <code>MegatronModule</code></p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class ExampleModelTrunk(MegatronModule):\n    def __init__(self, config: ModelParallelConfig) -&gt; None:\n        \"\"\"Constructor of the model.\n\n        Args:\n            config: The config object is responsible for telling the strategy what model to create.\n        \"\"\"\n        super().__init__(config)\n        # FIXME add an assertion that the user is not trying to do tensor parallelism since this doesn't use\n        #  parallelizable megatron linear layers.\n        self.model_type: ModelType = ModelType.encoder_or_decoder\n        self.vp_stage = None  # Add vp_stage attribute for compatibility with virtual pipeline\n        self.linear1 = nn.Linear(28 * 28, 64)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(64, 3)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        # we could return a dictionary of strings to tensors here, but let's demonstrate this is not necessary\n        x = x.view(x.size(0), -1)\n        z = self.linear1(x)\n        z = self.relu(z)\n        z = self.linear2(z)\n        return z\n\n    def set_input_tensor(self, input_tensor: Optional[Tensor]) -&gt; None:\n        \"\"\"This _would_ be needed for model parallel and other kinds of more complicated forward passes in megatron.\"\"\"\n        pass\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleModelTrunk.__init__","title":"<code>__init__(config)</code>","text":"<p>Constructor of the model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ModelParallelConfig</code> <p>The config object is responsible for telling the strategy what model to create.</p> required Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def __init__(self, config: ModelParallelConfig) -&gt; None:\n    \"\"\"Constructor of the model.\n\n    Args:\n        config: The config object is responsible for telling the strategy what model to create.\n    \"\"\"\n    super().__init__(config)\n    # FIXME add an assertion that the user is not trying to do tensor parallelism since this doesn't use\n    #  parallelizable megatron linear layers.\n    self.model_type: ModelType = ModelType.encoder_or_decoder\n    self.vp_stage = None  # Add vp_stage attribute for compatibility with virtual pipeline\n    self.linear1 = nn.Linear(28 * 28, 64)\n    self.relu = nn.ReLU()\n    self.linear2 = nn.Linear(64, 3)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleModelTrunk.set_input_tensor","title":"<code>set_input_tensor(input_tensor)</code>","text":"<p>This would be needed for model parallel and other kinds of more complicated forward passes in megatron.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def set_input_tensor(self, input_tensor: Optional[Tensor]) -&gt; None:\n    \"\"\"This _would_ be needed for model parallel and other kinds of more complicated forward passes in megatron.\"\"\"\n    pass\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MNISTCustomDataset","title":"<code>MNISTCustomDataset</code>","text":"<p>               Bases: <code>MNIST</code></p> <p>A Wrapper for the MNIST Dataset.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class MNISTCustomDataset(MNIST):\n    \"\"\"A Wrapper for the MNIST Dataset.\"\"\"\n\n    def __getitem__(self, idx: int) -&gt; MnistItem:\n        \"\"\"Wraps the getitem method of the MNIST dataset such that we return a Dict.\n\n        This is instead of a Tuple or tensor.\n\n        Args:\n            idx: The index we want to grab, an int.\n\n        Returns:\n            A dict containing the data (\"x\"), label (\"y\"), and index (\"idx\").\n        \"\"\"\n        data, label = super().__getitem__(idx)\n\n        return {\n            \"data\": data,\n            \"label\": label,\n            \"idx\": idx,\n        }\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MNISTCustomDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Wraps the getitem method of the MNIST dataset such that we return a Dict.</p> <p>This is instead of a Tuple or tensor.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>The index we want to grab, an int.</p> required <p>Returns:</p> Type Description <code>MnistItem</code> <p>A dict containing the data (\"x\"), label (\"y\"), and index (\"idx\").</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; MnistItem:\n    \"\"\"Wraps the getitem method of the MNIST dataset such that we return a Dict.\n\n    This is instead of a Tuple or tensor.\n\n    Args:\n        idx: The index we want to grab, an int.\n\n    Returns:\n        A dict containing the data (\"x\"), label (\"y\"), and index (\"idx\").\n    \"\"\"\n    data, label = super().__getitem__(idx)\n\n    return {\n        \"data\": data,\n        \"label\": label,\n        \"idx\": idx,\n    }\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MNISTDataModule","title":"<code>MNISTDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>A Megatron Compatible Data Module for MNIST.</p> <p>Attributes: data_dir: data directory micro_batch_size: batch_size global_batch_size: global batch size max_len: maximal sequence length for megatron sampler rampup_batch_size: ramp up batch size num_workers: number of workers data_sampler: data_sampler set to be a megatron one</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class MNISTDataModule(pl.LightningDataModule):\n    \"\"\"A Megatron Compatible Data Module for MNIST.\n\n    Attributes:\n    data_dir: data directory\n    micro_batch_size: batch_size\n    global_batch_size: global batch size\n    max_len: maximal sequence length for megatron sampler\n    rampup_batch_size: ramp up batch size\n    num_workers: number of workers\n    data_sampler: data_sampler set to be a megatron one\n    \"\"\"\n\n    def __init__(\n        self,\n        data_dir: str | os.PathLike = str(BIONEMO_CACHE_DIR),\n        batch_size: int = 32,\n        num_workers: int = 0,\n        global_batch_size: int | None = None,\n        output_log: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize class.\n\n        Args:\n            data_dir: data directory\n            batch_size: batch_size\n            global_batch_size: global batch size\n            num_workers: number of workers\n            output_log: whether to output logs\n\n        \"\"\"\n        super().__init__()\n        self.data_dir = data_dir\n        self.micro_batch_size = batch_size\n        self.global_batch_size = global_batch_size or batch_size\n        self.max_len = 1048\n        # We need to define a \"seq_length\" for OneLogger, but we just set it to max_len\n        self.seq_length = self.max_len\n        self.rampup_batch_size = None\n        self.num_workers = num_workers\n        #  Note that this sampler is sequential, meaning it does not do any shuffling. Let's wrap our data in a shuffler.\n        # Wraps the datasampler with the MegatronDataSampler. The MegatronDataSampler is a wrapper that allows the sampler\n        # to be used with megatron. It sets up the capability to utilize micro-batching and gradient accumulation. It is also\n        # the place where the global batch size is constructed.\n        self.data_sampler = MegatronDataSampler(\n            seq_len=self.max_len,\n            micro_batch_size=self.micro_batch_size,\n            global_batch_size=self.global_batch_size,\n            rampup_batch_size=self.rampup_batch_size,\n            output_log=output_log,\n        )\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Sets up the datasets.\n\n        Args:\n            stage: can be one of train / test / predict.\n        \"\"\"\n        self.mnist_test = MultiEpochDatasetResampler(\n            IdentityMultiEpochDatasetWrapper(\n                MNISTCustomDataset(self.data_dir, download=True, transform=transforms.ToTensor(), train=False)\n            ),\n            seed=43,\n            shuffle=False,\n        )\n        mnist_full = MNISTCustomDataset(self.data_dir, download=True, transform=transforms.ToTensor(), train=True)\n        mnist_train, mnist_val = torch.utils.data.random_split(\n            mnist_full, [55000, 5000], generator=torch.Generator().manual_seed(42)\n        )\n        self.mnist_train = MultiEpochDatasetResampler(\n            IdentityMultiEpochDatasetWrapper(mnist_train), seed=44, shuffle=True\n        )\n\n        self.mnist_val = MultiEpochDatasetResampler(\n            IdentityMultiEpochDatasetWrapper(mnist_val),\n            seed=45,\n            shuffle=False,\n        )\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"Returns the training dataloader.\"\"\"\n        return DataLoader(self.mnist_train, batch_size=self.micro_batch_size, num_workers=self.num_workers)\n\n    def val_dataloader(self) -&gt; DataLoader:\n        \"\"\"Returns the validation dataloader.\"\"\"\n        return DataLoader(self.mnist_val, batch_size=self.micro_batch_size, num_workers=self.num_workers)\n\n    def predict_dataloader(self) -&gt; DataLoader:\n        \"\"\"Returns the prediction dataloader.\"\"\"\n        return DataLoader(self.mnist_test, batch_size=self.micro_batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MNISTDataModule.__init__","title":"<code>__init__(data_dir=str(BIONEMO_CACHE_DIR), batch_size=32, num_workers=0, global_batch_size=None, output_log=True)</code>","text":"<p>Initialize class.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str | PathLike</code> <p>data directory</p> <code>str(BIONEMO_CACHE_DIR)</code> <code>batch_size</code> <code>int</code> <p>batch_size</p> <code>32</code> <code>global_batch_size</code> <code>int | None</code> <p>global batch size</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>number of workers</p> <code>0</code> <code>output_log</code> <code>bool</code> <p>whether to output logs</p> <code>True</code> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def __init__(\n    self,\n    data_dir: str | os.PathLike = str(BIONEMO_CACHE_DIR),\n    batch_size: int = 32,\n    num_workers: int = 0,\n    global_batch_size: int | None = None,\n    output_log: bool = True,\n) -&gt; None:\n    \"\"\"Initialize class.\n\n    Args:\n        data_dir: data directory\n        batch_size: batch_size\n        global_batch_size: global batch size\n        num_workers: number of workers\n        output_log: whether to output logs\n\n    \"\"\"\n    super().__init__()\n    self.data_dir = data_dir\n    self.micro_batch_size = batch_size\n    self.global_batch_size = global_batch_size or batch_size\n    self.max_len = 1048\n    # We need to define a \"seq_length\" for OneLogger, but we just set it to max_len\n    self.seq_length = self.max_len\n    self.rampup_batch_size = None\n    self.num_workers = num_workers\n    #  Note that this sampler is sequential, meaning it does not do any shuffling. Let's wrap our data in a shuffler.\n    # Wraps the datasampler with the MegatronDataSampler. The MegatronDataSampler is a wrapper that allows the sampler\n    # to be used with megatron. It sets up the capability to utilize micro-batching and gradient accumulation. It is also\n    # the place where the global batch size is constructed.\n    self.data_sampler = MegatronDataSampler(\n        seq_len=self.max_len,\n        micro_batch_size=self.micro_batch_size,\n        global_batch_size=self.global_batch_size,\n        rampup_batch_size=self.rampup_batch_size,\n        output_log=output_log,\n    )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MNISTDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Returns the prediction dataloader.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def predict_dataloader(self) -&gt; DataLoader:\n    \"\"\"Returns the prediction dataloader.\"\"\"\n    return DataLoader(self.mnist_test, batch_size=self.micro_batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MNISTDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Sets up the datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>can be one of train / test / predict.</p> required Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Sets up the datasets.\n\n    Args:\n        stage: can be one of train / test / predict.\n    \"\"\"\n    self.mnist_test = MultiEpochDatasetResampler(\n        IdentityMultiEpochDatasetWrapper(\n            MNISTCustomDataset(self.data_dir, download=True, transform=transforms.ToTensor(), train=False)\n        ),\n        seed=43,\n        shuffle=False,\n    )\n    mnist_full = MNISTCustomDataset(self.data_dir, download=True, transform=transforms.ToTensor(), train=True)\n    mnist_train, mnist_val = torch.utils.data.random_split(\n        mnist_full, [55000, 5000], generator=torch.Generator().manual_seed(42)\n    )\n    self.mnist_train = MultiEpochDatasetResampler(\n        IdentityMultiEpochDatasetWrapper(mnist_train), seed=44, shuffle=True\n    )\n\n    self.mnist_val = MultiEpochDatasetResampler(\n        IdentityMultiEpochDatasetWrapper(mnist_val),\n        seed=45,\n        shuffle=False,\n    )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MNISTDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns the training dataloader.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"Returns the training dataloader.\"\"\"\n    return DataLoader(self.mnist_train, batch_size=self.micro_batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MNISTDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Returns the validation dataloader.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n    \"\"\"Returns the validation dataloader.\"\"\"\n    return DataLoader(self.mnist_val, batch_size=self.micro_batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MSELossReduction","title":"<code>MSELossReduction</code>","text":"<p>               Bases: <code>MegatronLossReduction</code></p> <p>A class used for calculating the loss, and for logging the reduced loss across micro batches.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class MSELossReduction(MegatronLossReduction):\n    \"\"\"A class used for calculating the loss, and for logging the reduced loss across micro batches.\"\"\"\n\n    def forward(self, batch: MnistItem, forward_out: Dict[str, Tensor]) -&gt; Tuple[Tensor, SameSizeLossDict]:\n        \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n\n        Args:\n            batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n            forward_out: the output of the forward method inside LitAutoEncoder.\n\n        Returns:\n            A tuple containing [&lt;loss_tensor&gt;, ReductionT] where the loss tensor will be used for\n                backpropagation and the ReductionT will be passed to the reduce method\n                (which currently only works for logging.).\n        \"\"\"\n        x = batch[\"data\"]\n        x_hat = forward_out[\"x_hat\"]\n        xview = x.view(x.size(0), -1).to(x_hat.dtype)\n        loss = nn.functional.mse_loss(x_hat, xview)\n\n        return loss, {\"avg\": loss}\n\n    def reduce(self, losses_reduced_per_micro_batch: Sequence[SameSizeLossDict]) -&gt; Tensor:\n        \"\"\"Works across micro-batches. (data on single gpu).\n\n        Note: This currently only works for logging and this loss will not be used for backpropagation.\n\n        Args:\n            losses_reduced_per_micro_batch: a list of the outputs of forward\n\n        Returns:\n            A tensor that is the mean of the losses. (used for logging).\n        \"\"\"\n        mse_losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n        return mse_losses.mean()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MSELossReduction.forward","title":"<code>forward(batch, forward_out)</code>","text":"<p>Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>MnistItem</code> <p>A batch of data that gets passed to the original forward inside LitAutoEncoder.</p> required <code>forward_out</code> <code>Dict[str, Tensor]</code> <p>the output of the forward method inside LitAutoEncoder.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, SameSizeLossDict]</code> <p>A tuple containing [, ReductionT] where the loss tensor will be used for backpropagation and the ReductionT will be passed to the reduce method (which currently only works for logging.). Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def forward(self, batch: MnistItem, forward_out: Dict[str, Tensor]) -&gt; Tuple[Tensor, SameSizeLossDict]:\n    \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n\n    Args:\n        batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n        forward_out: the output of the forward method inside LitAutoEncoder.\n\n    Returns:\n        A tuple containing [&lt;loss_tensor&gt;, ReductionT] where the loss tensor will be used for\n            backpropagation and the ReductionT will be passed to the reduce method\n            (which currently only works for logging.).\n    \"\"\"\n    x = batch[\"data\"]\n    x_hat = forward_out[\"x_hat\"]\n    xview = x.view(x.size(0), -1).to(x_hat.dtype)\n    loss = nn.functional.mse_loss(x_hat, xview)\n\n    return loss, {\"avg\": loss}\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MSELossReduction.reduce","title":"<code>reduce(losses_reduced_per_micro_batch)</code>","text":"<p>Works across micro-batches. (data on single gpu).</p> <p>Note: This currently only works for logging and this loss will not be used for backpropagation.</p> <p>Parameters:</p> Name Type Description Default <code>losses_reduced_per_micro_batch</code> <code>Sequence[SameSizeLossDict]</code> <p>a list of the outputs of forward</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor that is the mean of the losses. (used for logging).</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def reduce(self, losses_reduced_per_micro_batch: Sequence[SameSizeLossDict]) -&gt; Tensor:\n    \"\"\"Works across micro-batches. (data on single gpu).\n\n    Note: This currently only works for logging and this loss will not be used for backpropagation.\n\n    Args:\n        losses_reduced_per_micro_batch: a list of the outputs of forward\n\n    Returns:\n        A tensor that is the mean of the losses. (used for logging).\n    \"\"\"\n    mse_losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n    return mse_losses.mean()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MSEPlusClassifierLossReduction","title":"<code>MSEPlusClassifierLossReduction</code>","text":"<p>               Bases: <code>MegatronLossReduction</code></p> <p>A class used for calculating the loss, and for logging the reduced loss across micro batches.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class MSEPlusClassifierLossReduction(MegatronLossReduction):\n    \"\"\"A class used for calculating the loss, and for logging the reduced loss across micro batches.\"\"\"\n\n    def forward(self, batch: MnistItem, forward_out: ExampleFineTuneOutput) -&gt; Tuple[Tensor, SameSizeLossDict]:\n        \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n\n        Args:\n            batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n            forward_out: the output of the forward method inside LitAutoEncoder.\n\n        Returns:\n            A tuple containing [&lt;loss_tensor&gt;, ReductionT] where the loss tensor will be used for\n                backpropagation and the ReductionT will be passed to the reduce method\n                (which currently only works for logging.).\n        \"\"\"\n        x = batch[\"data\"]\n        digits = batch[\"label\"]\n        x_hat = forward_out[\"x_hat\"]\n        digit_logits = forward_out[\"digit_logits\"]\n        xview = x.view(x.size(0), -1).to(x_hat.dtype)\n        mse_loss = nn.functional.mse_loss(x_hat, xview)\n        classifier_loss = nn.functional.cross_entropy(digit_logits, digits)\n        loss = classifier_loss + mse_loss\n        return loss, {\"avg\": loss}\n\n    def reduce(self, losses_reduced_per_micro_batch: Sequence[SameSizeLossDict]) -&gt; Tensor:\n        \"\"\"Works across micro-batches. (data on single gpu).\n\n        Note: This currently only works for logging and this loss will not be used for backpropagation.\n\n        Args:\n            losses_reduced_per_micro_batch: a list of the outputs of forward\n\n        Returns:\n            A tensor that is the mean of the losses. (used for logging).\n        \"\"\"\n        mse_losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n        return mse_losses.mean()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MSEPlusClassifierLossReduction.forward","title":"<code>forward(batch, forward_out)</code>","text":"<p>Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>MnistItem</code> <p>A batch of data that gets passed to the original forward inside LitAutoEncoder.</p> required <code>forward_out</code> <code>ExampleFineTuneOutput</code> <p>the output of the forward method inside LitAutoEncoder.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, SameSizeLossDict]</code> <p>A tuple containing [, ReductionT] where the loss tensor will be used for backpropagation and the ReductionT will be passed to the reduce method (which currently only works for logging.). Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def forward(self, batch: MnistItem, forward_out: ExampleFineTuneOutput) -&gt; Tuple[Tensor, SameSizeLossDict]:\n    \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n\n    Args:\n        batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n        forward_out: the output of the forward method inside LitAutoEncoder.\n\n    Returns:\n        A tuple containing [&lt;loss_tensor&gt;, ReductionT] where the loss tensor will be used for\n            backpropagation and the ReductionT will be passed to the reduce method\n            (which currently only works for logging.).\n    \"\"\"\n    x = batch[\"data\"]\n    digits = batch[\"label\"]\n    x_hat = forward_out[\"x_hat\"]\n    digit_logits = forward_out[\"digit_logits\"]\n    xview = x.view(x.size(0), -1).to(x_hat.dtype)\n    mse_loss = nn.functional.mse_loss(x_hat, xview)\n    classifier_loss = nn.functional.cross_entropy(digit_logits, digits)\n    loss = classifier_loss + mse_loss\n    return loss, {\"avg\": loss}\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MSEPlusClassifierLossReduction.reduce","title":"<code>reduce(losses_reduced_per_micro_batch)</code>","text":"<p>Works across micro-batches. (data on single gpu).</p> <p>Note: This currently only works for logging and this loss will not be used for backpropagation.</p> <p>Parameters:</p> Name Type Description Default <code>losses_reduced_per_micro_batch</code> <code>Sequence[SameSizeLossDict]</code> <p>a list of the outputs of forward</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor that is the mean of the losses. (used for logging).</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def reduce(self, losses_reduced_per_micro_batch: Sequence[SameSizeLossDict]) -&gt; Tensor:\n    \"\"\"Works across micro-batches. (data on single gpu).\n\n    Note: This currently only works for logging and this loss will not be used for backpropagation.\n\n    Args:\n        losses_reduced_per_micro_batch: a list of the outputs of forward\n\n    Returns:\n        A tensor that is the mean of the losses. (used for logging).\n    \"\"\"\n    mse_losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n    return mse_losses.mean()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MnistItem","title":"<code>MnistItem</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Training input for the MNIST dataset.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class MnistItem(TypedDict):\n    \"\"\"Training input for the MNIST dataset.\"\"\"\n\n    data: Tensor\n    label: Tensor\n    idx: int\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.PretrainConfig","title":"<code>PretrainConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ExampleGenericConfig['ExampleModel', 'MSELossReduction']</code>, <code>IOMixinWithGettersSetters</code></p> <p>PretrainConfig is a dataclass that is used to configure the model.</p> <p>Timers from ModelParallelConfig are required for megatron forward compatibility.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>@dataclass\nclass PretrainConfig(ExampleGenericConfig[\"ExampleModel\", \"MSELossReduction\"], iom.IOMixinWithGettersSetters):\n    \"\"\"PretrainConfig is a dataclass that is used to configure the model.\n\n    Timers from ModelParallelConfig are required for megatron forward compatibility.\n    \"\"\"\n\n    model_cls: Type[ExampleModel] = ExampleModel\n    loss_cls: Type[MSELossReduction] = MSELossReduction\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.SameSizeLossDict","title":"<code>SameSizeLossDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>This is the return type for a loss that is computed for the entire batch, where all microbatches are the same size.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class SameSizeLossDict(TypedDict):\n    \"\"\"This is the return type for a loss that is computed for the entire batch, where all microbatches are the same size.\"\"\"\n\n    avg: Tensor\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/training_scripts/finetune_mnist/","title":"Finetune mnist","text":""},{"location":"main/references/API_reference/bionemo/example_model/training_scripts/finetune_mnist/#bionemo.example_model.training_scripts.finetune_mnist.run_finetune","title":"<code>run_finetune(checkpoint_dir, name, directory_name)</code>","text":"<p>Run the finetuning step.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>str</code> <p>The directory with the previous model</p> required <code>name</code> <code>str</code> <p>The experiment name.</p> required <code>directory_name</code> <code>str</code> <p>The directory to write the output</p> required <p>Returns:     str: the path of the trained model.</p> Source code in <code>bionemo/example_model/training_scripts/finetune_mnist.py</code> <pre><code>def run_finetune(checkpoint_dir: str, name: str, directory_name: str):\n    \"\"\"Run the finetuning step.\n\n    Args:\n        checkpoint_dir: The directory with the previous model\n        name: The experiment name.\n        directory_name: The directory to write the output\n    Returns:\n        str: the path of the trained model.\n    \"\"\"\n    save_dir = Path(directory_name) / \"classifier\"\n    checkpoint_callback = nl_callbacks.ModelCheckpoint(\n        save_last=True,\n        save_on_train_epoch_end=True,\n        monitor=\"val_loss\",\n        always_save_context=True,  # Enables the .nemo file-like checkpointing where all IOMixins are under SerDe\n    )\n\n    nemo_logger2 = NeMoLogger(\n        log_dir=str(save_dir),\n        name=name,\n        tensorboard=TensorBoardLogger(save_dir=save_dir, name=name),\n        ckpt=checkpoint_callback,\n        extra_loggers=[CSVLogger(save_dir / \"logs\", name=name)],\n    )\n\n    lightning_module2 = BionemoLightningModule(\n        config=ExampleFineTuneConfig(\n            initial_ckpt_path=checkpoint_dir,\n            initial_ckpt_skip_keys_with_these_prefixes={\"digit_classifier\"},\n        )\n    )\n\n    strategy = nl.MegatronStrategy(\n        tensor_model_parallel_size=1,\n        pipeline_model_parallel_size=1,\n        ddp=\"megatron\",\n        find_unused_parameters=True,\n        always_save_context=True,\n    )\n\n    trainer = nl.Trainer(\n        accelerator=\"gpu\",\n        devices=1,\n        strategy=strategy,\n        limit_val_batches=5,\n        val_check_interval=5,\n        max_steps=100,\n        max_epochs=10,\n        num_nodes=1,\n        log_every_n_steps=5,\n        plugins=nl.MegatronMixedPrecision(precision=\"bf16-mixed\"),\n    )\n    llm.train(\n        model=lightning_module2,\n        data=data_module,\n        trainer=trainer,\n        log=nemo_logger2,\n        resume=resume.AutoResume(\n            resume_if_exists=True,\n            resume_ignore_no_checkpoint=True,\n        ),\n    )\n    finetune_dir = Path(checkpoint_callback.last_model_path.replace(\".ckpt\", \"\"))\n    return finetune_dir\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/training_scripts/predict_mnist/","title":"Predict mnist","text":""},{"location":"main/references/API_reference/bionemo/example_model/training_scripts/predict_mnist/#bionemo.example_model.training_scripts.predict_mnist.run_predict","title":"<code>run_predict(finetune_dir, test_length)</code>","text":"<p>Run the prediction step.</p> <p>Parameters:</p> Name Type Description Default <code>finetune_dir</code> <code>str</code> <p>The directory with the previous step</p> required <code>test_length</code> <code>int</code> <p>The length of the test step.</p> required <p>Returns:</p> Name Type Description <code>tensor</code> <p>the outputs of the model.</p> Source code in <code>bionemo/example_model/training_scripts/predict_mnist.py</code> <pre><code>def run_predict(finetune_dir: str, test_length: int):\n    \"\"\"Run the prediction step.\n\n    Args:\n        finetune_dir: The directory with the previous step\n        test_length: The length of the test step.\n\n    Returns:\n        tensor: the outputs of the model.\n    \"\"\"\n    strategy = nl.MegatronStrategy(\n        tensor_model_parallel_size=1,\n        pipeline_model_parallel_size=1,\n        ddp=\"megatron\",\n        find_unused_parameters=True,\n        always_save_context=True,\n    )\n\n    test_run_trainer = nl.Trainer(\n        accelerator=\"gpu\",\n        devices=1,\n        strategy=strategy,\n        num_nodes=1,\n        plugins=nl.MegatronMixedPrecision(precision=\"bf16-mixed\"),\n    )\n\n    lightning_module3 = BionemoLightningModule(config=ExampleFineTuneConfig(initial_ckpt_path=finetune_dir))\n    new_data_module = MNISTDataModule(data_dir=str(BIONEMO_CACHE_DIR), batch_size=test_length, output_log=False)\n\n    results = test_run_trainer.predict(lightning_module3, datamodule=new_data_module)\n    return results\n</code></pre>"},{"location":"main/references/API_reference/bionemo/example_model/training_scripts/pretrain_mnist/","title":"Pretrain mnist","text":""},{"location":"main/references/API_reference/bionemo/example_model/training_scripts/pretrain_mnist/#bionemo.example_model.training_scripts.pretrain_mnist.run_pretrain","title":"<code>run_pretrain(name, directory_name)</code>","text":"<p>Run the pretraining step.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The experiment name.</p> required <code>directory_name</code> <code>str</code> <p>The directory to write the output</p> required <p>Returns:     str: the path of the trained model.</p> Source code in <code>bionemo/example_model/training_scripts/pretrain_mnist.py</code> <pre><code>def run_pretrain(name: str, directory_name: str):\n    \"\"\"Run the pretraining step.\n\n    Args:\n        name: The experiment name.\n        directory_name: The directory to write the output\n    Returns:\n        str: the path of the trained model.\n    \"\"\"\n    # Setup the logger train the model\n    save_dir = Path(directory_name) / \"pretrain\"\n\n    nemo_logger = NeMoLogger(\n        log_dir=str(save_dir),\n        name=name,\n        tensorboard=TensorBoardLogger(save_dir=save_dir, name=name),\n        ckpt=checkpoint_callback,\n        extra_loggers=[CSVLogger(save_dir / \"logs\", name=name)],\n    )\n\n    # Set up the training module\n    lightning_module = BionemoLightningModule(config=PretrainConfig())\n    strategy = nl.MegatronStrategy(\n        tensor_model_parallel_size=1,\n        pipeline_model_parallel_size=1,\n        ddp=\"megatron\",\n        find_unused_parameters=True,\n        always_save_context=True,\n    )\n\n    trainer = nl.Trainer(\n        accelerator=\"gpu\",\n        devices=1,\n        strategy=strategy,\n        limit_val_batches=5,\n        val_check_interval=5,\n        max_steps=100,\n        max_epochs=10,\n        num_nodes=1,\n        log_every_n_steps=5,\n        plugins=nl.MegatronMixedPrecision(precision=\"bf16-mixed\"),\n    )\n\n    # This trains the model\n    llm.train(\n        model=lightning_module,\n        data=data_module,\n        trainer=trainer,\n        log=nemo_logger,\n        resume=resume.AutoResume(\n            resume_if_exists=True,  # Looks for the -last checkpoint to continue training.\n            resume_ignore_no_checkpoint=True,  # When false this will throw an error with no existing checkpoint.\n        ),\n    )\n    return Path(checkpoint_callback.last_model_path.replace(\".ckpt\", \"\"))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/api/","title":"Api","text":""},{"location":"main/references/API_reference/bionemo/llm/api/#bionemo.llm.api.BionemoMegatronModel","title":"<code>BionemoMegatronModel</code>","text":"<p>               Bases: <code>MegatronModule</code>, <code>Generic[DataT]</code>, <code>ABC</code></p> <p>Models that use Megatron must be a MegatronModule type.</p> <p>The only major difference is the explicit <code>forward</code> pass method signature that makes this class compatible with bionemo-core's <code>Model</code> structural type.</p> Source code in <code>bionemo/llm/api.py</code> <pre><code>class BionemoMegatronModel(MegatronModule, Generic[DataT], ABC):\n    \"\"\"Models that use Megatron must be a MegatronModule type.\n\n    The only major difference is the explicit `forward` pass method signature that makes this class compatible\n    with bionemo-core's `Model` structural type.\n    \"\"\"\n\n    @abstractmethod\n    def forward(self, *args, **kwargs) -&gt; DataT:  # noqa: D102\n        raise NotImplementedError()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/lightning/","title":"Lightning","text":""},{"location":"main/references/API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.DataStep","title":"<code>DataStep = Callable[[Iterator[DataT]], DataT]</code>  <code>module-attribute</code>","text":"<p>Batches together an iterator of individual examples.</p> <p>Necessary for compatability with Megatron. This function type is similiar to the collate function of PyTorch.</p> <p>A <code>DataStep</code> function takes an iterator over individual examples. Each example may be a tensor, sequence of tensors, or a set of named tensors (provided as a <code>dict</code> mapping <code>str</code> names to each <code>Tensor</code>). Each iteration must yield the same type.</p> <p>The output of this function will mirror the same structure of each yielded example. It will be a concatenation of all of the examples in the iterator.</p>"},{"location":"main/references/API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.ForwardStep","title":"<code>ForwardStep = Callable[[MegatronModelType, DataT], DataT]</code>  <code>module-attribute</code>","text":"<p>Megatron-compatible forward pass function.</p>"},{"location":"main/references/API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule","title":"<code>BionemoLightningModule</code>","text":"<p>               Bases: <code>Generic[MegatronModelType, MegatronLossType]</code>, <code>LightningModule</code>, <code>IOMixin</code>, <code>ConnectorMixin</code>, <code>LightningPassthroughPredictionMixin</code></p> <p>Reusable PyTorch Lightning module for Megatron models that is compatible with NeMo's conventions.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>class BionemoLightningModule(\n    Generic[MegatronModelType, MegatronLossType],\n    pl.LightningModule,\n    nlio.IOMixin,\n    nlio.ConnectorMixin,\n    LightningPassthroughPredictionMixin,\n):\n    \"\"\"Reusable PyTorch Lightning module for Megatron models that is compatible with NeMo's conventions.\"\"\"\n\n    def __init__(\n        self,\n        config: BionemoTrainableModelConfig[MegatronModelType, MegatronLossType],\n        forward_step: ForwardStep,\n        data_step: DataStep,\n        optimizer: MegatronOptimizerModule,\n        model_transform: Optional[Callable[[MegatronModelType], MegatronModelType]] = None,\n        configure_init_model_parallel: bool = False,\n        **model_construct_args,\n    ) -&gt; None:\n        \"\"\"Constructor.\n\n        Args:\n            config: Serializable configuration object that allows one to construct a new model instance and loss\n                function. Necessary for Megatron-based training as the model itself cannot be serialized and\n                distributed to nodes. Instead, we serialize the procedure for making the model and distribute that.\n            forward_step: Performs forward pass using the model and a batch of data.\n            data_step: Custom batch-creating function for the model.\n            optimizer: Megatron-compatible distributed optimizer instance. Defaults to using ADAM with a 1e-4 learning\n                rate.\n            model_construct_args: Optional. Any arguments necessary to construct the model in the `config`'s\n                `configure_model` method.\n            model_transform: Optional. The model transform function.\n            configure_init_model_parallel: Optional. Whether to initialize the model parallel at configuration time.\n            **model_construct_args: Optional. Arguments necessary for the supplied model configuration's\n                `configure_model` method, which will make an instance of the model.\n        \"\"\"\n        super().__init__()\n        self.config = config\n        self.module_construct_args: Optional[dict[str, Any]] = model_construct_args\n        # ***must** be set up in configure_model() -- megatron constraint\n        # also, must be called `module`: nemo expects the actual model to be stored this way\n        self.module: Optional[MegatronModelType] = None\n        self.loss_reduction_class: type[MegatronLossType] = config.get_loss_reduction_class()\n        self.optim = optimizer\n        self.optim.connect(self)  # This will bind the `configure_optimizers` method\n        self._data_step = data_step\n        self._forward_step = forward_step\n        self.model_transform = model_transform\n        self.configure_init_model_parallel = configure_init_model_parallel\n        # configure metrics\n        self.train_metric = self.config.train_metric.get_instance() if self.config.train_metric else None\n        self.valid_metric = self.config.valid_metric.get_instance() if self.config.valid_metric else None\n\n    def configure_model(self) -&gt; None:\n        \"\"\"Updates internal state: instantiates the model from the object's config, assigns to `model` attribute.\n\n        NOTE: this method is idempotent; successive calls have no effect. The model is only initialized once.\n\n        Raises:\n            ValueError iff the internal config's configure_model method returns None.\n        \"\"\"\n        if self.configure_init_model_parallel:\n            self.trainer.strategy._init_model_parallel = True\n        if self.module is None:\n            if self.module_construct_args is None:\n                module_construct_args = {}\n            elif \"model_construct_args\" in self.module_construct_args:\n                # Not sure why this is needed, but it seems \"model_construct_args\" ends up as a key inside this dict.\n                module_construct_args = self.module_construct_args[\"model_construct_args\"]\n            else:\n                module_construct_args = self.module_construct_args\n\n            model: MegatronModelType = self.config.configure_model(**module_construct_args)\n            self.module = model\n        if self.module is None:\n            raise ValueError(\"Invalid semantics: configure_model method **MUST** initialize the model.\")\n\n    def is_on_logging_device(self):\n        \"\"\"Return True if last stage of pipeline parallel and first tensor parallel rank.\"\"\"\n        return parallel_state.is_pipeline_last_stage() and parallel_state.get_tensor_model_parallel_rank() == 0\n\n    def forward(self, *args, **kwargs) -&gt; DataT:\n        \"\"\"Call the forward method of the underlying model, and return whatever it outputs.\"\"\"\n        # safe to do because configure_model is idempotent\n        self.configure_model()\n        assert self.module is not None, \"ERROR: configure_model() method has been incorrectly overridden!\"\n        prediction = self.module(*args, **kwargs)  # for now just pass through to the underlying model\n        return prediction\n\n    def data_step(self, dataloader_iter: Iterator[DataT]) -&gt; DataT:  # noqa: D102\n        return self._data_step(dataloader_iter)\n\n    def forward_step(self, batch) -&gt; Tensor:\n        \"\"\"Megatron-required: the training forward step for the model, which is required to produce the loss.\n\n        Normally, the forward pass of a model means its inference. Loss is computed using the predictions\n        from the forward pass against labels. Megatron unfortunately conflates these two different concepts\n        and instead has models \"forward\" method produce the loss. See the Megatron docs for details:\n        https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/pipeline_parallel/schedules.py#L170\n\n        To get actual predictions, use the :func:`forward` method instead.\n        \"\"\"\n        # safe to do because configure_model is idempotent\n        self.configure_model()\n        assert self.module is not None\n        return self._forward_step(self.module, batch)\n\n    def update_metric(\n        self, batch, outputs, metric, task: Literal[\"pretraining\", \"classification\", \"regression\"]\n    ) -&gt; None:\n        \"\"\"Update metric for logging.\"\"\"\n        match task:\n            case \"pretraining\":\n                logits = outputs[\"token_logits\"].detach().transpose(0, 1)  #  [s, b, v] -&gt; [b, s, v]\n                metric(logits, batch[\"labels\"])\n            case \"classification\":\n                classification_output = outputs[\"classification_output\"]\n                num_classes = classification_output.shape[-1]\n                labels = batch[\"labels\"]\n                if classification_output.ndim == 3:  # token-level classification\n                    classification_output = classification_output.reshape(-1, num_classes)[\n                        batch[\"loss_mask\"].view(-1)\n                    ]  # shape [-1, num_classes]\n                    assert classification_output.ndim == 2\n\n                    labels = batch[\"labels\"].reshape(-1)[batch[\"loss_mask\"].view(-1)]\n                metric(\n                    classification_output.reshape(-1, num_classes),\n                    labels.reshape(-1),\n                )\n            case \"regression\":\n                regression_output = outputs[\"regression_output\"]\n                metric(regression_output, batch[\"labels\"])\n            case _:\n                raise NotImplementedError(f\"unrecognized task {task}\")\n\n    def training_step(self, batch, batch_idx: Optional[int] = None) -&gt; Tensor:\n        \"\"\"In mcore the loss-function is part of the forward-pass when labels are provided.\"\"\"\n        outputs = self.forward_step(batch)\n        if self.train_metric is not None:\n            if self.is_on_logging_device():\n                self.update_metric(batch, outputs, self.train_metric, self.config.train_metric.task)\n\n            self.log(\n                self.config.train_metric.metric_name,\n                self.train_metric,\n                on_step=True,\n                on_epoch=False,\n                prog_bar=True,\n            )\n\n        return outputs\n\n    def validation_step(self, batch, batch_idx: Optional[int] = None) -&gt; Tensor:\n        \"\"\"In mcore the loss-function is part of the forward-pass when labels are provided.\"\"\"\n        outputs = self.forward_step(batch)\n        if self.valid_metric is not None and self.is_on_logging_device():\n            self.update_metric(batch, outputs, self.valid_metric, self.config.valid_metric.task)\n\n        return outputs\n\n    def predict_step(self, batch, batch_idx: Optional[int] = None) -&gt; Tensor:\n        \"\"\"Alias for forward_step.\"\"\"\n        if len(batch) == 0:\n            return\n        return self.forward_step(batch)\n\n    def training_loss_reduction(self) -&gt; MegatronLossType:\n        \"\"\"This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.\"\"\"\n        return self.loss_reduction_class()\n\n    def validation_loss_reduction(self) -&gt; MegatronLossType:  # noqa: D102\n        return self.loss_reduction_class(validation_step=True)\n\n    def test_loss_reduction(self) -&gt; MegatronLossType:  # noqa: D102\n        return self.loss_reduction_class(validation_step=True)\n\n    def on_validation_epoch_end(self):  # noqa: D102\n        if self.valid_metric is None:\n            return\n\n        if self.trainer.sanity_checking:\n            self.valid_metric.reset()  # clean up sanity runs\n            return\n\n        self.log(\n            self.config.valid_metric.metric_name,\n            self.valid_metric,\n            on_step=False,\n            on_epoch=True,\n            prog_bar=True,\n        )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.__init__","title":"<code>__init__(config, forward_step, data_step, optimizer, model_transform=None, configure_init_model_parallel=False, **model_construct_args)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>BionemoTrainableModelConfig[MegatronModelType, MegatronLossType]</code> <p>Serializable configuration object that allows one to construct a new model instance and loss function. Necessary for Megatron-based training as the model itself cannot be serialized and distributed to nodes. Instead, we serialize the procedure for making the model and distribute that.</p> required <code>forward_step</code> <code>ForwardStep</code> <p>Performs forward pass using the model and a batch of data.</p> required <code>data_step</code> <code>DataStep</code> <p>Custom batch-creating function for the model.</p> required <code>optimizer</code> <code>MegatronOptimizerModule</code> <p>Megatron-compatible distributed optimizer instance. Defaults to using ADAM with a 1e-4 learning rate.</p> required <code>model_construct_args</code> <p>Optional. Any arguments necessary to construct the model in the <code>config</code>'s <code>configure_model</code> method.</p> <code>{}</code> <code>model_transform</code> <code>Optional[Callable[[MegatronModelType], MegatronModelType]]</code> <p>Optional. The model transform function.</p> <code>None</code> <code>configure_init_model_parallel</code> <code>bool</code> <p>Optional. Whether to initialize the model parallel at configuration time.</p> <code>False</code> <code>**model_construct_args</code> <p>Optional. Arguments necessary for the supplied model configuration's <code>configure_model</code> method, which will make an instance of the model.</p> <code>{}</code> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def __init__(\n    self,\n    config: BionemoTrainableModelConfig[MegatronModelType, MegatronLossType],\n    forward_step: ForwardStep,\n    data_step: DataStep,\n    optimizer: MegatronOptimizerModule,\n    model_transform: Optional[Callable[[MegatronModelType], MegatronModelType]] = None,\n    configure_init_model_parallel: bool = False,\n    **model_construct_args,\n) -&gt; None:\n    \"\"\"Constructor.\n\n    Args:\n        config: Serializable configuration object that allows one to construct a new model instance and loss\n            function. Necessary for Megatron-based training as the model itself cannot be serialized and\n            distributed to nodes. Instead, we serialize the procedure for making the model and distribute that.\n        forward_step: Performs forward pass using the model and a batch of data.\n        data_step: Custom batch-creating function for the model.\n        optimizer: Megatron-compatible distributed optimizer instance. Defaults to using ADAM with a 1e-4 learning\n            rate.\n        model_construct_args: Optional. Any arguments necessary to construct the model in the `config`'s\n            `configure_model` method.\n        model_transform: Optional. The model transform function.\n        configure_init_model_parallel: Optional. Whether to initialize the model parallel at configuration time.\n        **model_construct_args: Optional. Arguments necessary for the supplied model configuration's\n            `configure_model` method, which will make an instance of the model.\n    \"\"\"\n    super().__init__()\n    self.config = config\n    self.module_construct_args: Optional[dict[str, Any]] = model_construct_args\n    # ***must** be set up in configure_model() -- megatron constraint\n    # also, must be called `module`: nemo expects the actual model to be stored this way\n    self.module: Optional[MegatronModelType] = None\n    self.loss_reduction_class: type[MegatronLossType] = config.get_loss_reduction_class()\n    self.optim = optimizer\n    self.optim.connect(self)  # This will bind the `configure_optimizers` method\n    self._data_step = data_step\n    self._forward_step = forward_step\n    self.model_transform = model_transform\n    self.configure_init_model_parallel = configure_init_model_parallel\n    # configure metrics\n    self.train_metric = self.config.train_metric.get_instance() if self.config.train_metric else None\n    self.valid_metric = self.config.valid_metric.get_instance() if self.config.valid_metric else None\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.configure_model","title":"<code>configure_model()</code>","text":"<p>Updates internal state: instantiates the model from the object's config, assigns to <code>model</code> attribute.</p> <p>NOTE: this method is idempotent; successive calls have no effect. The model is only initialized once.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def configure_model(self) -&gt; None:\n    \"\"\"Updates internal state: instantiates the model from the object's config, assigns to `model` attribute.\n\n    NOTE: this method is idempotent; successive calls have no effect. The model is only initialized once.\n\n    Raises:\n        ValueError iff the internal config's configure_model method returns None.\n    \"\"\"\n    if self.configure_init_model_parallel:\n        self.trainer.strategy._init_model_parallel = True\n    if self.module is None:\n        if self.module_construct_args is None:\n            module_construct_args = {}\n        elif \"model_construct_args\" in self.module_construct_args:\n            # Not sure why this is needed, but it seems \"model_construct_args\" ends up as a key inside this dict.\n            module_construct_args = self.module_construct_args[\"model_construct_args\"]\n        else:\n            module_construct_args = self.module_construct_args\n\n        model: MegatronModelType = self.config.configure_model(**module_construct_args)\n        self.module = model\n    if self.module is None:\n        raise ValueError(\"Invalid semantics: configure_model method **MUST** initialize the model.\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Call the forward method of the underlying model, and return whatever it outputs.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def forward(self, *args, **kwargs) -&gt; DataT:\n    \"\"\"Call the forward method of the underlying model, and return whatever it outputs.\"\"\"\n    # safe to do because configure_model is idempotent\n    self.configure_model()\n    assert self.module is not None, \"ERROR: configure_model() method has been incorrectly overridden!\"\n    prediction = self.module(*args, **kwargs)  # for now just pass through to the underlying model\n    return prediction\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.forward_step","title":"<code>forward_step(batch)</code>","text":"<p>Megatron-required: the training forward step for the model, which is required to produce the loss.</p> <p>Normally, the forward pass of a model means its inference. Loss is computed using the predictions from the forward pass against labels. Megatron unfortunately conflates these two different concepts and instead has models \"forward\" method produce the loss. See the Megatron docs for details: https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/pipeline_parallel/schedules.py#L170</p> <p>To get actual predictions, use the :func:<code>forward</code> method instead.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def forward_step(self, batch) -&gt; Tensor:\n    \"\"\"Megatron-required: the training forward step for the model, which is required to produce the loss.\n\n    Normally, the forward pass of a model means its inference. Loss is computed using the predictions\n    from the forward pass against labels. Megatron unfortunately conflates these two different concepts\n    and instead has models \"forward\" method produce the loss. See the Megatron docs for details:\n    https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/pipeline_parallel/schedules.py#L170\n\n    To get actual predictions, use the :func:`forward` method instead.\n    \"\"\"\n    # safe to do because configure_model is idempotent\n    self.configure_model()\n    assert self.module is not None\n    return self._forward_step(self.module, batch)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.is_on_logging_device","title":"<code>is_on_logging_device()</code>","text":"<p>Return True if last stage of pipeline parallel and first tensor parallel rank.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def is_on_logging_device(self):\n    \"\"\"Return True if last stage of pipeline parallel and first tensor parallel rank.\"\"\"\n    return parallel_state.is_pipeline_last_stage() and parallel_state.get_tensor_model_parallel_rank() == 0\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.predict_step","title":"<code>predict_step(batch, batch_idx=None)</code>","text":"<p>Alias for forward_step.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def predict_step(self, batch, batch_idx: Optional[int] = None) -&gt; Tensor:\n    \"\"\"Alias for forward_step.\"\"\"\n    if len(batch) == 0:\n        return\n    return self.forward_step(batch)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.training_loss_reduction","title":"<code>training_loss_reduction()</code>","text":"<p>This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def training_loss_reduction(self) -&gt; MegatronLossType:\n    \"\"\"This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.\"\"\"\n    return self.loss_reduction_class()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.training_step","title":"<code>training_step(batch, batch_idx=None)</code>","text":"<p>In mcore the loss-function is part of the forward-pass when labels are provided.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def training_step(self, batch, batch_idx: Optional[int] = None) -&gt; Tensor:\n    \"\"\"In mcore the loss-function is part of the forward-pass when labels are provided.\"\"\"\n    outputs = self.forward_step(batch)\n    if self.train_metric is not None:\n        if self.is_on_logging_device():\n            self.update_metric(batch, outputs, self.train_metric, self.config.train_metric.task)\n\n        self.log(\n            self.config.train_metric.metric_name,\n            self.train_metric,\n            on_step=True,\n            on_epoch=False,\n            prog_bar=True,\n        )\n\n    return outputs\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.update_metric","title":"<code>update_metric(batch, outputs, metric, task)</code>","text":"<p>Update metric for logging.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def update_metric(\n    self, batch, outputs, metric, task: Literal[\"pretraining\", \"classification\", \"regression\"]\n) -&gt; None:\n    \"\"\"Update metric for logging.\"\"\"\n    match task:\n        case \"pretraining\":\n            logits = outputs[\"token_logits\"].detach().transpose(0, 1)  #  [s, b, v] -&gt; [b, s, v]\n            metric(logits, batch[\"labels\"])\n        case \"classification\":\n            classification_output = outputs[\"classification_output\"]\n            num_classes = classification_output.shape[-1]\n            labels = batch[\"labels\"]\n            if classification_output.ndim == 3:  # token-level classification\n                classification_output = classification_output.reshape(-1, num_classes)[\n                    batch[\"loss_mask\"].view(-1)\n                ]  # shape [-1, num_classes]\n                assert classification_output.ndim == 2\n\n                labels = batch[\"labels\"].reshape(-1)[batch[\"loss_mask\"].view(-1)]\n            metric(\n                classification_output.reshape(-1, num_classes),\n                labels.reshape(-1),\n            )\n        case \"regression\":\n            regression_output = outputs[\"regression_output\"]\n            metric(regression_output, batch[\"labels\"])\n        case _:\n            raise NotImplementedError(f\"unrecognized task {task}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.validation_step","title":"<code>validation_step(batch, batch_idx=None)</code>","text":"<p>In mcore the loss-function is part of the forward-pass when labels are provided.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def validation_step(self, batch, batch_idx: Optional[int] = None) -&gt; Tensor:\n    \"\"\"In mcore the loss-function is part of the forward-pass when labels are provided.\"\"\"\n    outputs = self.forward_step(batch)\n    if self.valid_metric is not None and self.is_on_logging_device():\n        self.update_metric(batch, outputs, self.valid_metric, self.config.valid_metric.task)\n\n    return outputs\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.LightningPassthroughPredictionMixin","title":"<code>LightningPassthroughPredictionMixin</code>","text":"<p>A mixin that allows your model to do inference on the predict step by hijacking nemo's loss reduction mechanism.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>class LightningPassthroughPredictionMixin:\n    \"\"\"A mixin that allows your model to do inference on the predict step by hijacking nemo's loss reduction mechanism.\"\"\"\n\n    def predict_loss_reduction(self) -&gt; PassthroughLossReduction:\n        \"\"\"For the predict step, pass through the forward pass output.\"\"\"\n        return PassthroughLossReduction()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.LightningPassthroughPredictionMixin.predict_loss_reduction","title":"<code>predict_loss_reduction()</code>","text":"<p>For the predict step, pass through the forward pass output.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def predict_loss_reduction(self) -&gt; PassthroughLossReduction:\n    \"\"\"For the predict step, pass through the forward pass output.\"\"\"\n    return PassthroughLossReduction()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.PassthroughLossReduction","title":"<code>PassthroughLossReduction</code>","text":"<p>               Bases: <code>MegatronLossReduction</code>, <code>Generic[DataT]</code></p> <p>A workaround for nemo/megatron to perform inference.</p> <p>Internally in NeMo2.0 the forward step is always expected to return a loss reduction class, and forward is expected to return a loss. This class hijacks that mechanism to instead pass through the forward output unperturbed as the loss (to enable inference in the predict step), and then the reduce method is used to collate the batch of forward outputs into a single batch. This supports the model forward output being a tensor, dict, tuple, or list of tensors. The inner type must always be a Tensor.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>class PassthroughLossReduction(MegatronLossReduction, Generic[DataT]):\n    \"\"\"A workaround for nemo/megatron to perform inference.\n\n    Internally in NeMo2.0 the forward step is always expected to return a loss reduction class, and forward is\n    expected to return a loss. This class hijacks that mechanism to instead pass through the forward output unperturbed\n    as the loss (to enable inference in the predict step), and then the reduce method is used to collate the batch of\n    forward outputs into a single batch. This supports the model forward output being a tensor, dict, tuple, or list of\n    tensors. The inner type _must always be a Tensor_.\n    \"\"\"\n\n    def forward(self, batch: DataT, forward_out: DataT) -&gt; Tuple[Tensor, DataT]:\n        \"\"\"Passes through the `forward_out` value as the 2nd tuple element.\n\n        Args:\n            batch: The batch of data that was passed through the model to generate output. NOTE: this value is ignored.\n            forward_out: The output from your model's forward pass.\n\n        Returns:\n            A tuple containing the loss tensor (dummy in this case) and the forward output (unmodified).\n        \"\"\"\n        return torch.zeros((1, 1)), forward_out\n\n    def reduce(self, forward_out: List[DataT]) -&gt; DataT:\n        \"\"\"Collates list of model's outputs into a single output.\"\"\"\n        return batch_collator(forward_out)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.PassthroughLossReduction.forward","title":"<code>forward(batch, forward_out)</code>","text":"<p>Passes through the <code>forward_out</code> value as the 2nd tuple element.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>DataT</code> <p>The batch of data that was passed through the model to generate output. NOTE: this value is ignored.</p> required <code>forward_out</code> <code>DataT</code> <p>The output from your model's forward pass.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, DataT]</code> <p>A tuple containing the loss tensor (dummy in this case) and the forward output (unmodified).</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def forward(self, batch: DataT, forward_out: DataT) -&gt; Tuple[Tensor, DataT]:\n    \"\"\"Passes through the `forward_out` value as the 2nd tuple element.\n\n    Args:\n        batch: The batch of data that was passed through the model to generate output. NOTE: this value is ignored.\n        forward_out: The output from your model's forward pass.\n\n    Returns:\n        A tuple containing the loss tensor (dummy in this case) and the forward output (unmodified).\n    \"\"\"\n    return torch.zeros((1, 1)), forward_out\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.PassthroughLossReduction.reduce","title":"<code>reduce(forward_out)</code>","text":"<p>Collates list of model's outputs into a single output.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def reduce(self, forward_out: List[DataT]) -&gt; DataT:\n    \"\"\"Collates list of model's outputs into a single output.\"\"\"\n    return batch_collator(forward_out)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.batch_collator","title":"<code>batch_collator(batches, batch_dim=0, seq_dim=1, batch_dim_key_defaults={'token_logits': 1}, seq_dim_key_defaults={'token_logits': 0}, preferred_gpu=0)</code>","text":"<p>Takes a sequence of batches and collates them into a single batch.</p> <pre><code>This is distinct from the standard pytorch default_collator since it does\nnot add the batch dimension, it's assumed the batch\ndimension is already present in the input, as would be the case when\nparallelizing across minibatches.\n</code></pre> <p>IMPORTANT: The underlying data primitive must be a torch Tensor. The input to this function is a recurisve type, there can be any amount of nesting between dictionaries, tuples, and lists, as long as the inner type is a n-d Tensor.</p> <p>Examples:</p> <p>Outer container = Dict:     [{'a': Tensor([1]), 'b': Tensor([2])}, {'a': Tensor([2]), 'b': Tensor([3])}] -&gt; {'a': Tensor([1, 2]), 'b': Tensor([2, 3])} Outer container = List:     [[Tensor([1]), Tensor([2])], [Tensor([2]), Tensor([3])]] -&gt; [Tensor([1, 2]), Tensor([2, 3])] Outer container = Tuple:     ([Tensor([1]), Tensor([2])], [Tensor([2]), Tensor([3])]) -&gt; (Tensor([1, 2]), Tensor([2, 3]))</p> <p>Parameters:</p> Name Type Description Default <code>batches</code> <code>Optional[Sequence[ReductionT]]</code> <p>sequence of batches to collate into a single batch.</p> required <code>batch_dim</code> <code>int</code> <p>If you know that the batch dim for the batch you are concatenating is not the 0th dimension (for example it is sequence first) then supply that dimension.</p> <code>0</code> <code>seq_dim</code> <code>int</code> <p>If you know that the sequence dim for the batch you are concatenating is not the 1st dimension (for example it is sequence first) then supply that dimension. This is used for padding to the max length.</p> <code>1</code> <code>batch_dim_key_defaults</code> <code>dictionary of keys to integers</code> <p>If your batch is a dictionary and you know that some keys have non-standard (0) batch dimensions, supply those here. By default \"token_logits\" has batch dim 1 and otherwise all keys are assumed to have batch dim 0.</p> <code>{'token_logits': 1}</code> <code>seq_dim_key_defaults</code> <code>dictionary of keys to integers</code> <p>If your batch is a dictionary and you know that some keys have non-standard (1) sequence dimensions, supply those here. By default \"token_logits\" has seq dim 0 and otherwise all keys are assumed to have seq dim 1.</p> <code>{'token_logits': 0}</code> <code>preferred_gpu</code> <code>int</code> <p>If any of the tensors are on any GPU, all of them will be moved to this GPU. 0 by default.</p> <code>0</code> <p>Returns:</p> Type Description <code>Optional[ReductionT]</code> <p>A single batch of the same type as the elements of your input sequence.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def batch_collator(\n    batches: Optional[Union[Tuple[ReductionT], List[ReductionT]]],\n    batch_dim: int = 0,\n    seq_dim: int = 1,\n    batch_dim_key_defaults: dict[str, int] = {\"token_logits\": 1},\n    seq_dim_key_defaults: dict[str, int] = {\"token_logits\": 0},\n    preferred_gpu: int = 0,\n) -&gt; Optional[ReductionT]:\n    \"\"\"Takes a sequence of batches and collates them into a single batch.\n\n        This is distinct from the standard pytorch default_collator since it does\n        not add the batch dimension, it's assumed the batch\n        dimension is already present in the input, as would be the case when\n        parallelizing across minibatches.\n\n    IMPORTANT: The underlying data primitive _must_ be a torch Tensor. The input to this function is a recurisve type,\n    there can be any amount of nesting between dictionaries, tuples, and lists, as long as the inner type is a n-d Tensor.\n\n    Examples:\n        Outer container = Dict:\n            [{'a': Tensor([1]), 'b': Tensor([2])}, {'a': Tensor([2]), 'b': Tensor([3])}] -&gt; {'a': Tensor([1, 2]), 'b': Tensor([2, 3])}\n        Outer container = List:\n            [[Tensor([1]), Tensor([2])], [Tensor([2]), Tensor([3])]] -&gt; [Tensor([1, 2]), Tensor([2, 3])]\n        Outer container = Tuple:\n            ([Tensor([1]), Tensor([2])], [Tensor([2]), Tensor([3])]) -&gt; (Tensor([1, 2]), Tensor([2, 3]))\n\n    Args:\n        batches (Optional[Sequence[ReductionT]]): sequence of batches to collate into a single batch.\n        batch_dim: If you know that the batch dim for the batch you are concatenating is not the 0th dimension (for\n            example it is sequence first) then supply that dimension.\n        seq_dim: If you know that the sequence dim for the batch you are concatenating is not the 1st dimension (for\n            example it is sequence first) then supply that dimension. This is used for padding to the max length.\n        batch_dim_key_defaults (dictionary of keys to integers): If your batch is a dictionary and you know that some\n            keys have non-standard (0) batch dimensions, supply those here. By default \"token_logits\" has batch dim 1\n            and otherwise all keys are assumed to have batch dim 0.\n        seq_dim_key_defaults (dictionary of keys to integers): If your batch is a dictionary and you know that some\n            keys have non-standard (1) sequence dimensions, supply those here. By default \"token_logits\" has seq dim 0\n            and otherwise all keys are assumed to have seq dim 1.\n        preferred_gpu: If any of the tensors are on any GPU, all of them will be moved to this GPU. 0 by default.\n\n    Returns:\n        A single batch of the same type as the elements of your input sequence.\n    \"\"\"\n    match batches:\n        # Handle base-cases for batch concatenation, either a list of None or a list of tensors\n        case [None, *_]:\n            return None\n        case [Tensor(), *_]:\n            # If any tensor is on a GPU, move all to preferred GPU\n            if any(t.is_cuda for t in batches):\n                device = torch.device(f\"cuda:{preferred_gpu}\")\n                batches = [t.to(device) for t in batches]\n            # First shortcut if all tensors are 1D (they have at least one batch dim, and it must be at 0)\n            if len(batches) &gt; 0 and isinstance(batches[0], Tensor) and batches[0].ndim == 1:\n                return torch.cat(batches, dim=0)\n            # Find max sequence length across all tensors\n            max_seq_len = max(batch.size(seq_dim) for batch in batches)\n            # Pad each tensor to max length along seq_dim\n            padded_batches = []\n            for batch in batches:\n                # Initialize padding tuple - needs 2 values per dim, starting from last dim\n                # e.g. for 3D tensor: [left_pad_dim2, right_pad_dim2, left_pad_dim1, right_pad_dim1, left_pad_dim0, right_pad_dim0]\n                pad_size = [0] * (2 * batch.ndim)\n                # Calculate padding needed at end of sequence dimension\n                pad_amount = max_seq_len - batch.size(seq_dim)\n                # Pad end of sequence dimension by putting padding amount in correct position\n                # For seq_dim=1 in 3D tensor: [0, 0, 0, pad_amount, 0, 0]\n                pad_size[2 * (batch.ndim - 1 - seq_dim) + 1] = pad_amount\n                padded_batch = torch.nn.functional.pad(batch, tuple(pad_size))\n                padded_batches.append(padded_batch)\n            padded_batch = torch.cat(padded_batches, dim=batch_dim)\n            assert padded_batch.size(seq_dim) == max_seq_len\n            return padded_batch\n        # Next 3 calls are the recursive calls into the sub-structures of the batch. We handle dictionaries, tuples, and lists\n        case [dict(), *_]:\n            return {\n                key: batch_collator(\n                    [batch[key] for batch in batches],\n                    batch_dim=batch_dim_key_defaults.get(key, batch_dim),\n                    seq_dim=seq_dim_key_defaults.get(key, seq_dim),\n                    batch_dim_key_defaults=batch_dim_key_defaults,\n                    seq_dim_key_defaults=seq_dim_key_defaults,\n                    preferred_gpu=preferred_gpu,\n                )\n                for key in batches[0]\n            }\n        case [tuple(), *_]:\n            return tuple(\n                batch_collator(\n                    [batch[i] for batch in batches],\n                    batch_dim=batch_dim,\n                    seq_dim=seq_dim,\n                    batch_dim_key_defaults=batch_dim_key_defaults,\n                    seq_dim_key_defaults=seq_dim_key_defaults,\n                    preferred_gpu=preferred_gpu,\n                )\n                for i in range(len(batches[0]))\n            )\n        case [list(), *_]:\n            return [\n                batch_collator(\n                    [batch[i] for batch in batches],\n                    batch_dim=batch_dim,\n                    seq_dim=seq_dim,\n                    batch_dim_key_defaults=batch_dim_key_defaults,\n                    seq_dim_key_defaults=seq_dim_key_defaults,\n                    preferred_gpu=preferred_gpu,\n                )\n                for i in range(len(batches[0]))\n            ]\n        # Final cases shouldn't happen, an empty sequence (no batches), or \"other\".\n        case []:\n            raise ValueError(\"Cannot process an empty sequence\")\n        case _:\n            raise ValueError(\"Unsupported input structure in batch_collator\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.default_megatron_optimizer","title":"<code>default_megatron_optimizer()</code>","text":"<p>Default distributed optimizer uses Adam with a 1e-4 learning rate.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def default_megatron_optimizer() -&gt; MegatronOptimizerModule:\n    \"\"\"Default distributed optimizer uses Adam with a 1e-4 learning rate.\"\"\"\n    return MegatronOptimizerModule(\n        config=OptimizerConfig(lr=1e-4, optimizer=\"adam\", use_distributed_optimizer=True),\n    )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.some_first","title":"<code>some_first(seq)</code>","text":"<p>Returns the first non-None value from the sequence or fails</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def some_first(seq: Iterable[Optional[T]]) -&gt; T:\n    \"\"\"Returns the first non-None value from the sequence or fails\"\"\"  # noqa: D415\n    for s in seq:\n        if s is not None:\n            return s\n    raise ValueError(\"non-None value not found\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/train/","title":"Train","text":""},{"location":"main/references/API_reference/bionemo/llm/train/#bionemo.llm.train.NsysConfig","title":"<code>NsysConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for nsys profiling.</p> Source code in <code>bionemo/llm/train.py</code> <pre><code>class NsysConfig(BaseModel):\n    \"\"\"Configuration for nsys profiling.\"\"\"\n\n    start_step: int = 0\n    end_step: Optional[int] = None\n    ranks: list[int] = field(default_factory=lambda: [0])\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/train/#bionemo.llm.train.nemo_logger_factory","title":"<code>nemo_logger_factory(experiment_config, wandb_config)</code>","text":"<p>Creates and returns a NeMoLogger instance configured based on the provided experiment and wandb configurations.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_config</code> <code>ExperimentConfig</code> <p>Configuration object containing experiment settings such as result directory, experiment name, checkpoint settings, and logger preferences.</p> required <code>wandb_config</code> <code>Optional[WandbConfig]</code> <p>Optional configuration object for Weights and Biases logging.</p> required <p>Returns:</p> Type Description <code>NeMoLogger</code> <p>nl.NeMoLogger: An instance of NeMoLogger configured with the specified settings.</p> Source code in <code>bionemo/llm/train.py</code> <pre><code>def nemo_logger_factory(experiment_config: ExperimentConfig, wandb_config: Optional[WandbConfig]) -&gt; nl.NeMoLogger:\n    \"\"\"Creates and returns a NeMoLogger instance configured based on the provided experiment and wandb configurations.\n\n    Args:\n        experiment_config (ExperimentConfig): Configuration object containing experiment settings such as\n            result directory, experiment name, checkpoint settings, and logger preferences.\n        wandb_config (Optional[WandbConfig]): Optional configuration object for Weights and Biases logging.\n\n    Returns:\n        nl.NeMoLogger: An instance of NeMoLogger configured with the specified settings.\n    \"\"\"\n    if experiment_config.create_checkpoint_callback:\n        checkpoint_callback = nl_callbacks.ModelCheckpoint(\n            save_last=experiment_config.save_last_checkpoint,\n            monitor=experiment_config.metric_to_monitor_for_checkpoints,\n            save_top_k=experiment_config.save_top_k,\n            every_n_train_steps=experiment_config.save_every_n_steps,\n            always_save_context=True,\n            filename=\"{epoch}-{val_loss:.2f}-{step}-{consumed_samples}\",  # Including step and consumed_samples in the checkpoint filename prevents duplicate filenames and bugs related to this.\n        )\n    else:\n        checkpoint_callback = None\n\n    nemo_logger = setup_nemo_lightning_logger(\n        root_dir=experiment_config.result_dir,\n        name=experiment_config.experiment_name,\n        initialize_tensorboard_logger=experiment_config.create_tensorboard_logger,\n        wandb_config=wandb_config,\n        ckpt_callback=checkpoint_callback,\n    )\n    return nemo_logger\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/train/#bionemo.llm.train.setup_trainer","title":"<code>setup_trainer(parallel_config, training_config, callbacks=None, nsys_config=None)</code>","text":"<p>Set up the trainer for model training using the specified parallel and training configurations.</p> <p>Parameters:</p> Name Type Description Default <code>parallel_config</code> <code>ParallelConfig</code> <p>Configuration for parallelism, including tensor and pipeline model parallel sizes,                               number of devices, and number of nodes.</p> required <code>training_config</code> <code>TrainingConfig</code> <p>Configuration for training, including maximum steps, accelerator type,                               validation batch limit, validation check interval, and precision.</p> required <code>callbacks</code> <code>list</code> <p>List of callback functions to be used during training. Defaults to None,                         in which case default callbacks (RichModelSummary and LearningRateMonitor) are used.</p> <code>None</code> <code>nsys_config</code> <code>NsysConfig</code> <p>Configuration for nsys profiling. If None, is disabled.</p> <code>None</code> <p>Returns:</p> Type Description <code>Trainer</code> <p>nl.Trainer: Configured trainer object ready for model training.</p> Source code in <code>bionemo/llm/train.py</code> <pre><code>def setup_trainer(\n    parallel_config: ParallelConfig,\n    training_config: TrainingConfig,\n    callbacks=None,\n    nsys_config: NsysConfig | None = None,\n) -&gt; nl.Trainer:\n    \"\"\"Set up the trainer for model training using the specified parallel and training configurations.\n\n    Args:\n        parallel_config (ParallelConfig): Configuration for parallelism, including tensor and pipeline model parallel sizes,\n                                          number of devices, and number of nodes.\n        training_config (TrainingConfig): Configuration for training, including maximum steps, accelerator type,\n                                          validation batch limit, validation check interval, and precision.\n        callbacks (list, optional): List of callback functions to be used during training. Defaults to None,\n                                    in which case default callbacks (RichModelSummary and LearningRateMonitor) are used.\n        nsys_config (NsysConfig, optional): Configuration for nsys profiling. If None, is disabled.\n\n    Returns:\n        nl.Trainer: Configured trainer object ready for model training.\n    \"\"\"\n    strategy = nl.MegatronStrategy(\n        tensor_model_parallel_size=parallel_config.tensor_model_parallel_size,\n        pipeline_model_parallel_size=parallel_config.pipeline_model_parallel_size,\n        pipeline_dtype=get_autocast_dtype(training_config.precision),\n        ddp=DistributedDataParallelConfig(\n            check_for_nan_in_grad=True,\n            overlap_grad_reduce=True,\n            overlap_param_gather=False,  # TODO waiting for NeMo fix\n            average_in_collective=True,\n            use_distributed_optimizer=parallel_config.use_distributed_optimizer,\n        ),\n        find_unused_parameters=True,\n        gradient_as_bucket_view=True,\n        ckpt_include_optimizer=True,\n        ckpt_async_save=True,\n        ckpt_parallel_load=True,\n    )\n    if callbacks is None:\n        callbacks = [\n            RichModelSummary(max_depth=4),\n            LearningRateMonitor(),\n        ]\n\n    if training_config.gc_interval &gt; 0:\n        callbacks.append(\n            nl_callbacks.GarbageCollectionCallback(\n                gc_interval_train=training_config.gc_interval, gc_interval_val=training_config.gc_interval\n            )\n        )\n\n    if nsys_config:\n        if nsys_config.end_step is None:\n            nsys_config.end_step = training_config.max_steps\n        callbacks.append(\n            nl_callbacks.NsysCallback(\n                start_step=nsys_config.start_step,\n                end_step=nsys_config.end_step,\n                ranks=nsys_config.ranks,\n                gen_shape=True,\n            )\n        )\n\n    trainer = nl.Trainer(\n        devices=parallel_config.num_devices,\n        max_steps=training_config.max_steps,\n        accelerator=training_config.accelerator,\n        strategy=strategy,\n        limit_val_batches=training_config.limit_val_batches,\n        val_check_interval=training_config.val_check_interval,\n        num_nodes=parallel_config.num_nodes,\n        callbacks=callbacks,\n        plugins=nl.MegatronMixedPrecision(\n            precision=training_config.precision,\n            params_dtype=get_autocast_dtype(training_config.precision),\n            pipeline_dtype=get_autocast_dtype(training_config.precision),\n            grad_reduce_in_fp32=False,\n            autocast_enabled=False,\n        ),\n        enable_checkpointing=training_config.enable_checkpointing,\n    )\n    return trainer\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/train/#bionemo.llm.train.train","title":"<code>train(bionemo_exposed_model_config, data_config, parallel_config, training_config, optim_config, experiment_config, wandb_config, nsys_config=None, resume_if_exists=True)</code>","text":"<p>Train a BioNemo model using the provided configurations. Uses the ExposedModelConfig and DataConfig as the primary variants for this method.</p> <p>Parameters:</p> Name Type Description Default <code>bionemo_exposed_model_config</code> <code>ExposedModelConfig</code> <p>Configuration for the exposed BioNemo model.</p> required <code>data_config</code> <code>DataConfig[DataModuleT]</code> <p>Configuration for the data module.</p> required <code>parallel_config</code> <code>ParallelConfig</code> <p>Configuration for parallel training.</p> required <code>training_config</code> <code>TrainingConfig</code> <p>Configuration for training parameters.</p> required <code>optim_config</code> <code>OptimizerSchedulerConfig</code> <p>Configuration for the optimizer and scheduler.</p> required <code>experiment_config</code> <code>ExperimentConfig</code> <p>Configuration for the experiment.</p> required <code>wandb_config</code> <code>Optional[WandbConfig]</code> <p>Configuration for Weights and Biases logging.n</p> required <code>nsys_config</code> <code>Optional[NsysConfig]</code> <p>Configuration for nsys profiling. If None, is disabled.</p> <code>None</code> <code>resume_if_exists</code> <code>bool</code> <p>Flag to resume training if a checkpoint exists. Defaults to True.</p> <code>True</code> Source code in <code>bionemo/llm/train.py</code> <pre><code>def train(\n    bionemo_exposed_model_config: ExposedModelConfig,\n    data_config: DataConfig[DataModuleT],\n    parallel_config: ParallelConfig,\n    training_config: TrainingConfig,\n    optim_config: OptimizerSchedulerConfig,\n    experiment_config: ExperimentConfig,\n    wandb_config: Optional[WandbConfig],\n    nsys_config: Optional[NsysConfig] = None,\n    resume_if_exists: bool = True,\n):\n    \"\"\"Train a BioNemo model using the provided configurations. Uses the ExposedModelConfig and DataConfig as the primary variants for this method.\n\n    Args:\n        bionemo_exposed_model_config (ExposedModelConfig): Configuration for the exposed BioNemo model.\n        data_config (DataConfig[DataModuleT]): Configuration for the data module.\n        parallel_config (ParallelConfig): Configuration for parallel training.\n        training_config (TrainingConfig): Configuration for training parameters.\n        optim_config (OptimizerSchedulerConfig): Configuration for the optimizer and scheduler.\n        experiment_config (ExperimentConfig): Configuration for the experiment.\n        wandb_config (Optional[WandbConfig]): Configuration for Weights and Biases logging.n\n        nsys_config (Optional[NsysConfig], optional): Configuration for nsys profiling. If None, is disabled.\n        resume_if_exists (bool, optional): Flag to resume training if a checkpoint exists. Defaults to True.\n    \"\"\"\n    bionemo_model_config = bionemo_exposed_model_config.exposed_to_internal_bionemo_model_config()\n    pathlib.Path(data_config.result_dir).mkdir(parents=True, exist_ok=True)\n\n    if experiment_config.save_every_n_steps != training_config.val_check_interval:\n        logging.warning(\"Mutating training_config.save_every_n_steps to be equal to val_check_interval.\")\n        experiment_config.save_every_n_steps = training_config.val_check_interval\n\n    global_batch_size = infer_global_batch_size(\n        micro_batch_size=data_config.micro_batch_size,\n        num_nodes=parallel_config.num_nodes,\n        devices=parallel_config.num_devices,\n        accumulate_grad_batches=parallel_config.accumulate_grad_batches,\n        tensor_model_parallel_size=parallel_config.tensor_model_parallel_size,\n        pipeline_model_parallel_size=parallel_config.pipeline_model_parallel_size,\n    )\n\n    data: DataModuleT = data_config.construct_data_module(global_batch_size)\n    # TODO BioBertDataModule or BioBertTokenizer abstractions. We know all DataModuleT in this case has data.tokenizer,\n    # although this constraint is not documented.\n\n    # TODO: need an abstraction for LrSchedulerConfig\n    if optim_config.lr_scheduler == \"cosine\":\n        lr_scheduler = CosineAnnealingScheduler(\n            max_steps=training_config.max_steps if optim_config.max_steps is None else optim_config.max_steps,\n            min_lr=optim_config.lr / 100,\n            warmup_steps=math.ceil(training_config.max_steps * optim_config.cosine_rampup_frac),\n            interval=optim_config.interval,\n            monitor=optim_config.monitor,\n            constant_steps=math.ceil(training_config.max_steps * optim_config.cosine_hold_frac),\n        )\n    elif optim_config.lr_scheduler == \"warmup_anneal\":\n        lr_scheduler = WarmupAnnealDecayHoldScheduler(\n            warmup_steps=optim_config.warmup_steps,\n            max_steps=training_config.max_steps if optim_config.max_steps is None else optim_config.max_steps,\n            max_lr=optim_config.lr,\n            min_lr=optim_config.lr / 10.0,\n            anneal_percentage=0.10,\n        )\n    else:\n        raise NotImplementedError(f\"Scheduler {optim_config.lr_scheduler} not implemented.\")\n\n    optimizer = MegatronOptimizerModule(\n        config=OptimizerConfig(\n            lr=optim_config.lr,\n            weight_decay=optim_config.weight_decay,\n            sgd_momentum=optim_config.sgd_momentum,\n            adam_eps=optim_config.adam_eps,\n            optimizer=optim_config.optimizer,\n            use_distributed_optimizer=parallel_config.use_distributed_optimizer,\n            fp16=bionemo_model_config.fp16,\n            bf16=bionemo_model_config.bf16,\n        ),\n        lr_scheduler=lr_scheduler,\n    )\n\n    model: BionemoLightningModule = biobert_lightning_module(\n        config=bionemo_model_config,\n        tokenizer=data.tokenizer,\n        optimizer=optimizer,\n    )\n    # NOTE (SKH): lifted default callbacks out of setup_trainer\n    callbacks = [\n        RichModelSummary(max_depth=4),\n        LearningRateMonitor(),\n        TimingCallback(),  # Required for certain plugins such as FLOPsMeasurement\n    ]\n    if training_config.create_tflops_callback:\n        dummy_data_module = SimpleNamespace()\n        dummy_data_module.global_batch_size = (\n            global_batch_size  # TODO(dorotat): remove this change after FLOPsMeasurementCallback is refactored\n        )\n        dummy_data_module.tokenizer_vocab_size = data.vocab_size\n        flop_meas_callback = FLOPsMeasurementCallback(\n            bionemo_model_config,\n            dummy_data_module,\n            \"bert\",\n        )\n        callbacks.append(flop_meas_callback)\n    trainer: nl.Trainer = setup_trainer(parallel_config, training_config, nsys_config=nsys_config, callbacks=callbacks)\n    nemo_logger: nl.NeMoLogger = nemo_logger_factory(experiment_config, wandb_config=wandb_config)\n\n    llm.train(\n        model=model,\n        data=data,\n        trainer=trainer,\n        log=nemo_logger,\n        resume=resume.AutoResume(\n            resume_if_exists=resume_if_exists,\n            resume_ignore_no_checkpoint=True,\n        ),\n    )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/data/collate/","title":"Collate","text":""},{"location":"main/references/API_reference/bionemo/llm/data/collate/#bionemo.llm.data.collate.bert_padding_collate_fn","title":"<code>bert_padding_collate_fn(batch, padding_value, min_length=None, max_length=None)</code>","text":"<p>Padding collate function for BERT dataloaders.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>list</code> <p>List of samples.</p> required <code>padding_value</code> <code>int</code> <p>The tokenizer's pad token ID.</p> required <code>min_length</code> <code>int | None</code> <p>Minimum length of the output batch; tensors will be padded to this length. If not provided, no extra padding beyond the max_length will be added.</p> <code>None</code> <code>max_length</code> <code>int | None</code> <p>Maximum length of the sequence. If not provided, tensors will be padded to the longest sequence in the batch.</p> <code>None</code> Source code in <code>bionemo/llm/data/collate.py</code> <pre><code>def bert_padding_collate_fn(\n    batch: Sequence[types.BertSample],\n    padding_value: int,\n    min_length: int | None = None,\n    max_length: int | None = None,\n) -&gt; types.BertSample:\n    \"\"\"Padding collate function for BERT dataloaders.\n\n    Args:\n        batch (list): List of samples.\n        padding_value (int, optional): The tokenizer's pad token ID.\n        min_length: Minimum length of the output batch; tensors will be padded to this length. If not\n            provided, no extra padding beyond the max_length will be added.\n        max_length: Maximum length of the sequence. If not provided, tensors will be padded to the\n            longest sequence in the batch.\n    \"\"\"\n    padding_values = {\n        \"text\": padding_value,\n        \"types\": 0,\n        \"attention_mask\": False,\n        \"labels\": MLM_LOSS_IGNORE_INDEX,  # This should match the masked value used in the MLM loss mask.\n        \"loss_mask\": False,\n        \"is_random\": 0,\n    }\n    return padding_collate_fn(\n        batch=batch,  # type: ignore[assignment]\n        padding_values=padding_values,\n        min_length=min_length,\n        max_length=max_length,\n    )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/data/collate/#bionemo.llm.data.collate.padding_collate_fn","title":"<code>padding_collate_fn(batch, padding_values, min_length=None, max_length=None)</code>","text":"<p>Collate function with padding.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Sequence[_T]</code> <p>List of samples, each of which is a dictionary of tensors.</p> required <code>padding_values</code> <code>dict[str, int]</code> <p>A dictionary of padding values for each tensor key.</p> required <code>min_length</code> <code>int | None</code> <p>Minimum length of the output batch; tensors will be padded to this length. If not provided, no extra padding beyond the max_length will be added.</p> <code>None</code> <code>max_length</code> <code>int | None</code> <p>Maximum length of the sequence. If not provided, tensors will be padded to the longest sequence in the batch.</p> <code>None</code> <p>Returns:</p> Type Description <code>_T</code> <p>A collated batch with the same dictionary input structure.</p> Source code in <code>bionemo/llm/data/collate.py</code> <pre><code>def padding_collate_fn(\n    batch: Sequence[_T],\n    padding_values: dict[str, int],\n    min_length: int | None = None,\n    max_length: int | None = None,\n) -&gt; _T:\n    \"\"\"Collate function with padding.\n\n    Args:\n        batch: List of samples, each of which is a dictionary of tensors.\n        padding_values: A dictionary of padding values for each tensor key.\n        min_length: Minimum length of the output batch; tensors will be padded to this length. If not\n            provided, no extra padding beyond the max_length will be added.\n        max_length: Maximum length of the sequence. If not provided, tensors will be padded to the\n            longest sequence in the batch.\n\n    Returns:\n        A collated batch with the same dictionary input structure.\n    \"\"\"\n    global _warned_once\n    keys: set[str] | None = None\n\n    if len(batch) == 0:  # empty batches passed through in DDP inference\n        return {}\n\n    for entry in batch:\n        # First check that we have sane batches where keys align with each other.\n        if keys is None:\n            keys = set(entry.keys())\n        else:\n            if set(entry.keys()) != keys:\n                raise ValueError(f\"All keys in inputs must match each other. Got: {[sorted(e.keys()) for e in batch]}\")\n        if entry.keys() != padding_values.keys():\n            if not _warned_once:\n                extra_keys = {k for k in entry.keys() if k not in padding_values}\n                missing_keys = {k for k in padding_values.keys() if k not in entry}\n                logger.warning(\n                    f\"Extra keys in batch that will not be padded: {extra_keys}. Missing keys in batch: {missing_keys}\"\n                )\n                _warned_once = True\n\n    def _pad(tensors, padding_value):\n        if max_length is not None:\n            tensors = [t[:max_length] for t in tensors]\n        batched_tensors = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True, padding_value=padding_value)\n        if min_length is None:\n            return batched_tensors\n        return torch.nn.functional.pad(batched_tensors, (0, min_length - batched_tensors.size(1)), value=padding_value)\n\n    return {\n        k: _pad([s[k] for s in batch], padding_values[k])\n        if k in padding_values\n        else torch.stack([s[k] for s in batch])\n        for k in batch[0].keys()\n    }  # type: ignore[return-value]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/data/datamodule/","title":"Datamodule","text":""},{"location":"main/references/API_reference/bionemo/llm/data/datamodule/#bionemo.llm.data.datamodule.MegatronDataModule","title":"<code>MegatronDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>A mixin that adds a <code>state_dict</code> and <code>load_state_dict</code> method for datamodule training resumption in NeMo.</p> Source code in <code>bionemo/llm/data/datamodule.py</code> <pre><code>class MegatronDataModule(pl.LightningDataModule):\n    \"\"\"A mixin that adds a `state_dict` and `load_state_dict` method for datamodule training resumption in NeMo.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Set init_global_step to 0 for datamodule resumption.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.init_global_step = 0\n\n    def update_init_global_step(self):\n        \"\"\"Please always call this when you get a new dataloader... if you forget, your resumption will not work.\"\"\"\n        self.init_global_step = self.trainer.global_step  # Update the init_global_step whenever we re-init training\n        self.data_sampler.init_global_step = (\n            self.init_global_step\n        )  # Update the init_global_step whenever we re-init training\n\n    def state_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Called when saving a checkpoint, implement to generate and save datamodule state.\n\n        Returns:\n            A dictionary containing datamodule state.\n\n        \"\"\"\n        consumed_samples = self.data_sampler.compute_consumed_samples(self.trainer.global_step - self.init_global_step)\n        return {\"consumed_samples\": consumed_samples}\n\n    def load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        \"\"\"Called when loading a checkpoint, implement to reload datamodule state given datamodule stat.\n\n        Args:\n            state_dict: the datamodule state returned by ``state_dict``.\n\n        \"\"\"\n        consumed_samples = state_dict[\"consumed_samples\"]\n        self.data_sampler.init_consumed_samples = consumed_samples\n        self.data_sampler.prev_consumed_samples = consumed_samples\n\n        update_num_microbatches(\n            consumed_samples=consumed_samples,\n            consistency_check=False,\n        )\n        self.data_sampler.if_first_step = 1\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/data/datamodule/#bionemo.llm.data.datamodule.MegatronDataModule.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Set init_global_step to 0 for datamodule resumption.</p> Source code in <code>bionemo/llm/data/datamodule.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Set init_global_step to 0 for datamodule resumption.\"\"\"\n    super().__init__(*args, **kwargs)\n    self.init_global_step = 0\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/data/datamodule/#bionemo.llm.data.datamodule.MegatronDataModule.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"<p>Called when loading a checkpoint, implement to reload datamodule state given datamodule stat.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>Dict[str, Any]</code> <p>the datamodule state returned by <code>state_dict</code>.</p> required Source code in <code>bionemo/llm/data/datamodule.py</code> <pre><code>def load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n    \"\"\"Called when loading a checkpoint, implement to reload datamodule state given datamodule stat.\n\n    Args:\n        state_dict: the datamodule state returned by ``state_dict``.\n\n    \"\"\"\n    consumed_samples = state_dict[\"consumed_samples\"]\n    self.data_sampler.init_consumed_samples = consumed_samples\n    self.data_sampler.prev_consumed_samples = consumed_samples\n\n    update_num_microbatches(\n        consumed_samples=consumed_samples,\n        consistency_check=False,\n    )\n    self.data_sampler.if_first_step = 1\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/data/datamodule/#bionemo.llm.data.datamodule.MegatronDataModule.state_dict","title":"<code>state_dict()</code>","text":"<p>Called when saving a checkpoint, implement to generate and save datamodule state.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary containing datamodule state.</p> Source code in <code>bionemo/llm/data/datamodule.py</code> <pre><code>def state_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Called when saving a checkpoint, implement to generate and save datamodule state.\n\n    Returns:\n        A dictionary containing datamodule state.\n\n    \"\"\"\n    consumed_samples = self.data_sampler.compute_consumed_samples(self.trainer.global_step - self.init_global_step)\n    return {\"consumed_samples\": consumed_samples}\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/data/datamodule/#bionemo.llm.data.datamodule.MegatronDataModule.update_init_global_step","title":"<code>update_init_global_step()</code>","text":"<p>Please always call this when you get a new dataloader... if you forget, your resumption will not work.</p> Source code in <code>bionemo/llm/data/datamodule.py</code> <pre><code>def update_init_global_step(self):\n    \"\"\"Please always call this when you get a new dataloader... if you forget, your resumption will not work.\"\"\"\n    self.init_global_step = self.trainer.global_step  # Update the init_global_step whenever we re-init training\n    self.data_sampler.init_global_step = (\n        self.init_global_step\n    )  # Update the init_global_step whenever we re-init training\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/data/datamodule/#bionemo.llm.data.datamodule.MockDataModule","title":"<code>MockDataModule</code>","text":"<p>               Bases: <code>MegatronDataModule</code></p> <p>A simple data module that just wraps input datasets with dataloaders.</p> Source code in <code>bionemo/llm/data/datamodule.py</code> <pre><code>class MockDataModule(MegatronDataModule):\n    \"\"\"A simple data module that just wraps input datasets with dataloaders.\"\"\"\n\n    def __init__(\n        self,\n        train_dataset: Dataset | None = None,\n        valid_dataset: Dataset | None = None,\n        test_dataset: Dataset | None = None,\n        predict_dataset: Dataset | None = None,\n        pad_token_id: int = 0,\n        min_seq_length: int | None = None,\n        max_seq_length: int = 512,\n        micro_batch_size: int = 16,\n        global_batch_size: int = 16,\n        num_workers: int = 4,\n    ) -&gt; None:\n        \"\"\"Initialize the MockDataModule.\"\"\"\n        super().__init__()\n        self.train_dataset = train_dataset\n        self.valid_dataset = valid_dataset\n        self.test_dataset = test_dataset\n        self.predict_dataset = predict_dataset\n        self.pad_token_id = pad_token_id\n        self.min_seq_length = min_seq_length\n        self.max_seq_length = max_seq_length\n        self.batch_size = micro_batch_size\n        self.num_workers = num_workers\n        self.data_sampler = MegatronDataSampler(\n            seq_len=max_seq_length,\n            micro_batch_size=micro_batch_size,\n            global_batch_size=global_batch_size,\n            dataloader_type=\"single\",\n            output_log=False,\n        )\n\n    def setup(self, stage: str | None = None) -&gt; None:  # noqa: D102\n        pass\n\n    def _make_dataloader(\n        self, dataset: Dataset, mode: Literal[\"train\", \"validation\", \"test\", \"predict\"]\n    ) -&gt; WrappedDataLoader:\n        if mode not in [\"predict\", \"test\"]:\n            self.update_init_global_step()\n\n        return WrappedDataLoader(\n            mode=mode,\n            dataset=dataset,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            collate_fn=functools.partial(\n                collate.bert_padding_collate_fn,\n                padding_value=self.pad_token_id,\n                min_length=self.min_seq_length,\n                max_length=self.max_seq_length,\n            ),\n        )\n\n    def train_dataloader(self) -&gt; DataLoader:  # noqa: D102\n        if self.train_dataset is None:\n            raise ValueError(\"No train_dataset was provided\")\n        return self._make_dataloader(\n            self.train_dataset,\n            mode=\"train\",\n        )\n\n    def val_dataloader(self) -&gt; DataLoader:  # noqa: D102\n        if self.valid_dataset is None:\n            raise ValueError(\"No valid_dataset was provided\")\n        return self._make_dataloader(\n            self.valid_dataset,\n            mode=\"validation\",\n        )\n\n    def test_dataloader(self) -&gt; DataLoader:  # noqa: D102\n        if self.test_dataset is None:\n            raise ValueError(\"No test_dataset was provided\")\n        return self._make_dataloader(\n            self.test_dataset,\n            mode=\"test\",\n        )\n\n    def predict_dataloader(self) -&gt; DataLoader:  # noqa: D102\n        if self.predict_dataset is None:\n            raise ValueError(\"No predict_dataset was provided\")\n        return self._make_dataloader(\n            self.predict_dataset,\n            mode=\"predict\",\n        )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/data/datamodule/#bionemo.llm.data.datamodule.MockDataModule.__init__","title":"<code>__init__(train_dataset=None, valid_dataset=None, test_dataset=None, predict_dataset=None, pad_token_id=0, min_seq_length=None, max_seq_length=512, micro_batch_size=16, global_batch_size=16, num_workers=4)</code>","text":"<p>Initialize the MockDataModule.</p> Source code in <code>bionemo/llm/data/datamodule.py</code> <pre><code>def __init__(\n    self,\n    train_dataset: Dataset | None = None,\n    valid_dataset: Dataset | None = None,\n    test_dataset: Dataset | None = None,\n    predict_dataset: Dataset | None = None,\n    pad_token_id: int = 0,\n    min_seq_length: int | None = None,\n    max_seq_length: int = 512,\n    micro_batch_size: int = 16,\n    global_batch_size: int = 16,\n    num_workers: int = 4,\n) -&gt; None:\n    \"\"\"Initialize the MockDataModule.\"\"\"\n    super().__init__()\n    self.train_dataset = train_dataset\n    self.valid_dataset = valid_dataset\n    self.test_dataset = test_dataset\n    self.predict_dataset = predict_dataset\n    self.pad_token_id = pad_token_id\n    self.min_seq_length = min_seq_length\n    self.max_seq_length = max_seq_length\n    self.batch_size = micro_batch_size\n    self.num_workers = num_workers\n    self.data_sampler = MegatronDataSampler(\n        seq_len=max_seq_length,\n        micro_batch_size=micro_batch_size,\n        global_batch_size=global_batch_size,\n        dataloader_type=\"single\",\n        output_log=False,\n    )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/data/label2id_tokenizer/","title":"Label2id tokenizer","text":""},{"location":"main/references/API_reference/bionemo/llm/data/label2id_tokenizer/#bionemo.llm.data.label2id_tokenizer.Label2IDTokenizer","title":"<code>Label2IDTokenizer</code>","text":"<p>               Bases: <code>TokenizerSpec</code></p> <p>Initializes simple Char Tokenizer.</p> <p>Intended to be used for extracting class labels for classification models such as secondary structure prediction model, where each class is encoded with a character (ex. \"C\", \"H\", \"E\")</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tokenizer = Label2IDTokenizer()\n&gt;&gt;&gt; seqs = ['CHE', 'CCC', 'EHH']\n&gt;&gt;&gt; tokenizer = tokenizer.build_vocab(s)\n</code></pre> Source code in <code>bionemo/llm/data/label2id_tokenizer.py</code> <pre><code>class Label2IDTokenizer(TokenizerSpec):\n    \"\"\"Initializes simple Char Tokenizer.\n\n    Intended to be used for extracting class labels\n    for classification models such as secondary\n    structure prediction model, where each class is\n    encoded with a character (ex. \"C\", \"H\", \"E\")\n\n    Examples:\n            &gt;&gt;&gt; tokenizer = Label2IDTokenizer()\n            &gt;&gt;&gt; seqs = ['CHE', 'CCC', 'EHH']\n            &gt;&gt;&gt; tokenizer = tokenizer.build_vocab(s)\n\n    \"\"\"\n\n    def __init__(self) -&gt; None:  # noqa: D107\n        super().__init__()\n        self.vocab: Dict[str, int] = {}\n        self.decode_vocab: Dict[int, str] = {id_: token for token, id_ in self.vocab.items()}\n\n    @property\n    def vocab_size(self) -&gt; int:\n        \"\"\"Return the size of the vocab being used.\"\"\"\n        return len(self.vocab)\n\n    def text_to_tokens(self, text: str) -&gt; List[str]:  # noqa: D102\n        return list(text)\n\n    def tokens_to_text(self, tokens: List[str]) -&gt; str:  # noqa: D102\n        return \"\".join(tokens)\n\n    def tokens_to_ids(self, tokens: List[str]) -&gt; List[int]:\n        \"\"\"Convert tokens to indexes/ids.\n\n        Args:\n            tokens: Containing tokens\n        Returns:\n            Containing ID's for each token\n        \"\"\"\n        ids = []\n        for token in tokens:\n            id_ = self.vocab.get(token)\n            if id_ is None:\n                raise ValueError(f\"Do not recognize token: {token}\")\n            else:\n                ids.append(id_)\n        return ids\n\n    def ids_to_tokens(self, ids: List[int]) -&gt; List[str]:\n        \"\"\"Convert Ids to tokens.\n\n        Args:\n            ids: Containg ids for each token\n        Returns:\n            Containing tokens\n        \"\"\"\n        tokens = []\n        for id_ in ids:\n            token = self.decode_vocab.get(id_)\n            if token is None:\n                raise ValueError(f\"Do not recognize ID: {id_}\")\n            tokens.append(token)\n        return tokens\n\n    def text_to_ids(self, text: str) -&gt; List[int]:\n        \"\"\"Converts text to ids.\n\n        Args:\n            text (str): String containing text to convert\n        Returns:\n            (List[int]): Id's corresponding to the tokenization\n            of the text\n        \"\"\"\n        tokens = self.text_to_tokens(text)\n        return self.tokens_to_ids(tokens)\n\n    def ids_to_text(self, ids: List[int]) -&gt; str:  # noqa: D102\n        tokens = self.ids_to_tokens(ids)\n        return self.tokens_to_text(tokens)\n\n    def build_vocab(self, strings: Union[str, Iterable[str]]) -&gt; \"Label2IDTokenizer\":\n        \"\"\"Builds the vocabulary of the tokenizer from strings\n        Args:\n            strings: (Union[str, Iterable[str]]): Strings to\n                build the vocabulary with. If a string is supplied,\n                then the vocabulary is built from the single string.\n                Otherwise, the vocabulary is progressively built\n                from all the strings in `strings`.\n        \"\"\"  # noqa: D205\n        if isinstance(strings, str):\n            strings = [strings]\n\n        for string in strings:\n            for token in string:\n                if token not in self.vocab:\n                    self.vocab[token] = len(self.vocab)\n                    self.decode_vocab[self.vocab[token]] = token\n\n        return self\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/data/label2id_tokenizer/#bionemo.llm.data.label2id_tokenizer.Label2IDTokenizer.vocab_size","title":"<code>vocab_size</code>  <code>property</code>","text":"<p>Return the size of the vocab being used.</p>"},{"location":"main/references/API_reference/bionemo/llm/data/label2id_tokenizer/#bionemo.llm.data.label2id_tokenizer.Label2IDTokenizer.build_vocab","title":"<code>build_vocab(strings)</code>","text":"<p>Builds the vocabulary of the tokenizer from strings Args:     strings: (Union[str, Iterable[str]]): Strings to         build the vocabulary with. If a string is supplied,         then the vocabulary is built from the single string.         Otherwise, the vocabulary is progressively built         from all the strings in <code>strings</code>.</p> Source code in <code>bionemo/llm/data/label2id_tokenizer.py</code> <pre><code>def build_vocab(self, strings: Union[str, Iterable[str]]) -&gt; \"Label2IDTokenizer\":\n    \"\"\"Builds the vocabulary of the tokenizer from strings\n    Args:\n        strings: (Union[str, Iterable[str]]): Strings to\n            build the vocabulary with. If a string is supplied,\n            then the vocabulary is built from the single string.\n            Otherwise, the vocabulary is progressively built\n            from all the strings in `strings`.\n    \"\"\"  # noqa: D205\n    if isinstance(strings, str):\n        strings = [strings]\n\n    for string in strings:\n        for token in string:\n            if token not in self.vocab:\n                self.vocab[token] = len(self.vocab)\n                self.decode_vocab[self.vocab[token]] = token\n\n    return self\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/data/label2id_tokenizer/#bionemo.llm.data.label2id_tokenizer.Label2IDTokenizer.ids_to_tokens","title":"<code>ids_to_tokens(ids)</code>","text":"<p>Convert Ids to tokens.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>List[int]</code> <p>Containg ids for each token</p> required <p>Returns:     Containing tokens</p> Source code in <code>bionemo/llm/data/label2id_tokenizer.py</code> <pre><code>def ids_to_tokens(self, ids: List[int]) -&gt; List[str]:\n    \"\"\"Convert Ids to tokens.\n\n    Args:\n        ids: Containg ids for each token\n    Returns:\n        Containing tokens\n    \"\"\"\n    tokens = []\n    for id_ in ids:\n        token = self.decode_vocab.get(id_)\n        if token is None:\n            raise ValueError(f\"Do not recognize ID: {id_}\")\n        tokens.append(token)\n    return tokens\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/data/label2id_tokenizer/#bionemo.llm.data.label2id_tokenizer.Label2IDTokenizer.text_to_ids","title":"<code>text_to_ids(text)</code>","text":"<p>Converts text to ids.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>String containing text to convert</p> required <p>Returns:     (List[int]): Id's corresponding to the tokenization     of the text</p> Source code in <code>bionemo/llm/data/label2id_tokenizer.py</code> <pre><code>def text_to_ids(self, text: str) -&gt; List[int]:\n    \"\"\"Converts text to ids.\n\n    Args:\n        text (str): String containing text to convert\n    Returns:\n        (List[int]): Id's corresponding to the tokenization\n        of the text\n    \"\"\"\n    tokens = self.text_to_tokens(text)\n    return self.tokens_to_ids(tokens)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/data/label2id_tokenizer/#bionemo.llm.data.label2id_tokenizer.Label2IDTokenizer.tokens_to_ids","title":"<code>tokens_to_ids(tokens)</code>","text":"<p>Convert tokens to indexes/ids.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>List[str]</code> <p>Containing tokens</p> required <p>Returns:     Containing ID's for each token</p> Source code in <code>bionemo/llm/data/label2id_tokenizer.py</code> <pre><code>def tokens_to_ids(self, tokens: List[str]) -&gt; List[int]:\n    \"\"\"Convert tokens to indexes/ids.\n\n    Args:\n        tokens: Containing tokens\n    Returns:\n        Containing ID's for each token\n    \"\"\"\n    ids = []\n    for token in tokens:\n        id_ = self.vocab.get(token)\n        if id_ is None:\n            raise ValueError(f\"Do not recognize token: {token}\")\n        else:\n            ids.append(id_)\n    return ids\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/data/masking/","title":"Masking","text":""},{"location":"main/references/API_reference/bionemo/llm/data/masking/#bionemo.llm.data.masking.BertMaskConfig","title":"<code>BertMaskConfig</code>  <code>dataclass</code>","text":"<p>Configuration for masking tokens in a BERT-style model.</p> <p>Attributes:</p> Name Type Description <code>mask_prob</code> <code>float</code> <p>Probability of masking a token.</p> <code>mask_token_prob</code> <code>float</code> <p>Probability of replacing a masked token with the mask token.</p> <code>random_token_prob</code> <code>float</code> <p>Probability of replacing a masked token with a random token.</p> Source code in <code>bionemo/llm/data/masking.py</code> <pre><code>@dataclass(frozen=True)\nclass BertMaskConfig:\n    \"\"\"Configuration for masking tokens in a BERT-style model.\n\n    Attributes:\n        mask_prob: Probability of masking a token.\n        mask_token_prob: Probability of replacing a masked token with the mask token.\n        random_token_prob: Probability of replacing a masked token with a random token.\n    \"\"\"\n\n    tokenizer: Tokenizer\n    random_tokens: range\n    mask_prob: float = 0.15\n    mask_token_prob: float = 0.8\n    random_token_prob: float = 0.1\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Check that the sum of `mask_token_prob` and `random_token_prob` is less than or equal to 1.0.\n\n        Raises:\n            ValueError: If the sum of `mask_token_prob` and `random_token_prob` is greater than 1.0.\n        \"\"\"\n        if self.random_token_prob + self.mask_token_prob &gt; 1.0:\n            raise ValueError(\"Sum of random_token_prob and mask_token_prob must be less than or equal to 1.0.\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/data/masking/#bionemo.llm.data.masking.BertMaskConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Check that the sum of <code>mask_token_prob</code> and <code>random_token_prob</code> is less than or equal to 1.0.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the sum of <code>mask_token_prob</code> and <code>random_token_prob</code> is greater than 1.0.</p> Source code in <code>bionemo/llm/data/masking.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Check that the sum of `mask_token_prob` and `random_token_prob` is less than or equal to 1.0.\n\n    Raises:\n        ValueError: If the sum of `mask_token_prob` and `random_token_prob` is greater than 1.0.\n    \"\"\"\n    if self.random_token_prob + self.mask_token_prob &gt; 1.0:\n        raise ValueError(\"Sum of random_token_prob and mask_token_prob must be less than or equal to 1.0.\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/data/masking/#bionemo.llm.data.masking.add_cls_and_eos_tokens","title":"<code>add_cls_and_eos_tokens(sequence, labels, loss_mask, cls_token=None, eos_token=None)</code>","text":"<p>Prepends the CLS token and appends the EOS token to the masked sequence, updating the loss mask and labels.</p> <p>These labels should never be masked, so this is done after the masking step.</p> <p>Parameters:</p> Name Type Description Default <code>sequence</code> <code>Tensor</code> <p>The input (likely masked) sequence.</p> required <code>labels</code> <code>Tensor</code> <p>The true values of the input sequence at the mask positions.</p> required <code>loss_mask</code> <code>Tensor</code> <p>A boolean tensor indicating which tokens should be included in the loss.</p> required <code>cls_token</code> <code>int | None</code> <p>The token to use for the CLS token. If None, no CLS token is added.</p> <code>None</code> <code>eos_token</code> <code>int | None</code> <p>The token to use for the EOS token. If None, no EOS token is added.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor, Tensor]</code> <p>The same input tensors with the CLS and EOS tokens added, and the labels and loss_mask updated accordingly.</p> Source code in <code>bionemo/llm/data/masking.py</code> <pre><code>def add_cls_and_eos_tokens(\n    sequence: torch.Tensor,\n    labels: torch.Tensor,\n    loss_mask: torch.Tensor,\n    cls_token: int | None = None,\n    eos_token: int | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Prepends the CLS token and appends the EOS token to the masked sequence, updating the loss mask and labels.\n\n    These labels should never be masked, so this is done after the masking step.\n\n    Args:\n        sequence: The input (likely masked) sequence.\n        labels: The true values of the input sequence at the mask positions.\n        loss_mask: A boolean tensor indicating which tokens should be included in the loss.\n        cls_token: The token to use for the CLS token. If None, no CLS token is added.\n        eos_token: The token to use for the EOS token. If None, no EOS token is added.\n\n    Returns:\n        The same input tensors with the CLS and EOS tokens added, and the labels and loss_mask updated accordingly.\n    \"\"\"\n    # Prepend the CLS token and append the EOS token, and update the loss mask and labels accordingly.\n    sequence = torch.cat(\n        [\n            torch.tensor([cls_token], dtype=sequence.dtype)\n            if cls_token is not None\n            else torch.tensor([], dtype=sequence.dtype),\n            sequence,\n            torch.tensor([eos_token], dtype=sequence.dtype)\n            if eos_token is not None\n            else torch.tensor([], dtype=sequence.dtype),\n        ]\n    )\n\n    labels = torch.cat(\n        [\n            torch.tensor([-1], dtype=labels.dtype) if cls_token is not None else torch.tensor([], dtype=labels.dtype),\n            labels,\n            torch.tensor([-1], dtype=labels.dtype) if eos_token is not None else torch.tensor([], dtype=labels.dtype),\n        ]\n    )\n\n    loss_mask = torch.cat(\n        [\n            torch.tensor([False]) if cls_token is not None else torch.tensor([], dtype=loss_mask.dtype),\n            loss_mask,\n            torch.tensor([False]) if eos_token is not None else torch.tensor([], dtype=loss_mask.dtype),\n        ]\n    )\n\n    return sequence, labels, loss_mask\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/data/masking/#bionemo.llm.data.masking.apply_bert_pretraining_mask","title":"<code>apply_bert_pretraining_mask(tokenized_sequence, random_seed, mask_config)</code>","text":"<p>Applies the pretraining mask to a tokenized sequence.</p> <p>Parameters:</p> Name Type Description Default <code>tokenized_sequence</code> <code>Tensor</code> <p>Tokenized protein sequence.</p> required <code>random_seed</code> <code>int</code> <p>Random seed for reproducibility.</p> required <code>mask_config</code> <code>BertMaskConfig</code> <p>Configuration for masking tokens in a BERT-style model.</p> required <p>Returns:</p> Name Type Description <code>masked_sequence</code> <code>Tensor</code> <p>The tokenized sequence with some tokens masked.</p> <code>labels</code> <code>Tensor</code> <p>A tensor the same shape as <code>masked_sequence</code> containing labels for the masked tokens, with -1 for non-masked tokens.</p> <code>loss_mask</code> <code>Tensor</code> <p>A boolean tensor the same shape as <code>masked_sequence</code>, where 'True' indicates which tokens should be included in the loss.</p> Source code in <code>bionemo/llm/data/masking.py</code> <pre><code>def apply_bert_pretraining_mask(\n    tokenized_sequence: torch.Tensor, random_seed: int, mask_config: BertMaskConfig\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Applies the pretraining mask to a tokenized sequence.\n\n    Args:\n        tokenized_sequence: Tokenized protein sequence.\n        random_seed: Random seed for reproducibility.\n        mask_config: Configuration for masking tokens in a BERT-style model.\n\n    Returns:\n        masked_sequence:\n            The tokenized sequence with some tokens masked.\n        labels:\n            A tensor the same shape as `masked_sequence` containing labels for the masked tokens, with -1 for non-masked\n            tokens.\n        loss_mask:\n            A boolean tensor the same shape as `masked_sequence`, where 'True' indicates which tokens should be included\n            in the loss.\n    \"\"\"\n    if mask_config.tokenizer.mask_token_id is None:\n        raise ValueError(\"Tokenizer must have a mask token.\")\n\n    if mask_config.random_token_prob + mask_config.mask_token_prob &gt; 1.0:\n        raise ValueError(\"Sum of random_token_prob and mask_token_prob must be less than or equal to 1.0.\")\n\n    # Set the seed so that __getitem__(idx) is always deterministic.\n    # This is required by Megatron-LM's parallel strategies.\n    generator = torch.Generator().manual_seed(random_seed)\n\n    mask_stop_1 = mask_config.mask_prob * mask_config.mask_token_prob\n    mask_stop_2 = mask_config.mask_prob * (mask_config.mask_token_prob + mask_config.random_token_prob)\n\n    random_draws = torch.rand(tokenized_sequence.shape, generator=generator)  # Random draws for each token in [0, 1).\n\n    # Overall mask for a token being masked in some capacity - either mask token, random token, or left as-is\n    # (identity). We don't want to mask special tokens.\n    loss_mask = ~torch.isin(tokenized_sequence, torch.tensor(mask_config.tokenizer.all_special_ids))\n    loss_mask &amp;= random_draws &lt; mask_config.mask_prob\n\n    # The first `mask_token_prob` fraction of the `mask_prob` tokens are replaced with the mask token.\n    mask_token_mask = (random_draws &lt; mask_stop_1) &amp; loss_mask\n\n    # The next `random_token_prob` fraction of the `mask_prob` tokens are replaced with a random token.\n    random_token_mask = ((random_draws &gt;= mask_stop_1) &amp; (random_draws &lt; mask_stop_2)) &amp; loss_mask\n\n    # The remaining tokens are implicitly left as-is, representing an identity mask.\n\n    # Mask the tokens.\n    masked_sequence = tokenized_sequence.clone()\n    masked_sequence[mask_token_mask] = mask_config.tokenizer.mask_token_id\n    num_random_tokens: int = random_token_mask.sum().item()  # type: ignore[assignment]\n    masked_sequence[random_token_mask] = torch.randint(\n        low=mask_config.random_tokens.start,\n        high=mask_config.random_tokens.stop,\n        size=(num_random_tokens,),\n        dtype=masked_sequence.dtype,\n        generator=generator,\n    )\n\n    # Create the labels for the masked tokens.\n    labels = tokenized_sequence.clone()\n    labels[~loss_mask] = -100  # Ignore loss for non-masked tokens.\n\n    return masked_sequence, labels, loss_mask\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/data/types/","title":"Types","text":""},{"location":"main/references/API_reference/bionemo/llm/data/types/#bionemo.llm.data.types.BertSample","title":"<code>BertSample</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>The type expected by NeMo/Megatron for a single dataset item.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>Tensor</code> <p>The tokenized, masked input text.</p> <code>types</code> <code>Tensor</code> <p>The token type ids, if applicable.</p> <code>attention_mask</code> <code>Tensor</code> <p>A mask over all valid tokens, excluding padding.</p> <code>labels</code> <code>Tensor</code> <p>The true values of the masked tokens at each position covered by loss_mask.</p> <code>loss_mask</code> <code>Tensor</code> <p>The mask over the text indicating which tokens are masked and should be predicted.</p> <code>is_random</code> <code>Tensor</code> <p>??</p> Source code in <code>bionemo/llm/data/types.py</code> <pre><code>class BertSample(TypedDict):\n    \"\"\"The type expected by NeMo/Megatron for a single dataset item.\n\n    Attributes:\n        text: The tokenized, masked input text.\n        types: The token type ids, if applicable.\n        attention_mask: A mask over all valid tokens, excluding padding.\n        labels: The true values of the masked tokens at each position covered by loss_mask.\n        loss_mask: The mask over the text indicating which tokens are masked and should be predicted.\n        is_random: ??\n    \"\"\"\n\n    text: Tensor\n    types: Tensor\n    attention_mask: Tensor\n    labels: Tensor\n    loss_mask: Tensor\n    is_random: Tensor\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/data/types/#bionemo.llm.data.types.Tokenizer","title":"<code>Tokenizer</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Required attributes for a tokenizers provided to apply_bert_pretraining_mask.</p> Source code in <code>bionemo/llm/data/types.py</code> <pre><code>class Tokenizer(Protocol):\n    \"\"\"Required attributes for a tokenizers provided to apply_bert_pretraining_mask.\"\"\"\n\n    @property\n    def mask_token_id(self) -&gt; int | None:  # noqa: D102\n        ...\n\n    @property\n    def all_special_ids(self) -&gt; list[int]:  # noqa: D102\n        ...\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/config/","title":"Config","text":""},{"location":"main/references/API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.IOMixinProto","title":"<code>IOMixinProto</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>A Protocol for the get/set hparam functions of the IOMixin class from NeMo.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>class IOMixinProto(Protocol):\n    \"\"\"A Protocol for the get/set hparam functions of the IOMixin class from NeMo.\"\"\"\n\n    def set_hparam(self, attribute: str, value: Any, also_change_value: bool = True) -&gt; None:\n        \"\"\"Set the value of an attribute in the config attached to the class by the IOMixin.\"\"\"\n        ...\n\n    def get_hparam(self, attribute: str) -&gt; Any:\n        \"\"\"Get the value of an attribute in the config attached to the class by the IOMixin.\"\"\"\n        ...\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.IOMixinProto.get_hparam","title":"<code>get_hparam(attribute)</code>","text":"<p>Get the value of an attribute in the config attached to the class by the IOMixin.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>def get_hparam(self, attribute: str) -&gt; Any:\n    \"\"\"Get the value of an attribute in the config attached to the class by the IOMixin.\"\"\"\n    ...\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.IOMixinProto.set_hparam","title":"<code>set_hparam(attribute, value, also_change_value=True)</code>","text":"<p>Set the value of an attribute in the config attached to the class by the IOMixin.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>def set_hparam(self, attribute: str, value: Any, also_change_value: bool = True) -&gt; None:\n    \"\"\"Set the value of an attribute in the config attached to the class by the IOMixin.\"\"\"\n    ...\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.MegatronBioNeMoModelConfig","title":"<code>MegatronBioNeMoModelConfig</code>","text":"<p>               Bases: <code>BionemoModelConfig[MegatronModelType]</code>, <code>TransformerConfig</code>, <code>WillHaveGetSetHparam</code></p> <p>A ModelConfig class for bionemo that supports usage with Megatron models, for example as NeMo2 requires.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>class MegatronBioNeMoModelConfig(BionemoModelConfig[MegatronModelType], TransformerConfig, iom.WillHaveGetSetHparam):\n    \"\"\"A ModelConfig class for bionemo that supports usage with Megatron models, for example as NeMo2 requires.\"\"\"\n\n    model_cls: Type[MegatronModelType]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.MegatronBioNeMoTrainableModelConfig","title":"<code>MegatronBioNeMoTrainableModelConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MegatronBioNeMoModelConfig[MegatronModelType]</code>, <code>BionemoTrainableModelConfig[MegatronModelType, MegatronLossType]</code>, <code>Generic[MegatronModelType, MegatronLossType]</code></p> <p>A TrainableModelConfig class for bionemo that supports usage with Megatron models, for example as NeMo2 requires.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>@dataclass\nclass MegatronBioNeMoTrainableModelConfig(\n    MegatronBioNeMoModelConfig[MegatronModelType],\n    BionemoTrainableModelConfig[MegatronModelType, MegatronLossType],\n    Generic[MegatronModelType, MegatronLossType],\n):\n    \"\"\"A TrainableModelConfig class for bionemo that supports usage with Megatron models, for example as NeMo2 requires.\"\"\"\n\n    initial_ckpt_path: str | None = None\n    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=list)\n    override_parent_fields: List[str] = field(default_factory=lambda: _OVERRIDE_BIONEMO_CONFIG_DEFAULTS)\n\n    def load_settings_from_checkpoint(self, initial_ckpt_path: str) -&gt; None:\n        \"\"\"Load settings into self from the checkpoint saved in self.\n\n        Any setting in self.override_parent_fields is not overriden. Note that this function will also update the hyper\n        parameters in this config, as well as the associated attributes in self in case they were modified post-init.\n\n        Args:\n            initial_ckpt_path: The path to the checkpoint to load, note that everything is loaded from this checkpoint\n                other than the settings in self.override_parent_fields.\n\n        Returns:\n            None, the settings are loaded into self in place, and the hyper-parameters that will later be saved into\n                a checkpoint are updated.\n        \"\"\"\n        logger.warning(f\"Loading {self.initial_ckpt_path}\")\n        # 1. get the config from the trainer io context by querying the `model.config` subpath of the trainer.\n        initial_config: MegatronBioNeMoTrainableModelConfig = io.load_context(\n            path=Path(initial_ckpt_path) / \"context\", subpath=\"model.config\"\n        )  # type: ignore\n        initial_fields = {f.name for f in fields(initial_config)}\n        my_fields = [f.name for f in fields(self)]\n        skip_fields = set(self.override_parent_fields)\n        override_fields = [f for f in my_fields if f in initial_fields and f not in skip_fields]\n        override_mutate_possibly_extra_mutated_fiddle(self, initial_config, override_fields)\n\n    def update_model_from_checkpoint(self, model: MegatronModelType, initial_ckpt_path: str) -&gt; None:\n        \"\"\"Utility function to standardize how to load a megatron model from a checkpoint ignoring user-specified keys.\n\n        Update the model with the weights from the provided checkpoint path, skipping the keys with the prefixes in\n            self.initial_ckpt_skip_keys_with_these_prefixes.\n\n        Args:\n            model: The Megatron model to update.\n            initial_ckpt_path: The path to the megatron checkpoint to load.\n\n        Returns:\n            None, the model is updated in place, supporting megatron model parallelism abstractions, and ignoring\n                any extra keys that are provided in self.initial_ckpt_skip_keys_with_these_prefixes.\n        \"\"\"\n        load_weights_sharded_inplace_nemo2_to_mcore(\n            model=model,  # type: ignore\n            distributed_checkpoint_dir=initial_ckpt_path,\n            skip_keys_with_these_prefixes=set(self.initial_ckpt_skip_keys_with_these_prefixes),\n        )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.MegatronBioNeMoTrainableModelConfig.load_settings_from_checkpoint","title":"<code>load_settings_from_checkpoint(initial_ckpt_path)</code>","text":"<p>Load settings into self from the checkpoint saved in self.</p> <p>Any setting in self.override_parent_fields is not overriden. Note that this function will also update the hyper parameters in this config, as well as the associated attributes in self in case they were modified post-init.</p> <p>Parameters:</p> Name Type Description Default <code>initial_ckpt_path</code> <code>str</code> <p>The path to the checkpoint to load, note that everything is loaded from this checkpoint other than the settings in self.override_parent_fields.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None, the settings are loaded into self in place, and the hyper-parameters that will later be saved into a checkpoint are updated.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>def load_settings_from_checkpoint(self, initial_ckpt_path: str) -&gt; None:\n    \"\"\"Load settings into self from the checkpoint saved in self.\n\n    Any setting in self.override_parent_fields is not overriden. Note that this function will also update the hyper\n    parameters in this config, as well as the associated attributes in self in case they were modified post-init.\n\n    Args:\n        initial_ckpt_path: The path to the checkpoint to load, note that everything is loaded from this checkpoint\n            other than the settings in self.override_parent_fields.\n\n    Returns:\n        None, the settings are loaded into self in place, and the hyper-parameters that will later be saved into\n            a checkpoint are updated.\n    \"\"\"\n    logger.warning(f\"Loading {self.initial_ckpt_path}\")\n    # 1. get the config from the trainer io context by querying the `model.config` subpath of the trainer.\n    initial_config: MegatronBioNeMoTrainableModelConfig = io.load_context(\n        path=Path(initial_ckpt_path) / \"context\", subpath=\"model.config\"\n    )  # type: ignore\n    initial_fields = {f.name for f in fields(initial_config)}\n    my_fields = [f.name for f in fields(self)]\n    skip_fields = set(self.override_parent_fields)\n    override_fields = [f for f in my_fields if f in initial_fields and f not in skip_fields]\n    override_mutate_possibly_extra_mutated_fiddle(self, initial_config, override_fields)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.MegatronBioNeMoTrainableModelConfig.update_model_from_checkpoint","title":"<code>update_model_from_checkpoint(model, initial_ckpt_path)</code>","text":"<p>Utility function to standardize how to load a megatron model from a checkpoint ignoring user-specified keys.</p> <p>Update the model with the weights from the provided checkpoint path, skipping the keys with the prefixes in     self.initial_ckpt_skip_keys_with_these_prefixes.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>MegatronModelType</code> <p>The Megatron model to update.</p> required <code>initial_ckpt_path</code> <code>str</code> <p>The path to the megatron checkpoint to load.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None, the model is updated in place, supporting megatron model parallelism abstractions, and ignoring any extra keys that are provided in self.initial_ckpt_skip_keys_with_these_prefixes.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>def update_model_from_checkpoint(self, model: MegatronModelType, initial_ckpt_path: str) -&gt; None:\n    \"\"\"Utility function to standardize how to load a megatron model from a checkpoint ignoring user-specified keys.\n\n    Update the model with the weights from the provided checkpoint path, skipping the keys with the prefixes in\n        self.initial_ckpt_skip_keys_with_these_prefixes.\n\n    Args:\n        model: The Megatron model to update.\n        initial_ckpt_path: The path to the megatron checkpoint to load.\n\n    Returns:\n        None, the model is updated in place, supporting megatron model parallelism abstractions, and ignoring\n            any extra keys that are provided in self.initial_ckpt_skip_keys_with_these_prefixes.\n    \"\"\"\n    load_weights_sharded_inplace_nemo2_to_mcore(\n        model=model,  # type: ignore\n        distributed_checkpoint_dir=initial_ckpt_path,\n        skip_keys_with_these_prefixes=set(self.initial_ckpt_skip_keys_with_these_prefixes),\n    )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.TorchmetricsConfig","title":"<code>TorchmetricsConfig</code>  <code>dataclass</code>","text":"<p>TorchmetricsConfig to instantiate torchmetrics.Metric class.</p> <p>Fiddle requires all objects in config serializable and torchmetric.Metric is not. Its instantiation must be deferred into BionemoLightningModule.init. Only support torchmetrics currently, e.g. users can provide 'text.Perplexity' to 'class_path' to use 'torchmetrics.text.Perplexity'.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>@dataclass\nclass TorchmetricsConfig:\n    \"\"\"TorchmetricsConfig to instantiate torchmetrics.Metric class.\n\n    Fiddle requires all objects in config serializable and torchmetric.Metric is not. Its instantiation must be deferred into BionemoLightningModule.__init__.\n    Only support torchmetrics currently, e.g. users can provide 'text.Perplexity' to 'class_path' to use 'torchmetrics.text.Perplexity'.\n    \"\"\"\n\n    class_path: str\n    task: Literal[\"lm\", \"classification\", \"regression\"]\n    metric_name: str\n    kwargs: Optional[dict[str, Any]] = None\n\n    def __post_init__(self):\n        \"\"\"__post_init__ in dataclass.\"\"\"\n        self.kwargs = {} if self.kwargs is None else self.kwargs\n\n    def get_instance(self) -&gt; torchmetrics.Metric:\n        \"\"\"Dynamically imports and instantiates the metric class.\"\"\"\n        if \".\" in self.class_path:\n            module_path, class_name = self.class_path.rsplit(\".\", 1)\n            module = importlib.import_module(f\"torchmetrics.{module_path}\")\n        else:\n            class_name = self.class_path\n            module = importlib.import_module(\"torchmetrics\")\n\n        cls_ = getattr(module, class_name)\n        return cls_(**self.kwargs)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.TorchmetricsConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>post_init in dataclass.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>def __post_init__(self):\n    \"\"\"__post_init__ in dataclass.\"\"\"\n    self.kwargs = {} if self.kwargs is None else self.kwargs\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.TorchmetricsConfig.get_instance","title":"<code>get_instance()</code>","text":"<p>Dynamically imports and instantiates the metric class.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>def get_instance(self) -&gt; torchmetrics.Metric:\n    \"\"\"Dynamically imports and instantiates the metric class.\"\"\"\n    if \".\" in self.class_path:\n        module_path, class_name = self.class_path.rsplit(\".\", 1)\n        module = importlib.import_module(f\"torchmetrics.{module_path}\")\n    else:\n        class_name = self.class_path\n        module = importlib.import_module(\"torchmetrics\")\n\n    cls_ = getattr(module, class_name)\n    return cls_(**self.kwargs)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.override_mutate_possibly_extra_mutated_fiddle","title":"<code>override_mutate_possibly_extra_mutated_fiddle(target_cfg, source_cfg, maybe_mutated_elements_to_clone)</code>","text":"<p>Override the values of the target config with the values of the source config for the given elements.</p> <p>This will modify the tracked init hyper-parameter values, as well as modifying the associated attributes in     self incase they were modified later by post_init code.</p> <p>Parameters:</p> Name Type Description Default <code>target_cfg</code> <code>IOMixinProto</code> <p>The config to update.</p> required <code>source_cfg</code> <code>IOMixinProto</code> <p>The config to copy values from.</p> required <code>maybe_mutated_elements_to_clone</code> <code>List[str]</code> <p>The list of elements to copy from the source config to the target config.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None, the target config is updated in place.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>def override_mutate_possibly_extra_mutated_fiddle(\n    target_cfg: IOMixinProto, source_cfg: IOMixinProto, maybe_mutated_elements_to_clone: List[str]\n) -&gt; None:\n    \"\"\"Override the values of the target config with the values of the source config for the given elements.\n\n    This will modify the tracked init hyper-parameter values, as well as modifying the associated attributes in\n        self incase they were modified later by post_init code.\n\n    Args:\n        target_cfg: The config to update.\n        source_cfg: The config to copy values from.\n        maybe_mutated_elements_to_clone: The list of elements to copy from the source config to the target config.\n\n    Returns:\n        None, the target config is updated in place.\n    \"\"\"\n    for f in maybe_mutated_elements_to_clone:\n        # 1. Update the tracked config values. Note that the associated attribute in self may have been modified\n        #  post-init, so we don't want to change the value in self here. We do that separately next.\n        target_cfg.set_hparam(f, source_cfg.get_hparam(f), also_change_value=False)\n        # 2. Update the lazily untracked values (if the same variable name is used post-init)\n        setattr(target_cfg, f, getattr(source_cfg, f))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/layers/","title":"Layers","text":""},{"location":"main/references/API_reference/bionemo/llm/model/layers/#bionemo.llm.model.layers.ESM2QueryScaling","title":"<code>ESM2QueryScaling</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>bionemo/llm/model/layers.py</code> <pre><code>class ESM2QueryScaling(torch.nn.Module):  # noqa: D101\n    def __init__(self, config: TransformerConfig, *args, **kwargs) -&gt; None:  # noqa: D417\n        \"\"\"A custom layer that scales quary values.\n\n        This layer should replace the q_layernorm=IdentityOp in ESM2 ModuleSpec to reproduce ESM2\n        which apply 1/sqrt(hidden_size_per_attention_head) scaling prior to apply_rotary_pos_emb()\n\n        Args:\n            config (TransformerConfig): The megatron config. This is used for computing projection_size\n        \"\"\"\n        super().__init__()\n        projection_size = config.kv_channels * config.num_attention_heads\n        self.hidden_size_per_attention_head = divide(projection_size, config.num_attention_heads)\n        self.sqrt_val = math.sqrt(self.hidden_size_per_attention_head)\n\n    @torch.compile\n    def forward(self, query, *args, **kwargs):  # noqa: D102\n        return query / self.sqrt_val\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/layers/#bionemo.llm.model.layers.ESM2QueryScaling.__init__","title":"<code>__init__(config, *args, **kwargs)</code>","text":"<p>A custom layer that scales quary values.</p> <p>This layer should replace the q_layernorm=IdentityOp in ESM2 ModuleSpec to reproduce ESM2 which apply 1/sqrt(hidden_size_per_attention_head) scaling prior to apply_rotary_pos_emb()</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TransformerConfig</code> <p>The megatron config. This is used for computing projection_size</p> required Source code in <code>bionemo/llm/model/layers.py</code> <pre><code>def __init__(self, config: TransformerConfig, *args, **kwargs) -&gt; None:  # noqa: D417\n    \"\"\"A custom layer that scales quary values.\n\n    This layer should replace the q_layernorm=IdentityOp in ESM2 ModuleSpec to reproduce ESM2\n    which apply 1/sqrt(hidden_size_per_attention_head) scaling prior to apply_rotary_pos_emb()\n\n    Args:\n        config (TransformerConfig): The megatron config. This is used for computing projection_size\n    \"\"\"\n    super().__init__()\n    projection_size = config.kv_channels * config.num_attention_heads\n    self.hidden_size_per_attention_head = divide(projection_size, config.num_attention_heads)\n    self.sqrt_val = math.sqrt(self.hidden_size_per_attention_head)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/layers/#bionemo.llm.model.layers.TELayerNorm","title":"<code>TELayerNorm</code>","text":"<p>               Bases: <code>LayerNorm</code></p> Source code in <code>bionemo/llm/model/layers.py</code> <pre><code>class TELayerNorm(te.pytorch.LayerNorm):  # noqa: D101\n    def __init__(self, config: TransformerConfig, *args, **kwargs) -&gt; None:  # noqa: D417\n        \"\"\"A wrapper around transformer engine layernorm that allows it to be initialized with a TransformerConfig.\n            This allows this method to be used in a megatron layerspec.\n\n        Args:\n            config (TransformerConfig): The megatron config. This is used for extracing sequence_parallel and zero_centered_gamma.\n                The rest of the config is not used.\n        \"\"\"  # noqa: D205\n        # Eps tends to get passed through properly, as does hidden_size, but not other params from the config.\n        super().__init__(\n            *args,\n            zero_centered_gamma=config.layernorm_zero_centered_gamma,\n            sequence_parallel=config.sequence_parallel,\n            **kwargs,\n        )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/layers/#bionemo.llm.model.layers.TELayerNorm.__init__","title":"<code>__init__(config, *args, **kwargs)</code>","text":"<p>A wrapper around transformer engine layernorm that allows it to be initialized with a TransformerConfig.     This allows this method to be used in a megatron layerspec.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TransformerConfig</code> <p>The megatron config. This is used for extracing sequence_parallel and zero_centered_gamma. The rest of the config is not used.</p> required Source code in <code>bionemo/llm/model/layers.py</code> <pre><code>def __init__(self, config: TransformerConfig, *args, **kwargs) -&gt; None:  # noqa: D417\n    \"\"\"A wrapper around transformer engine layernorm that allows it to be initialized with a TransformerConfig.\n        This allows this method to be used in a megatron layerspec.\n\n    Args:\n        config (TransformerConfig): The megatron config. This is used for extracing sequence_parallel and zero_centered_gamma.\n            The rest of the config is not used.\n    \"\"\"  # noqa: D205\n    # Eps tends to get passed through properly, as does hidden_size, but not other params from the config.\n    super().__init__(\n        *args,\n        zero_centered_gamma=config.layernorm_zero_centered_gamma,\n        sequence_parallel=config.sequence_parallel,\n        **kwargs,\n    )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/loss/","title":"Loss","text":""},{"location":"main/references/API_reference/bionemo/llm/model/loss/#bionemo.llm.model.loss.BERTMLMLossWithReduction","title":"<code>BERTMLMLossWithReduction</code>","text":"<p>               Bases: <code>MegatronLossReduction</code></p> Source code in <code>bionemo/llm/model/loss.py</code> <pre><code>class BERTMLMLossWithReduction(MegatronLossReduction):  # noqa: D101\n    def __init__(self, validation_step: bool = False, val_drop_last: bool = True) -&gt; None:  # noqa: D107\n        super().__init__()\n        self.validation_step = validation_step\n        self.val_drop_last = val_drop_last\n\n    def forward(\n        self, batch: Dict[str, Tensor], forward_out: Dict[str, Tensor]\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, Dict[str, torch.Tensor]]:\n        \"\"\"Forward impl.\n\n        https://github.com/NVIDIA/NeMo/blob/main/nemo/lightning/megatron_parallel.py#L1733\n\n        Note that Method signature is slightly different from NeMo as the NeMo signature is incorrect.\n        \"\"\"\n        # neva returns (logits, loss_mask)\n        if isinstance(forward_out, tuple):\n            # NOTE(SKH): this comes from NeMo- when does this occur? Directly related to the incorrect method signature.\n            forward_out, loss_mask = forward_out\n            batch[\"loss_mask\"] = loss_mask\n\n        if \"labels\" not in batch:\n            raise ValueError(\"Labels not provided in the batch. These are required for this loss computation.\")\n\n        # NOTE: token_logits is [sequence, batch] but labels and other fields, including the loss are [batch, sequence]\n        unreduced_token_loss = unreduced_token_loss_fn(forward_out[\"token_logits\"], batch[\"labels\"])  # [b s]\n\n        loss_sum, num_valid_tokens = masked_token_loss(unreduced_token_loss, batch[\"loss_mask\"])\n\n        if self.validation_step and not self.val_drop_last and loss_sum.isnan():\n            assert num_valid_tokens == 0, \"Got NaN loss with non-empty input\"\n            if batch[\"loss_mask\"].count_nonzero() != 0:\n                raise ValueError(\"Got NaN loss with non-empty input\")\n            loss_sum = torch.zeros_like(num_valid_tokens)\n\n        num_valid_tokens = num_valid_tokens.clone().detach().to(torch.int)\n        loss_sum_and_ub_size = torch.cat([loss_sum.clone().detach().view(1), num_valid_tokens.view(1)])\n        # Set to 1 to avoid divide by zero in the megatron scheduler:\n        #  https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/pipeline_parallel/schedules.py#L303-L308\n        if num_valid_tokens.item() == 0:\n            num_valid_tokens = torch.ones_like(num_valid_tokens)\n\n        return loss_sum, num_valid_tokens, {\"loss_sum_and_ub_size\": loss_sum_and_ub_size}\n\n    def reduce(self, losses_reduced_per_micro_batch) -&gt; torch.Tensor:\n        \"\"\"Loss reduction impl.\n\n        Taken from: https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L534-L552 .\n        \"\"\"\n        if losses_reduced_per_micro_batch:\n            if \"avg\" in losses_reduced_per_micro_batch[0]:\n                # legacy behavior, average over the number of microbatches\n                avg = [x[\"avg\"] for x in losses_reduced_per_micro_batch]\n                loss = torch.cat(avg).mean()\n                return loss\n\n            from megatron.core import parallel_state\n\n            loss_sum_and_ub_size = [\n                x[\"loss_sum_and_ub_size\"] for x in losses_reduced_per_micro_batch if x[\"loss_sum_and_ub_size\"][1] &gt; 0\n            ]\n            loss = (\n                torch.vstack(loss_sum_and_ub_size).sum(dim=0)\n                if len(loss_sum_and_ub_size) &gt; 0\n                else torch.tensor([0.0, 0.0], device=torch.cuda.current_device())\n            )\n            torch.distributed.all_reduce(\n                loss,\n                group=parallel_state.get_data_parallel_group(with_context_parallel=True),\n            )\n            # average over the total number of tokens across the global batch.\n            loss = loss[0] / loss[1]\n\n            return loss\n\n        return torch.tensor(0.0, device=torch.cuda.current_device())\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/loss/#bionemo.llm.model.loss.BERTMLMLossWithReduction.forward","title":"<code>forward(batch, forward_out)</code>","text":"<p>Forward impl.</p> <p>https://github.com/NVIDIA/NeMo/blob/main/nemo/lightning/megatron_parallel.py#L1733</p> <p>Note that Method signature is slightly different from NeMo as the NeMo signature is incorrect.</p> Source code in <code>bionemo/llm/model/loss.py</code> <pre><code>def forward(\n    self, batch: Dict[str, Tensor], forward_out: Dict[str, Tensor]\n) -&gt; Tuple[torch.Tensor, torch.Tensor, Dict[str, torch.Tensor]]:\n    \"\"\"Forward impl.\n\n    https://github.com/NVIDIA/NeMo/blob/main/nemo/lightning/megatron_parallel.py#L1733\n\n    Note that Method signature is slightly different from NeMo as the NeMo signature is incorrect.\n    \"\"\"\n    # neva returns (logits, loss_mask)\n    if isinstance(forward_out, tuple):\n        # NOTE(SKH): this comes from NeMo- when does this occur? Directly related to the incorrect method signature.\n        forward_out, loss_mask = forward_out\n        batch[\"loss_mask\"] = loss_mask\n\n    if \"labels\" not in batch:\n        raise ValueError(\"Labels not provided in the batch. These are required for this loss computation.\")\n\n    # NOTE: token_logits is [sequence, batch] but labels and other fields, including the loss are [batch, sequence]\n    unreduced_token_loss = unreduced_token_loss_fn(forward_out[\"token_logits\"], batch[\"labels\"])  # [b s]\n\n    loss_sum, num_valid_tokens = masked_token_loss(unreduced_token_loss, batch[\"loss_mask\"])\n\n    if self.validation_step and not self.val_drop_last and loss_sum.isnan():\n        assert num_valid_tokens == 0, \"Got NaN loss with non-empty input\"\n        if batch[\"loss_mask\"].count_nonzero() != 0:\n            raise ValueError(\"Got NaN loss with non-empty input\")\n        loss_sum = torch.zeros_like(num_valid_tokens)\n\n    num_valid_tokens = num_valid_tokens.clone().detach().to(torch.int)\n    loss_sum_and_ub_size = torch.cat([loss_sum.clone().detach().view(1), num_valid_tokens.view(1)])\n    # Set to 1 to avoid divide by zero in the megatron scheduler:\n    #  https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/pipeline_parallel/schedules.py#L303-L308\n    if num_valid_tokens.item() == 0:\n        num_valid_tokens = torch.ones_like(num_valid_tokens)\n\n    return loss_sum, num_valid_tokens, {\"loss_sum_and_ub_size\": loss_sum_and_ub_size}\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/loss/#bionemo.llm.model.loss.BERTMLMLossWithReduction.reduce","title":"<code>reduce(losses_reduced_per_micro_batch)</code>","text":"<p>Loss reduction impl.</p> <p>Taken from: https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L534-L552 .</p> Source code in <code>bionemo/llm/model/loss.py</code> <pre><code>def reduce(self, losses_reduced_per_micro_batch) -&gt; torch.Tensor:\n    \"\"\"Loss reduction impl.\n\n    Taken from: https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L534-L552 .\n    \"\"\"\n    if losses_reduced_per_micro_batch:\n        if \"avg\" in losses_reduced_per_micro_batch[0]:\n            # legacy behavior, average over the number of microbatches\n            avg = [x[\"avg\"] for x in losses_reduced_per_micro_batch]\n            loss = torch.cat(avg).mean()\n            return loss\n\n        from megatron.core import parallel_state\n\n        loss_sum_and_ub_size = [\n            x[\"loss_sum_and_ub_size\"] for x in losses_reduced_per_micro_batch if x[\"loss_sum_and_ub_size\"][1] &gt; 0\n        ]\n        loss = (\n            torch.vstack(loss_sum_and_ub_size).sum(dim=0)\n            if len(loss_sum_and_ub_size) &gt; 0\n            else torch.tensor([0.0, 0.0], device=torch.cuda.current_device())\n        )\n        torch.distributed.all_reduce(\n            loss,\n            group=parallel_state.get_data_parallel_group(with_context_parallel=True),\n        )\n        # average over the total number of tokens across the global batch.\n        loss = loss[0] / loss[1]\n\n        return loss\n\n    return torch.tensor(0.0, device=torch.cuda.current_device())\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/loss/#bionemo.llm.model.loss.DataParallelGroupLossAndIO","title":"<code>DataParallelGroupLossAndIO</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Average losses across the data parallel group + the original batch and inference output.</p> Source code in <code>bionemo/llm/model/loss.py</code> <pre><code>class DataParallelGroupLossAndIO(TypedDict):\n    \"\"\"Average losses across the data parallel group + the original batch and inference output.\"\"\"\n\n    avg: Tensor\n    batch: dict[str, Tensor]\n    forward_out: dict[str, Tensor]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/loss/#bionemo.llm.model.loss.PerTokenLossDict","title":"<code>PerTokenLossDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Tensor dictionary for loss.</p> <p>This is the return type for a loss that is computed per token in the batch, supporting microbatches of varying sizes.</p> Source code in <code>bionemo/llm/model/loss.py</code> <pre><code>class PerTokenLossDict(TypedDict):\n    \"\"\"Tensor dictionary for loss.\n\n    This is the return type for a loss that is computed per token in the batch, supporting microbatches of varying sizes.\n    \"\"\"\n\n    loss_sum_and_microbatch_size: Tensor\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/loss/#bionemo.llm.model.loss.SameSizeLossDict","title":"<code>SameSizeLossDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Tensor dictionary for loss.</p> <p>This is the return type for a loss that is computed for the entire batch, where all microbatches are the same size.</p> Source code in <code>bionemo/llm/model/loss.py</code> <pre><code>class SameSizeLossDict(TypedDict):\n    \"\"\"Tensor dictionary for loss.\n\n    This is the return type for a loss that is computed for the entire batch, where all microbatches are the same size.\n    \"\"\"\n\n    avg: Tensor\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/loss/#bionemo.llm.model.loss.unreduced_token_loss_fn","title":"<code>unreduced_token_loss_fn(logits, labels, cross_entropy_loss_fusion=False)</code>","text":"<p>Computes the unreduced token loss given the logits and labels without regard to the loss mask.</p> <p>WARNING: This function does not apply a loss mask. Also, it does inplace operation on the inputs.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>The predicted logits of shape [sequence_length, batch_size, num_classes].</p> required <code>labels</code> <code>Tensor</code> <p>The true labels of shape [batch_size, sequence_length].</p> required <code>cross_entropy_loss_fusion</code> <code>bool</code> <p>If True, use the fused kernel version of vocab parallel cross entropy. This should generally be preferred for speed as it packs more operations into a single kernel on the GPU. However some users have observed reduced training stability when using this method.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The unreduced token loss of shape [batch_size, sequence_length].</p> Source code in <code>bionemo/llm/model/loss.py</code> <pre><code>def unreduced_token_loss_fn(logits: Tensor, labels: Tensor, cross_entropy_loss_fusion: bool = False) -&gt; Tensor:\n    \"\"\"Computes the unreduced token loss given the logits and labels without regard to the loss mask.\n\n    WARNING: This function does not apply a loss mask. Also, it does inplace operation on the inputs.\n\n    Args:\n        logits (Tensor): The predicted logits of shape [sequence_length, batch_size, num_classes].\n        labels (Tensor): The true labels of shape [batch_size, sequence_length].\n        cross_entropy_loss_fusion (bool): If True, use the fused kernel version of vocab parallel cross entropy. This\n            should generally be preferred for speed as it packs more operations into a single kernel on the GPU. However\n            some users have observed reduced training stability when using this method.\n\n    Returns:\n        Tensor: The unreduced token loss of shape [batch_size, sequence_length].\n    \"\"\"\n    labels = labels.transpose(0, 1).contiguous()  # [b, s] -&gt; [s, b]\n    if cross_entropy_loss_fusion:\n        loss = fused_vocab_parallel_cross_entropy(logits, labels)\n    else:\n        loss = tensor_parallel.vocab_parallel_cross_entropy(logits, labels)\n    # [s b] =&gt; [b, s]\n    loss = loss.transpose(0, 1).contiguous()\n    return loss\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/lr_scheduler/","title":"Lr scheduler","text":""},{"location":"main/references/API_reference/bionemo/llm/model/lr_scheduler/#bionemo.llm.model.lr_scheduler.SchedulerOutput","title":"<code>SchedulerOutput</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Output of the scheduler method.</p> Source code in <code>bionemo/llm/model/lr_scheduler.py</code> <pre><code>class SchedulerOutput(TypedDict):\n    \"\"\"Output of the scheduler method.\"\"\"\n\n    optimizer: MegatronOptimizerModule\n    lr_scheduler: dict\n    monitor: str\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/lr_scheduler/#bionemo.llm.model.lr_scheduler.WarmupAnnealDecayHold","title":"<code>WarmupAnnealDecayHold</code>","text":"<p>               Bases: <code>_LRScheduler</code></p> <p>Warmup Anneal Decay Hold learning rate scheduler.</p> Source code in <code>bionemo/llm/model/lr_scheduler.py</code> <pre><code>class WarmupAnnealDecayHold(_LRScheduler):\n    \"\"\"Warmup Anneal Decay Hold learning rate scheduler.\"\"\"\n\n    def __init__(\n        self,\n        optimizer: MegatronOptimizerModule,\n        *,\n        warmup_steps: Optional[int] = None,\n        max_steps: Optional[int] = None,\n        max_lr: Optional[float] = None,\n        min_lr: float = 4e-5,\n        anneal_percentage: float = 0.10,\n        last_epoch: int = -1,\n    ) -&gt; None:\n        \"\"\"Initializes the WarmupAnnealDecayHold learning rate scheduler.\n\n        Args:\n            optimizer: Optimizer to apply the learning rate scheduler.\n            warmup_steps (int): Number of steps for the linear warm-up.\n            max_steps (int): Total number of training steps.\n            max_lr (float): Peak learning rate to be achieved after warm-up.\n            min_lr (float): Minimum learning rate.\n            anneal_percentage (float): Percentage of the max_lr to hold after decay.\n            last_epoch (int): The index of the last epoch.\n        \"\"\"\n        self.warmup_steps = warmup_steps\n        self.max_steps = max_steps\n        self.max_lr = max_lr\n        self.min_lr = min_lr\n        self.anneal_percentage = anneal_percentage\n        self.last_epoch = last_epoch\n\n        for group in optimizer.param_groups:\n            group.setdefault(\"initial_lr\", max_lr)\n\n        super(WarmupAnnealDecayHold, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self) -&gt; List[float]:\n        \"\"\"Get the learning rate at the current step.\"\"\"\n        step_num = self.last_epoch\n        if step_num &lt; self.warmup_steps:\n            lr = self.min_lr + (self.max_lr - self.min_lr) * step_num / self.warmup_steps\n        else:\n            decay_steps = self.max_steps - self.warmup_steps\n            lr = self.max_lr * (1 - (step_num - self.warmup_steps) / decay_steps)\n            lr = max(lr, self.max_lr * self.anneal_percentage)\n\n        return [lr for _ in self.optimizer.param_groups]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/lr_scheduler/#bionemo.llm.model.lr_scheduler.WarmupAnnealDecayHold.__init__","title":"<code>__init__(optimizer, *, warmup_steps=None, max_steps=None, max_lr=None, min_lr=4e-05, anneal_percentage=0.1, last_epoch=-1)</code>","text":"<p>Initializes the WarmupAnnealDecayHold learning rate scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>MegatronOptimizerModule</code> <p>Optimizer to apply the learning rate scheduler.</p> required <code>warmup_steps</code> <code>int</code> <p>Number of steps for the linear warm-up.</p> <code>None</code> <code>max_steps</code> <code>int</code> <p>Total number of training steps.</p> <code>None</code> <code>max_lr</code> <code>float</code> <p>Peak learning rate to be achieved after warm-up.</p> <code>None</code> <code>min_lr</code> <code>float</code> <p>Minimum learning rate.</p> <code>4e-05</code> <code>anneal_percentage</code> <code>float</code> <p>Percentage of the max_lr to hold after decay.</p> <code>0.1</code> <code>last_epoch</code> <code>int</code> <p>The index of the last epoch.</p> <code>-1</code> Source code in <code>bionemo/llm/model/lr_scheduler.py</code> <pre><code>def __init__(\n    self,\n    optimizer: MegatronOptimizerModule,\n    *,\n    warmup_steps: Optional[int] = None,\n    max_steps: Optional[int] = None,\n    max_lr: Optional[float] = None,\n    min_lr: float = 4e-5,\n    anneal_percentage: float = 0.10,\n    last_epoch: int = -1,\n) -&gt; None:\n    \"\"\"Initializes the WarmupAnnealDecayHold learning rate scheduler.\n\n    Args:\n        optimizer: Optimizer to apply the learning rate scheduler.\n        warmup_steps (int): Number of steps for the linear warm-up.\n        max_steps (int): Total number of training steps.\n        max_lr (float): Peak learning rate to be achieved after warm-up.\n        min_lr (float): Minimum learning rate.\n        anneal_percentage (float): Percentage of the max_lr to hold after decay.\n        last_epoch (int): The index of the last epoch.\n    \"\"\"\n    self.warmup_steps = warmup_steps\n    self.max_steps = max_steps\n    self.max_lr = max_lr\n    self.min_lr = min_lr\n    self.anneal_percentage = anneal_percentage\n    self.last_epoch = last_epoch\n\n    for group in optimizer.param_groups:\n        group.setdefault(\"initial_lr\", max_lr)\n\n    super(WarmupAnnealDecayHold, self).__init__(optimizer, last_epoch)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/lr_scheduler/#bionemo.llm.model.lr_scheduler.WarmupAnnealDecayHold.get_lr","title":"<code>get_lr()</code>","text":"<p>Get the learning rate at the current step.</p> Source code in <code>bionemo/llm/model/lr_scheduler.py</code> <pre><code>def get_lr(self) -&gt; List[float]:\n    \"\"\"Get the learning rate at the current step.\"\"\"\n    step_num = self.last_epoch\n    if step_num &lt; self.warmup_steps:\n        lr = self.min_lr + (self.max_lr - self.min_lr) * step_num / self.warmup_steps\n    else:\n        decay_steps = self.max_steps - self.warmup_steps\n        lr = self.max_lr * (1 - (step_num - self.warmup_steps) / decay_steps)\n        lr = max(lr, self.max_lr * self.anneal_percentage)\n\n    return [lr for _ in self.optimizer.param_groups]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/lr_scheduler/#bionemo.llm.model.lr_scheduler.WarmupAnnealDecayHoldScheduler","title":"<code>WarmupAnnealDecayHoldScheduler</code>","text":"<p>               Bases: <code>LRSchedulerModule</code></p> <p>Warmup Policy Learning Rate Scheduler.</p> Source code in <code>bionemo/llm/model/lr_scheduler.py</code> <pre><code>class WarmupAnnealDecayHoldScheduler(LRSchedulerModule):\n    \"\"\"Warmup Policy Learning Rate Scheduler.\"\"\"\n\n    def __init__(\n        self,\n        warmup_steps: int = 2000,\n        max_steps: int = 500_000,\n        max_lr: float = 4e-4,\n        min_lr: float = 4e-5,\n        anneal_percentage: float = 0.10,\n        interval: str = \"step\",\n        frequency: int = 1,\n        monitor: str = \"val_loss\",\n    ) -&gt; None:\n        \"\"\"Initializes the WarmupAnnealDecayHoldScheduler.\"\"\"\n        super().__init__()\n        self.warmup_steps = warmup_steps\n        self.max_steps = max_steps\n        self.max_lr = max_lr\n        self.min_lr = min_lr\n        self.anneal_percentage = anneal_percentage\n        self.interval = interval\n        self.frequency = frequency\n        self.monitor = monitor\n\n    def scheduler(self, model: MegatronBioBertModel, optimizer: MegatronOptimizerModule) -&gt; SchedulerOutput:\n        \"\"\"Returns the scheduler output.\"\"\"\n        lr_scheduler = WarmupAnnealDecayHold(\n            optimizer,\n            warmup_steps=self.warmup_steps,\n            max_steps=self.max_steps,\n            max_lr=self.max_lr,\n            min_lr=self.min_lr,\n            anneal_percentage=self.anneal_percentage,\n        )\n        return {\n            \"optimizer\": optimizer,\n            # REQUIRED: The scheduler instance\n            \"lr_scheduler\": {\n                \"scheduler\": lr_scheduler,\n                # `interval` is the unit of the scheduler's step size, could also be 'step'.\n                # 'epoch' updates the scheduler on epoch end whereas 'step'\n                # updates it after a optimizer update.\n                \"interval\": self.interval,\n                # How many epochs/steps should pass between calls to\n                # `scheduler.step()`. 1 corresponds to updating the learning\n                # rate after every epoch/step.\n                \"frequency\": self.frequency,\n            },\n            # Metric to to monitor for schedulers like `ReduceLROnPlateau`\n            \"monitor\": self.monitor,\n        }\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/lr_scheduler/#bionemo.llm.model.lr_scheduler.WarmupAnnealDecayHoldScheduler.__init__","title":"<code>__init__(warmup_steps=2000, max_steps=500000, max_lr=0.0004, min_lr=4e-05, anneal_percentage=0.1, interval='step', frequency=1, monitor='val_loss')</code>","text":"<p>Initializes the WarmupAnnealDecayHoldScheduler.</p> Source code in <code>bionemo/llm/model/lr_scheduler.py</code> <pre><code>def __init__(\n    self,\n    warmup_steps: int = 2000,\n    max_steps: int = 500_000,\n    max_lr: float = 4e-4,\n    min_lr: float = 4e-5,\n    anneal_percentage: float = 0.10,\n    interval: str = \"step\",\n    frequency: int = 1,\n    monitor: str = \"val_loss\",\n) -&gt; None:\n    \"\"\"Initializes the WarmupAnnealDecayHoldScheduler.\"\"\"\n    super().__init__()\n    self.warmup_steps = warmup_steps\n    self.max_steps = max_steps\n    self.max_lr = max_lr\n    self.min_lr = min_lr\n    self.anneal_percentage = anneal_percentage\n    self.interval = interval\n    self.frequency = frequency\n    self.monitor = monitor\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/lr_scheduler/#bionemo.llm.model.lr_scheduler.WarmupAnnealDecayHoldScheduler.scheduler","title":"<code>scheduler(model, optimizer)</code>","text":"<p>Returns the scheduler output.</p> Source code in <code>bionemo/llm/model/lr_scheduler.py</code> <pre><code>def scheduler(self, model: MegatronBioBertModel, optimizer: MegatronOptimizerModule) -&gt; SchedulerOutput:\n    \"\"\"Returns the scheduler output.\"\"\"\n    lr_scheduler = WarmupAnnealDecayHold(\n        optimizer,\n        warmup_steps=self.warmup_steps,\n        max_steps=self.max_steps,\n        max_lr=self.max_lr,\n        min_lr=self.min_lr,\n        anneal_percentage=self.anneal_percentage,\n    )\n    return {\n        \"optimizer\": optimizer,\n        # REQUIRED: The scheduler instance\n        \"lr_scheduler\": {\n            \"scheduler\": lr_scheduler,\n            # `interval` is the unit of the scheduler's step size, could also be 'step'.\n            # 'epoch' updates the scheduler on epoch end whereas 'step'\n            # updates it after a optimizer update.\n            \"interval\": self.interval,\n            # How many epochs/steps should pass between calls to\n            # `scheduler.step()`. 1 corresponds to updating the learning\n            # rate after every epoch/step.\n            \"frequency\": self.frequency,\n        },\n        # Metric to to monitor for schedulers like `ReduceLROnPlateau`\n        \"monitor\": self.monitor,\n    }\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/biobert/lightning/","title":"Lightning","text":""},{"location":"main/references/API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.BertBatch","title":"<code>BertBatch</code>","text":"<p>               Bases: <code>BertBatchCore</code></p> <p>Input datatype for inference with BERT-like models.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>class BertBatch(BertBatchCore, total=False):\n    \"\"\"Input datatype for inference with BERT-like models.\"\"\"\n\n    cu_seqlens: Tensor\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.BertBatchCore","title":"<code>BertBatchCore</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Input datatype for inference with BERT-like models.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>class BertBatchCore(TypedDict):\n    \"\"\"Input datatype for inference with BERT-like models.\"\"\"\n\n    text: Tensor\n    attention_mask: Tensor\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.BertModel","title":"<code>BertModel</code>","text":"<p>               Bases: <code>Protocol[DataT]</code></p> <p>Interface for BERT-like models.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>class BertModel(Protocol[DataT]):\n    \"\"\"Interface for BERT-like models.\"\"\"\n\n    def forward(\n        self, input_ids: Tensor, attention_mask: Tensor, packed_seq_params: Optional[PackedSeqParams] = None\n    ) -&gt; DataT:\n        \"\"\"Inference for BERT-like models.\n\n        Inference for BERT-like models require their tokenized inputs by IDs, an attention mask over the input,\n        and the original sequence lengths if the sequences are packed into a dense batch.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.BertModel.forward","title":"<code>forward(input_ids, attention_mask, packed_seq_params=None)</code>","text":"<p>Inference for BERT-like models.</p> <p>Inference for BERT-like models require their tokenized inputs by IDs, an attention mask over the input, and the original sequence lengths if the sequences are packed into a dense batch.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>def forward(\n    self, input_ids: Tensor, attention_mask: Tensor, packed_seq_params: Optional[PackedSeqParams] = None\n) -&gt; DataT:\n    \"\"\"Inference for BERT-like models.\n\n    Inference for BERT-like models require their tokenized inputs by IDs, an attention mask over the input,\n    and the original sequence lengths if the sequences are packed into a dense batch.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.BioBertLightningModule","title":"<code>BioBertLightningModule</code>","text":"<p>               Bases: <code>BionemoLightningModule</code></p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>class BioBertLightningModule(BionemoLightningModule):\n    def __init__(\n        self,\n        *args,\n        data_step_function: DataStepFunction = biobert_data_step,\n        forward_step_function: ForwardStepFunction = bert_forward_step,\n        **kwargs,\n    ):\n        \"\"\"DEPRECATED! Please use BionemoLightningModule. This is here so we can load older checkpoints.\n        This maps the old name `forward_step_function` to the new name `forward_step` and `data_step_function` to\n        `data_step`.\n\n        Args:\n            *args: all args are passed through to BionemoLightningModule\n            data_step_function (DataStepFunction, optional): The data step function. Defaults to biobert_data_step.\n            forward_step_function (ForwardStepFunction, optional): The forward step function. Defaults to bert_forward_step.\n            **kwargs: all other kwargs are passed through to BionemoLightningModule.\n        \"\"\"  # noqa: D205\n        super().__init__(*args, forward_step=forward_step_function, data_step=data_step_function, **kwargs)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.BioBertLightningModule.__init__","title":"<code>__init__(*args, data_step_function=biobert_data_step, forward_step_function=bert_forward_step, **kwargs)</code>","text":"<p>DEPRECATED! Please use BionemoLightningModule. This is here so we can load older checkpoints. This maps the old name <code>forward_step_function</code> to the new name <code>forward_step</code> and <code>data_step_function</code> to <code>data_step</code>.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>all args are passed through to BionemoLightningModule</p> <code>()</code> <code>data_step_function</code> <code>DataStepFunction</code> <p>The data step function. Defaults to biobert_data_step.</p> <code>biobert_data_step</code> <code>forward_step_function</code> <code>ForwardStepFunction</code> <p>The forward step function. Defaults to bert_forward_step.</p> <code>bert_forward_step</code> <code>**kwargs</code> <p>all other kwargs are passed through to BionemoLightningModule.</p> <code>{}</code> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    data_step_function: DataStepFunction = biobert_data_step,\n    forward_step_function: ForwardStepFunction = bert_forward_step,\n    **kwargs,\n):\n    \"\"\"DEPRECATED! Please use BionemoLightningModule. This is here so we can load older checkpoints.\n    This maps the old name `forward_step_function` to the new name `forward_step` and `data_step_function` to\n    `data_step`.\n\n    Args:\n        *args: all args are passed through to BionemoLightningModule\n        data_step_function (DataStepFunction, optional): The data step function. Defaults to biobert_data_step.\n        forward_step_function (ForwardStepFunction, optional): The forward step function. Defaults to bert_forward_step.\n        **kwargs: all other kwargs are passed through to BionemoLightningModule.\n    \"\"\"  # noqa: D205\n    super().__init__(*args, forward_step=forward_step_function, data_step=data_step_function, **kwargs)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.SequenceBatch","title":"<code>SequenceBatch</code>","text":"<p>               Bases: <code>SequenceBatchCore</code></p> <p>Input datatype for inference with BERT-like models.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>class SequenceBatch(SequenceBatchCore, total=False):\n    \"\"\"Input datatype for inference with BERT-like models.\"\"\"\n\n    cu_seqlens_argmin: Tensor\n    max_seqlen: Tensor\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.SequenceBatchCore","title":"<code>SequenceBatchCore</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Input datatype for inference with BERT-like models.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>class SequenceBatchCore(TypedDict):\n    \"\"\"Input datatype for inference with BERT-like models.\"\"\"\n\n    cu_seqlens: Tensor\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.bert_forward_step","title":"<code>bert_forward_step(model, batch)</code>","text":"<p>Performs the model's forward pass using the batch, for Megatron compatibility.</p> <p>This subsets the batch keys to the ones actually used by forward pass of the model, and then calls the model's forward pass. if \"cu_seqsens\" are defined in the batch, then the packed sequence parameters are also passed to the model for forward pass efficiency.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>def bert_forward_step(model: BertModel[DataT], batch: BertBatch) -&gt; DataT:\n    \"\"\"Performs the model's forward pass using the batch, for Megatron compatibility.\n\n    This subsets the batch keys to the ones actually used by forward pass of the model, and then calls the model's\n    forward pass. if \"cu_seqsens\" are defined in the batch, then the packed sequence parameters are also passed to the\n    model for forward pass efficiency.\n    \"\"\"\n    if \"cu_seqlens\" in batch:\n        forward_results = model.forward(\n            input_ids=batch[\"text\"],\n            attention_mask=batch[\"attention_mask\"],\n            packed_seq_params=get_packed_seq_params(cast(SequenceBatch, batch)),\n        )\n    else:\n        forward_results = model.forward(input_ids=batch[\"text\"], attention_mask=batch[\"attention_mask\"])\n    # TODO support losses that also include the binary head, this means doing something more fancy than the one\n    #      default GPT reduction function above MaskedTokenLossReduction()\n    return forward_results\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.biobert_data_step","title":"<code>biobert_data_step(dataloader_iter)</code>","text":"<p>Preprocesses a batch of data for the GeneFormer model, and ingest a single batch of data from the dataloader iterator.     only necessary batch keys are subsetted and passed to the model's forward pass, and the loss forward pass, depending on stage.     TODO document how parallel_state pipeline stages work.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader_iter</code> <p>An iterator over the dataloader.</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>Dict[str, Tensor]</code> <p>A dictionary of this batch limiting to relevant keys.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>def biobert_data_step(dataloader_iter) -&gt; Dict[str, Tensor]:\n    \"\"\"Preprocesses a batch of data for the GeneFormer model, and ingest a single batch of data from the dataloader iterator.\n        only necessary batch keys are subsetted and passed to the model's forward pass, and the loss forward pass, depending on stage.\n        TODO document how parallel_state pipeline stages work.\n\n    Args:\n        dataloader_iter: An iterator over the dataloader.\n\n    Returns:\n        output: A dictionary of this batch limiting to relevant keys.\n\n    \"\"\"  # noqa: D205\n    # Based on: https://github.com/NVIDIA/Megatron-LM/blob/main/pretrain_gpt.py#L87\n    # https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L828-L842\n\n    batch = next(dataloader_iter)\n\n    if isinstance(batch, tuple) and len(batch) == 3:\n        _batch: dict = batch[0]\n    else:\n        _batch = batch\n\n    required_keys = set()\n    required_keys.add(\"attention_mask\")\n    if parallel_state.is_pipeline_first_stage():\n        required_keys.add(\"text\")\n    if parallel_state.is_pipeline_last_stage():\n        required_keys.update((\"labels\", \"loss_mask\", \"types\", \"is_random\"))\n    # if self.get_attention_mask_from_fusion:\n    #     required_keys.remove('attention_mask')\n\n    _batch = {key: val.cuda(non_blocking=True) if key in required_keys else None for key, val in _batch.items()}\n    # slice batch along sequence dimension for context parallelism\n    output = get_batch_on_this_context_parallel_rank(_batch)\n\n    return output\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.biobert_lightning_module","title":"<code>biobert_lightning_module(config, optimizer=None, tokenizer=None, data_step=biobert_data_step, forward_step=bert_forward_step, model_transform=None, **model_construct_args)</code>","text":"<p>A pytorch lightning module for BioBert-derived models.</p> <p>This module is designed to be used with the Megatron-LM strategy and nemo 2.0 conventions. To change your loss, pass in a different config object that returns a different loss reduction class. To change your model and what it outputs, pass in a different config object that returns a different model. Do not modify this function unless you need to change higher level logic. You may need to modify the various step and forward functions towards the bottom of this file to handle new/different keys in the batch. In the future some of those functions may need to be refactored out into the config object or a different place so that they live closer to the model definition.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>def biobert_lightning_module(\n    config: BioBertConfig[MegatronBioBertModel, MegatronLossReduction],\n    optimizer: Optional[MegatronOptimizerModule] = None,\n    tokenizer: Optional[TokenizerSpec | PreTrainedTokenizerBase] = None,\n    data_step: DataStep = biobert_data_step,\n    forward_step: ForwardStep = bert_forward_step,\n    model_transform: Optional[Callable] = None,\n    **model_construct_args,\n) -&gt; BionemoLightningModule[MegatronBioBertModel, MegatronLossReduction]:\n    \"\"\"A pytorch lightning module for BioBert-derived models.\n\n    This module is designed to be used with the Megatron-LM strategy and nemo 2.0 conventions.\n    To change your loss, pass in a different config object that returns a different loss reduction class.\n    To change your model and what it outputs, pass in a different config object that returns a different model.\n    Do not modify this function unless you need to change higher level logic. You may need to modify the various step\n    and forward functions towards the bottom of this file to handle new/different keys in the batch. In the future some\n    of those functions may need to be refactored out into the config object or a different place so that they live\n    closer to the model definition.\n    \"\"\"\n    return BionemoLightningModule(\n        config=config,\n        optimizer=optimizer if optimizer is not None else default_megatron_optimizer(),\n        data_step=data_step,\n        forward_step=forward_step,\n        tokenizer=tokenizer,\n        model_transform=model_transform,\n        **model_construct_args,\n    )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.get_batch_on_this_context_parallel_rank","title":"<code>get_batch_on_this_context_parallel_rank(batch, in_place=True)</code>","text":"<p>Ensures that the input batch is in the right format for context parallel rank.</p> <p>Modifies the batch data based on the context parallel rank, if the context parallel world size is greater than 1. Otherwise, the batch is returned as-is.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Tensor]</code> <p>The input batch data.</p> required <code>in_place</code> <code>bool</code> <p>If true, then the input is mutated. The returned dict is a reference to the input.       Otherwise, the input data is always shallow-copied and this copy is modified and returned.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Tensor]</code> <p>The modified batch data based on the context parallel rank.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>def get_batch_on_this_context_parallel_rank(batch: Dict[str, Tensor], in_place: bool = True) -&gt; Dict[str, Tensor]:\n    \"\"\"Ensures that the input batch is in the right format for context parallel rank.\n\n    Modifies the batch data based on the context parallel rank, if the context parallel world size is greater than 1.\n    Otherwise, the batch is returned as-is.\n\n\n    Args:\n        batch: The input batch data.\n        in_place: If true, then the input is mutated. The returned dict is a reference to the input.\n                  Otherwise, the input data is always shallow-copied and this copy is modified and returned.\n\n    Returns:\n        dict: The modified batch data based on the context parallel rank.\n    \"\"\"\n    if not in_place:\n        batch: dict[str, Tensor] = dict(**batch)\n\n    if cp_size := parallel_state.get_context_parallel_world_size() &gt; 1:\n        num_valid_tokens_in_ub: Tensor | None = None\n        if \"loss_mask\" in batch and batch[\"loss_mask\"] is not None:\n            num_valid_tokens_in_ub = batch[\"loss_mask\"].sum()\n\n        cp_rank = parallel_state.get_context_parallel_rank()\n        for key, val in batch.items():\n            if val is not None:\n                seq_dim = 1 if key != \"attention_mask\" else 2\n                _val = val.view(\n                    *val.shape[0:seq_dim],\n                    2 * cp_size,\n                    val.shape[seq_dim] // (2 * cp_size),\n                    *val.shape[(seq_dim + 1) :],\n                )\n                index = torch.tensor([cp_rank, (2 * cp_size - cp_rank - 1)], device=\"cpu\", pin_memory=True).cuda(\n                    non_blocking=True\n                )\n                _val = _val.index_select(seq_dim, index)\n                _val = _val.view(*val.shape[0:seq_dim], -1, *_val.shape[(seq_dim + 2) :])\n                batch[key] = _val\n        batch[\"num_valid_tokens_in_ub\"] = num_valid_tokens_in_ub  # type: ignore\n\n    return batch\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.get_packed_seq_params","title":"<code>get_packed_seq_params(batch)</code>","text":"<p>Get the packed sequence parameters for the given batch.</p> <p>This function should only be called if <code>cu_seqlens</code> is defined in the batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>SequenceBatch</code> <p>The input batch to pack.</p> required <p>Returns:</p> Name Type Description <code>PackedSeqParams</code> <code>PackedSeqParams</code> <p>The packed sequence parameters containing the following attributes: - cu_seqlens_q (Tensor): The sequence lengths for query. - cu_seqlens_kv (Tensor): The sequence lengths for key and value. - max_seqlen_q (Tensor, optional): The maximum sequence length for query. - max_seqlen_kv (Tensor, optional): The maximum sequence length for key and value. - qkv_format (str): The format of query, key, and value tensors.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>def get_packed_seq_params(batch: SequenceBatch) -&gt; PackedSeqParams:\n    \"\"\"Get the packed sequence parameters for the given batch.\n\n    This function should only be called if `cu_seqlens` is defined in the batch.\n\n    Args:\n        batch: The input batch to pack.\n\n    Returns:\n        PackedSeqParams: The packed sequence parameters containing the following attributes:\n            - cu_seqlens_q (Tensor): The sequence lengths for query.\n            - cu_seqlens_kv (Tensor): The sequence lengths for key and value.\n            - max_seqlen_q (Tensor, optional): The maximum sequence length for query.\n            - max_seqlen_kv (Tensor, optional): The maximum sequence length for key and value.\n            - qkv_format (str): The format of query, key, and value tensors.\n\n    \"\"\"\n    cu_seqlens = batch[\"cu_seqlens\"].squeeze()  # remove batch size dimension (mbs=1)\n    # remove -1 \"paddings\" added in collate_fn\n    if cu_seqlens_argmin := batch.get(\"cu_seqlens_argmin\", None) is not None:\n        # pre-compute cu_seqlens_argmin in dataset class for perf\n        cu_seqlens = cu_seqlens[: cu_seqlens_argmin.item()]\n    else:\n        cu_seqlens = cu_seqlens[: torch.argmin(cu_seqlens)]\n\n    # pre-compute max_seqlens in dataset class for perf\n    max_seqlen = batch[\"max_seqlen\"].squeeze() if \"max_seqlen\" in batch else None\n\n    # these args are passed eventually into TEDotProductAttention.forward()\n    return PackedSeqParams(\n        cu_seqlens_q=cu_seqlens,\n        cu_seqlens_kv=cu_seqlens,\n        max_seqlen_q=max_seqlen,\n        max_seqlen_kv=max_seqlen,\n        qkv_format=\"thd\",\n    )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/biobert/model/","title":"Model","text":""},{"location":"main/references/API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.MegatronBioBertModelType","title":"<code>MegatronBioBertModelType = TypeVar('MegatronBioBertModelType', bound=MegatronBioBertModel)</code>  <code>module-attribute</code>","text":"<p>A megatron model that is or extends the MegatronBioBertModel.</p>"},{"location":"main/references/API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.PositionEmbeddingKinds","title":"<code>PositionEmbeddingKinds = Literal['learned_absolute', 'rope']</code>  <code>module-attribute</code>","text":"<p>Kinds of supported positional embeddings.</p>"},{"location":"main/references/API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.BioBertConfig","title":"<code>BioBertConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MegatronBioNeMoTrainableModelConfig[MegatronBioBertModelType, MegatronLossType]</code></p> <p>Config class for BioBert model, responsible for the partial configuration of Transformer models.</p> <p>NOTE: do not use this config directly, define a child config that overrides items from this parent config</p> <p><code>configure_model()</code> is ultimately called by the LightningModule using PTL lightning module hooks.</p> Source code in <code>bionemo/llm/model/biobert/model.py</code> <pre><code>@dataclass\nclass BioBertConfig(\n    MegatronBioNeMoTrainableModelConfig[MegatronBioBertModelType, MegatronLossType],\n):\n    \"\"\"Config class for BioBert model, responsible for the partial configuration of Transformer models.\n\n    NOTE: do not use this config directly, define a child config that overrides items from this parent config\n\n    `configure_model()` is ultimately called by the LightningModule using PTL lightning module hooks.\n    \"\"\"\n\n    # From megatron.core.models.gpt.bert_model.GPTModel\n    kv_channels: int | None = None\n    fp16_lm_cross_entropy: bool = False\n    apply_rope_fusion: bool = True\n    parallel_output: bool = True\n    bias_dropout_fusion: bool = True\n    bias_activation_fusion: bool = True\n    masked_softmax_fusion: bool = True\n    persist_layer_norm: bool = True\n    get_attention_mask_from_fusion: bool = True\n    share_embeddings_and_output_weights: bool = False  # try True\n    make_vocab_size_divisible_by: int = 128\n    position_embedding_type: PositionEmbeddingKinds = \"learned_absolute\"\n    rotary_base: int = 10000\n    rotary_percent: float = 1.0\n    seq_len_interpolation_factor: Optional[float] = None\n    seq_length: int = 1024\n    hidden_size: int = 512\n    num_attention_heads: int = 8\n    num_layers: int = 6\n    init_method_std: float = 0.02\n    biobert_spec_option: BiobertSpecOption = BiobertSpecOption.bert_layer_with_transformer_engine_spec\n\n    optimizer_fn: Optional[Callable[[\"MegatronBioBertModel\"], Optimizer]] = None\n    # TODO (@skothenhill,@georgea) update to use the nemo2 checkpoint mixins\n    #  support HF (requires weight interleaving on qkv layer) and nemo1 checkpoints ideally.\n    # TODO (@skothenhill,@jstjohn) come up with a nice way of doing fine-tuning checkpoint loading,\n    #  where some acceptible layers (eg lm_head) may or may not be absent from the model, and others\n    #  (like a new head) may be new and missing from the initial checkpoint.\n    nemo1_ckpt_path: Optional[str] = None\n\n    initial_ckpt_path: Optional[str] = None\n    # TODO(@jstjohn, @skothenhill) Was this supposed to be only on the child?\n    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=list)\n    # Used if initializing from a checkpoint, set this to any fields you want to override rather than re-set.\n    #  by default all fields will be overridden.\n    override_parent_fields: List[str] = field(default_factory=lambda: _OVERRIDE_BIOBERT_CONFIG_DEFAULTS)\n    return_embeddings: bool = False\n    include_embeddings: bool = False\n    return_only_hidden_states: bool = False\n    include_hiddens: bool = False  # Include hidden layers in the output of the model\n    include_input_ids: bool = False\n    skip_logits: bool = False  # useful for inference\n    core_attention_override: Type[torch.nn.Module] | None = None\n\n    # loss reduction class\n    loss_reduction_class: Type[MegatronLossType] = BERTMLMLossWithReduction\n\n    # metric logging\n    train_metric: Optional[TorchmetricsConfig] = None\n    valid_metric: Optional[TorchmetricsConfig] = None\n\n    def configure_model(self, tokenizer: AutoTokenizer) -&gt; MegatronBioBertModelType:  # noqa: D102\n        vp_size = self.virtual_pipeline_model_parallel_size\n        if vp_size:\n            p_size = self.pipeline_model_parallel_size\n            assert (self.num_layers // p_size) % vp_size == 0, (\n                \"Make sure the number of model chunks is the same across all pipeline stages.\"\n            )\n\n        # The local specs all require the standard full attention mask.\n        use_full_attention_mask: bool = \"transformer_engine\" not in self.biobert_spec_option\n        do_next_sentence = False\n        if self.model_cls is None:\n            raise ValueError(\n                f\"You must supply `model_cls` to the {type(self)} for module to initialization in `configure_model`.\"\n            )\n\n        if self.initial_ckpt_path:\n            self.load_settings_from_checkpoint(self.initial_ckpt_path)\n        model = self.model_cls(\n            self,\n            transformer_layer_spec=get_biobert_spec(\n                self.biobert_spec_option,\n                qk_layernorm=self.qk_layernorm,\n                core_attention=self.core_attention_override,\n            ),\n            num_tokentypes=2 if do_next_sentence else 0,\n            vocab_size=get_vocab_size(self, tokenizer.vocab_size, self.make_vocab_size_divisible_by),\n            max_sequence_length=self.seq_length,\n            tokenizer=tokenizer,\n            fp16_lm_cross_entropy=self.fp16_lm_cross_entropy,\n            parallel_output=self.parallel_output,\n            share_embeddings_and_output_weights=self.share_embeddings_and_output_weights,\n            position_embedding_type=self.position_embedding_type,\n            rotary_percent=self.rotary_percent,\n            seq_len_interpolation_factor=self.seq_len_interpolation_factor,\n            return_embeddings=self.return_embeddings,\n            include_embeddings=self.include_embeddings,\n            pre_process=parallel_state.is_pipeline_first_stage(),\n            post_process=parallel_state.is_pipeline_last_stage(),  # set to False for inference\n            add_binary_head=do_next_sentence,\n            use_full_attention_mask=use_full_attention_mask,\n            include_hiddens=self.include_hiddens,\n            skip_logits=self.skip_logits,\n            include_input_ids=self.include_input_ids,\n        )\n        # TODO (@skothenhill) this is a hack to load the old checkpoint.\n        # This should be removed once we have a proper checkpoint conversion\n        # see NeMo/nemo/collections/llm/gpt/model/mixtral.py for how we should do it.\n        # We should eventually have an adapter for nemo1 checkpoints, HF checkpoints (at least for ESM2 @georgea)\n        # and an adapter may also be the right way to handle expected missing/extra keys when importing\n        # a checkpoint for fine-tuning (eg ignore misisng lm_head, if not there in model, etc).\n        if self.nemo1_ckpt_path is not None:\n            assert self.initial_ckpt_path is None, \"Mutually exclusive checkpoint path used twice\"\n            te_mapping = \"transformer_engine\" in self.biobert_spec_option.value\n            with tarfile.open(self.nemo1_ckpt_path, \"r\") as old_ckpt:\n                ckpt_file = old_ckpt.extractfile(\"./model_weights.ckpt\")\n                if ckpt_file is None:\n                    raise ValueError(f\"Failure to read checkpoint file: {old_ckpt}/model_weights/ckpt\")\n                old_weights = torch.load(ckpt_file)\n                new_state_dict_from_old = {}\n                for k, v in old_weights.items():\n                    new_key = nemo1_to_nemo2_biobert_key_mapping(k, new_model_prefix=\"\", te_mapping=te_mapping)\n                    new_state_dict_from_old[new_key] = v\n                # TE adds non-null ._extra_state objects to layers, which store some kind of buffer bits\n                #  so we need to allow those to pass through if we're loading from bionemo1 which did not\n                #  use TE.\n                model.load_state_dict(new_state_dict_from_old, strict=not te_mapping)\n        if self.initial_ckpt_path is not None:\n            assert self.nemo1_ckpt_path is None, \"Mutually exclusive checkpoint path used twice\"\n            self.update_model_from_checkpoint(model, self.initial_ckpt_path)\n\n        # TODO (@jstjohn) come up with a cleaner way in the biobert module to return hidden states.\n        #  maybe a suite of options like hugging face has so a user can ask for several or only one thing.\n        if self.return_only_hidden_states:\n            # this applies the final layernorm in the encoder to the hidden states which was\n            #  the default in nemo1.\n            model.post_process = False\n            model.encoder.post_process = True\n            model.encoder.post_layer_norm = True\n        return model\n\n    def get_loss_reduction_class(self) -&gt; Type[MegatronLossType]:  # noqa: D102\n        # You could optionally return a different loss reduction class here based on the config settings.\n        return self.loss_reduction_class\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.BioBertOutput","title":"<code>BioBertOutput</code>","text":"<p>               Bases: <code>BioBertOutputCore</code></p> <p>The megatron bionemo bert model inference type.</p> Source code in <code>bionemo/llm/model/biobert/model.py</code> <pre><code>class BioBertOutput(BioBertOutputCore, total=False):\n    \"\"\"The megatron bionemo bert model inference type.\"\"\"\n\n    hidden_states: Tensor\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.BioBertOutputCore","title":"<code>BioBertOutputCore</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Keys always present in the bionemo bert model inference output.</p> Source code in <code>bionemo/llm/model/biobert/model.py</code> <pre><code>class BioBertOutputCore(TypedDict):\n    \"\"\"Keys always present in the bionemo bert model inference output.\"\"\"\n\n    token_logits: Tensor\n    binary_logits: Optional[Tensor]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.MegatronBioBertModel","title":"<code>MegatronBioBertModel</code>","text":"<p>               Bases: <code>LanguageModule</code></p> <p>Transformer language model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TransformerConfig</code> <p>transformer config</p> required <code>num_tokentypes</code> <code>int</code> <p>Set to 2 when args.bert_binary_head is True, and 0 otherwise. Defaults to 0.</p> required <code>transformer_layer_spec</code> <code>ModuleSpec</code> <p>Specifies module to use for transformer layers</p> required <code>vocab_size</code> <code>int</code> <p>vocabulary size</p> required <code>max_sequence_length</code> <code>int</code> <p>maximum size of sequence. This is used for positional embedding</p> required <code>pre_process</code> <code>bool</code> <p>Include embedding layer (used with pipeline parallelism)</p> <code>True</code> <code>post_process</code> <code>bool</code> <p>Include an output layer (used with pipeline parallelism)</p> <code>True</code> <code>parallel_output</code> <code>bool</code> <p>Do not gather the outputs, keep them split across tensor parallel ranks</p> <code>True</code> <code>share_embeddings_and_output_weights</code> <code>bool</code> <p>When True, input embeddings and output logit weights are shared. Defaults to False.</p> <code>False</code> <code>position_embedding_type</code> <code>PositionEmbeddingKinds</code> <p>Position embedding type. Options [\"learned_absolute\", \"rope\"]. Defaults is 'learned_absolute'.</p> <code>'learned_absolute'</code> <code>rotary_percent</code> <code>float</code> <p>Percent of rotary dimension to use for rotary position embeddings. Defaults to 1.0 (100%). Ignored unless position_embedding_type is 'rope'.</p> <code>1.0</code> Source code in <code>bionemo/llm/model/biobert/model.py</code> <pre><code>class MegatronBioBertModel(LanguageModule):\n    \"\"\"Transformer language model.\n\n    Args:\n        config: transformer config\n        num_tokentypes: Set to 2 when args.bert_binary_head is True, and 0 otherwise. Defaults to 0.\n        transformer_layer_spec: Specifies module to use for transformer layers\n        vocab_size: vocabulary size\n        max_sequence_length: maximum size of sequence. This is used for positional embedding\n        pre_process: Include embedding layer (used with pipeline parallelism)\n        post_process: Include an output layer (used with pipeline parallelism)\n        parallel_output: Do not gather the outputs, keep them split across tensor parallel ranks\n        share_embeddings_and_output_weights: When True, input embeddings and output logit weights are shared.\n            Defaults to False.\n        position_embedding_type: Position embedding type. Options [\"learned_absolute\", \"rope\"].\n            Defaults is 'learned_absolute'.\n        rotary_percent: Percent of rotary dimension to use for rotary position embeddings.\n            Defaults to 1.0 (100%). Ignored unless position_embedding_type is 'rope'.\n    \"\"\"\n\n    def __init__(  # noqa: D107\n        self,\n        config: TransformerConfig,\n        num_tokentypes: int,\n        transformer_layer_spec: ModuleSpec,\n        vocab_size: int,\n        max_sequence_length: int,\n        tokenizer: Optional[AutoTokenizer] = None,\n        pre_process: bool = True,\n        post_process: bool = True,\n        fp16_lm_cross_entropy: bool = False,\n        parallel_output: bool = True,\n        share_embeddings_and_output_weights: bool = False,\n        position_embedding_type: PositionEmbeddingKinds = \"learned_absolute\",\n        rotary_percent: float = 1.0,\n        seq_len_interpolation_factor: Optional[float] = None,\n        add_binary_head: bool = False,\n        return_embeddings: bool = False,\n        include_embeddings: bool = False,\n        use_full_attention_mask: bool = False,\n        include_hiddens: bool = False,\n        include_input_ids: bool = False,\n        skip_logits: bool = False,  # Useful for inference time.\n    ):\n        # TODO (@jstjohn) come up with a cleaner way for this model to return a set of things the user wants.\n        #  hidden states, embeddings, logits, etc. The defaults should work for training but we need to make it\n        #  customizable and easy to tell how to make it work well for inference as well as trouble shooting.\n        #  Also make sure that everything returned that the user wants gets transposed to the b,s,h format.\n        super(MegatronBioBertModel, self).__init__(config=config)\n        self.post_process = post_process\n        self.add_binary_head = add_binary_head\n        self.skip_logits = skip_logits\n        if return_embeddings:\n            assert self.post_process, \"only return embeddings on the last pipeline stage\"\n        # `b` = batch, `s` = sequence.\n        # The old flash attention mechanism apparently wants you to use a b x 1 x s x s attention mask while\n        #  the new one wants a b x 1 x 1 x s attention mask. This is a hack to allow us to switch between the two.\n        self.use_full_attention_mask = use_full_attention_mask\n        self.config: TransformerConfig = config\n        self.transformer_layer_spec: ModuleSpec = transformer_layer_spec\n        self.vocab_size = vocab_size\n        self.max_sequence_length = max_sequence_length\n        self.tokenizer = tokenizer\n        self.pre_process = pre_process\n        self.post_process = post_process\n        self.fp16_lm_cross_entropy = fp16_lm_cross_entropy\n        self.parallel_output = parallel_output\n        self.share_embeddings_and_output_weights = share_embeddings_and_output_weights\n        self.position_embedding_type = position_embedding_type\n        self.add_binary_head = add_binary_head\n        self.return_embeddings = return_embeddings\n        self.include_embeddings = include_embeddings\n        self.include_hiddens = include_hiddens\n        self.include_input_ids = include_input_ids\n        self.skip_logits = skip_logits\n\n        # megatron core pipelining currently depends on model type\n        self.model_type = ModelType.encoder_or_decoder\n        # Embeddings.\n        if self.pre_process:\n            self.register_buffer(\n                \"bert_position_id_tensor\",\n                torch.arange(max_sequence_length, dtype=torch.long, requires_grad=False).unsqueeze(0),\n                persistent=False,\n            )\n            self.embedding = LanguageModelEmbedding(\n                config=self.config,\n                vocab_size=self.vocab_size,\n                max_sequence_length=self.max_sequence_length,\n                position_embedding_type=position_embedding_type,\n                num_tokentypes=num_tokentypes,\n            )\n\n        if self.position_embedding_type == \"rope\":\n            self.rotary_pos_emb = RotaryEmbedding(\n                kv_channels=self.config.kv_channels,\n                rotary_percent=rotary_percent,\n                rotary_interleaved=self.config.rotary_interleaved,\n                # bug in megatron: they list the type as `float` but they default to `None` so it should be `Optional[float]`\n                seq_len_interpolation_factor=seq_len_interpolation_factor,  # type: ignore\n            )\n\n        # Transformer.\n        self.encoder = TransformerBlock(\n            config=self.config,\n            spec=self.transformer_layer_spec,\n            pre_process=self.pre_process,\n            post_process=self.post_process,  # NOTE: in bionemo1 this is hard-coded to True\n        )\n\n        # Output\n        if post_process:\n            # TODO: Make sure you are passing in the mpu_vocab_size properly\n            if self.config.defer_embedding_wgrad_compute:\n                # The embedding activation buffer preserves a reference to the input activations\n                # of the final embedding projection layer GEMM. It will hold the activations for\n                # all the micro-batches of a global batch for the last pipeline stage. Once we are\n                # done with all the back props for all the microbatches for the last pipeline stage,\n                # it will be in the pipeline flush stage. During this pipeline flush we use the\n                # input activations stored in embedding activation buffer and gradient outputs\n                # stored in gradient buffer to calculate the weight gradients for the embedding\n                # final linear layer.\n                self.embedding_activation_buffer = []\n                self.grad_output_buffer = []\n            else:\n                self.embedding_activation_buffer = None\n                self.grad_output_buffer = None\n\n            self.lm_head = BertLMHead(\n                config.hidden_size,\n                config,\n            )\n\n            self.output_layer = tensor_parallel.ColumnParallelLinear(\n                config.hidden_size,\n                self.vocab_size,\n                config=config,\n                init_method=config.init_method,\n                is_expert=False,\n                bias=True,\n                skip_bias_add=False,\n                gather_output=not self.parallel_output,\n                skip_weight_param_allocation=pre_process and share_embeddings_and_output_weights,\n                embedding_activation_buffer=self.embedding_activation_buffer,\n                grad_output_buffer=self.grad_output_buffer,\n            )\n\n            self.binary_head = None\n            if self.add_binary_head:\n                # TODO: Shoudl switch this to TE ?\n                self.binary_head = get_linear_layer(\n                    config.hidden_size, 2, config.init_method, config.perform_initialization\n                )\n                self.pooler = Pooler(config.hidden_size, config.init_method, config, config.sequence_parallel)\n\n        if self.pre_process or self.post_process:\n            self.setup_embeddings_and_output_layer()\n\n    def bert_extended_attention_mask(self, attention_mask: Tensor) -&gt; Tensor:\n        \"\"\"Creates the extended attention mask\n\n        Converts the attention mask of dimension [batch size, 1, seq len] to [batch size, 1, seq len, seq len] and makes it binary\n\n        Args:\n            attention_mask (Tensor): The input attention mask\n\n        Returns:\n            Tensor: The extended binary attention mask\n        \"\"\"  # noqa: D415\n        # We create a 3D attention mask from a 2D tensor mask.\n        # [b, 1, s]\n        attention_mask_b1s = attention_mask.unsqueeze(1)\n\n        if self.use_full_attention_mask:\n            # [b, s, 1]\n            attention_mask_bs1 = attention_mask.unsqueeze(2)\n            # [b, s, s]\n            attention_mask_bss = attention_mask_b1s * attention_mask_bs1\n            # [b, 1, s, s]\n            extended_attention_mask = attention_mask_bss.unsqueeze(1)\n        else:\n            # Tensor Engine requires a 1x1xS attention mask which it internally\n            #  converts into a 1xSxS mask.\n            # [b, 1, 1, s]\n            extended_attention_mask = attention_mask_b1s.unsqueeze(1)\n\n        # Convert attention mask to binary, and flip the values from 0 to 1 and vice versa so that\n        #  extended_attention_mask._mask_fill(-1000) that megatron does internally result in\n        #  masking out pad positions.\n        extended_attention_mask = extended_attention_mask &lt; 0.5\n\n        return extended_attention_mask\n\n    def bert_position_ids(self, token_ids):  # noqa: D102\n        # Create position ids\n        seq_length = token_ids.size(1)\n        if seq_length != self.max_sequence_length:\n            return self.bert_position_id_tensor[:, :seq_length]\n        return self.bert_position_id_tensor  # No need to subset so skip the slice op\n\n    def embedding_forward(\n        self,\n        input_ids: Tensor,\n        position_ids: Tensor,\n        tokentype_ids: Optional[Tensor] = None,\n        attention_mask: Optional[Tensor] = None,\n    ) -&gt; Tensor:\n        \"\"\"Produce embeddings.\"\"\"\n        return self.embedding(input_ids=input_ids, position_ids=position_ids, tokentype_ids=tokentype_ids)\n\n    def set_input_tensor(self, input_tensor: Tensor | list[Tensor]) -&gt; None:\n        \"\"\"Sets input tensor to the model.\n\n        See megatron.model.transformer.set_input_tensor()\n\n        Args:\n            input_tensor: Sets the input tensor for the model.\n\n        Raises:\n            ValueError: Iff the input tensor is a list that doesn't have exactly 1 tensor.\n        \"\"\"\n        # This is usually handled in schedules.py but some inference code still gives us non-lists or None.\n        if isinstance(input_tensor, list):\n            if len(input_tensor) != 1:\n                raise ValueError(f\"input_tensor should only be length 1 for gpt/bert, not length: {len(input_tensor)}\")\n            single_input_tensor: Tensor = input_tensor[0]\n        else:\n            single_input_tensor = input_tensor\n        self.encoder.set_input_tensor(single_input_tensor)\n\n    def forward(\n        self,\n        input_ids: Tensor,\n        attention_mask: Tensor,\n        tokentype_ids: Optional[Tensor] = None,\n        lm_labels: Optional[Tensor] = None,\n        inference_params: Any | None = None,\n        runtime_gather_output: Optional[bool] = None,\n    ) -&gt; BioBertOutput | Tensor:\n        \"\"\"Forward function of BERT model\n\n        Forward function of the BERT Model This function passes the input tensors\n        through the embedding layer, and then the encoder and finally into the post\n        processing layer (optional).\n\n        It either returns the Loss values if labels are given or the final hidden units.\n        \"\"\"  # noqa: D415\n        # TODO! If we upgrade to TE 1.7 why does bit flipping back to 1 help the loss in TE 1.7? It claimed that they now follow standards, did\n        #  nemo/megatron flip again internally to be compatible wtih TE somewhere?\n        #  change the following line to ~self.bert... and see if it helps if we upgrade to TE 1.7 and NeMo/Megatron have not compensated.\n        extended_attention_mask = self.bert_extended_attention_mask(attention_mask)\n\n        if parallel_state.is_pipeline_first_stage():\n            using_input_ids: Optional[Tensor] = input_ids\n            using_position_ids: Optional[Tensor] = self.bert_position_ids(input_ids)\n        else:\n            using_input_ids = None\n            using_position_ids = None\n\n        # Encoder embedding.\n        if self.pre_process:\n            encoder_input: Optional[Tensor] = self.embedding_forward(\n                input_ids=using_input_ids,\n                position_ids=using_position_ids,\n                tokentype_ids=tokentype_ids,\n                attention_mask=attention_mask,\n            )\n        else:\n            # intermediate stage of pipeline\n            # encoder will get hidden_states from encoder.input_tensor\n            encoder_input = None\n\n        # Rotary positional embeddings (Why not move this into BERT/GPTEmberdding ?)\n        rotary_pos_emb = None\n        if self.position_embedding_type == \"rope\":\n            rotary_seq_len = self.rotary_pos_emb.get_rotary_seq_len(\n                inference_params,\n                self.encoder,\n                encoder_input,\n                self.config,\n                packed_seq_params=None,  # TODO @sichu: upstream to Megatron-LM\n            )\n            rotary_pos_emb = self.rotary_pos_emb(rotary_seq_len)\n\n        # Run encoder.\n        hidden_states = self.encoder(\n            hidden_states=encoder_input,\n            attention_mask=extended_attention_mask,\n            inference_params=inference_params,\n            rotary_pos_emb=rotary_pos_emb,\n        )\n\n        if not self.post_process:\n            return hidden_states\n\n        if self.add_binary_head:\n            pooled_output = self.pooler(hidden_states, 0)\n\n        if self.return_embeddings or self.include_embeddings:\n            embeddings = torch.transpose(hidden_states, 0, 1)\n            masks = torch.sum(attention_mask, dim=1)\n            # Collect masked embeddings.\n            output_embeddings = torch.zeros(\n                size=(embeddings.shape[0], embeddings.shape[2]),\n                dtype=embeddings.dtype,\n                device=torch.cuda.current_device(),\n            )\n            for i, (embedding, mask) in enumerate(zip(embeddings, masks)):\n                output_embeddings[i, :] = torch.mean(embedding[1 : mask - 1], dim=0)\n\n        if self.return_embeddings:\n            return output_embeddings\n\n        # logits and loss\n        output_weight = None\n        if self.share_embeddings_and_output_weights:\n            output_weight = self.shared_embedding_or_output_weight()\n\n        hidden_states_after_lm_head = self.lm_head(hidden_states=hidden_states)\n        if not self.skip_logits:\n            # TODO add , runtime_gather_output=runtime_gather_output once supported in ColumnParallelLinear\n            logits, _ = self.output_layer(hidden_states_after_lm_head, weight=output_weight)\n        else:\n            logits = None\n\n        binary_logits = None\n        if self.binary_head is not None:\n            binary_logits = self.binary_head(pooled_output)\n\n        output = {\"token_logits\": logits, \"binary_logits\": binary_logits}\n        if self.include_hiddens:\n            output[\"hidden_states\"] = hidden_states.transpose(0, 1).contiguous()  # [s b h] =&gt; [b s h]\n        if self.include_input_ids:\n            output[\"input_ids\"] = input_ids\n        if self.include_embeddings:\n            output[\"embeddings\"] = output_embeddings\n        return output\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.MegatronBioBertModel.bert_extended_attention_mask","title":"<code>bert_extended_attention_mask(attention_mask)</code>","text":"<p>Creates the extended attention mask</p> <p>Converts the attention mask of dimension [batch size, 1, seq len] to [batch size, 1, seq len, seq len] and makes it binary</p> <p>Parameters:</p> Name Type Description Default <code>attention_mask</code> <code>Tensor</code> <p>The input attention mask</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The extended binary attention mask</p> Source code in <code>bionemo/llm/model/biobert/model.py</code> <pre><code>def bert_extended_attention_mask(self, attention_mask: Tensor) -&gt; Tensor:\n    \"\"\"Creates the extended attention mask\n\n    Converts the attention mask of dimension [batch size, 1, seq len] to [batch size, 1, seq len, seq len] and makes it binary\n\n    Args:\n        attention_mask (Tensor): The input attention mask\n\n    Returns:\n        Tensor: The extended binary attention mask\n    \"\"\"  # noqa: D415\n    # We create a 3D attention mask from a 2D tensor mask.\n    # [b, 1, s]\n    attention_mask_b1s = attention_mask.unsqueeze(1)\n\n    if self.use_full_attention_mask:\n        # [b, s, 1]\n        attention_mask_bs1 = attention_mask.unsqueeze(2)\n        # [b, s, s]\n        attention_mask_bss = attention_mask_b1s * attention_mask_bs1\n        # [b, 1, s, s]\n        extended_attention_mask = attention_mask_bss.unsqueeze(1)\n    else:\n        # Tensor Engine requires a 1x1xS attention mask which it internally\n        #  converts into a 1xSxS mask.\n        # [b, 1, 1, s]\n        extended_attention_mask = attention_mask_b1s.unsqueeze(1)\n\n    # Convert attention mask to binary, and flip the values from 0 to 1 and vice versa so that\n    #  extended_attention_mask._mask_fill(-1000) that megatron does internally result in\n    #  masking out pad positions.\n    extended_attention_mask = extended_attention_mask &lt; 0.5\n\n    return extended_attention_mask\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.MegatronBioBertModel.embedding_forward","title":"<code>embedding_forward(input_ids, position_ids, tokentype_ids=None, attention_mask=None)</code>","text":"<p>Produce embeddings.</p> Source code in <code>bionemo/llm/model/biobert/model.py</code> <pre><code>def embedding_forward(\n    self,\n    input_ids: Tensor,\n    position_ids: Tensor,\n    tokentype_ids: Optional[Tensor] = None,\n    attention_mask: Optional[Tensor] = None,\n) -&gt; Tensor:\n    \"\"\"Produce embeddings.\"\"\"\n    return self.embedding(input_ids=input_ids, position_ids=position_ids, tokentype_ids=tokentype_ids)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.MegatronBioBertModel.forward","title":"<code>forward(input_ids, attention_mask, tokentype_ids=None, lm_labels=None, inference_params=None, runtime_gather_output=None)</code>","text":"<p>Forward function of BERT model</p> <p>Forward function of the BERT Model This function passes the input tensors through the embedding layer, and then the encoder and finally into the post processing layer (optional).</p> <p>It either returns the Loss values if labels are given or the final hidden units.</p> Source code in <code>bionemo/llm/model/biobert/model.py</code> <pre><code>def forward(\n    self,\n    input_ids: Tensor,\n    attention_mask: Tensor,\n    tokentype_ids: Optional[Tensor] = None,\n    lm_labels: Optional[Tensor] = None,\n    inference_params: Any | None = None,\n    runtime_gather_output: Optional[bool] = None,\n) -&gt; BioBertOutput | Tensor:\n    \"\"\"Forward function of BERT model\n\n    Forward function of the BERT Model This function passes the input tensors\n    through the embedding layer, and then the encoder and finally into the post\n    processing layer (optional).\n\n    It either returns the Loss values if labels are given or the final hidden units.\n    \"\"\"  # noqa: D415\n    # TODO! If we upgrade to TE 1.7 why does bit flipping back to 1 help the loss in TE 1.7? It claimed that they now follow standards, did\n    #  nemo/megatron flip again internally to be compatible wtih TE somewhere?\n    #  change the following line to ~self.bert... and see if it helps if we upgrade to TE 1.7 and NeMo/Megatron have not compensated.\n    extended_attention_mask = self.bert_extended_attention_mask(attention_mask)\n\n    if parallel_state.is_pipeline_first_stage():\n        using_input_ids: Optional[Tensor] = input_ids\n        using_position_ids: Optional[Tensor] = self.bert_position_ids(input_ids)\n    else:\n        using_input_ids = None\n        using_position_ids = None\n\n    # Encoder embedding.\n    if self.pre_process:\n        encoder_input: Optional[Tensor] = self.embedding_forward(\n            input_ids=using_input_ids,\n            position_ids=using_position_ids,\n            tokentype_ids=tokentype_ids,\n            attention_mask=attention_mask,\n        )\n    else:\n        # intermediate stage of pipeline\n        # encoder will get hidden_states from encoder.input_tensor\n        encoder_input = None\n\n    # Rotary positional embeddings (Why not move this into BERT/GPTEmberdding ?)\n    rotary_pos_emb = None\n    if self.position_embedding_type == \"rope\":\n        rotary_seq_len = self.rotary_pos_emb.get_rotary_seq_len(\n            inference_params,\n            self.encoder,\n            encoder_input,\n            self.config,\n            packed_seq_params=None,  # TODO @sichu: upstream to Megatron-LM\n        )\n        rotary_pos_emb = self.rotary_pos_emb(rotary_seq_len)\n\n    # Run encoder.\n    hidden_states = self.encoder(\n        hidden_states=encoder_input,\n        attention_mask=extended_attention_mask,\n        inference_params=inference_params,\n        rotary_pos_emb=rotary_pos_emb,\n    )\n\n    if not self.post_process:\n        return hidden_states\n\n    if self.add_binary_head:\n        pooled_output = self.pooler(hidden_states, 0)\n\n    if self.return_embeddings or self.include_embeddings:\n        embeddings = torch.transpose(hidden_states, 0, 1)\n        masks = torch.sum(attention_mask, dim=1)\n        # Collect masked embeddings.\n        output_embeddings = torch.zeros(\n            size=(embeddings.shape[0], embeddings.shape[2]),\n            dtype=embeddings.dtype,\n            device=torch.cuda.current_device(),\n        )\n        for i, (embedding, mask) in enumerate(zip(embeddings, masks)):\n            output_embeddings[i, :] = torch.mean(embedding[1 : mask - 1], dim=0)\n\n    if self.return_embeddings:\n        return output_embeddings\n\n    # logits and loss\n    output_weight = None\n    if self.share_embeddings_and_output_weights:\n        output_weight = self.shared_embedding_or_output_weight()\n\n    hidden_states_after_lm_head = self.lm_head(hidden_states=hidden_states)\n    if not self.skip_logits:\n        # TODO add , runtime_gather_output=runtime_gather_output once supported in ColumnParallelLinear\n        logits, _ = self.output_layer(hidden_states_after_lm_head, weight=output_weight)\n    else:\n        logits = None\n\n    binary_logits = None\n    if self.binary_head is not None:\n        binary_logits = self.binary_head(pooled_output)\n\n    output = {\"token_logits\": logits, \"binary_logits\": binary_logits}\n    if self.include_hiddens:\n        output[\"hidden_states\"] = hidden_states.transpose(0, 1).contiguous()  # [s b h] =&gt; [b s h]\n    if self.include_input_ids:\n        output[\"input_ids\"] = input_ids\n    if self.include_embeddings:\n        output[\"embeddings\"] = output_embeddings\n    return output\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.MegatronBioBertModel.set_input_tensor","title":"<code>set_input_tensor(input_tensor)</code>","text":"<p>Sets input tensor to the model.</p> <p>See megatron.model.transformer.set_input_tensor()</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor | list[Tensor]</code> <p>Sets the input tensor for the model.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Iff the input tensor is a list that doesn't have exactly 1 tensor.</p> Source code in <code>bionemo/llm/model/biobert/model.py</code> <pre><code>def set_input_tensor(self, input_tensor: Tensor | list[Tensor]) -&gt; None:\n    \"\"\"Sets input tensor to the model.\n\n    See megatron.model.transformer.set_input_tensor()\n\n    Args:\n        input_tensor: Sets the input tensor for the model.\n\n    Raises:\n        ValueError: Iff the input tensor is a list that doesn't have exactly 1 tensor.\n    \"\"\"\n    # This is usually handled in schedules.py but some inference code still gives us non-lists or None.\n    if isinstance(input_tensor, list):\n        if len(input_tensor) != 1:\n            raise ValueError(f\"input_tensor should only be length 1 for gpt/bert, not length: {len(input_tensor)}\")\n        single_input_tensor: Tensor = input_tensor[0]\n    else:\n        single_input_tensor = input_tensor\n    self.encoder.set_input_tensor(single_input_tensor)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/biobert/testing_utils/","title":"Testing utils","text":""},{"location":"main/references/API_reference/bionemo/llm/model/biobert/testing_utils/#bionemo.llm.model.biobert.testing_utils.compute_biobert_loss_singlegpu","title":"<code>compute_biobert_loss_singlegpu(trainer, pl_module)</code>","text":"<p>Computes the loss for BioBert models on a single GPU.</p> <p>This will not function in multi-gpu settings nor with models that do not conform to BioBert.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The Lightning Trainer object.</p> required <code>pl_module</code> <code>LightningModule</code> <p>The LightningModule being trained.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The mean loss.</p> <p>See Also: - :class: BioBertModel</p> Source code in <code>bionemo/llm/model/biobert/testing_utils.py</code> <pre><code>def compute_biobert_loss_singlegpu(trainer: pl.Trainer, pl_module: pl.LightningModule):\n    \"\"\"Computes the loss for BioBert models on a single GPU.\n\n    This will not function in multi-gpu settings nor with models that do not conform to BioBert.\n\n    Args:\n        trainer (pl.Trainer): The Lightning Trainer object.\n        pl_module (pl.LightningModule): The LightningModule being trained.\n\n    Returns:\n        float: The mean loss.\n\n    See Also:\n    - :class: BioBertModel\n    \"\"\"\n    model = pl_module\n    dl = trainer.datamodule.val_dataloader()\n\n    n, loss = -1, 0.0\n    model.eval()\n    # batch = next(iter(dl))\n    batch = model.data_step(iter(dl))\n    result = model(\n        input_ids=batch[\"text\"].cuda(),  # 'tokens' also a valid input for MockGPTDataModule\n        attention_mask=batch[\"attention_mask\"].cuda(),\n    )\n    loss_mask = batch[\"loss_mask\"].cuda()\n    # Not guaranteed i guess?\n    logits = result[\"token_logits\"]\n    target = batch[\"labels\"].cuda()\n    loss += F.cross_entropy(logits[loss_mask].float(), target[loss_mask], reduction=\"sum\")\n    n += loss_mask.sum()\n\n    mean_loss: float = (loss / n).detach().cpu().numpy().item()\n    model.train()\n    return mean_loss\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/biobert/transformer_specs/","title":"Transformer specs","text":""},{"location":"main/references/API_reference/bionemo/llm/model/biobert/transformer_specs/#bionemo.llm.model.biobert.transformer_specs.BiobertSpecOption","title":"<code>BiobertSpecOption</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Options for the BiobertSpec. The spec defines the architecture of the transformer (BERT) block in the biobert model. This is a <code>str, Enum</code> type so that argparse can use the string names as choices.</p> Source code in <code>bionemo/llm/model/biobert/transformer_specs.py</code> <pre><code>class BiobertSpecOption(str, Enum):\n    \"\"\"Options for the BiobertSpec. The spec defines the architecture of the transformer (BERT) block in the biobert model.\n    This is a `str, Enum` type so that argparse can use the string names as choices.\n    \"\"\"  # noqa: D205\n\n    bert_layer_local_spec = \"bert_layer_local_spec\"\n    bert_layer_local_spec_with_qk_ln = \"bert_layer_local_spec_with_qk_ln\"\n    bert_layer_with_transformer_engine_spec = \"bert_layer_with_transformer_engine_spec\"\n    bert_layer_with_transformer_engine_and_qk_ln_spec = \"bert_layer_with_transformer_engine_and_qk_ln_spec\"\n    # ESM2 spec\n    esm2_bert_layer_local_spec = \"esm2_bert_layer_local_spec\"\n    esm2_bert_layer_with_transformer_engine_spec = \"esm2_bert_layer_with_transformer_engine_spec\"\n    amplify_with_transformer_engine_spec = \"amplify_with_transformer_engine_spec\"\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/model/biobert/transformer_specs/#bionemo.llm.model.biobert.transformer_specs.get_biobert_spec","title":"<code>get_biobert_spec(biobert_spec_option, qk_layernorm=False, core_attention=None)</code>","text":"<p>Get the spec for the Biobert model.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>ModelType</code> <p>The model type.</p> required <code>spec_option</code> <code>BiobertSpecOption</code> <p>The spec option.</p> required <p>Returns:</p> Name Type Description <code>TransformerConfig</code> <code>ModuleSpec</code> <p>The Biobert spec.</p> Source code in <code>bionemo/llm/model/biobert/transformer_specs.py</code> <pre><code>def get_biobert_spec(  # noqa: D417\n    biobert_spec_option: BiobertSpecOption,\n    qk_layernorm: bool = False,\n    core_attention: Optional[Type[Module]] = None,\n) -&gt; spec_utils.ModuleSpec:\n    \"\"\"Get the spec for the Biobert model.\n\n    Args:\n        model_type (ModelType): The model type.\n        spec_option (BiobertSpecOption): The spec option.\n\n    Returns:\n        TransformerConfig: The Biobert spec.\n    \"\"\"\n    #\n    # BEGIN define several specs that are a function of `qk_layernorm`\n    #\n\n    match biobert_spec_option:\n        case BiobertSpecOption.bert_layer_local_spec:\n            return bert_layer_specs.bert_layer_local_spec\n\n        case BiobertSpecOption.bert_layer_local_spec_with_qk_ln:\n            # Use this spec for an implementation using only modules in megatron core\n\n            if core_attention is None:\n                core_attention = DotProductAttention\n\n            bert_layer_local_spec_with_qk_ln = spec_utils.ModuleSpec(\n                module=TransformerLayer,\n                submodules=TransformerLayerSubmodules(\n                    input_layernorm=FusedLayerNorm,\n                    self_attention=spec_utils.ModuleSpec(\n                        module=SelfAttention,\n                        params={\"attn_mask_type\": AttnMaskType.padding},\n                        submodules=SelfAttentionSubmodules(\n                            linear_qkv=ColumnParallelLinear,\n                            core_attention=core_attention,\n                            linear_proj=RowParallelLinear,\n                            q_layernorm=FusedLayerNorm if qk_layernorm else IdentityOp,\n                            k_layernorm=FusedLayerNorm if qk_layernorm else IdentityOp,\n                        ),\n                    ),\n                    self_attn_bda=get_bias_dropout_add,\n                    pre_mlp_layernorm=FusedLayerNorm,\n                    mlp=spec_utils.ModuleSpec(\n                        module=MLP,\n                        submodules=MLPSubmodules(\n                            linear_fc1=ColumnParallelLinear,\n                            linear_fc2=RowParallelLinear,\n                        ),\n                    ),\n                    mlp_bda=get_bias_dropout_add,\n                    sharded_state_dict_keys_map={\n                        \"input_layernorm.\": \"self_attention.linear_qkv.layer_norm_\",\n                        \"pre_mlp_layernorm.\": \"mlp.linear_fc1.layer_norm_\",\n                    },\n                ),\n            )\n            return bert_layer_local_spec_with_qk_ln\n\n        case BiobertSpecOption.bert_layer_with_transformer_engine_spec:\n            return bert_layer_specs.bert_layer_with_transformer_engine_spec\n\n        case BiobertSpecOption.bert_layer_with_transformer_engine_and_qk_ln_spec:\n            if core_attention is None:\n                core_attention = TEDotProductAttention\n\n            bert_layer_with_transformer_engine_and_qk_ln_spec = spec_utils.ModuleSpec(\n                module=TransformerLayer,\n                submodules=TransformerLayerSubmodules(\n                    self_attention=spec_utils.ModuleSpec(\n                        module=SelfAttention,\n                        params={\"attn_mask_type\": AttnMaskType.padding},\n                        submodules=SelfAttentionSubmodules(\n                            linear_qkv=TELayerNormColumnParallelLinear,\n                            core_attention=core_attention,\n                            linear_proj=TERowParallelLinear,\n                            q_layernorm=TELayerNorm if qk_layernorm else IdentityOp,\n                            k_layernorm=TELayerNorm if qk_layernorm else IdentityOp,\n                        ),\n                    ),\n                    self_attn_bda=get_bias_dropout_add,\n                    mlp=spec_utils.ModuleSpec(\n                        module=MLP,\n                        submodules=MLPSubmodules(\n                            linear_fc1=TELayerNormColumnParallelLinear,\n                            linear_fc2=TERowParallelLinear,\n                        ),\n                    ),\n                    mlp_bda=get_bias_dropout_add,\n                ),\n            )\n            return bert_layer_with_transformer_engine_and_qk_ln_spec\n\n        case BiobertSpecOption.esm2_bert_layer_local_spec:\n            if core_attention is None:\n                raise ValueError(f\"Must supply core_attention with {BiobertSpecOption.esm2_bert_layer_local_spec} !\")\n\n            esm2_bert_layer_local_spec = spec_utils.ModuleSpec(\n                module=TransformerLayer,\n                submodules=TransformerLayerSubmodules(\n                    input_layernorm=FusedLayerNorm,\n                    self_attention=spec_utils.ModuleSpec(\n                        module=SelfAttention,\n                        params={\"attn_mask_type\": AttnMaskType.padding},\n                        submodules=SelfAttentionSubmodules(\n                            linear_qkv=ColumnParallelLinear,\n                            core_attention=core_attention,\n                            linear_proj=RowParallelLinear,\n                            q_layernorm=ESM2QueryScaling,\n                            k_layernorm=IdentityOp,\n                        ),\n                    ),\n                    self_attn_bda=get_bias_dropout_add,\n                    pre_mlp_layernorm=FusedLayerNorm,\n                    mlp=spec_utils.ModuleSpec(\n                        module=MLP,\n                        submodules=MLPSubmodules(\n                            linear_fc1=ColumnParallelLinear,\n                            linear_fc2=RowParallelLinear,\n                        ),\n                    ),\n                    mlp_bda=get_bias_dropout_add,\n                    sharded_state_dict_keys_map={\n                        \"input_layernorm.\": \"self_attention.linear_qkv.layer_norm_\",\n                        \"pre_mlp_layernorm.\": \"mlp.linear_fc1.layer_norm_\",\n                    },\n                ),\n            )\n            return esm2_bert_layer_local_spec\n\n        case BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec:\n            if core_attention is None:\n                core_attention = TEDotProductAttention\n\n            esm2_bert_layer_local_spec = spec_utils.ModuleSpec(\n                module=TransformerLayer,\n                submodules=TransformerLayerSubmodules(\n                    self_attention=spec_utils.ModuleSpec(\n                        module=SelfAttention,\n                        params={\"attn_mask_type\": AttnMaskType.padding},\n                        submodules=SelfAttentionSubmodules(\n                            linear_qkv=TELayerNormColumnParallelLinear,\n                            core_attention=core_attention,\n                            linear_proj=TERowParallelLinear,\n                            q_layernorm=ESM2QueryScaling,\n                            k_layernorm=IdentityOp,\n                        ),\n                    ),\n                    self_attn_bda=get_bias_dropout_add,\n                    mlp=spec_utils.ModuleSpec(\n                        module=MLP,\n                        submodules=MLPSubmodules(\n                            linear_fc1=TELayerNormColumnParallelLinear,\n                            linear_fc2=TERowParallelLinear,\n                        ),\n                    ),\n                    mlp_bda=get_bias_dropout_add,\n                ),\n            )\n            return esm2_bert_layer_local_spec\n\n        case BiobertSpecOption.amplify_with_transformer_engine_spec:\n            if core_attention is None:\n                core_attention = TEDotProductAttention\n\n            amplify_with_transformer_engine_spec = spec_utils.ModuleSpec(\n                module=TransformerLayer,\n                submodules=TransformerLayerSubmodules(\n                    self_attention=spec_utils.ModuleSpec(\n                        module=SelfAttention,\n                        params={\"attn_mask_type\": AttnMaskType.padding},\n                        submodules=SelfAttentionSubmodules(\n                            linear_qkv=TELayerNormColumnParallelLinear,\n                            core_attention=core_attention,\n                            linear_proj=TERowParallelLinear,\n                            q_layernorm=IdentityOp,\n                            k_layernorm=IdentityOp,\n                        ),\n                    ),\n                    self_attn_bda=get_bias_dropout_add,\n                    mlp=spec_utils.ModuleSpec(\n                        module=MLP,\n                        submodules=MLPSubmodules(\n                            linear_fc1=TELayerNormColumnParallelLinear,\n                            linear_fc2=TERowParallelLinear,\n                        ),\n                    ),\n                    mlp_bda=get_bias_dropout_add,\n                ),\n            )\n            return amplify_with_transformer_engine_spec\n\n        case _:\n            raise NotImplementedError(f\"Spec option {biobert_spec_option} not implemented\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/run/config_models/","title":"Config models","text":""},{"location":"main/references/API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.DataConfig","title":"<code>DataConfig</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[DataModuleT]</code>, <code>ABC</code></p> <p>Base class for all data configurations.</p> <p>This class is used to define the interface for all data configurations. It is used to define the data module that will be used in the training loop.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>class DataConfig(BaseModel, Generic[DataModuleT], ABC):\n    \"\"\"Base class for all data configurations.\n\n    This class is used to define the interface for all data configurations. It is used to define the data module that\n    will be used in the training loop.\n    \"\"\"\n\n    micro_batch_size: int = 8\n    result_dir: str | pathlib.Path = \"./results\"\n    num_dataset_workers: int = 0\n    seq_length: int = 128\n\n    @field_serializer(\"result_dir\")\n    def serialize_paths(self, value: pathlib.Path) -&gt; str:  # noqa: D102\n        return serialize_path_or_str(value)\n\n    @field_validator(\"result_dir\")\n    def deserialize_paths(cls, value: str) -&gt; pathlib.Path:  # noqa: D102\n        return deserialize_str_to_path(value)\n\n    @abstractmethod\n    def construct_data_module(self, global_batch_size: int) -&gt; DataModuleT:\n        \"\"\"Construct the data module from the configuration. Cannot be defined generically.\"\"\"\n        ...\n\n    def custom_model_validator(self, global_cfg: \"MainConfig\") -&gt; \"MainConfig\":\n        \"\"\"Use custom implementation of this method to define the things inside global_config.\n\n        The following expression will always be true:\n\n        global_cfg.data_config == self\n        \"\"\"\n        return global_cfg\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.DataConfig.construct_data_module","title":"<code>construct_data_module(global_batch_size)</code>  <code>abstractmethod</code>","text":"<p>Construct the data module from the configuration. Cannot be defined generically.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@abstractmethod\ndef construct_data_module(self, global_batch_size: int) -&gt; DataModuleT:\n    \"\"\"Construct the data module from the configuration. Cannot be defined generically.\"\"\"\n    ...\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.DataConfig.custom_model_validator","title":"<code>custom_model_validator(global_cfg)</code>","text":"<p>Use custom implementation of this method to define the things inside global_config.</p> <p>The following expression will always be true:</p> <p>global_cfg.data_config == self</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>def custom_model_validator(self, global_cfg: \"MainConfig\") -&gt; \"MainConfig\":\n    \"\"\"Use custom implementation of this method to define the things inside global_config.\n\n    The following expression will always be true:\n\n    global_cfg.data_config == self\n    \"\"\"\n    return global_cfg\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExperimentConfig","title":"<code>ExperimentConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration class for setting up and managing experiment parameters.</p> <p>Attributes:</p> Name Type Description <code>save_every_n_steps</code> <code>int</code> <p>Number of steps between saving checkpoints.</p> <code>result_dir</code> <code>str | Path</code> <p>Directory where results will be saved.</p> <code>experiment_name</code> <code>str</code> <p>Name of the experiment.</p> <code>restore_from_checkpoint_path</code> <code>Optional[str]</code> <p>Path to restore from a checkpoint. Note: This does not invoke the checkpoint callback as expected.</p> <code>save_last_checkpoint</code> <code>bool</code> <p>Flag to save the last checkpoint. Default is True.</p> <code>metric_to_monitor_for_checkpoints</code> <code>str</code> <p>Metric to monitor for saving top-k checkpoints. Default is \"reduced_train_loss\".</p> <code>save_top_k</code> <code>int</code> <p>Number of top checkpoints to save based on the monitored metric. Default is 2.</p> <code>create_tensorboard_logger</code> <code>bool</code> <p>Flag to create a TensorBoard logger. Default is False.</p> <code>create_checkpoint_callback</code> <code>bool</code> <p>Flag to create a ModelCheckpoint callback</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>class ExperimentConfig(BaseModel):\n    \"\"\"Configuration class for setting up and managing experiment parameters.\n\n    Attributes:\n        save_every_n_steps (int): Number of steps between saving checkpoints.\n        result_dir (str | pathlib.Path): Directory where results will be saved.\n        experiment_name (str): Name of the experiment.\n        restore_from_checkpoint_path (Optional[str]): Path to restore from a checkpoint. Note: This does not invoke the checkpoint callback as expected.\n        save_last_checkpoint (bool): Flag to save the last checkpoint. Default is True.\n        metric_to_monitor_for_checkpoints (str): Metric to monitor for saving top-k checkpoints. Default is \"reduced_train_loss\".\n        save_top_k (int): Number of top checkpoints to save based on the monitored metric. Default is 2.\n        create_tensorboard_logger (bool): Flag to create a TensorBoard logger. Default is False.\n        create_checkpoint_callback (bool): Flag to create a ModelCheckpoint callback\n    \"\"\"\n\n    save_every_n_steps: int\n    result_dir: str | pathlib.Path\n    experiment_name: str\n    # NOTE: restore_from_checkpoint_path does not invoke the checkpoint callback in the way we'd like. Avoid using.\n    restore_from_checkpoint_path: Optional[str]\n    save_last_checkpoint: bool = True\n    metric_to_monitor_for_checkpoints: str = \"reduced_train_loss\"\n    save_top_k: int = 2\n    create_tensorboard_logger: bool = False\n    create_checkpoint_callback: bool = True\n\n    @field_serializer(\"result_dir\")\n    def serialize_paths(self, value: pathlib.Path) -&gt; str:  # noqa: D102\n        return serialize_path_or_str(value)\n\n    @field_validator(\"result_dir\")\n    def deserialize_paths(cls, value: str) -&gt; pathlib.Path:  # noqa: D102\n        return deserialize_str_to_path(value)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExposedModelConfig","title":"<code>ExposedModelConfig</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[ModelConfigT]</code>, <code>ABC</code></p> <p>BioNeMo model configuration class, wraps TransformerConfig and friends.</p> <p>This class is used to define the interface for all model configurations. It is Exposed to guard against ill-typed or poorly defined fields in the underlying configuration objects. <code>ModelConfigT</code> declares the associated type of the underlying config (most commonly a BioBertGenericConfig, but could also be a TransformerConfig or something similar). Children should try to expose the minimal set of fields necessary for the user to configure the model while keeping the more esoteric configuration private to the underlying ModelConfigT.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>class ExposedModelConfig(BaseModel, Generic[ModelConfigT], ABC):\n    \"\"\"BioNeMo model configuration class, wraps TransformerConfig and friends.\n\n    This class is used to define the interface for all model configurations. It is **Exposed** to guard against ill-typed\n    or poorly defined fields in the underlying configuration objects. `ModelConfigT` declares the associated type of the\n    underlying config (most commonly a BioBertGenericConfig, but could also be a TransformerConfig or something similar).\n    Children should try to expose the minimal set of fields necessary for the user to configure the model while keeping\n    the more esoteric configuration private to the underlying ModelConfigT.\n    \"\"\"\n\n    # Restores weights from a pretrained checkpoint\n    initial_ckpt_path: Optional[str] = None\n    # Does not attempt to load keys with these prefixes (useful if you attached extra parameters and still want to load a set of weights)\n    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=list)\n\n    # Pydantic stuff to allow arbitrary types + validators + serializers\n    class Config:  # noqa: D106\n        arbitrary_types_allowed = True\n\n    def model_class(self) -&gt; Type[ModelConfigT]:\n        \"\"\"Returns the underlying model class that this config wraps.\"\"\"\n        raise NotImplementedError\n\n    def custom_model_validator(self, global_cfg: \"MainConfig\") -&gt; \"MainConfig\":\n        \"\"\"Use custom implementation of this method to define the things inside global_config.\n\n        The following expression will always be true:\n\n        global_cfg.bionemo_model_config == self\n        \"\"\"\n        return global_cfg\n\n    def exposed_to_internal_bionemo_model_config(self) -&gt; ModelConfigT:\n        \"\"\"Converts the exposed dataclass to the underlying Transformer config.\n\n        The underlying ModelConfigT may both be incomplete and unserializable. We use this transformation as a way to\n        hide fields that are either not serializable by Pydantic or that we do not want to expose.\n        \"\"\"\n        cls: Type[ModelConfigT] = self.model_class()\n        model_dict = {}\n        for attr in self.model_fields:\n            if attr not in model_dict and attr in cls.__dataclass_fields__:\n                model_dict[attr] = getattr(self, attr)\n\n        # Now set fp16 and bf16 based on the precision for the underlying TransformerConfig=&gt;ParallelConfig\n        #   the only constraint is that both must not be true.\n        model_dict[\"bf16\"] = self.pipeline_dtype == dtypes.precision_to_dtype[\"bf16-mixed\"]\n        model_dict[\"fp16\"] = self.pipeline_dtype == dtypes.precision_to_dtype[\"16-mixed\"]\n        result = cls(**model_dict)\n\n        return result\n\n    # NOTE: See PrecisionTypes for a list of valid literals that may be deserialized.\n    params_dtype: torch.dtype\n    pipeline_dtype: torch.dtype\n    autocast_dtype: torch.dtype\n\n    num_layers: int = 6\n    hidden_size: int = 256\n    ffn_hidden_size: int = 512\n    num_attention_heads: int = 4\n    seq_length: int = 512\n    fp32_residual_connection: bool = False\n    hidden_dropout: float = 0.02\n    init_method_std: float = 0.02\n    kv_channels: Optional[int] = None\n    apply_query_key_layer_scaling: bool = False\n    make_vocab_size_divisible_by: int = 128\n    masked_softmax_fusion: bool = True\n    fp16_lm_cross_entropy: bool = False\n    gradient_accumulation_fusion: bool = False\n    layernorm_zero_centered_gamma: bool = False\n    layernorm_epsilon: float = 1.0e-12\n    activation_func: Callable[[torch.Tensor, Any], torch.Tensor] = F.gelu\n    qk_layernorm: bool = False\n    apply_residual_connection_post_layernorm: bool = False\n    bias_activation_fusion: bool = True\n    bias_dropout_fusion: bool = True\n    get_attention_mask_from_fusion: bool = False\n    attention_dropout: float = 0.1\n    share_embeddings_and_output_weights: bool = True\n    enable_autocast: bool = False\n    nemo1_ckpt_path: Optional[str] = None\n    biobert_spec_option: BiobertSpecOption = BiobertSpecOption.bert_layer_with_transformer_engine_spec\n\n    @field_serializer(\"biobert_spec_option\")\n    def serialize_spec_option(self, value: BiobertSpecOption) -&gt; str:  # noqa: D102\n        return value.value\n\n    @field_validator(\"biobert_spec_option\", mode=\"before\")\n    def deserialize_spec_option(cls, value: str) -&gt; BiobertSpecOption:  # noqa: D102\n        return BiobertSpecOption(value)\n\n    @field_validator(\"activation_func\", mode=\"before\")\n    @classmethod\n    def validate_activation_func(cls, activation_func: str) -&gt; Callable:\n        \"\"\"Validates the activation function, assumes this function exists in torch.nn.functional.\n\n        For custom activation functions, use the CUSTOM_ACTIVATION_FUNCTIONS dictionary in the module. This method\n        validates the provided activation function string and returns a callable function based on the validation\n        context using the provided validator in the base class.\n\n        Args:\n            activation_func (str): The activation function to be validated.\n            context (ValidationInfo): The context for validation.\n\n        Returns:\n            Callable: A callable function after validation.\n\n        See Also:\n            CUSTOM_ACTIVATION_FNS\n        \"\"\"\n        func = getattr(torch.nn.functional, activation_func.lower(), None)\n        if func is None and activation_func in CUSTOM_ACTIVATION_FNS:\n            func = CUSTOM_ACTIVATION_FNS[activation_func]\n            return func\n        elif func is None:\n            raise ValueError(\n                f\"activation_func must be a valid function in `torch.nn.functional`, got {activation_func=}\"\n            )\n        else:\n            return func\n\n    @field_serializer(\"activation_func\")\n    def serialize_activation_func(self, v: Callable[[torch.Tensor, Any], torch.Tensor]) -&gt; str:\n        \"\"\"Serializes a given activation function to its corresponding string representation.\n\n        By default, all activation functions from `torch.nn.functional` are serialized to their name. User defined\n        activation functions should also be defined here with a custom mapping in CUSTOM_ACTIVATION_FNS defined at the\n        top of this file. This allows our Pydantic model to serialize and deserialize the activation function.\n\n        Args:\n            v (Callable[[torch.Tensor, Any], torch.Tensor]): The activation function to serialize.\n\n        Returns:\n            str: The name of the activation function if it is a standard PyTorch function,\n                 or the corresponding serialization key if it is a custom activation function.\n\n        Raises:\n            ValueError: If the activation function is not supported.\n        \"\"\"\n        func_name = v.__name__\n        func = getattr(torch.nn.functional, func_name, None)\n        if func is not None:\n            return func_name\n        elif func in REVERSE_CUSTOM_ACTIVATION_FNS:\n            return REVERSE_CUSTOM_ACTIVATION_FNS[func]  # Get the serialization key\n        else:\n            raise ValueError(f\"Unsupported activation function: {v}\")\n\n    @field_validator(\"params_dtype\", \"pipeline_dtype\", \"autocast_dtype\", mode=\"before\")\n    @classmethod\n    def precision_validator(cls, v: dtypes.PrecisionTypes) -&gt; torch.dtype:\n        \"\"\"Validates the precision type and returns the corresponding torch dtype.\"\"\"\n        return dtypes.get_autocast_dtype(v)\n\n    @field_serializer(\"params_dtype\", \"pipeline_dtype\", \"autocast_dtype\")\n    def serialize_dtypes(self, v: torch.dtype) -&gt; dtypes.PrecisionTypes:\n        \"\"\"Serializes the torch dtype to the corresponding precision type.\"\"\"\n        return dtypes.dtype_to_precision[v]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExposedModelConfig.custom_model_validator","title":"<code>custom_model_validator(global_cfg)</code>","text":"<p>Use custom implementation of this method to define the things inside global_config.</p> <p>The following expression will always be true:</p> <p>global_cfg.bionemo_model_config == self</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>def custom_model_validator(self, global_cfg: \"MainConfig\") -&gt; \"MainConfig\":\n    \"\"\"Use custom implementation of this method to define the things inside global_config.\n\n    The following expression will always be true:\n\n    global_cfg.bionemo_model_config == self\n    \"\"\"\n    return global_cfg\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExposedModelConfig.exposed_to_internal_bionemo_model_config","title":"<code>exposed_to_internal_bionemo_model_config()</code>","text":"<p>Converts the exposed dataclass to the underlying Transformer config.</p> <p>The underlying ModelConfigT may both be incomplete and unserializable. We use this transformation as a way to hide fields that are either not serializable by Pydantic or that we do not want to expose.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>def exposed_to_internal_bionemo_model_config(self) -&gt; ModelConfigT:\n    \"\"\"Converts the exposed dataclass to the underlying Transformer config.\n\n    The underlying ModelConfigT may both be incomplete and unserializable. We use this transformation as a way to\n    hide fields that are either not serializable by Pydantic or that we do not want to expose.\n    \"\"\"\n    cls: Type[ModelConfigT] = self.model_class()\n    model_dict = {}\n    for attr in self.model_fields:\n        if attr not in model_dict and attr in cls.__dataclass_fields__:\n            model_dict[attr] = getattr(self, attr)\n\n    # Now set fp16 and bf16 based on the precision for the underlying TransformerConfig=&gt;ParallelConfig\n    #   the only constraint is that both must not be true.\n    model_dict[\"bf16\"] = self.pipeline_dtype == dtypes.precision_to_dtype[\"bf16-mixed\"]\n    model_dict[\"fp16\"] = self.pipeline_dtype == dtypes.precision_to_dtype[\"16-mixed\"]\n    result = cls(**model_dict)\n\n    return result\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExposedModelConfig.model_class","title":"<code>model_class()</code>","text":"<p>Returns the underlying model class that this config wraps.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>def model_class(self) -&gt; Type[ModelConfigT]:\n    \"\"\"Returns the underlying model class that this config wraps.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExposedModelConfig.precision_validator","title":"<code>precision_validator(v)</code>  <code>classmethod</code>","text":"<p>Validates the precision type and returns the corresponding torch dtype.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@field_validator(\"params_dtype\", \"pipeline_dtype\", \"autocast_dtype\", mode=\"before\")\n@classmethod\ndef precision_validator(cls, v: dtypes.PrecisionTypes) -&gt; torch.dtype:\n    \"\"\"Validates the precision type and returns the corresponding torch dtype.\"\"\"\n    return dtypes.get_autocast_dtype(v)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExposedModelConfig.serialize_activation_func","title":"<code>serialize_activation_func(v)</code>","text":"<p>Serializes a given activation function to its corresponding string representation.</p> <p>By default, all activation functions from <code>torch.nn.functional</code> are serialized to their name. User defined activation functions should also be defined here with a custom mapping in CUSTOM_ACTIVATION_FNS defined at the top of this file. This allows our Pydantic model to serialize and deserialize the activation function.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>Callable[[Tensor, Any], Tensor]</code> <p>The activation function to serialize.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The name of the activation function if it is a standard PyTorch function,  or the corresponding serialization key if it is a custom activation function.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the activation function is not supported.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@field_serializer(\"activation_func\")\ndef serialize_activation_func(self, v: Callable[[torch.Tensor, Any], torch.Tensor]) -&gt; str:\n    \"\"\"Serializes a given activation function to its corresponding string representation.\n\n    By default, all activation functions from `torch.nn.functional` are serialized to their name. User defined\n    activation functions should also be defined here with a custom mapping in CUSTOM_ACTIVATION_FNS defined at the\n    top of this file. This allows our Pydantic model to serialize and deserialize the activation function.\n\n    Args:\n        v (Callable[[torch.Tensor, Any], torch.Tensor]): The activation function to serialize.\n\n    Returns:\n        str: The name of the activation function if it is a standard PyTorch function,\n             or the corresponding serialization key if it is a custom activation function.\n\n    Raises:\n        ValueError: If the activation function is not supported.\n    \"\"\"\n    func_name = v.__name__\n    func = getattr(torch.nn.functional, func_name, None)\n    if func is not None:\n        return func_name\n    elif func in REVERSE_CUSTOM_ACTIVATION_FNS:\n        return REVERSE_CUSTOM_ACTIVATION_FNS[func]  # Get the serialization key\n    else:\n        raise ValueError(f\"Unsupported activation function: {v}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExposedModelConfig.serialize_dtypes","title":"<code>serialize_dtypes(v)</code>","text":"<p>Serializes the torch dtype to the corresponding precision type.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@field_serializer(\"params_dtype\", \"pipeline_dtype\", \"autocast_dtype\")\ndef serialize_dtypes(self, v: torch.dtype) -&gt; dtypes.PrecisionTypes:\n    \"\"\"Serializes the torch dtype to the corresponding precision type.\"\"\"\n    return dtypes.dtype_to_precision[v]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExposedModelConfig.validate_activation_func","title":"<code>validate_activation_func(activation_func)</code>  <code>classmethod</code>","text":"<p>Validates the activation function, assumes this function exists in torch.nn.functional.</p> <p>For custom activation functions, use the CUSTOM_ACTIVATION_FUNCTIONS dictionary in the module. This method validates the provided activation function string and returns a callable function based on the validation context using the provided validator in the base class.</p> <p>Parameters:</p> Name Type Description Default <code>activation_func</code> <code>str</code> <p>The activation function to be validated.</p> required <code>context</code> <code>ValidationInfo</code> <p>The context for validation.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>A callable function after validation.</p> See Also <p>CUSTOM_ACTIVATION_FNS</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@field_validator(\"activation_func\", mode=\"before\")\n@classmethod\ndef validate_activation_func(cls, activation_func: str) -&gt; Callable:\n    \"\"\"Validates the activation function, assumes this function exists in torch.nn.functional.\n\n    For custom activation functions, use the CUSTOM_ACTIVATION_FUNCTIONS dictionary in the module. This method\n    validates the provided activation function string and returns a callable function based on the validation\n    context using the provided validator in the base class.\n\n    Args:\n        activation_func (str): The activation function to be validated.\n        context (ValidationInfo): The context for validation.\n\n    Returns:\n        Callable: A callable function after validation.\n\n    See Also:\n        CUSTOM_ACTIVATION_FNS\n    \"\"\"\n    func = getattr(torch.nn.functional, activation_func.lower(), None)\n    if func is None and activation_func in CUSTOM_ACTIVATION_FNS:\n        func = CUSTOM_ACTIVATION_FNS[activation_func]\n        return func\n    elif func is None:\n        raise ValueError(\n            f\"activation_func must be a valid function in `torch.nn.functional`, got {activation_func=}\"\n        )\n    else:\n        return func\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.MainConfig","title":"<code>MainConfig</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[ExModelConfigT, DataConfigT]</code></p> <p>Main configuration class for BioNeMo. All serialized configs that are a valid MainConfig should be Runnable.</p> <p>This class is used to define the main configuration for BioNeMo. It defines the minimal pieces of configuration to execution a training job with the NeMo2 training api. It accepts two generic type parameters which users must define in their own environment for execution.</p> <p>Additionally, this class assumes that the configs for ExposedModelConfig and DataConfig may have custom validators implemented that operate on the entire MainConfig. This prevents the need from type based conditionals inside this class while still allowing for custom validation global logic to be implemented in the underlying classes. For example, some models may want to restrict their Datamodules seq_length to a certain value.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <p>Generic config type that contains instructions on instantiating the required DataModule.</p> required <code>parallel_config</code> <p>The parallel configuration for the model.</p> required <code>training_config</code> <p>The training configuration for the model.</p> required <code>bionemo_model_config</code> <p>Generic ExposedModelConfig type. This class hides extra configuration parameters in the underlying model configuration as well as providing</p> required <code>optim_config</code> <p>The optimizer/scheduler configuration for the model.</p> required <code>experiment_config</code> <p>The experiment configuration for the model.</p> required <code>wandb_config</code> <p>Optional, the wandb configuration for the model.</p> required Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>class MainConfig(BaseModel, Generic[ExModelConfigT, DataConfigT]):\n    \"\"\"Main configuration class for BioNeMo. All serialized configs that are a valid MainConfig should be Runnable.\n\n    This class is used to define the main configuration for BioNeMo. It defines the minimal pieces of configuration\n    to execution a training job with the NeMo2 training api. It accepts two generic type parameters which users\n    must define in their own environment for execution.\n\n    Additionally, this class assumes that the configs for ExposedModelConfig and DataConfig may have custom validators\n    implemented that operate on the entire MainConfig. This prevents the need from type based conditionals inside this\n    class while still allowing for custom validation global logic to be implemented in the underlying classes. For example,\n    some models may want to restrict their Datamodules seq_length to a certain value.\n\n\n    Args:\n        data_config: Generic config type that contains instructions on instantiating the required DataModule.\n        parallel_config: The parallel configuration for the model.\n        training_config: The training configuration for the model.\n        bionemo_model_config: Generic ExposedModelConfig type. This class hides extra configuration parameters in the\n            underlying model configuration as well as providing\n        optim_config: The optimizer/scheduler configuration for the model.\n        experiment_config: The experiment configuration for the model.\n        wandb_config: Optional, the wandb configuration for the model.\n    \"\"\"\n\n    data_config: DataConfigT\n    parallel_config: ParallelConfig\n    training_config: TrainingConfig\n    bionemo_model_config: ExModelConfigT\n    optim_config: OptimizerSchedulerConfig\n    experiment_config: ExperimentConfig\n    wandb_config: Optional[WandbConfig] = None\n\n    @model_validator(mode=\"after\")\n    def validate_master_config(self) -&gt; \"MainConfig\":\n        \"\"\"Validates the master configuration object.\"\"\"\n        self.bionemo_model_config.seq_length = self.data_config.seq_length\n        return self\n\n    @model_validator(mode=\"after\")\n    def run_bionemo_model_config_model_validators(self) -&gt; \"MainConfig\":\n        \"\"\"Runs the model validators on the bionemo_model_config.\"\"\"\n        return self.bionemo_model_config.custom_model_validator(self)\n\n    @model_validator(mode=\"after\")\n    def run_data_config_model_validators(self) -&gt; \"MainConfig\":\n        \"\"\"Runs the model validators on the data_config.\"\"\"\n        return self.data_config.custom_model_validator(self)\n\n    @model_validator(mode=\"after\")\n    def validate_checkpointing_setting(self) -&gt; \"MainConfig\":\n        \"\"\"Validates the master configuration object.\"\"\"\n        self.training_config.enable_checkpointing = self.experiment_config.create_checkpoint_callback\n        return self\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.MainConfig.run_bionemo_model_config_model_validators","title":"<code>run_bionemo_model_config_model_validators()</code>","text":"<p>Runs the model validators on the bionemo_model_config.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef run_bionemo_model_config_model_validators(self) -&gt; \"MainConfig\":\n    \"\"\"Runs the model validators on the bionemo_model_config.\"\"\"\n    return self.bionemo_model_config.custom_model_validator(self)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.MainConfig.run_data_config_model_validators","title":"<code>run_data_config_model_validators()</code>","text":"<p>Runs the model validators on the data_config.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef run_data_config_model_validators(self) -&gt; \"MainConfig\":\n    \"\"\"Runs the model validators on the data_config.\"\"\"\n    return self.data_config.custom_model_validator(self)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.MainConfig.validate_checkpointing_setting","title":"<code>validate_checkpointing_setting()</code>","text":"<p>Validates the master configuration object.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_checkpointing_setting(self) -&gt; \"MainConfig\":\n    \"\"\"Validates the master configuration object.\"\"\"\n    self.training_config.enable_checkpointing = self.experiment_config.create_checkpoint_callback\n    return self\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.MainConfig.validate_master_config","title":"<code>validate_master_config()</code>","text":"<p>Validates the master configuration object.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_master_config(self) -&gt; \"MainConfig\":\n    \"\"\"Validates the master configuration object.\"\"\"\n    self.bionemo_model_config.seq_length = self.data_config.seq_length\n    return self\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.OptimizerSchedulerConfig","title":"<code>OptimizerSchedulerConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the optimizer and learning rate scheduler.</p> <p>Attributes:</p> Name Type Description <code>lr</code> <code>float</code> <p>Learning rate for the optimizer. Default is 1e-4.</p> <code>optimizer</code> <code>str</code> <p>Type of optimizer to use. Default is \"adam\".</p> <code>interval</code> <code>str</code> <p>Interval for updating the learning rate scheduler. Default is \"step\".</p> <code>monitor</code> <code>str</code> <p>Metric to monitor for learning rate adjustments. Default is \"val_loss\".</p> <code>interval</code> <code>str</code> <p>Interval for updating the learning rate scheduler. Default is \"step\".</p> <code>monitor</code> <code>str</code> <p>Metric to monitor for learning rate adjustments. Default is \"val_loss\".</p> <code>warmup_steps</code> <code>int</code> <p>Number of warmup steps for use with the warmup annealing learning rate scheduler. Default is 0.</p> <code>lr_scheduler</code> <code>Literal['warmup_anneal', 'cosine']</code> <p>Type of learning rate scheduler to use. Default is 'warmup_anneal'. NOTE this is likely to change.</p> <code>max_steps</code> <code>Optional[int]</code> <p>max_steps used in optimizer. Default to None which uses max_steps from TrainingConfig.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>class OptimizerSchedulerConfig(BaseModel):\n    \"\"\"Configuration for the optimizer and learning rate scheduler.\n\n    Attributes:\n        lr (float): Learning rate for the optimizer. Default is 1e-4.\n        optimizer (str): Type of optimizer to use. Default is \"adam\".\n        interval (str): Interval for updating the learning rate scheduler. Default is \"step\".\n        monitor (str): Metric to monitor for learning rate adjustments. Default is \"val_loss\".\n        interval (str): Interval for updating the learning rate scheduler. Default is \"step\".\n        monitor (str): Metric to monitor for learning rate adjustments. Default is \"val_loss\".\n        warmup_steps (int): Number of warmup steps for use with the warmup annealing learning rate scheduler. Default is 0.\n        lr_scheduler (Literal['warmup_anneal', 'cosine']): Type of learning rate scheduler to use. Default is 'warmup_anneal'. NOTE this is likely to change.\n        max_steps (Optional[int]): max_steps used in optimizer. Default to None which uses max_steps from TrainingConfig.\n    \"\"\"\n\n    lr: float = 1e-4\n    sgd_momentum: float = 0.9\n    adam_eps: float = 1e-8\n    weight_decay: float = 0.01\n    use_distributed_optimizer: bool = True\n    optimizer: str = \"adam\"\n    interval: str = \"step\"\n    monitor: str = \"val_loss\"\n    cosine_rampup_frac: float = 0.01\n    cosine_hold_frac: float = 0.05\n    warmup_steps: int = 0\n    lr_scheduler: Literal[\"warmup_anneal\", \"cosine\"] = \"warmup_anneal\"\n    max_steps: Optional[int] = None\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ParallelConfig","title":"<code>ParallelConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>ParallelConfig is a configuration class for setting up parallelism in model training.</p> <p>Attributes:</p> Name Type Description <code>tensor_model_parallel_size</code> <code>int</code> <p>The size of the tensor model parallelism. Default is 1.</p> <code>pipeline_model_parallel_size</code> <code>int</code> <p>The size of the pipeline model parallelism. Default is 1.</p> <code>accumulate_grad_batches</code> <code>int</code> <p>The number of batches to accumulate gradients over. Default is 1.</p> <code>ddp</code> <code>Literal['megatron']</code> <p>The distributed data parallel method to use. Default is \"megatron\".</p> <code>remove_unused_parameters</code> <code>bool</code> <p>Whether to remove unused parameters. Default is True.</p> <code>num_devices</code> <code>int</code> <p>The number of devices to use. Default is 1.</p> <code>num_nodes</code> <code>int</code> <p>The number of nodes to use. Default is 1.</p> <p>Methods:</p> Name Description <code>validate_devices</code> <p>Validates the number of devices based on the tensor and pipeline model parallel sizes.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>class ParallelConfig(BaseModel):\n    \"\"\"ParallelConfig is a configuration class for setting up parallelism in model training.\n\n    Attributes:\n        tensor_model_parallel_size (int): The size of the tensor model parallelism. Default is 1.\n        pipeline_model_parallel_size (int): The size of the pipeline model parallelism. Default is 1.\n        accumulate_grad_batches (int): The number of batches to accumulate gradients over. Default is 1.\n        ddp (Literal[\"megatron\"]): The distributed data parallel method to use. Default is \"megatron\".\n        remove_unused_parameters (bool): Whether to remove unused parameters. Default is True.\n        num_devices (int): The number of devices to use. Default is 1.\n        num_nodes (int): The number of nodes to use. Default is 1.\n\n    Methods:\n        validate_devices(): Validates the number of devices based on the tensor and pipeline model parallel sizes.\n    \"\"\"\n\n    tensor_model_parallel_size: int = 1\n    pipeline_model_parallel_size: int = 1\n    accumulate_grad_batches: int = 1\n    ddp: Literal[\"megatron\"] = \"megatron\"\n    remove_unused_parameters: bool = True\n    use_distributed_optimizer: bool = True\n    num_devices: int = 1\n    num_nodes: int = 1\n\n    @model_validator(mode=\"after\")\n    def validate_devices(self):\n        \"\"\"Validates the number of devices based on the tensor and pipeline model parallel sizes.\"\"\"\n        if self.num_devices &lt; self.tensor_model_parallel_size * self.pipeline_model_parallel_size:\n            raise ValueError(\"devices must be divisible by tensor_model_parallel_size * pipeline_model_parallel_size\")\n        return self\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ParallelConfig.validate_devices","title":"<code>validate_devices()</code>","text":"<p>Validates the number of devices based on the tensor and pipeline model parallel sizes.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_devices(self):\n    \"\"\"Validates the number of devices based on the tensor and pipeline model parallel sizes.\"\"\"\n    if self.num_devices &lt; self.tensor_model_parallel_size * self.pipeline_model_parallel_size:\n        raise ValueError(\"devices must be divisible by tensor_model_parallel_size * pipeline_model_parallel_size\")\n    return self\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.TrainingConfig","title":"<code>TrainingConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>TrainingConfig is a configuration class for training models.</p> <p>Attributes:</p> Name Type Description <code>max_steps</code> <code>int</code> <p>The maximum number of training steps.</p> <code>limit_val_batches</code> <code>int | float</code> <p>The number of validation batches to use. Can be a fraction or a count.</p> <code>val_check_interval</code> <code>int</code> <p>The interval (in steps) at which to check validation.</p> <code>precision</code> <code>Literal['32', 'bf16-mixed', '16-mixed']</code> <p>The precision to use for training. Defaults to \"bf16-mixed\".</p> <code>accelerator</code> <code>str</code> <p>The type of accelerator to use for training. Defaults to \"gpu\".</p> <code>gc_interval</code> <code>int</code> <p>The interval of global steps at which to run synchronized garbage collection. Useful for synchronizing garbage collection when performing distributed training. Defaults to 0.</p> <code>include_perplexity</code> <code>bool</code> <p>Whether to include perplexity in the validation logs. Defaults to False.</p> <code>enable_checkpointing</code> <code>bool</code> <p>Whether to enable checkpointing and configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint. Corresponds to the same parameter name in pl.Trainer</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>class TrainingConfig(BaseModel):\n    \"\"\"TrainingConfig is a configuration class for training models.\n\n    Attributes:\n        max_steps (int): The maximum number of training steps.\n        limit_val_batches (int | float): The number of validation batches to use. Can be a fraction or a count.\n        val_check_interval (int): The interval (in steps) at which to check validation.\n        precision (Literal[\"32\", \"bf16-mixed\", \"16-mixed\"], optional): The precision to use for training. Defaults to \"bf16-mixed\".\n        accelerator (str, optional): The type of accelerator to use for training. Defaults to \"gpu\".\n        gc_interval (int, optional): The interval of global steps at which to run synchronized garbage collection. Useful for synchronizing garbage collection when performing distributed training. Defaults to 0.\n        include_perplexity (bool, optional): Whether to include perplexity in the validation logs. Defaults to False.\n        enable_checkpointing (bool, optional): Whether to enable checkpointing and configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint. Corresponds to the same parameter name in pl.Trainer\n    \"\"\"\n\n    max_steps: int\n    limit_val_batches: int | float  # Because this can be a fraction or a count...\n    val_check_interval: int\n    precision: Literal[\"32\", \"bf16-mixed\", \"16-mixed\"] = \"bf16-mixed\"\n    accelerator: str = \"gpu\"\n    # NOTE: VERY important for distributed training performance.\n    gc_interval: int = 0\n    log_train_ppl: bool = False\n    log_val_ppl: bool = True\n    enable_checkpointing: bool = True\n    create_tflops_callback: bool = False\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.deserialize_str_to_path","title":"<code>deserialize_str_to_path(path)</code>","text":"<p>General purpose deserialize for string/path objects. Since YAML has no native representation for pathlib.Path, we serialize to strings. Import this method as a @field_validator.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>def deserialize_str_to_path(path: str) -&gt; pathlib.Path:\n    \"\"\"General purpose deserialize for string/path objects. Since YAML has no native representation for pathlib.Path, we serialize to strings. Import this method as a @field_validator.\"\"\"\n    return pathlib.Path(path)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.serialize_path_or_str","title":"<code>serialize_path_or_str(path)</code>","text":"<p>General purpose serialization for string/path objects. Since YAML has no native representation for pathlib.Path, we serialize to strings. Import this method as a @field_serializer.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>def serialize_path_or_str(path: str | pathlib.Path) -&gt; str:\n    \"\"\"General purpose serialization for string/path objects. Since YAML has no native representation for pathlib.Path, we serialize to strings. Import this method as a @field_serializer.\"\"\"\n    if isinstance(path, pathlib.Path):\n        return str(path)\n    elif isinstance(path, str):\n        return path\n    else:\n        raise ValueError(f\"Expected str or pathlib.Path, got {type(path)}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/callbacks/","title":"Callbacks","text":""},{"location":"main/references/API_reference/bionemo/llm/utils/callbacks/#bionemo.llm.utils.callbacks.PredictionWriter","title":"<code>PredictionWriter</code>","text":"<p>               Bases: <code>BasePredictionWriter</code>, <code>Callback</code></p> <p>A callback that writes predictions to disk at specified intervals during training.</p> <p>Logits, Embeddings, Hiddens, Input IDs, and Labels may all be saved to the disk depending on trainer configuration. Batch Idxs are provided for each prediction in the same dictionary. These must be used to maintain order between multi device predictions and single device predictions.</p> Source code in <code>bionemo/llm/utils/callbacks.py</code> <pre><code>class PredictionWriter(BasePredictionWriter, pl.Callback):\n    \"\"\"A callback that writes predictions to disk at specified intervals during training.\n\n    Logits, Embeddings, Hiddens, Input IDs, and Labels may all be saved to the disk depending on trainer configuration.\n    Batch Idxs are provided for each prediction in the same dictionary. These must be used to maintain order between\n    multi device predictions and single device predictions.\n    \"\"\"\n\n    def __init__(\n        self,\n        output_dir: str | os.PathLike,\n        write_interval: IntervalT,\n        batch_dim_key_defaults: dict[str, int] | None = None,\n        seq_dim_key_defaults: dict[str, int] | None = None,\n        save_all_model_parallel_ranks: bool = False,\n        files_per_subdir: int | None = None,\n    ):\n        \"\"\"Initializes the callback.\n\n        Args:\n            output_dir: The directory where predictions will be written.\n            write_interval: The interval at which predictions will be written (batch, epoch). Epoch may not be used with\n                multi-device trainers.\n            batch_dim_key_defaults: The default batch dimension for each key, if different from the standard 0.\n            seq_dim_key_defaults: The default sequence dimension for each key, if different from the standard 1.\n            save_all_model_parallel_ranks: Whether to save predictions for all model parallel ranks. Generally these\n                will be redundant.\n            files_per_subdir: Number of files to write to each subdirectory. If provided, subdirectories with N files\n                each will be created. Ignored unless write_interval is 'batch'.\n        \"\"\"\n        super().__init__(write_interval)\n        self.write_interval = write_interval\n        self.output_dir = str(output_dir)\n        self.base_dir = self.output_dir  # start out like this, but output_dir will be updated if files_per_subdir&gt;0\n        self.batch_dim_key_defaults = batch_dim_key_defaults\n        self.seq_dim_key_defaults = seq_dim_key_defaults\n        self.save_all_model_parallel_ranks = save_all_model_parallel_ranks\n        self.files_per_subdir = files_per_subdir\n        # Initialize to infinity if files_per_subdir is provided so that we create a new subdirectory before writing\n        #   any files.\n        self.num_files_written = float(\"inf\") if files_per_subdir else 0\n        self.num_subdirs_written = 0\n\n    def setup(self, trainer: pl.Trainer, pl_module: pl.LightningModule, *args, **kwargs) -&gt; None:  # noqa: D417\n        \"\"\"Invoked with Trainer.fit, validate, test, and predict are called. Will immediately fail when 'write_interval' is 'epoch' and 'trainer.num_devices' &gt; 1.\n\n        Args:\n            trainer: The Trainer instance.\n            pl_module: The LightningModule instance.\n        \"\"\"\n        if trainer.num_devices &gt; 1 and self.write_interval == \"epoch\":\n            logger.warning(\n                \"Multi-GPU predictions could result in shuffled inputs. Verify that the original indices are included \"\n                \"in the model's predictions as outputs are not ordered and batch indices do not track input order.\"\n            )\n\n    @staticmethod\n    def _assert_initialized():\n        \"\"\"Asserts that the environment is initialized.\"\"\"\n        if not (\n            torch.distributed.is_available() and torch.distributed.is_initialized() and parallel_state.is_initialized()\n        ):\n            raise RuntimeError(\"This function is only defined within an initialized megatron parallel environment.\")\n\n    @property\n    def data_parallel_world_size(self) -&gt; int:\n        \"\"\"Returns the data parallel world size.\"\"\"\n        self._assert_initialized()\n        return torch.distributed.get_world_size(parallel_state.get_data_parallel_group(with_context_parallel=False))\n\n    @property\n    def data_parallel_rank(self) -&gt; int:\n        \"\"\"Returns the data parallel rank.\"\"\"\n        self._assert_initialized()\n        return torch.distributed.get_rank(parallel_state.get_data_parallel_group(with_context_parallel=False))\n\n    @property\n    def should_write_predictions(self) -&gt; bool:\n        \"\"\"Ensures that predictions are only written on TP/CP rank 0 and that it is the last stage of the pipeline.\"\"\"\n        self._assert_initialized()\n        if not parallel_state.is_pipeline_last_stage():\n            return False\n        if self.save_all_model_parallel_ranks:\n            return True\n        # TODO: handle expert parallelism and other kinds of parallelism\n        return parallel_state.get_tensor_model_parallel_rank() == 0 and parallel_state.get_context_parallel_rank() == 0\n\n    @override\n    def write_on_batch_end(\n        self,\n        trainer: pl.Trainer,\n        pl_module: pl.LightningModule,\n        prediction: Any,\n        batch_indices: Sequence[int] | None,\n        batch: Any,\n        batch_idx: int,\n        dataloader_idx: int,\n    ) -&gt; None:\n        \"\"\"Writes predictions to disk at the end of each batch.\n\n        Predictions files follow the naming pattern, where rank is the active GPU in which the predictions were made.\n        predictions__rank_{rank}__batch_{batch_idx}.pt\n\n        Args:\n            trainer: The Trainer instance.\n            pl_module: The LightningModule instance.\n            prediction: The prediction made by the model.\n            batch_indices: The indices of the batch.\n            batch: The batch data.\n            batch_idx: The index of the batch.\n            dataloader_idx: The index of the dataloader.\n        \"\"\"\n        # this will create N (num processes) files in `output_dir` each containing\n        # the predictions of it's respective rank\n        if self.should_write_predictions:\n            if (\n                self.files_per_subdir is not None\n                and (self.num_files_written * self.data_parallel_world_size) &gt;= self.files_per_subdir\n            ):\n                self.num_subdirs_written += 1\n                self.output_dir = os.path.join(self.base_dir, f\"subdir_{self.num_subdirs_written}\")\n                os.makedirs(self.output_dir, exist_ok=True)\n                self.num_files_written = 0\n            result_path = os.path.join(\n                self.output_dir,\n                f\"predictions__rank_{trainer.global_rank}__dp_rank_{self.data_parallel_rank}__batch_{batch_idx}.pt\",\n            )\n\n            # batch_indices is not captured due to a lightning bug when return_predictions = False\n            # we use input IDs in the prediction to map the result to input.\n\n            # NOTE store the batch_idx so we do not need to rely on filenames for reconstruction of inputs. This is wrapped\n            # in a tensor and list container to ensure compatibility with batch_collator.\n            prediction[\"batch_idx\"] = torch.tensor([batch_idx], dtype=torch.int64)\n\n            torch.save(prediction, result_path)\n            logger.info(f\"Inference predictions are stored in {result_path}\\n{prediction.keys()}\")\n            self.num_files_written += 1\n\n    @override\n    def write_on_epoch_end(\n        self,\n        trainer: pl.Trainer,\n        pl_module: pl.LightningModule,\n        predictions: Any,\n        batch_indices: Sequence[int],\n    ) -&gt; None:\n        \"\"\"Writes predictions to disk at the end of each epoch.\n\n        Writing all predictions on epoch end is memory intensive. It is recommended to use the batch writer instead for\n        large predictions.\n\n        Multi-device predictions will likely yield predictions in an order that is inconsistent with single device predictions and the input data.\n\n        Args:\n            trainer: The Trainer instance.\n            pl_module: The LightningModule instance.\n            predictions: The predictions made by the model.\n            batch_indices: The indices of the batch.\n\n        Raises:\n            Multi-GPU predictions are output in an inconsistent order with multiple devices.\n        \"\"\"\n        # this will create N (num processes) files in `output_dir` each containing\n        # the predictions of it's respective rank\n        if self.should_write_predictions:\n            result_path = os.path.join(\n                self.output_dir,\n                f\"predictions__rank_{trainer.global_rank}__dp_rank_{self.data_parallel_rank}.pt\",\n            )\n\n            # collate multiple batches / ignore empty ones\n            collate_kwargs = {}\n            if self.batch_dim_key_defaults is not None:\n                collate_kwargs[\"batch_dim_key_defaults\"] = self.batch_dim_key_defaults\n            if self.seq_dim_key_defaults is not None:\n                collate_kwargs[\"seq_dim_key_defaults\"] = self.seq_dim_key_defaults\n\n            prediction = batch_collator([item for item in predictions if item is not None], **collate_kwargs)\n\n            # batch_indices is not captured due to a lightning bug when return_predictions = False\n            # we use input IDs in the prediction to map the result to input\n            if isinstance(prediction, dict):\n                keys = prediction.keys()\n            else:\n                keys = \"tensor\"\n            torch.save(prediction, result_path)\n            logger.info(f\"Inference predictions are stored in {result_path}\\n{keys}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/callbacks/#bionemo.llm.utils.callbacks.PredictionWriter.data_parallel_rank","title":"<code>data_parallel_rank</code>  <code>property</code>","text":"<p>Returns the data parallel rank.</p>"},{"location":"main/references/API_reference/bionemo/llm/utils/callbacks/#bionemo.llm.utils.callbacks.PredictionWriter.data_parallel_world_size","title":"<code>data_parallel_world_size</code>  <code>property</code>","text":"<p>Returns the data parallel world size.</p>"},{"location":"main/references/API_reference/bionemo/llm/utils/callbacks/#bionemo.llm.utils.callbacks.PredictionWriter.should_write_predictions","title":"<code>should_write_predictions</code>  <code>property</code>","text":"<p>Ensures that predictions are only written on TP/CP rank 0 and that it is the last stage of the pipeline.</p>"},{"location":"main/references/API_reference/bionemo/llm/utils/callbacks/#bionemo.llm.utils.callbacks.PredictionWriter.__init__","title":"<code>__init__(output_dir, write_interval, batch_dim_key_defaults=None, seq_dim_key_defaults=None, save_all_model_parallel_ranks=False, files_per_subdir=None)</code>","text":"<p>Initializes the callback.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str | PathLike</code> <p>The directory where predictions will be written.</p> required <code>write_interval</code> <code>IntervalT</code> <p>The interval at which predictions will be written (batch, epoch). Epoch may not be used with multi-device trainers.</p> required <code>batch_dim_key_defaults</code> <code>dict[str, int] | None</code> <p>The default batch dimension for each key, if different from the standard 0.</p> <code>None</code> <code>seq_dim_key_defaults</code> <code>dict[str, int] | None</code> <p>The default sequence dimension for each key, if different from the standard 1.</p> <code>None</code> <code>save_all_model_parallel_ranks</code> <code>bool</code> <p>Whether to save predictions for all model parallel ranks. Generally these will be redundant.</p> <code>False</code> <code>files_per_subdir</code> <code>int | None</code> <p>Number of files to write to each subdirectory. If provided, subdirectories with N files each will be created. Ignored unless write_interval is 'batch'.</p> <code>None</code> Source code in <code>bionemo/llm/utils/callbacks.py</code> <pre><code>def __init__(\n    self,\n    output_dir: str | os.PathLike,\n    write_interval: IntervalT,\n    batch_dim_key_defaults: dict[str, int] | None = None,\n    seq_dim_key_defaults: dict[str, int] | None = None,\n    save_all_model_parallel_ranks: bool = False,\n    files_per_subdir: int | None = None,\n):\n    \"\"\"Initializes the callback.\n\n    Args:\n        output_dir: The directory where predictions will be written.\n        write_interval: The interval at which predictions will be written (batch, epoch). Epoch may not be used with\n            multi-device trainers.\n        batch_dim_key_defaults: The default batch dimension for each key, if different from the standard 0.\n        seq_dim_key_defaults: The default sequence dimension for each key, if different from the standard 1.\n        save_all_model_parallel_ranks: Whether to save predictions for all model parallel ranks. Generally these\n            will be redundant.\n        files_per_subdir: Number of files to write to each subdirectory. If provided, subdirectories with N files\n            each will be created. Ignored unless write_interval is 'batch'.\n    \"\"\"\n    super().__init__(write_interval)\n    self.write_interval = write_interval\n    self.output_dir = str(output_dir)\n    self.base_dir = self.output_dir  # start out like this, but output_dir will be updated if files_per_subdir&gt;0\n    self.batch_dim_key_defaults = batch_dim_key_defaults\n    self.seq_dim_key_defaults = seq_dim_key_defaults\n    self.save_all_model_parallel_ranks = save_all_model_parallel_ranks\n    self.files_per_subdir = files_per_subdir\n    # Initialize to infinity if files_per_subdir is provided so that we create a new subdirectory before writing\n    #   any files.\n    self.num_files_written = float(\"inf\") if files_per_subdir else 0\n    self.num_subdirs_written = 0\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/callbacks/#bionemo.llm.utils.callbacks.PredictionWriter.setup","title":"<code>setup(trainer, pl_module, *args, **kwargs)</code>","text":"<p>Invoked with Trainer.fit, validate, test, and predict are called. Will immediately fail when 'write_interval' is 'epoch' and 'trainer.num_devices' &gt; 1.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The Trainer instance.</p> required <code>pl_module</code> <code>LightningModule</code> <p>The LightningModule instance.</p> required Source code in <code>bionemo/llm/utils/callbacks.py</code> <pre><code>def setup(self, trainer: pl.Trainer, pl_module: pl.LightningModule, *args, **kwargs) -&gt; None:  # noqa: D417\n    \"\"\"Invoked with Trainer.fit, validate, test, and predict are called. Will immediately fail when 'write_interval' is 'epoch' and 'trainer.num_devices' &gt; 1.\n\n    Args:\n        trainer: The Trainer instance.\n        pl_module: The LightningModule instance.\n    \"\"\"\n    if trainer.num_devices &gt; 1 and self.write_interval == \"epoch\":\n        logger.warning(\n            \"Multi-GPU predictions could result in shuffled inputs. Verify that the original indices are included \"\n            \"in the model's predictions as outputs are not ordered and batch indices do not track input order.\"\n        )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/callbacks/#bionemo.llm.utils.callbacks.PredictionWriter.write_on_batch_end","title":"<code>write_on_batch_end(trainer, pl_module, prediction, batch_indices, batch, batch_idx, dataloader_idx)</code>","text":"<p>Writes predictions to disk at the end of each batch.</p> <p>Predictions files follow the naming pattern, where rank is the active GPU in which the predictions were made. predictions__rank_{rank}__batch_{batch_idx}.pt</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The Trainer instance.</p> required <code>pl_module</code> <code>LightningModule</code> <p>The LightningModule instance.</p> required <code>prediction</code> <code>Any</code> <p>The prediction made by the model.</p> required <code>batch_indices</code> <code>Sequence[int] | None</code> <p>The indices of the batch.</p> required <code>batch</code> <code>Any</code> <p>The batch data.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>The index of the dataloader.</p> required Source code in <code>bionemo/llm/utils/callbacks.py</code> <pre><code>@override\ndef write_on_batch_end(\n    self,\n    trainer: pl.Trainer,\n    pl_module: pl.LightningModule,\n    prediction: Any,\n    batch_indices: Sequence[int] | None,\n    batch: Any,\n    batch_idx: int,\n    dataloader_idx: int,\n) -&gt; None:\n    \"\"\"Writes predictions to disk at the end of each batch.\n\n    Predictions files follow the naming pattern, where rank is the active GPU in which the predictions were made.\n    predictions__rank_{rank}__batch_{batch_idx}.pt\n\n    Args:\n        trainer: The Trainer instance.\n        pl_module: The LightningModule instance.\n        prediction: The prediction made by the model.\n        batch_indices: The indices of the batch.\n        batch: The batch data.\n        batch_idx: The index of the batch.\n        dataloader_idx: The index of the dataloader.\n    \"\"\"\n    # this will create N (num processes) files in `output_dir` each containing\n    # the predictions of it's respective rank\n    if self.should_write_predictions:\n        if (\n            self.files_per_subdir is not None\n            and (self.num_files_written * self.data_parallel_world_size) &gt;= self.files_per_subdir\n        ):\n            self.num_subdirs_written += 1\n            self.output_dir = os.path.join(self.base_dir, f\"subdir_{self.num_subdirs_written}\")\n            os.makedirs(self.output_dir, exist_ok=True)\n            self.num_files_written = 0\n        result_path = os.path.join(\n            self.output_dir,\n            f\"predictions__rank_{trainer.global_rank}__dp_rank_{self.data_parallel_rank}__batch_{batch_idx}.pt\",\n        )\n\n        # batch_indices is not captured due to a lightning bug when return_predictions = False\n        # we use input IDs in the prediction to map the result to input.\n\n        # NOTE store the batch_idx so we do not need to rely on filenames for reconstruction of inputs. This is wrapped\n        # in a tensor and list container to ensure compatibility with batch_collator.\n        prediction[\"batch_idx\"] = torch.tensor([batch_idx], dtype=torch.int64)\n\n        torch.save(prediction, result_path)\n        logger.info(f\"Inference predictions are stored in {result_path}\\n{prediction.keys()}\")\n        self.num_files_written += 1\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/callbacks/#bionemo.llm.utils.callbacks.PredictionWriter.write_on_epoch_end","title":"<code>write_on_epoch_end(trainer, pl_module, predictions, batch_indices)</code>","text":"<p>Writes predictions to disk at the end of each epoch.</p> <p>Writing all predictions on epoch end is memory intensive. It is recommended to use the batch writer instead for large predictions.</p> <p>Multi-device predictions will likely yield predictions in an order that is inconsistent with single device predictions and the input data.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The Trainer instance.</p> required <code>pl_module</code> <code>LightningModule</code> <p>The LightningModule instance.</p> required <code>predictions</code> <code>Any</code> <p>The predictions made by the model.</p> required <code>batch_indices</code> <code>Sequence[int]</code> <p>The indices of the batch.</p> required Source code in <code>bionemo/llm/utils/callbacks.py</code> <pre><code>@override\ndef write_on_epoch_end(\n    self,\n    trainer: pl.Trainer,\n    pl_module: pl.LightningModule,\n    predictions: Any,\n    batch_indices: Sequence[int],\n) -&gt; None:\n    \"\"\"Writes predictions to disk at the end of each epoch.\n\n    Writing all predictions on epoch end is memory intensive. It is recommended to use the batch writer instead for\n    large predictions.\n\n    Multi-device predictions will likely yield predictions in an order that is inconsistent with single device predictions and the input data.\n\n    Args:\n        trainer: The Trainer instance.\n        pl_module: The LightningModule instance.\n        predictions: The predictions made by the model.\n        batch_indices: The indices of the batch.\n\n    Raises:\n        Multi-GPU predictions are output in an inconsistent order with multiple devices.\n    \"\"\"\n    # this will create N (num processes) files in `output_dir` each containing\n    # the predictions of it's respective rank\n    if self.should_write_predictions:\n        result_path = os.path.join(\n            self.output_dir,\n            f\"predictions__rank_{trainer.global_rank}__dp_rank_{self.data_parallel_rank}.pt\",\n        )\n\n        # collate multiple batches / ignore empty ones\n        collate_kwargs = {}\n        if self.batch_dim_key_defaults is not None:\n            collate_kwargs[\"batch_dim_key_defaults\"] = self.batch_dim_key_defaults\n        if self.seq_dim_key_defaults is not None:\n            collate_kwargs[\"seq_dim_key_defaults\"] = self.seq_dim_key_defaults\n\n        prediction = batch_collator([item for item in predictions if item is not None], **collate_kwargs)\n\n        # batch_indices is not captured due to a lightning bug when return_predictions = False\n        # we use input IDs in the prediction to map the result to input\n        if isinstance(prediction, dict):\n            keys = prediction.keys()\n        else:\n            keys = \"tensor\"\n        torch.save(prediction, result_path)\n        logger.info(f\"Inference predictions are stored in {result_path}\\n{keys}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/datamodule_utils/","title":"Datamodule utils","text":""},{"location":"main/references/API_reference/bionemo/llm/utils/datamodule_utils/#bionemo.llm.utils.datamodule_utils.float_or_int_or_none","title":"<code>float_or_int_or_none(value)</code>","text":"<p>Converts a given value into a float, int, or None.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, float, int, None]</code> <p>A value that can be either a string, float, int, or None.</p> required <p>Returns:</p> Type Description <code>Union[float, int, None]</code> <p>Union[float, int, None]: A float, int, or None based on the input value.</p> <p>If the input value is None or \"None\", it returns None. If the input value is an int or float, it returns the same value. If the input value is a string, it tries to convert it into an int if possible, otherwise into a float.</p> Source code in <code>bionemo/llm/utils/datamodule_utils.py</code> <pre><code>def float_or_int_or_none(value: Union[str, float, int, None]) -&gt; Union[float, int, None]:\n    \"\"\"Converts a given value into a float, int, or None.\n\n    Args:\n        value (Union[str, float, int, None]): A value that can be either a string, float, int, or None.\n\n    Returns:\n        Union[float, int, None]: A float, int, or None based on the input value.\n\n    If the input value is None or \"None\", it returns None.\n    If the input value is an int or float, it returns the same value.\n    If the input value is a string, it tries to convert it into an int if possible, otherwise into a float.\n    \"\"\"\n    if value is None or value == \"None\":\n        return\n    if isinstance(value, (int, float)):\n        return value\n    if value.isdigit():\n        return int(value)\n    return float(value)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/datamodule_utils/#bionemo.llm.utils.datamodule_utils.infer_global_batch_size","title":"<code>infer_global_batch_size(micro_batch_size, num_nodes, devices, accumulate_grad_batches=1, tensor_model_parallel_size=1, pipeline_model_parallel_size=1, context_model_parallel_size=1)</code>","text":"<p>Infers the global batch size based on the micro batch size, number of nodes, devices, accumulation of gradient batches, and model parallel sizes.</p> <p>Parameters:</p> Name Type Description Default <code>micro_batch_size</code> <code>int</code> <p>The micro batch size.</p> required <code>num_nodes</code> <code>int</code> <p>The number of nodes.</p> required <code>devices</code> <code>int</code> <p>The number of devices.</p> required <code>accumulate_grad_batches</code> <code>int</code> <p>The accumulation of gradient batches. Defaults to 1.</p> <code>1</code> <code>tensor_model_parallel_size</code> <code>int</code> <p>The tensor model parallel size. Defaults to 1.</p> <code>1</code> <code>pipeline_model_parallel_size</code> <code>int</code> <p>The pipeline model parallel size. Defaults to 1.</p> <code>1</code> <code>context_model_parallel_size</code> <code>int</code> <p>The context model parallel size. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The global batch size.</p> Source code in <code>bionemo/llm/utils/datamodule_utils.py</code> <pre><code>def infer_global_batch_size(\n    micro_batch_size: int,\n    num_nodes: int,\n    devices: int,\n    accumulate_grad_batches: int = 1,\n    tensor_model_parallel_size: int = 1,\n    pipeline_model_parallel_size: int = 1,\n    context_model_parallel_size: int = 1,\n) -&gt; int:\n    \"\"\"Infers the global batch size based on the micro batch size, number of nodes, devices, accumulation of gradient batches, and model parallel sizes.\n\n    Args:\n        micro_batch_size (int): The micro batch size.\n        num_nodes (int): The number of nodes.\n        devices (int): The number of devices.\n        accumulate_grad_batches (int): The accumulation of gradient batches. Defaults to 1.\n        tensor_model_parallel_size (int): The tensor model parallel size. Defaults to 1.\n        pipeline_model_parallel_size (int): The pipeline model parallel size. Defaults to 1.\n        context_model_parallel_size (int): The context model parallel size. Defaults to 1.\n\n    Returns:\n        int: The global batch size.\n    \"\"\"\n    if not all(\n        isinstance(arg, int)\n        for arg in [\n            micro_batch_size,\n            num_nodes,\n            devices,\n            accumulate_grad_batches,\n            tensor_model_parallel_size,\n            pipeline_model_parallel_size,\n            context_model_parallel_size,\n        ]\n    ):\n        raise ValueError(\n            f\"All arguments must be of type int, got {type(micro_batch_size)}, {type(num_nodes)}, {type(devices)}, \"\n            f\"{type(accumulate_grad_batches)}, {type(tensor_model_parallel_size)}, {type(pipeline_model_parallel_size)}, and {type(context_model_parallel_size)}\"\n        )\n    if micro_batch_size &lt;= 0:\n        raise ValueError(f\"micro_batch_size must be greater than 0, got {micro_batch_size}\")\n    if num_nodes &lt;= 0:\n        raise ValueError(f\"num_nodes must be greater than 0, got {num_nodes}\")\n    if devices &lt;= 0:\n        raise ValueError(f\"devices must be greater than 0, got {devices}\")\n    if accumulate_grad_batches &lt;= 0:\n        raise ValueError(f\"accumulate_grad_batches must be greater than 0, got {accumulate_grad_batches}\")\n    if tensor_model_parallel_size &lt;= 0:\n        raise ValueError(f\"tensor_model_parallel_size must be greater than 0, got {tensor_model_parallel_size}\")\n    if pipeline_model_parallel_size &lt;= 0:\n        raise ValueError(f\"pipeline_model_parallel_size must be greater than 0, got {pipeline_model_parallel_size}\")\n    if context_model_parallel_size &lt;= 0:\n        raise ValueError(f\"context_model_parallel_size must be greater than 0, got {context_model_parallel_size}\")\n\n    world_size = num_nodes * devices\n    if world_size % (tensor_model_parallel_size * pipeline_model_parallel_size * context_model_parallel_size) != 0:\n        raise ValueError(\n            f\"world_size must be divisible by tensor_model_parallel_size * pipeline_model_parallel_size * context_model_parallel_size, \"\n            f\"got {world_size} and TP{tensor_model_parallel_size} * PP{pipeline_model_parallel_size} * CP{context_model_parallel_size}\"\n        )\n\n    model_parallel_size = tensor_model_parallel_size * pipeline_model_parallel_size * context_model_parallel_size\n    data_parallel_size = world_size // model_parallel_size\n    global_batch_size = micro_batch_size * data_parallel_size * accumulate_grad_batches\n    return global_batch_size\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/datamodule_utils/#bionemo.llm.utils.datamodule_utils.infer_num_samples","title":"<code>infer_num_samples(limit_batches, num_samples_in_dataset, global_batch_size, stage)</code>","text":"<p>Infers the number of samples based on the limit_batches parameter, the length of the dataset, and the global batch size.</p> <p>Parameters:</p> Name Type Description Default <code>limit_batches</code> <code>Union[float, int, str, None]</code> <p>The limit on the number of batches. Can be a float between 0 and 1, an integer, a string, or None. If None, defaults to 1.0.</p> required <code>num_samples_in_dataset</code> <code>int</code> <p>The number of samples in the dataset.</p> required <code>global_batch_size</code> <code>int</code> <p>The global batch size.</p> required <code>stage</code> <code>str</code> <p>The stage of the training.</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>The number of samples from the limit.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the limited number of samples is less than the global batch size, or if the limit_batches parameter is invalid.</p> <p>If limit_batches is a float between 0 and 1, the number of samples is inferred as a fraction of the number of samples in the dataset. If limit_batches is an integer greater than or equal to 1, the number of limited samples is inferred as the product of limit_batches and global batch size. If limit_batches is None, it defaults to 1.0, indicating that all dataset samples should be used.</p> Source code in <code>bionemo/llm/utils/datamodule_utils.py</code> <pre><code>def infer_num_samples(\n    limit_batches: Union[float, int, str, None], num_samples_in_dataset: int, global_batch_size: int, stage: str\n):\n    \"\"\"Infers the number of samples based on the limit_batches parameter, the length of the dataset, and the global batch size.\n\n    Args:\n        limit_batches (Union[float, int, str, None]): The limit on the number of batches. Can be a float\n            between 0 and 1, an integer, a string, or None. If None, defaults to 1.0.\n        num_samples_in_dataset (int): The number of samples in the dataset.\n        global_batch_size (int): The global batch size.\n        stage (str): The stage of the training.\n\n    Returns:\n        int: The number of samples from the limit.\n\n    Raises:\n        ValueError: If the limited number of samples is less than the global batch size, or if the\n            limit_batches parameter is invalid.\n\n    If limit_batches is a float between 0 and 1, the number of samples is inferred as a fraction of the number of samples\n    in the dataset. If limit_batches is an integer greater than or equal to 1, the number of limited samples is inferred\n    as the product of limit_batches and global batch size. If limit_batches is None, it defaults to 1.0, indicating that\n    all dataset samples should be used.\n    \"\"\"\n    limit_batches = 1.0 if limit_batches is None else limit_batches  # validation data does not require upsampling\n    if 0 &lt; limit_batches &lt;= 1.0 and isinstance(limit_batches, float):\n        num_limited_samples = int(num_samples_in_dataset * limit_batches)\n        if num_limited_samples &lt; global_batch_size:\n            raise ValueError(\n                \"The limited number of %s samples %s is less than the global batch size %s\"\n                % (stage, num_limited_samples, global_batch_size)\n            )\n    elif limit_batches &gt;= 1 and isinstance(limit_batches, int):\n        num_limited_samples = int(limit_batches * global_batch_size)\n    else:\n        raise ValueError(\"Invalid choice of limit_%s_batches size: %s\" % (stage, limit_batches))\n\n    return num_limited_samples\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/datamodule_utils/#bionemo.llm.utils.datamodule_utils.parse_kwargs_to_arglist","title":"<code>parse_kwargs_to_arglist(kwargs)</code>","text":"<p>Converts a dictionary of keyword arguments into a list of command-line arguments.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Dict[str, Any]</code> <p>A dictionary where keys are argument names and values are argument values.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of strings, where each string is a command-line argument in the format '--argument-name value'.</p> Source code in <code>bionemo/llm/utils/datamodule_utils.py</code> <pre><code>def parse_kwargs_to_arglist(kwargs: Dict[str, Any]) -&gt; List[str]:\n    \"\"\"Converts a dictionary of keyword arguments into a list of command-line arguments.\n\n    Args:\n        kwargs (Dict[str, Any]): A dictionary where keys are argument names and values are argument values.\n\n    Returns:\n        A list of strings, where each string is a command-line argument in the format '--argument-name value'.\n    \"\"\"\n    arglist = []\n    for k, v in kwargs.items():\n        arglist.extend([f\"--{k.replace('_', '-')}\", str(v)])\n    return arglist\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/iomixin_utils/","title":"Iomixin utils","text":""},{"location":"main/references/API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.IOMixinWithGettersSetters","title":"<code>IOMixinWithGettersSetters</code>","text":"<p>               Bases: <code>WillHaveGetSetHparam</code>, <code>IOMixin</code></p> <p>An implementation of WillHaveGetSetHparam which makes use of the io.IOMixin.io added to your classes.</p> <p>This enables you to mutate the hyper-parameters of your classes which will later be saved in configs.</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>class IOMixinWithGettersSetters(WillHaveGetSetHparam, io.IOMixin):\n    \"\"\"An implementation of WillHaveGetSetHparam which makes use of the io.IOMixin.__io__ added to your classes.\n\n    This enables you to mutate the hyper-parameters of your classes which will later be saved in configs.\n    \"\"\"\n\n    def set_hparam(self, attribute: str, value: Any, also_change_value: bool = True) -&gt; None:\n        \"\"\"Mutates the saved hyper-parameter for the io mixed class.\n\n        If you would like to only change the saved hyper-param\n            for example in the case of loading a dataclass where the same variables are mutated to other non-savable\n            entities by deterministic rules after init, then use `also_change_value=False` to only update the\n            hyper-parameter.\n\n        Args:\n            attribute: The element name to modify within the saved init settings for self\n            value: New parameter for the saved init settings\n            also_change_value: If you also want to mutate the attribute of this same name in self to be the desired\n                value, set this to True, otherwise if the init arg and self arg are expected to be divergent, then\n                do not set this and modify the self attribute separately in the normal pythonic way.\n\n        Returns:\n            None.\n        \"\"\"\n        # Change the attribute of self and also change the io tracker so it gets updated in the config\n        if also_change_value:\n            setattr(self, attribute, value)\n        setattr(self.__io__, attribute, value)\n\n    def get_hparam(self, attribute: str) -&gt; Any:\n        \"\"\"Looks up the saved hyper-parameter for the io mixed class.\n\n        Args:\n            attribute: The element name to look up within the saved init settings for self\n        Returns:\n            Value\n        Raises:\n            KeyError if the attribute does not exist in the saved init settings.\n        \"\"\"\n        if attribute not in dir(self.__io__):\n            raise KeyError(\n                f\"Attribute '{attribute}' not found in hyper-parameters. Options: {sorted(self.get_hparams().keys())}\"\n            )\n        return getattr(self.__io__, attribute)\n\n    def get_non_default_hparams(self) -&gt; List[str]:\n        \"\"\"Returns a list of hyper-parameters that have been changed from their default values.\n\n        Returns:\n            List[str]: A list of hyper-parameters that have been changed from their default values.\n        \"\"\"\n        return [k for k in self.__io__.__dict__[\"__argument_history__\"].keys() if k != \"__fn_or_cls__\"]\n\n    def get_hparams(self) -&gt; Dict[str, Any]:\n        \"\"\"Returns the hyper-parameters of init in a dictionary format.\n\n        Returns:\n            Dict[str, Any]: A dictionary of the init hyper-parameters on this object.\n        \"\"\"\n        return {k: getattr(self.__io__, k) for k in self.get_non_default_hparams()}\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.IOMixinWithGettersSetters.get_hparam","title":"<code>get_hparam(attribute)</code>","text":"<p>Looks up the saved hyper-parameter for the io mixed class.</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>str</code> <p>The element name to look up within the saved init settings for self</p> required <p>Returns:     Value Raises:     KeyError if the attribute does not exist in the saved init settings.</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>def get_hparam(self, attribute: str) -&gt; Any:\n    \"\"\"Looks up the saved hyper-parameter for the io mixed class.\n\n    Args:\n        attribute: The element name to look up within the saved init settings for self\n    Returns:\n        Value\n    Raises:\n        KeyError if the attribute does not exist in the saved init settings.\n    \"\"\"\n    if attribute not in dir(self.__io__):\n        raise KeyError(\n            f\"Attribute '{attribute}' not found in hyper-parameters. Options: {sorted(self.get_hparams().keys())}\"\n        )\n    return getattr(self.__io__, attribute)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.IOMixinWithGettersSetters.get_hparams","title":"<code>get_hparams()</code>","text":"<p>Returns the hyper-parameters of init in a dictionary format.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary of the init hyper-parameters on this object.</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>def get_hparams(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns the hyper-parameters of init in a dictionary format.\n\n    Returns:\n        Dict[str, Any]: A dictionary of the init hyper-parameters on this object.\n    \"\"\"\n    return {k: getattr(self.__io__, k) for k in self.get_non_default_hparams()}\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.IOMixinWithGettersSetters.get_non_default_hparams","title":"<code>get_non_default_hparams()</code>","text":"<p>Returns a list of hyper-parameters that have been changed from their default values.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of hyper-parameters that have been changed from their default values.</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>def get_non_default_hparams(self) -&gt; List[str]:\n    \"\"\"Returns a list of hyper-parameters that have been changed from their default values.\n\n    Returns:\n        List[str]: A list of hyper-parameters that have been changed from their default values.\n    \"\"\"\n    return [k for k in self.__io__.__dict__[\"__argument_history__\"].keys() if k != \"__fn_or_cls__\"]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.IOMixinWithGettersSetters.set_hparam","title":"<code>set_hparam(attribute, value, also_change_value=True)</code>","text":"<p>Mutates the saved hyper-parameter for the io mixed class.</p> <p>If you would like to only change the saved hyper-param     for example in the case of loading a dataclass where the same variables are mutated to other non-savable     entities by deterministic rules after init, then use <code>also_change_value=False</code> to only update the     hyper-parameter.</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>str</code> <p>The element name to modify within the saved init settings for self</p> required <code>value</code> <code>Any</code> <p>New parameter for the saved init settings</p> required <code>also_change_value</code> <code>bool</code> <p>If you also want to mutate the attribute of this same name in self to be the desired value, set this to True, otherwise if the init arg and self arg are expected to be divergent, then do not set this and modify the self attribute separately in the normal pythonic way.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>def set_hparam(self, attribute: str, value: Any, also_change_value: bool = True) -&gt; None:\n    \"\"\"Mutates the saved hyper-parameter for the io mixed class.\n\n    If you would like to only change the saved hyper-param\n        for example in the case of loading a dataclass where the same variables are mutated to other non-savable\n        entities by deterministic rules after init, then use `also_change_value=False` to only update the\n        hyper-parameter.\n\n    Args:\n        attribute: The element name to modify within the saved init settings for self\n        value: New parameter for the saved init settings\n        also_change_value: If you also want to mutate the attribute of this same name in self to be the desired\n            value, set this to True, otherwise if the init arg and self arg are expected to be divergent, then\n            do not set this and modify the self attribute separately in the normal pythonic way.\n\n    Returns:\n        None.\n    \"\"\"\n    # Change the attribute of self and also change the io tracker so it gets updated in the config\n    if also_change_value:\n        setattr(self, attribute, value)\n    setattr(self.__io__, attribute, value)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.WillHaveGetSetHparam","title":"<code>WillHaveGetSetHparam</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An ABC that states that a particular class will have our mutatable IO Mixin variant added to it.</p> <p>This is a placeholder until a similar piece of functionality is added in NeMo.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>You must implement set_hparam, get_hparam, and get_hparams</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>class WillHaveGetSetHparam(ABC):\n    \"\"\"An ABC that states that a particular class _will_ have our mutatable IO Mixin variant added to it.\n\n    This is a placeholder until a similar piece of functionality is added in NeMo.\n\n\n    Raises:\n        NotImplementedError: You must implement set_hparam, get_hparam, and get_hparams\n    \"\"\"\n\n    @abstractmethod\n    def set_hparam(self, attribute: str, value: Any, also_change_value: bool = True) -&gt; None:\n        \"\"\"Mutates the saved hyper-parameter for the io mixed class.\n\n        If you would like to only change the saved hyper-param\n            for example in the case of loading a dataclass where the same variables are mutated to other non-savable\n            entities by deterministic rules after init, then use `also_change_value=False` to only update the\n            hyper-parameter.\n\n        Args:\n            attribute: The element name to modify within the saved init settings for self\n            value: New parameter for the saved init settings\n            also_change_value: If you also want to mutate the attribute of this same name in self to be the desired\n                value, set this to True, otherwise if the init arg and self arg are expected to be divergent, then\n                do not set this and modify the self attribute separately in the normal pythonic way.\n\n        Returns:\n            None.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def get_hparam(self, attribute: str) -&gt; Any:\n        \"\"\"Looks up the saved hyper-parameter for the io mixed class.\n\n        Args:\n            attribute: The element name to look up within the saved init settings for self\n        Returns:\n            Value\n        Raises:\n            KeyError if the attribute does not exist in the saved init settings.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def get_hparams(self) -&gt; Dict[str, Any]:\n        \"\"\"Returns the hyper-parameters of init in a dictionary format.\n\n        Returns:\n            Dict[str, Any]: A dictionary of the init hyper-parameters on this object.\n        \"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.WillHaveGetSetHparam.get_hparam","title":"<code>get_hparam(attribute)</code>  <code>abstractmethod</code>","text":"<p>Looks up the saved hyper-parameter for the io mixed class.</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>str</code> <p>The element name to look up within the saved init settings for self</p> required <p>Returns:     Value Raises:     KeyError if the attribute does not exist in the saved init settings.</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>@abstractmethod\ndef get_hparam(self, attribute: str) -&gt; Any:\n    \"\"\"Looks up the saved hyper-parameter for the io mixed class.\n\n    Args:\n        attribute: The element name to look up within the saved init settings for self\n    Returns:\n        Value\n    Raises:\n        KeyError if the attribute does not exist in the saved init settings.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.WillHaveGetSetHparam.get_hparams","title":"<code>get_hparams()</code>  <code>abstractmethod</code>","text":"<p>Returns the hyper-parameters of init in a dictionary format.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary of the init hyper-parameters on this object.</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>@abstractmethod\ndef get_hparams(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns the hyper-parameters of init in a dictionary format.\n\n    Returns:\n        Dict[str, Any]: A dictionary of the init hyper-parameters on this object.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.WillHaveGetSetHparam.set_hparam","title":"<code>set_hparam(attribute, value, also_change_value=True)</code>  <code>abstractmethod</code>","text":"<p>Mutates the saved hyper-parameter for the io mixed class.</p> <p>If you would like to only change the saved hyper-param     for example in the case of loading a dataclass where the same variables are mutated to other non-savable     entities by deterministic rules after init, then use <code>also_change_value=False</code> to only update the     hyper-parameter.</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>str</code> <p>The element name to modify within the saved init settings for self</p> required <code>value</code> <code>Any</code> <p>New parameter for the saved init settings</p> required <code>also_change_value</code> <code>bool</code> <p>If you also want to mutate the attribute of this same name in self to be the desired value, set this to True, otherwise if the init arg and self arg are expected to be divergent, then do not set this and modify the self attribute separately in the normal pythonic way.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>@abstractmethod\ndef set_hparam(self, attribute: str, value: Any, also_change_value: bool = True) -&gt; None:\n    \"\"\"Mutates the saved hyper-parameter for the io mixed class.\n\n    If you would like to only change the saved hyper-param\n        for example in the case of loading a dataclass where the same variables are mutated to other non-savable\n        entities by deterministic rules after init, then use `also_change_value=False` to only update the\n        hyper-parameter.\n\n    Args:\n        attribute: The element name to modify within the saved init settings for self\n        value: New parameter for the saved init settings\n        also_change_value: If you also want to mutate the attribute of this same name in self to be the desired\n            value, set this to True, otherwise if the init arg and self arg are expected to be divergent, then\n            do not set this and modify the self attribute separately in the normal pythonic way.\n\n    Returns:\n        None.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/logger_utils/","title":"Logger utils","text":""},{"location":"main/references/API_reference/bionemo/llm/utils/logger_utils/#bionemo.llm.utils.logger_utils.WandbConfig","title":"<code>WandbConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Note: <code>name</code> controls the exp name is handled by the NeMoLogger so it is ommitted here. <code>directory</code> is also omitted since it is set by the NeMoLogger.</p> <p>Parameters:</p> Name Type Description Default <code>entity</code> <p>The team posting this run (default: your username or your default team)</p> required <code>project</code> <p>The name of the project to which this run will belong.</p> required <code>name</code> <p>Display name for the run. By default it is set by NeMoLogger to experiment name</p> required <code>tags</code> <p>Tags associated with this run.</p> required <code>group</code> <p>A unique string shared by all runs in a given group</p> required <code>job_type</code> <p>Type of run, which is useful when you're grouping runs together into larger experiments.</p> required <code>offline</code> <p>Run offline (data can be streamed later to wandb servers).</p> required <code>id</code> <p>Sets the version, mainly used to resume a previous run.</p> required <code>anonymous</code> <p>Enables or explicitly disables anonymous logging.</p> required Source code in <code>bionemo/llm/utils/logger_utils.py</code> <pre><code>class WandbConfig(BaseModel):\n    \"\"\"Note: `name` controls the exp name is handled by the NeMoLogger so it is ommitted here.\n    `directory` is also omitted since it is set by the NeMoLogger.\n\n    Args:\n        entity: The team posting this run (default: your username or your default team)\n        project: The name of the project to which this run will belong.\n        name: Display name for the run. By default it is set by NeMoLogger to experiment name\n        tags: Tags associated with this run.\n        group: A unique string shared by all runs in a given group\n        job_type: Type of run, which is useful when you're grouping runs together into larger experiments.\n        offline: Run offline (data can be streamed later to wandb servers).\n        id: Sets the version, mainly used to resume a previous run.\n        anonymous: Enables or explicitly disables anonymous logging.\n    \"\"\"  # noqa: D205\n\n    entity: str | None  # The team posting this run (default: your username or your default team)\n    project: str  # The name of the project to which this run will belong.\n    name: str | None = None  # Display name for the run. By default, it is set by NeMoLogger to experiment name\n    # save_dir: #Path where data is saved. \"This is handled by NeMoLogger\"\n    tags: List[str] | None  # Tags associated with this run.\n    group: str | None  # A unique string shared by all runs in a given group.\n    job_type: str | None = (\n        None  # Type of run, which is useful when you're grouping runs together into larger experiments.\n    )\n    offline: bool  # Run offline (data can be streamed later to wandb servers).\n    id: str | None  # Sets the version, mainly used to resume a previous run.\n    anonymous: bool  # Enables or explicitly disables anonymous logging.\n    log_model: bool  # Save checkpoints in wandb dir to upload on W&amp;B servers.\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/logger_utils/#bionemo.llm.utils.logger_utils.setup_nemo_lightning_logger","title":"<code>setup_nemo_lightning_logger(name=None, root_dir='./results', initialize_tensorboard_logger=False, wandb_config=None, ckpt_callback=None, **kwargs)</code>","text":"<p>Setup the logger for the experiment.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str | None</code> <p>The name of the experiment. Results go into <code>root_dir</code>/<code>name</code></p> <code>None</code> <code>root_dir</code> <code>str | Path</code> <p>The root directory to create the <code>name</code> directory in for saving run results.</p> <code>'./results'</code> <code>initialize_tensorboard_logger</code> <code>bool</code> <p>Whether to initialize the tensorboard logger.</p> <code>False</code> <code>wandb_config</code> <code>Optional[WandbConfig]</code> <p>The remaining configuration options for the wandb logger.</p> <code>None</code> <code>ckpt_callback</code> <code>Optional[ModelCheckpoint]</code> <p>The checkpoint callback to use, must be a child of the pytorch lightning ModelCheckpoint callback. NOTE the type annotation in the underlying NeMoCheckpoint constructor is incorrect.</p> <code>None</code> <code>**kwargs</code> <code>Dict[str, Any]</code> <p>The kwargs for the NeMoLogger.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>NeMoLogger</code> <code>NeMoLogger</code> <p>NeMo logger instance.</p> Source code in <code>bionemo/llm/utils/logger_utils.py</code> <pre><code>def setup_nemo_lightning_logger(\n    name: str | None = None,\n    root_dir: str | pathlib.Path = \"./results\",\n    initialize_tensorboard_logger: bool = False,\n    wandb_config: Optional[WandbConfig] = None,\n    ckpt_callback: Optional[nemo_callbacks.ModelCheckpoint] = None,\n    **kwargs: Dict[str, Any],\n) -&gt; NeMoLogger:\n    \"\"\"Setup the logger for the experiment.\n\n    Arguments:\n        name: The name of the experiment. Results go into `root_dir`/`name`\n        root_dir: The root directory to create the `name` directory in for saving run results.\n        initialize_tensorboard_logger: Whether to initialize the tensorboard logger.\n        wandb_config: The remaining configuration options for the wandb logger.\n        ckpt_callback: The checkpoint callback to use, must be a child of the pytorch lightning ModelCheckpoint callback.\n            NOTE the type annotation in the underlying NeMoCheckpoint constructor is incorrect.\n        **kwargs: The kwargs for the NeMoLogger.\n\n    Returns:\n        NeMoLogger: NeMo logger instance.\n    \"\"\"\n    # The directory that the logger will save to\n    save_dir = pathlib.Path(root_dir) / name\n    save_dir.mkdir(parents=True, exist_ok=True)\n\n    version = \"dev\"\n    if wandb_config is not None:\n        if wandb_config.name is None:\n            wandb_config.name = name\n        wandb_logger = WandbLogger(save_dir=save_dir, **wandb_config.model_dump())\n    else:\n        wandb_logger = None\n        logging.warning(\"WandB is currently turned off.\")\n    if initialize_tensorboard_logger:\n        tb_logger = TensorBoardLogger(save_dir=root_dir, name=name, version=version)\n    else:\n        tb_logger = None\n        logging.warning(\"User-set tensorboard is currently turned off. Internally one may still be set by NeMo2.\")\n\n    logger: NeMoLogger = NeMoLogger(\n        name=name,\n        log_dir=str(root_dir),\n        tensorboard=tb_logger,\n        wandb=wandb_logger,\n        ckpt=ckpt_callback,\n        version=version,\n        update_logger_directory=False,\n        **kwargs,\n    )\n    # Needed so that the trainer can find an output directory for the profiler\n    logger.save_dir = save_dir\n    return logger\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/megatron_utils/","title":"Megatron utils","text":""},{"location":"main/references/API_reference/bionemo/llm/utils/megatron_utils/#bionemo.llm.utils.megatron_utils.average_losses_across_data_parallel_group","title":"<code>average_losses_across_data_parallel_group(losses, with_context_parallel=False)</code>","text":"<p>Reduce a tensor of losses across all GPUs.</p> Source code in <code>bionemo/llm/utils/megatron_utils.py</code> <pre><code>def average_losses_across_data_parallel_group(losses, with_context_parallel: bool = False):\n    \"\"\"Reduce a tensor of losses across all GPUs.\"\"\"\n    averaged_losses = torch.cat([loss.clone().detach().view(1) for loss in losses])\n    # Reduce across the DP (or optionally, the flattened DP + CP) group.\n    # Refer to the ring attention algorithm on why we always must reduce across the CP group.\n    torch.distributed.all_reduce(\n        averaged_losses, group=parallel_state.get_data_parallel_group(with_context_parallel=with_context_parallel)\n    )\n    averaged_losses = averaged_losses / torch.distributed.get_world_size(\n        # Only average losses across the data parallel group, not the context parallel group!\n        group=parallel_state.get_data_parallel_group()\n    )\n    return averaged_losses\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/megatron_utils/#bionemo.llm.utils.megatron_utils.is_only_data_parallel","title":"<code>is_only_data_parallel()</code>","text":"<p>Checks to see if you are in a distributed megatron environment with only data parallelism active.</p> <p>This is useful if you are working on a model, loss, etc and you know that you do not yet support megatron model parallelism. You can test that the only kind of parallelism in use is data parallelism.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if data parallel is the only parallel mode, False otherwise.</p> Source code in <code>bionemo/llm/utils/megatron_utils.py</code> <pre><code>def is_only_data_parallel() -&gt; bool:\n    \"\"\"Checks to see if you are in a distributed megatron environment with only data parallelism active.\n\n    This is useful if you are working on a model, loss, etc and you know that you do not yet support megatron model\n    parallelism. You can test that the only kind of parallelism in use is data parallelism.\n\n    Returns:\n        True if data parallel is the only parallel mode, False otherwise.\n    \"\"\"\n    if not (torch.distributed.is_available() and parallel_state.is_initialized()):\n        raise RuntimeError(\"This function is only defined within an initialized megatron parallel environment.\")\n    # Idea: when world_size == data_parallel_world_size, then you know that you are fully DDP, which means you are not\n    #  using model parallelism (meaning virtual GPUs composed of several underlying GPUs that you need to reduce over).\n\n    world_size: int = torch.distributed.get_world_size()\n    dp_world_size: int = parallel_state.get_data_parallel_world_size()\n    return world_size == dp_world_size\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/remote/","title":"Remote","text":""},{"location":"main/references/API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.FTPRemoteResource","title":"<code>FTPRemoteResource</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RemoteResource</code></p> Source code in <code>bionemo/llm/utils/remote.py</code> <pre><code>class FTPRemoteResource(RemoteResource):  # noqa: D101\n    def download_resource(self, overwrite=False) -&gt; str:\n        \"\"\"Downloads the resource to its specified fully_qualified_dest name.\n\n        Returns: the fully qualified destination filename.\n        \"\"\"\n        self.exists_or_create_destination_directory()\n\n        if not self.check_exists() or overwrite:\n            request.urlretrieve(self.url, self.fully_qualified_dest_filename)\n\n        self.check_exists()\n        return self.fully_qualified_dest_filename\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.FTPRemoteResource.download_resource","title":"<code>download_resource(overwrite=False)</code>","text":"<p>Downloads the resource to its specified fully_qualified_dest name.</p> <p>Returns: the fully qualified destination filename.</p> Source code in <code>bionemo/llm/utils/remote.py</code> <pre><code>def download_resource(self, overwrite=False) -&gt; str:\n    \"\"\"Downloads the resource to its specified fully_qualified_dest name.\n\n    Returns: the fully qualified destination filename.\n    \"\"\"\n    self.exists_or_create_destination_directory()\n\n    if not self.check_exists() or overwrite:\n        request.urlretrieve(self.url, self.fully_qualified_dest_filename)\n\n    self.check_exists()\n    return self.fully_qualified_dest_filename\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.RemoteResource","title":"<code>RemoteResource</code>  <code>dataclass</code>","text":"<p>Responsible for downloading remote files, along with optional processing of downloaded files for downstream usecases.</p> <p>Each object is invoked through either its constructor (setting up the destination and checksum), or through a pre-configured class method. <code>download_resource()</code> contains the core functionality, which is to download the file at <code>url</code> to the fully qualified filename. Class methods can be used to further configure this process.</p> Receive <p>a file, its checksum, a destination directory, and a root directory</p> <p>Our dataclass then provides some useful things:     - fully qualified destination folder (property)     - fully qualified destination file (property)     - check_exists()     - download_resource()</p> <p>Form the fully qualified destination folder. Create a fully qualified path for the file</p> <p>(all lives in the download routine) Check that the fq destination folder exists, otherwise create it Download the file. Checksum the download. Done.</p> <p>Postprocessing should be their own method with their own configuration.</p> Example usage <p>Attributes:</p> Name Type Description <code>dest_directory</code> <code>str</code> <p>The directory to place the desired file upon completing the download. Should have the form {dest_directory}/{dest_filename}</p> <code>dest_filename</code> <code>str</code> <p>The desired name for the file upon completing the download.</p> <code>checksum</code> <code>Optional[str]</code> <p>checksum associated with the file located at url. If set to None, check_exists only checks for the existance of <code>{dest_directory}/{dest_filename}</code></p> <code>url</code> <code>Optional[str]</code> <p>URL of the file to download</p> <code>root_directory</code> <code>str | PathLike</code> <p>the bottom-level directory, the fully qualified path is formed by joining root_directory, dest_directory, and dest_filename.</p> Source code in <code>bionemo/llm/utils/remote.py</code> <pre><code>@dataclass\nclass RemoteResource:\n    \"\"\"Responsible for downloading remote files, along with optional processing of downloaded files for downstream usecases.\n\n    Each object is invoked through either its constructor (setting up the destination and checksum), or through a pre-configured class method.\n    `download_resource()` contains the core functionality, which is to download the file at `url` to the fully qualified filename. Class methods\n    can be used to further configure this process.\n\n    Receive:\n        a file, its checksum, a destination directory, and a root directory\n\n        Our dataclass then provides some useful things:\n            - fully qualified destination folder (property)\n            - fully qualified destination file (property)\n            - check_exists()\n            - download_resource()\n\n        Form the fully qualified destination folder.\n        Create a fully qualified path for the file\n\n        (all lives in the download routine)\n        Check that the fq destination folder exists, otherwise create it\n        Download the file.\n        Checksum the download.\n        Done.\n\n        Postprocessing should be their own method with their own configuration.\n\n    Example usage:\n        &gt;&gt;&gt; # The following will download and preprocess the prepackaged resources.\n        &gt;&gt;&gt; GRCh38Ensembl99ResourcePreparer().prepare()\n        &gt;&gt;&gt; Hg38chromResourcePreparer().prepare()\n        &gt;&gt;&gt; GRCh38p13_ResourcePreparer().prepare()\n\n\n    Attributes:\n        dest_directory: The directory to place the desired file upon completing the download. Should have the form {dest_directory}/{dest_filename}\n        dest_filename: The desired name for the file upon completing the download.\n        checksum: checksum associated with the file located at url. If set to None, check_exists only checks for the existance of `{dest_directory}/{dest_filename}`\n        url: URL of the file to download\n        root_directory: the bottom-level directory, the fully qualified path is formed by joining root_directory, dest_directory, and dest_filename.\n    \"\"\"\n\n    checksum: Optional[str]\n    dest_filename: str\n    dest_directory: str\n    root_directory: str | os.PathLike = BIONEMO_CACHE_DIR\n    url: Optional[str] = None\n\n    @property\n    def fully_qualified_dest_folder(self):  # noqa: D102\n        return Path(self.root_directory) / self.dest_directory\n\n    @property\n    def fully_qualified_dest_filename(self):\n        \"\"\"Returns the fully qualified destination path of the file.\n\n        Example:\n            /tmp/my_folder/file.tar.gz\n        \"\"\"\n        return os.path.join(self.fully_qualified_dest_folder, self.dest_filename)\n\n    def exists_or_create_destination_directory(self, exist_ok=True):\n        \"\"\"Checks that the `fully_qualified_destination_directory` exists, if it does not, the directory is created (or fails).\n\n        exists_ok: Triest to create `fully_qualified_dest_folder` if it doesnt already exist.\n        \"\"\"\n        os.makedirs(self.fully_qualified_dest_folder, exist_ok=exist_ok)\n\n    @staticmethod\n    def get_env_tmpdir():\n        \"\"\"Convenience method that exposes the environment TMPDIR variable.\"\"\"\n        return os.environ.get(\"TMPDIR\", \"/tmp\")\n\n    def download_resource(self, overwrite=False) -&gt; str:\n        \"\"\"Downloads the resource to its specified fully_qualified_dest name.\n\n        Returns: the fully qualified destination filename.\n        \"\"\"\n        self.exists_or_create_destination_directory()\n\n        if not self.check_exists() or overwrite:\n            logging.info(f\"Downloading resource: {self.url}\")\n            with requests.get(self.url, stream=True) as r, open(self.fully_qualified_dest_filename, \"wb\") as fd:\n                r.raise_for_status()\n                for bytes in r:\n                    fd.write(bytes)\n        else:\n            logging.info(f\"Resource already exists, skipping download: {self.url}\")\n\n        self.check_exists()\n        return self.fully_qualified_dest_filename\n\n    def check_exists(self):\n        \"\"\"Returns true if `fully_qualified_dest_filename` exists and the checksum matches `self.checksum`\"\"\"  # noqa: D415\n        if os.path.exists(self.fully_qualified_dest_filename):\n            with open(self.fully_qualified_dest_filename, \"rb\") as fd:\n                data = fd.read()\n                result = md5(data).hexdigest()\n            if self.checksum is None:\n                logging.info(\"No checksum provided, filename exists. Assuming it is complete.\")\n                matches = True\n            else:\n                matches = result == self.checksum\n            return matches\n\n        return False\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.RemoteResource--the-following-will-download-and-preprocess-the-prepackaged-resources","title":"The following will download and preprocess the prepackaged resources.","text":"<p>GRCh38Ensembl99ResourcePreparer().prepare() Hg38chromResourcePreparer().prepare() GRCh38p13_ResourcePreparer().prepare()</p>"},{"location":"main/references/API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.RemoteResource.fully_qualified_dest_filename","title":"<code>fully_qualified_dest_filename</code>  <code>property</code>","text":"<p>Returns the fully qualified destination path of the file.</p> Example <p>/tmp/my_folder/file.tar.gz</p>"},{"location":"main/references/API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.RemoteResource.check_exists","title":"<code>check_exists()</code>","text":"<p>Returns true if <code>fully_qualified_dest_filename</code> exists and the checksum matches <code>self.checksum</code></p> Source code in <code>bionemo/llm/utils/remote.py</code> <pre><code>def check_exists(self):\n    \"\"\"Returns true if `fully_qualified_dest_filename` exists and the checksum matches `self.checksum`\"\"\"  # noqa: D415\n    if os.path.exists(self.fully_qualified_dest_filename):\n        with open(self.fully_qualified_dest_filename, \"rb\") as fd:\n            data = fd.read()\n            result = md5(data).hexdigest()\n        if self.checksum is None:\n            logging.info(\"No checksum provided, filename exists. Assuming it is complete.\")\n            matches = True\n        else:\n            matches = result == self.checksum\n        return matches\n\n    return False\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.RemoteResource.download_resource","title":"<code>download_resource(overwrite=False)</code>","text":"<p>Downloads the resource to its specified fully_qualified_dest name.</p> <p>Returns: the fully qualified destination filename.</p> Source code in <code>bionemo/llm/utils/remote.py</code> <pre><code>def download_resource(self, overwrite=False) -&gt; str:\n    \"\"\"Downloads the resource to its specified fully_qualified_dest name.\n\n    Returns: the fully qualified destination filename.\n    \"\"\"\n    self.exists_or_create_destination_directory()\n\n    if not self.check_exists() or overwrite:\n        logging.info(f\"Downloading resource: {self.url}\")\n        with requests.get(self.url, stream=True) as r, open(self.fully_qualified_dest_filename, \"wb\") as fd:\n            r.raise_for_status()\n            for bytes in r:\n                fd.write(bytes)\n    else:\n        logging.info(f\"Resource already exists, skipping download: {self.url}\")\n\n    self.check_exists()\n    return self.fully_qualified_dest_filename\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.RemoteResource.exists_or_create_destination_directory","title":"<code>exists_or_create_destination_directory(exist_ok=True)</code>","text":"<p>Checks that the <code>fully_qualified_destination_directory</code> exists, if it does not, the directory is created (or fails).</p> <p>exists_ok: Triest to create <code>fully_qualified_dest_folder</code> if it doesnt already exist.</p> Source code in <code>bionemo/llm/utils/remote.py</code> <pre><code>def exists_or_create_destination_directory(self, exist_ok=True):\n    \"\"\"Checks that the `fully_qualified_destination_directory` exists, if it does not, the directory is created (or fails).\n\n    exists_ok: Triest to create `fully_qualified_dest_folder` if it doesnt already exist.\n    \"\"\"\n    os.makedirs(self.fully_qualified_dest_folder, exist_ok=exist_ok)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.RemoteResource.get_env_tmpdir","title":"<code>get_env_tmpdir()</code>  <code>staticmethod</code>","text":"<p>Convenience method that exposes the environment TMPDIR variable.</p> Source code in <code>bionemo/llm/utils/remote.py</code> <pre><code>@staticmethod\ndef get_env_tmpdir():\n    \"\"\"Convenience method that exposes the environment TMPDIR variable.\"\"\"\n    return os.environ.get(\"TMPDIR\", \"/tmp\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/weight_utils/","title":"Weight utils","text":""},{"location":"main/references/API_reference/bionemo/llm/utils/weight_utils/#bionemo.llm.utils.weight_utils.load_weights_sharded_inplace_nemo2_to_mcore","title":"<code>load_weights_sharded_inplace_nemo2_to_mcore(model, distributed_checkpoint_dir, skip_keys_with_these_prefixes)</code>","text":"<p>Given a megatron module, this function will determine which keys/subsets of weights to load given the     parallel/distributed state. This operates assuming a checkpoint was saved by a nemo2 trainer which places     the <code>module.</code> prefix on all key names, but we are then going to load directly in to the megatron module     without the <code>module.</code> prefix. Note that if there are any extra keys that you do not want to search the     checkpoint for, for example if you add new layers/heads onto your module, you need to supply the prefix     path to those keys in your model and they will be ignored. This latter feature is key for flexible fine-tuning     strategies where you load weights partially from other models with partially overlapping structures.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>MegatronModelType</code> <p>Megatron model that you want to load weights into.</p> required <code>distributed_checkpoint_dir</code> <code>str | Path</code> <p>description</p> required <code>skip_keys_with_these_prefixes</code> <code>Set[str]</code> <p>description</p> required Source code in <code>bionemo/llm/utils/weight_utils.py</code> <pre><code>def load_weights_sharded_inplace_nemo2_to_mcore(\n    model: MegatronModelType, distributed_checkpoint_dir: str | Path, skip_keys_with_these_prefixes: Set[str]\n) -&gt; None:\n    \"\"\"Given a megatron module, this function will determine which keys/subsets of weights to load given the\n        parallel/distributed state. This operates assuming a checkpoint was saved by a nemo2 trainer which places\n        the `module.` prefix on all key names, but we are then going to load directly in to the megatron module\n        without the `module.` prefix. Note that if there are any _extra_ keys that you do not want to search the\n        checkpoint for, for example if you add new layers/heads onto your module, you need to supply the prefix\n        path to those keys in your model and they will be ignored. This latter feature is key for flexible fine-tuning\n        strategies where you load weights partially from other models with partially overlapping structures.\n\n    Args:\n        model: Megatron model that you want to load weights into.\n        distributed_checkpoint_dir: _description_\n        skip_keys_with_these_prefixes: _description_\n    \"\"\"  # noqa: D205\n    sharded_state_dict = {\n        _munge_key_megatron_to_nemo2(k): _munge_sharded_tensor_key_megatron_to_nemo2(v)\n        for k, v in model.sharded_state_dict().items()\n        if not _key_in_filter(k, skip_keys_with_these_prefixes) and \"_extra_state\" not in k\n    }\n    dist_checkpointing.load(\n        sharded_state_dict=sharded_state_dict,\n        checkpoint_dir=str(Path(distributed_checkpoint_dir) / \"weights\"),\n        strict=dist_checkpointing.serialization.StrictHandling.ASSUME_OK_UNEXPECTED,\n    )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/llm/utils/weight_utils/#bionemo.llm.utils.weight_utils.nemo1_to_nemo2_biobert_key_mapping","title":"<code>nemo1_to_nemo2_biobert_key_mapping(old_key, new_model_prefix='module', old_model_prefix='model', te_mapping=False)</code>","text":"<p>This function is used to map the keys from the old nemo BERT models to the new BioBERT models</p> <p>Parameters:</p> Name Type Description Default <code>old_key</code> <code>str</code> <p>old key we want to map to the expected new key name.</p> required <code>new_model_prefix</code> <code>str</code> <p>The new key for the base weights. If you point this at the core megatron model set it to \"\". For the regular nemo2 lightning module following standards, set it to \"module\". Defaults to \"module\".</p> <code>'module'</code> <code>old_model_prefix</code> <code>str</code> <p>The previous saved weight prefix. Defaults to \"model\" which was the standard in nemo1.</p> <code>'model'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>New key name</p> Source code in <code>bionemo/llm/utils/weight_utils.py</code> <pre><code>def nemo1_to_nemo2_biobert_key_mapping(  # noqa: D417\n    old_key: str,\n    new_model_prefix: str = \"module\",\n    old_model_prefix: str = \"model\",\n    te_mapping: bool = False,\n) -&gt; str:\n    \"\"\"This function is used to map the keys from the old nemo BERT models to the new BioBERT models\n\n    Args:\n        old_key (str): old key we want to map to the expected new key name.\n        new_model_prefix (str, optional): The new key for the base weights.\n            If you point this at the core megatron model set it to \"\".\n            For the regular nemo2 lightning module following standards, set it to \"module\".\n            Defaults to \"module\".\n        old_model_prefix (str, optional): The previous saved weight prefix. Defaults to \"model\" which was the standard in nemo1.\n\n    Returns:\n        str: New key name\n    \"\"\"  # noqa: D415\n    # add the . to the end of the input prefixes if they are not the empty string,\n    #  unless the user has already done so.\n    if old_model_prefix != \"\":\n        old_model_prefix = f\"{old_model_prefix.rstrip('.')}.\"\n    if new_model_prefix != \"\":\n        new_model_prefix = f\"{new_model_prefix.rstrip('.')}.\"\n\n    # This function is used to map the keys from the old nemo BERT models to the new BioBERT models\n    base_rename = old_key.replace(f\"{old_model_prefix}language_model.\", f\"{new_model_prefix}\")\n    base_rename = base_rename.replace(f\"{old_model_prefix}\", f\"{new_model_prefix}\")\n    if \"dense_h_to_4h\" in base_rename:\n        return base_rename.replace(\"dense_h_to_4h\", \"linear_fc1\")\n    if \"dense_4h_to_h\" in base_rename:\n        return base_rename.replace(\"dense_4h_to_h\", \"linear_fc2\")\n    if \"query_key_value\" in base_rename:\n        return base_rename.replace(\"query_key_value\", \"linear_qkv\")\n    if \"self_attention.dense\" in base_rename:\n        #  This is definitely the linear_proj and not the qkv. The linear_proj shapes are 256x256\n        #   which match dense but not query_key_value\n        # (Pdb) new_state_dict['encoder.layers.4.self_attention.linear_proj.weight'].shape\n        #  torch.Size([256, 256])\n        # (Pdb) new_state_dict['encoder.layers.4.self_attention.linear_qkv.weight'].shape\n        # torch.Size([768, 256])\n        # (Pdb) new_state_dict['encoder.layers.4.self_attention.linear_qkv.bias'].shape\n        # torch.Size([768])\n        return base_rename.replace(\"self_attention.dense\", \"self_attention.linear_proj\")\n    if \"lm_head.bias\" in base_rename:\n        return base_rename.replace(\"lm_head.bias\", \"output_layer.bias\")\n    if \"lm_head.weight\" in base_rename:\n        return base_rename.replace(\"lm_head.weight\", \"output_layer.weight\")\n    if \"lm_head.layernorm\" in base_rename:\n        return base_rename.replace(\"lm_head.layernorm\", \"lm_head.layer_norm\")\n\n    if \"post_attention_layernorm\" in base_rename:\n        base_rename = base_rename.replace(\"post_attention_layernorm\", \"pre_mlp_layernorm\")\n\n    # Handle the transformer engine spec's differences in layer naming and where things like layernorm are stored.\n    #  TE moves layernorm from  an object that's part of the main attention layer to being an internal component of\n    #  the linear layers, probably for efficiency/fusion of some sort.\n    if te_mapping:\n        if \".input_layernorm.weight\" in base_rename:\n            return base_rename.replace(\".input_layernorm.weight\", \".self_attention.linear_qkv.layer_norm_weight\")\n        if \".input_layernorm.bias\" in base_rename:\n            return base_rename.replace(\".input_layernorm.bias\", \".self_attention.linear_qkv.layer_norm_bias\")\n        if \".pre_mlp_layernorm.bias\" in base_rename:\n            return base_rename.replace(\".pre_mlp_layernorm.bias\", \".mlp.linear_fc1.layer_norm_bias\")\n        if \".pre_mlp_layernorm.weight\" in base_rename:\n            return base_rename.replace(\".pre_mlp_layernorm.weight\", \".mlp.linear_fc1.layer_norm_weight\")\n    return base_rename\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/distribution/","title":"Distribution","text":""},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/distribution/#bionemo.moco.distributions.prior.distribution.DiscretePriorDistribution","title":"<code>DiscretePriorDistribution</code>","text":"<p>               Bases: <code>PriorDistribution</code></p> <p>An abstract base class representing a discrete prior distribution.</p> Source code in <code>bionemo/moco/distributions/prior/distribution.py</code> <pre><code>class DiscretePriorDistribution(PriorDistribution):\n    \"\"\"An abstract base class representing a discrete prior distribution.\"\"\"\n\n    def __init__(self, num_classes: int, prior_dist: Tensor):\n        \"\"\"Initializes a DiscretePriorDistribution instance.\n\n        Args:\n        num_classes (int): The number of classes in the discrete distribution.\n        prior_dist (Tensor): The prior distribution over the classes.\n\n        Returns:\n        None\n        \"\"\"\n        self.num_classes = num_classes\n        self.prior_dist = prior_dist\n\n    def get_num_classes(self) -&gt; int:\n        \"\"\"Getter for num_classes.\"\"\"\n        return self.num_classes\n\n    def get_prior_dist(self) -&gt; Tensor:\n        \"\"\"Getter for prior_dist.\"\"\"\n        return self.prior_dist\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/distribution/#bionemo.moco.distributions.prior.distribution.DiscretePriorDistribution.__init__","title":"<code>__init__(num_classes, prior_dist)</code>","text":"<p>Initializes a DiscretePriorDistribution instance.</p> <p>Args: num_classes (int): The number of classes in the discrete distribution. prior_dist (Tensor): The prior distribution over the classes.</p> <p>Returns: None</p> Source code in <code>bionemo/moco/distributions/prior/distribution.py</code> <pre><code>def __init__(self, num_classes: int, prior_dist: Tensor):\n    \"\"\"Initializes a DiscretePriorDistribution instance.\n\n    Args:\n    num_classes (int): The number of classes in the discrete distribution.\n    prior_dist (Tensor): The prior distribution over the classes.\n\n    Returns:\n    None\n    \"\"\"\n    self.num_classes = num_classes\n    self.prior_dist = prior_dist\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/distribution/#bionemo.moco.distributions.prior.distribution.DiscretePriorDistribution.get_num_classes","title":"<code>get_num_classes()</code>","text":"<p>Getter for num_classes.</p> Source code in <code>bionemo/moco/distributions/prior/distribution.py</code> <pre><code>def get_num_classes(self) -&gt; int:\n    \"\"\"Getter for num_classes.\"\"\"\n    return self.num_classes\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/distribution/#bionemo.moco.distributions.prior.distribution.DiscretePriorDistribution.get_prior_dist","title":"<code>get_prior_dist()</code>","text":"<p>Getter for prior_dist.</p> Source code in <code>bionemo/moco/distributions/prior/distribution.py</code> <pre><code>def get_prior_dist(self) -&gt; Tensor:\n    \"\"\"Getter for prior_dist.\"\"\"\n    return self.prior_dist\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/distribution/#bionemo.moco.distributions.prior.distribution.PriorDistribution","title":"<code>PriorDistribution</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract base class representing a prior distribution.</p> Source code in <code>bionemo/moco/distributions/prior/distribution.py</code> <pre><code>class PriorDistribution(ABC):\n    \"\"\"An abstract base class representing a prior distribution.\"\"\"\n\n    @abstractmethod\n    def sample(self, shape: Tuple, mask: Optional[Tensor] = None, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n        \"\"\"Generates a specified number of samples from the time distribution.\n\n        Args:\n        shape (Tuple): The shape of the samples to generate.\n        mask (Optional[Tensor], optional): A tensor indicating which samples should be masked. Defaults to None.\n        device (str, optional): The device on which to generate the samples. Defaults to \"cpu\".\n\n        Returns:\n            Float: A tensor of samples.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/distribution/#bionemo.moco.distributions.prior.distribution.PriorDistribution.sample","title":"<code>sample(shape, mask=None, device='cpu')</code>  <code>abstractmethod</code>","text":"<p>Generates a specified number of samples from the time distribution.</p> <p>Args: shape (Tuple): The shape of the samples to generate. mask (Optional[Tensor], optional): A tensor indicating which samples should be masked. Defaults to None. device (str, optional): The device on which to generate the samples. Defaults to \"cpu\".</p> <p>Returns:</p> Name Type Description <code>Float</code> <code>Tensor</code> <p>A tensor of samples.</p> Source code in <code>bionemo/moco/distributions/prior/distribution.py</code> <pre><code>@abstractmethod\ndef sample(self, shape: Tuple, mask: Optional[Tensor] = None, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n    \"\"\"Generates a specified number of samples from the time distribution.\n\n    Args:\n    shape (Tuple): The shape of the samples to generate.\n    mask (Optional[Tensor], optional): A tensor indicating which samples should be masked. Defaults to None.\n    device (str, optional): The device on which to generate the samples. Defaults to \"cpu\".\n\n    Returns:\n        Float: A tensor of samples.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/continuous/gaussian/","title":"Gaussian","text":""},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/continuous/gaussian/#bionemo.moco.distributions.prior.continuous.gaussian.GaussianPrior","title":"<code>GaussianPrior</code>","text":"<p>               Bases: <code>PriorDistribution</code></p> <p>A subclass representing a Gaussian prior distribution.</p> Source code in <code>bionemo/moco/distributions/prior/continuous/gaussian.py</code> <pre><code>class GaussianPrior(PriorDistribution):\n    \"\"\"A subclass representing a Gaussian prior distribution.\"\"\"\n\n    def __init__(\n        self,\n        mean: Float = 0.0,\n        std: Float = 1.0,\n        center: Bool = False,\n        rng_generator: Optional[torch.Generator] = None,\n    ) -&gt; None:\n        \"\"\"Gaussian prior distribution.\n\n        Args:\n            mean (Float): The mean of the Gaussian distribution. Defaults to 0.0.\n            std (Float): The standard deviation of the Gaussian distribution. Defaults to 1.0.\n            center (bool): Whether to center the samples around the mean. Defaults to False.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n        \"\"\"\n        self.mean = mean\n        self.std = std\n        self.center = center\n        self.rng_generator = rng_generator\n\n    def sample(\n        self,\n        shape: Tuple,\n        mask: Optional[Tensor] = None,\n        device: Union[str, torch.device] = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n    ) -&gt; Tensor:\n        \"\"\"Generates a specified number of samples from the Gaussian prior distribution.\n\n        Args:\n            shape (Tuple): The shape of the samples to generate.\n            device (str): cpu or gpu.\n            mask (Optional[Tensor]): An optional mask to apply to the samples. Defaults to None.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n        Returns:\n            Float: A tensor of samples.\n        \"\"\"\n        if rng_generator is None:\n            rng_generator = self.rng_generator\n        samples = torch.randn(*shape, device=device, generator=rng_generator)\n        if self.std != 1:\n            samples = samples * self.std\n        if self.mean != 0:\n            samples = samples + self.mean\n\n        if self.center:\n            samples = remove_center_of_mass(samples, mask)\n        if mask is not None:\n            samples = samples * mask.unsqueeze(-1)\n        return samples\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/continuous/gaussian/#bionemo.moco.distributions.prior.continuous.gaussian.GaussianPrior.__init__","title":"<code>__init__(mean=0.0, std=1.0, center=False, rng_generator=None)</code>","text":"<p>Gaussian prior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>Float</code> <p>The mean of the Gaussian distribution. Defaults to 0.0.</p> <code>0.0</code> <code>std</code> <code>Float</code> <p>The standard deviation of the Gaussian distribution. Defaults to 1.0.</p> <code>1.0</code> <code>center</code> <code>bool</code> <p>Whether to center the samples around the mean. Defaults to False.</p> <code>False</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/moco/distributions/prior/continuous/gaussian.py</code> <pre><code>def __init__(\n    self,\n    mean: Float = 0.0,\n    std: Float = 1.0,\n    center: Bool = False,\n    rng_generator: Optional[torch.Generator] = None,\n) -&gt; None:\n    \"\"\"Gaussian prior distribution.\n\n    Args:\n        mean (Float): The mean of the Gaussian distribution. Defaults to 0.0.\n        std (Float): The standard deviation of the Gaussian distribution. Defaults to 1.0.\n        center (bool): Whether to center the samples around the mean. Defaults to False.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n    \"\"\"\n    self.mean = mean\n    self.std = std\n    self.center = center\n    self.rng_generator = rng_generator\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/continuous/gaussian/#bionemo.moco.distributions.prior.continuous.gaussian.GaussianPrior.sample","title":"<code>sample(shape, mask=None, device='cpu', rng_generator=None)</code>","text":"<p>Generates a specified number of samples from the Gaussian prior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>Tuple</code> <p>The shape of the samples to generate.</p> required <code>device</code> <code>str</code> <p>cpu or gpu.</p> <code>'cpu'</code> <code>mask</code> <code>Optional[Tensor]</code> <p>An optional mask to apply to the samples. Defaults to None.</p> <code>None</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Float</code> <code>Tensor</code> <p>A tensor of samples.</p> Source code in <code>bionemo/moco/distributions/prior/continuous/gaussian.py</code> <pre><code>def sample(\n    self,\n    shape: Tuple,\n    mask: Optional[Tensor] = None,\n    device: Union[str, torch.device] = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n) -&gt; Tensor:\n    \"\"\"Generates a specified number of samples from the Gaussian prior distribution.\n\n    Args:\n        shape (Tuple): The shape of the samples to generate.\n        device (str): cpu or gpu.\n        mask (Optional[Tensor]): An optional mask to apply to the samples. Defaults to None.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n    Returns:\n        Float: A tensor of samples.\n    \"\"\"\n    if rng_generator is None:\n        rng_generator = self.rng_generator\n    samples = torch.randn(*shape, device=device, generator=rng_generator)\n    if self.std != 1:\n        samples = samples * self.std\n    if self.mean != 0:\n        samples = samples + self.mean\n\n    if self.center:\n        samples = remove_center_of_mass(samples, mask)\n    if mask is not None:\n        samples = samples * mask.unsqueeze(-1)\n    return samples\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/continuous/harmonic/","title":"Harmonic","text":""},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/continuous/harmonic/#bionemo.moco.distributions.prior.continuous.harmonic.LinearHarmonicPrior","title":"<code>LinearHarmonicPrior</code>","text":"<p>               Bases: <code>PriorDistribution</code></p> <p>A subclass representing a Linear Harmonic prior distribution from Jing et al. https://arxiv.org/abs/2304.02198.</p> Source code in <code>bionemo/moco/distributions/prior/continuous/harmonic.py</code> <pre><code>class LinearHarmonicPrior(PriorDistribution):\n    \"\"\"A subclass representing a Linear Harmonic prior distribution from Jing et al. https://arxiv.org/abs/2304.02198.\"\"\"\n\n    def __init__(\n        self,\n        length: Optional[int] = None,\n        distance: Float = 3.8,\n        center: Bool = False,\n        rng_generator: Optional[torch.Generator] = None,\n        device: Union[str, torch.device] = \"cpu\",\n    ) -&gt; None:\n        \"\"\"Linear Harmonic prior distribution.\n\n        Args:\n            length (Optional[int]): The number of points in a batch.\n            distance (Float): RMS distance between adjacent points in the line graph.\n            center (bool): Whether to center the samples around the mean. Defaults to False.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n        \"\"\"\n        self.distance = distance\n        self.length = length\n        self.center = center\n        self.rng_generator = rng_generator\n        self.device = device\n        if length:\n            self._calculate_terms(length, device)\n\n    def _calculate_terms(self, N, device):\n        a = 3 / (self.distance * self.distance)\n        J = torch.zeros(N, N)\n        for i, j in zip(torch.arange(N - 1), torch.arange(1, N)):\n            J[i, i] += a\n            J[j, j] += a\n            J[i, j] = J[j, i] = -a\n        D, P = torch.linalg.eigh(J)\n        D_inv = 1 / D\n        D_inv[0] = 0\n        self.P, self.D_inv = P.to(device), D_inv.to(device)\n\n    def sample(\n        self,\n        shape: Tuple,\n        mask: Optional[Tensor] = None,\n        device: Union[str, torch.device] = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n    ) -&gt; Tensor:\n        \"\"\"Generates a specified number of samples from the Harmonic prior distribution.\n\n        Args:\n            shape (Tuple): The shape of the samples to generate.\n            device (str): cpu or gpu.\n            mask (Optional[Tensor]): An optional mask to apply to the samples. Defaults to None.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n        Returns:\n            Float: A tensor of samples.\n        \"\"\"\n        if rng_generator is None:\n            rng_generator = self.rng_generator\n\n        samples = torch.randn(*shape, device=device, generator=rng_generator)\n        N = shape[-2]\n\n        if N != self.length:\n            self._calculate_terms(N, device)\n\n        std = torch.sqrt(self.D_inv.to(device)).unsqueeze(-1)\n        samples = self.P.to(device) @ (std * samples)\n        # torch broadcasting avoids shape errors NxN @ (N x 1 * B x N x D)\n        if self.center:\n            samples = remove_center_of_mass(samples, mask)\n\n        if mask is not None:\n            samples = samples * mask.unsqueeze(-1)\n        return samples\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/continuous/harmonic/#bionemo.moco.distributions.prior.continuous.harmonic.LinearHarmonicPrior.__init__","title":"<code>__init__(length=None, distance=3.8, center=False, rng_generator=None, device='cpu')</code>","text":"<p>Linear Harmonic prior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>length</code> <code>Optional[int]</code> <p>The number of points in a batch.</p> <code>None</code> <code>distance</code> <code>Float</code> <p>RMS distance between adjacent points in the line graph.</p> <code>3.8</code> <code>center</code> <code>bool</code> <p>Whether to center the samples around the mean. Defaults to False.</p> <code>False</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>'cpu'</code> Source code in <code>bionemo/moco/distributions/prior/continuous/harmonic.py</code> <pre><code>def __init__(\n    self,\n    length: Optional[int] = None,\n    distance: Float = 3.8,\n    center: Bool = False,\n    rng_generator: Optional[torch.Generator] = None,\n    device: Union[str, torch.device] = \"cpu\",\n) -&gt; None:\n    \"\"\"Linear Harmonic prior distribution.\n\n    Args:\n        length (Optional[int]): The number of points in a batch.\n        distance (Float): RMS distance between adjacent points in the line graph.\n        center (bool): Whether to center the samples around the mean. Defaults to False.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n    \"\"\"\n    self.distance = distance\n    self.length = length\n    self.center = center\n    self.rng_generator = rng_generator\n    self.device = device\n    if length:\n        self._calculate_terms(length, device)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/continuous/harmonic/#bionemo.moco.distributions.prior.continuous.harmonic.LinearHarmonicPrior.sample","title":"<code>sample(shape, mask=None, device='cpu', rng_generator=None)</code>","text":"<p>Generates a specified number of samples from the Harmonic prior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>Tuple</code> <p>The shape of the samples to generate.</p> required <code>device</code> <code>str</code> <p>cpu or gpu.</p> <code>'cpu'</code> <code>mask</code> <code>Optional[Tensor]</code> <p>An optional mask to apply to the samples. Defaults to None.</p> <code>None</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Float</code> <code>Tensor</code> <p>A tensor of samples.</p> Source code in <code>bionemo/moco/distributions/prior/continuous/harmonic.py</code> <pre><code>def sample(\n    self,\n    shape: Tuple,\n    mask: Optional[Tensor] = None,\n    device: Union[str, torch.device] = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n) -&gt; Tensor:\n    \"\"\"Generates a specified number of samples from the Harmonic prior distribution.\n\n    Args:\n        shape (Tuple): The shape of the samples to generate.\n        device (str): cpu or gpu.\n        mask (Optional[Tensor]): An optional mask to apply to the samples. Defaults to None.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n    Returns:\n        Float: A tensor of samples.\n    \"\"\"\n    if rng_generator is None:\n        rng_generator = self.rng_generator\n\n    samples = torch.randn(*shape, device=device, generator=rng_generator)\n    N = shape[-2]\n\n    if N != self.length:\n        self._calculate_terms(N, device)\n\n    std = torch.sqrt(self.D_inv.to(device)).unsqueeze(-1)\n    samples = self.P.to(device) @ (std * samples)\n    # torch broadcasting avoids shape errors NxN @ (N x 1 * B x N x D)\n    if self.center:\n        samples = remove_center_of_mass(samples, mask)\n\n    if mask is not None:\n        samples = samples * mask.unsqueeze(-1)\n    return samples\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/continuous/utils/","title":"Utils","text":""},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/continuous/utils/#bionemo.moco.distributions.prior.continuous.utils.remove_center_of_mass","title":"<code>remove_center_of_mass(data, mask=None)</code>","text":"<p>Calculates the center of mass (CoM) of the given data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>The input data with shape (..., nodes, features).</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>An optional binary mask to apply to the data with shape (..., nodes) to mask out interaction from CoM calculation. Defaults to None.</p> <code>None</code> <p>Returns: The CoM of the data with shape (..., 1, features).</p> Source code in <code>bionemo/moco/distributions/prior/continuous/utils.py</code> <pre><code>def remove_center_of_mass(data: Tensor, mask: Optional[Tensor] = None) -&gt; Tensor:\n    \"\"\"Calculates the center of mass (CoM) of the given data.\n\n    Args:\n        data: The input data with shape (..., nodes, features).\n        mask: An optional binary mask to apply to the data with shape (..., nodes) to mask out interaction from CoM calculation. Defaults to None.\n\n    Returns:\n    The CoM of the data with shape (..., 1, features).\n    \"\"\"\n    if mask is None:\n        com = data.mean(dim=-2, keepdim=True)\n    else:\n        masked_data = data * mask.unsqueeze(-1)\n        num_nodes = mask.sum(dim=-1, keepdim=True).unsqueeze(-1)\n        com = masked_data.sum(dim=-2, keepdim=True) / num_nodes\n    return data - com\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/discrete/custom/","title":"Custom","text":""},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/discrete/custom/#bionemo.moco.distributions.prior.discrete.custom.DiscreteCustomPrior","title":"<code>DiscreteCustomPrior</code>","text":"<p>               Bases: <code>DiscretePriorDistribution</code></p> <p>A subclass representing a discrete custom prior distribution.</p> <p>This class allows for the creation of a prior distribution with a custom probability mass function defined by the <code>prior_dist</code> tensor. For example if my data has 4 classes and I want [.3, .2, .4, .1] as the probabilities of the 4 classes.</p> Source code in <code>bionemo/moco/distributions/prior/discrete/custom.py</code> <pre><code>class DiscreteCustomPrior(DiscretePriorDistribution):\n    \"\"\"A subclass representing a discrete custom prior distribution.\n\n    This class allows for the creation of a prior distribution with a custom\n    probability mass function defined by the `prior_dist` tensor. For example if my data has 4 classes and I want [.3, .2, .4, .1] as the probabilities of the 4 classes.\n    \"\"\"\n\n    def __init__(self, prior_dist: Tensor, num_classes: int = 10) -&gt; None:\n        \"\"\"Initializes a DiscreteCustomPrior distribution.\n\n        Args:\n            prior_dist: A tensor representing the probability mass function of the prior distribution.\n            num_classes: The number of classes in the prior distribution. Defaults to 10.\n\n        Note:\n            The `prior_dist` tensor should have a sum close to 1.0, as it represents a probability mass function.\n        \"\"\"\n        super().__init__(num_classes, prior_dist)\n        if torch.sum(self.prior_dist).item() - 1.0 &gt; 1e-5:\n            raise ValueError(\"Prior distribution probabilities do not sum up to 1.0\")\n\n    def sample(\n        self,\n        shape: Tuple,\n        mask: Optional[Tensor] = None,\n        device: Union[str, torch.device] = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n    ) -&gt; Tensor:\n        \"\"\"Samples from the discrete custom prior distribution.\n\n        Args:\n            shape: A tuple specifying the shape of the samples to generate.\n            mask: An optional tensor mask to apply to the samples, broadcastable to the sample shape. Defaults to None.\n            device: The device on which to generate the samples, specified as a string or a :class:`torch.device`. Defaults to \"cpu\".\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n        Returns:\n            A tensor of samples drawn from the prior distribution.\n        \"\"\"\n        samples = (\n            torch.multinomial(self.prior_dist, math.prod(shape), replacement=True, generator=rng_generator)\n            .to(device)\n            .reshape(shape)\n        )\n        if mask is not None:\n            samples = samples * mask[(...,) + (None,) * (len(samples.shape) - len(mask.shape))]\n        return samples\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/discrete/custom/#bionemo.moco.distributions.prior.discrete.custom.DiscreteCustomPrior.__init__","title":"<code>__init__(prior_dist, num_classes=10)</code>","text":"<p>Initializes a DiscreteCustomPrior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>prior_dist</code> <code>Tensor</code> <p>A tensor representing the probability mass function of the prior distribution.</p> required <code>num_classes</code> <code>int</code> <p>The number of classes in the prior distribution. Defaults to 10.</p> <code>10</code> Note <p>The <code>prior_dist</code> tensor should have a sum close to 1.0, as it represents a probability mass function.</p> Source code in <code>bionemo/moco/distributions/prior/discrete/custom.py</code> <pre><code>def __init__(self, prior_dist: Tensor, num_classes: int = 10) -&gt; None:\n    \"\"\"Initializes a DiscreteCustomPrior distribution.\n\n    Args:\n        prior_dist: A tensor representing the probability mass function of the prior distribution.\n        num_classes: The number of classes in the prior distribution. Defaults to 10.\n\n    Note:\n        The `prior_dist` tensor should have a sum close to 1.0, as it represents a probability mass function.\n    \"\"\"\n    super().__init__(num_classes, prior_dist)\n    if torch.sum(self.prior_dist).item() - 1.0 &gt; 1e-5:\n        raise ValueError(\"Prior distribution probabilities do not sum up to 1.0\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/discrete/custom/#bionemo.moco.distributions.prior.discrete.custom.DiscreteCustomPrior.sample","title":"<code>sample(shape, mask=None, device='cpu', rng_generator=None)</code>","text":"<p>Samples from the discrete custom prior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>Tuple</code> <p>A tuple specifying the shape of the samples to generate.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>An optional tensor mask to apply to the samples, broadcastable to the sample shape. Defaults to None.</p> <code>None</code> <code>device</code> <code>Union[str, device]</code> <p>The device on which to generate the samples, specified as a string or a :class:<code>torch.device</code>. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of samples drawn from the prior distribution.</p> Source code in <code>bionemo/moco/distributions/prior/discrete/custom.py</code> <pre><code>def sample(\n    self,\n    shape: Tuple,\n    mask: Optional[Tensor] = None,\n    device: Union[str, torch.device] = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n) -&gt; Tensor:\n    \"\"\"Samples from the discrete custom prior distribution.\n\n    Args:\n        shape: A tuple specifying the shape of the samples to generate.\n        mask: An optional tensor mask to apply to the samples, broadcastable to the sample shape. Defaults to None.\n        device: The device on which to generate the samples, specified as a string or a :class:`torch.device`. Defaults to \"cpu\".\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n    Returns:\n        A tensor of samples drawn from the prior distribution.\n    \"\"\"\n    samples = (\n        torch.multinomial(self.prior_dist, math.prod(shape), replacement=True, generator=rng_generator)\n        .to(device)\n        .reshape(shape)\n    )\n    if mask is not None:\n        samples = samples * mask[(...,) + (None,) * (len(samples.shape) - len(mask.shape))]\n    return samples\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/discrete/mask/","title":"Mask","text":""},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/discrete/mask/#bionemo.moco.distributions.prior.discrete.mask.DiscreteMaskedPrior","title":"<code>DiscreteMaskedPrior</code>","text":"<p>               Bases: <code>DiscretePriorDistribution</code></p> <p>A subclass representing a Discrete Masked prior distribution.</p> Source code in <code>bionemo/moco/distributions/prior/discrete/mask.py</code> <pre><code>class DiscreteMaskedPrior(DiscretePriorDistribution):\n    \"\"\"A subclass representing a Discrete Masked prior distribution.\"\"\"\n\n    def __init__(self, num_classes: int = 10, mask_dim: Optional[int] = None, inclusive: bool = True) -&gt; None:\n        \"\"\"Discrete Masked prior distribution.\n\n        Theres 3 ways I can think of defining the problem that are hard to mesh together.\n\n        1. [..., M, ....] inclusive anywhere --&gt; exisiting LLM tokenizer where the mask has a specific location not at the end\n        2. [......, M] inclusive on end --&gt; mask_dim = None with inclusive set to True default stick on the end\n        3. [.....] + [M] exclusive --&gt; the number of classes representes the number of data classes and one wishes to add a separate MASK dimension.\n            - Note the pad_sample function is provided to help add this extra external dimension.\n\n        Args:\n            num_classes (int): The number of classes in the distribution. Defaults to 10.\n            mask_dim (int): The index for the mask token. Defaults to num_classes - 1 if inclusive or num_classes if exclusive.\n            inclusive (bool): Whether the mask is included in the specified number of classes.\n                                If True, the mask is considered as one of the classes.\n                                If False, the mask is considered as an additional class. Defaults to True.\n        \"\"\"\n        if inclusive:\n            if mask_dim is None:\n                mask_dim = num_classes - 1\n            else:\n                if mask_dim &gt;= num_classes:\n                    raise ValueError(\n                        \"As Inclusive accounts for the mask as one of the specified num_classes, the provided mask_dim cannot be &gt;= to num_classes\"\n                    )\n            prior_dist = torch.zeros((num_classes))\n            prior_dist[-1] = 1.0\n            super().__init__(num_classes, prior_dist)\n            self.mask_dim = mask_dim\n        else:\n            prior_dist = torch.zeros((num_classes + 1))\n            prior_dist[-1] = 1.0\n            super().__init__(num_classes + 1, prior_dist)\n            self.mask_dim = num_classes\n        if torch.sum(self.prior_dist).item() - 1.0 &gt;= 1e-5:\n            raise ValueError(\"Invalid probability distribution. Must sum to 1.0\")\n\n    def sample(\n        self,\n        shape: Tuple,\n        mask: Optional[Tensor] = None,\n        device: Union[str, torch.device] = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n    ) -&gt; Tensor:\n        \"\"\"Generates a specified number of samples.\n\n        Args:\n            shape (Tuple): The shape of the samples to generate.\n            device (str): cpu or gpu.\n            mask (Optional[Tensor]): An optional mask to apply to the samples. Defaults to None.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n        Returns:\n            Float: A tensor of samples.\n        \"\"\"\n        samples = torch.ones(shape, dtype=torch.int64, device=device) * self.mask_dim\n        if mask is not None:\n            samples = samples * mask[(...,) + (None,) * (len(samples.shape) - len(mask.shape))]\n        return samples\n\n    def is_masked(self, sample: Tensor) -&gt; Tensor:\n        \"\"\"Creates a mask for whether a state is masked.\n\n        Args:\n            sample (Tensor): The sample to check.\n\n        Returns:\n            Tensor: A float tensor indicating whether the sample is masked.\n        \"\"\"\n        return (sample == self.mask_dim).float()\n\n    def pad_sample(self, sample: Tensor) -&gt; Tensor:\n        \"\"\"Pads the input sample with zeros along the last dimension.\n\n        Args:\n            sample (Tensor): The input sample to be padded.\n\n        Returns:\n            Tensor: The padded sample.\n        \"\"\"\n        # Create a zeros tensor with the same shape as the original tensor, except the last dimension is 1\n        zeros = torch.zeros((*sample.shape[:-1], 1), dtype=torch.float, device=sample.device)\n        # Concatenate along the last dimension to make the shape (..., N+1)\n        padded_sample = torch.cat((sample, zeros), dim=-1)\n        return padded_sample\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/discrete/mask/#bionemo.moco.distributions.prior.discrete.mask.DiscreteMaskedPrior.__init__","title":"<code>__init__(num_classes=10, mask_dim=None, inclusive=True)</code>","text":"<p>Discrete Masked prior distribution.</p> <p>Theres 3 ways I can think of defining the problem that are hard to mesh together.</p> <ol> <li>[..., M, ....] inclusive anywhere --&gt; exisiting LLM tokenizer where the mask has a specific location not at the end</li> <li>[......, M] inclusive on end --&gt; mask_dim = None with inclusive set to True default stick on the end</li> <li>[.....] + [M] exclusive --&gt; the number of classes representes the number of data classes and one wishes to add a separate MASK dimension.<ul> <li>Note the pad_sample function is provided to help add this extra external dimension.</li> </ul> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>The number of classes in the distribution. Defaults to 10.</p> <code>10</code> <code>mask_dim</code> <code>int</code> <p>The index for the mask token. Defaults to num_classes - 1 if inclusive or num_classes if exclusive.</p> <code>None</code> <code>inclusive</code> <code>bool</code> <p>Whether the mask is included in the specified number of classes.                 If True, the mask is considered as one of the classes.                 If False, the mask is considered as an additional class. Defaults to True.</p> <code>True</code> Source code in <code>bionemo/moco/distributions/prior/discrete/mask.py</code> <pre><code>def __init__(self, num_classes: int = 10, mask_dim: Optional[int] = None, inclusive: bool = True) -&gt; None:\n    \"\"\"Discrete Masked prior distribution.\n\n    Theres 3 ways I can think of defining the problem that are hard to mesh together.\n\n    1. [..., M, ....] inclusive anywhere --&gt; exisiting LLM tokenizer where the mask has a specific location not at the end\n    2. [......, M] inclusive on end --&gt; mask_dim = None with inclusive set to True default stick on the end\n    3. [.....] + [M] exclusive --&gt; the number of classes representes the number of data classes and one wishes to add a separate MASK dimension.\n        - Note the pad_sample function is provided to help add this extra external dimension.\n\n    Args:\n        num_classes (int): The number of classes in the distribution. Defaults to 10.\n        mask_dim (int): The index for the mask token. Defaults to num_classes - 1 if inclusive or num_classes if exclusive.\n        inclusive (bool): Whether the mask is included in the specified number of classes.\n                            If True, the mask is considered as one of the classes.\n                            If False, the mask is considered as an additional class. Defaults to True.\n    \"\"\"\n    if inclusive:\n        if mask_dim is None:\n            mask_dim = num_classes - 1\n        else:\n            if mask_dim &gt;= num_classes:\n                raise ValueError(\n                    \"As Inclusive accounts for the mask as one of the specified num_classes, the provided mask_dim cannot be &gt;= to num_classes\"\n                )\n        prior_dist = torch.zeros((num_classes))\n        prior_dist[-1] = 1.0\n        super().__init__(num_classes, prior_dist)\n        self.mask_dim = mask_dim\n    else:\n        prior_dist = torch.zeros((num_classes + 1))\n        prior_dist[-1] = 1.0\n        super().__init__(num_classes + 1, prior_dist)\n        self.mask_dim = num_classes\n    if torch.sum(self.prior_dist).item() - 1.0 &gt;= 1e-5:\n        raise ValueError(\"Invalid probability distribution. Must sum to 1.0\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/discrete/mask/#bionemo.moco.distributions.prior.discrete.mask.DiscreteMaskedPrior.is_masked","title":"<code>is_masked(sample)</code>","text":"<p>Creates a mask for whether a state is masked.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>Tensor</code> <p>The sample to check.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A float tensor indicating whether the sample is masked.</p> Source code in <code>bionemo/moco/distributions/prior/discrete/mask.py</code> <pre><code>def is_masked(self, sample: Tensor) -&gt; Tensor:\n    \"\"\"Creates a mask for whether a state is masked.\n\n    Args:\n        sample (Tensor): The sample to check.\n\n    Returns:\n        Tensor: A float tensor indicating whether the sample is masked.\n    \"\"\"\n    return (sample == self.mask_dim).float()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/discrete/mask/#bionemo.moco.distributions.prior.discrete.mask.DiscreteMaskedPrior.pad_sample","title":"<code>pad_sample(sample)</code>","text":"<p>Pads the input sample with zeros along the last dimension.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>Tensor</code> <p>The input sample to be padded.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The padded sample.</p> Source code in <code>bionemo/moco/distributions/prior/discrete/mask.py</code> <pre><code>def pad_sample(self, sample: Tensor) -&gt; Tensor:\n    \"\"\"Pads the input sample with zeros along the last dimension.\n\n    Args:\n        sample (Tensor): The input sample to be padded.\n\n    Returns:\n        Tensor: The padded sample.\n    \"\"\"\n    # Create a zeros tensor with the same shape as the original tensor, except the last dimension is 1\n    zeros = torch.zeros((*sample.shape[:-1], 1), dtype=torch.float, device=sample.device)\n    # Concatenate along the last dimension to make the shape (..., N+1)\n    padded_sample = torch.cat((sample, zeros), dim=-1)\n    return padded_sample\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/discrete/mask/#bionemo.moco.distributions.prior.discrete.mask.DiscreteMaskedPrior.sample","title":"<code>sample(shape, mask=None, device='cpu', rng_generator=None)</code>","text":"<p>Generates a specified number of samples.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>Tuple</code> <p>The shape of the samples to generate.</p> required <code>device</code> <code>str</code> <p>cpu or gpu.</p> <code>'cpu'</code> <code>mask</code> <code>Optional[Tensor]</code> <p>An optional mask to apply to the samples. Defaults to None.</p> <code>None</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Float</code> <code>Tensor</code> <p>A tensor of samples.</p> Source code in <code>bionemo/moco/distributions/prior/discrete/mask.py</code> <pre><code>def sample(\n    self,\n    shape: Tuple,\n    mask: Optional[Tensor] = None,\n    device: Union[str, torch.device] = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n) -&gt; Tensor:\n    \"\"\"Generates a specified number of samples.\n\n    Args:\n        shape (Tuple): The shape of the samples to generate.\n        device (str): cpu or gpu.\n        mask (Optional[Tensor]): An optional mask to apply to the samples. Defaults to None.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n    Returns:\n        Float: A tensor of samples.\n    \"\"\"\n    samples = torch.ones(shape, dtype=torch.int64, device=device) * self.mask_dim\n    if mask is not None:\n        samples = samples * mask[(...,) + (None,) * (len(samples.shape) - len(mask.shape))]\n    return samples\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/discrete/uniform/","title":"Uniform","text":""},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/discrete/uniform/#bionemo.moco.distributions.prior.discrete.uniform.DiscreteUniformPrior","title":"<code>DiscreteUniformPrior</code>","text":"<p>               Bases: <code>DiscretePriorDistribution</code></p> <p>A subclass representing a discrete uniform prior distribution.</p> Source code in <code>bionemo/moco/distributions/prior/discrete/uniform.py</code> <pre><code>class DiscreteUniformPrior(DiscretePriorDistribution):\n    \"\"\"A subclass representing a discrete uniform prior distribution.\"\"\"\n\n    def __init__(self, num_classes: int = 10) -&gt; None:\n        \"\"\"Initializes a discrete uniform prior distribution.\n\n        Args:\n            num_classes (int): The number of classes in the discrete uniform distribution. Defaults to 10.\n        \"\"\"\n        prior_dist = torch.ones((num_classes)) * 1 / num_classes\n        super().__init__(num_classes, prior_dist)\n        if torch.sum(self.prior_dist).item() - 1.0 &gt; 1e-5:\n            raise ValueError(\"Prior distribution probabilities do not sum up to 1.0\")\n\n    def sample(\n        self,\n        shape: Tuple,\n        mask: Optional[Tensor] = None,\n        device: Union[str, torch.device] = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n    ) -&gt; Tensor:\n        \"\"\"Generates a specified number of samples.\n\n        Args:\n            shape (Tuple): The shape of the samples to generate.\n            device (str): cpu or gpu.\n            mask (Optional[Tensor]): An optional mask to apply to the samples. Defaults to None.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n        Returns:\n            Float: A tensor of samples.\n        \"\"\"\n        samples = torch.randint(0, self.num_classes, shape, device=device, generator=rng_generator)\n        if mask is not None:\n            samples = samples * mask[(...,) + (None,) * (len(samples.shape) - len(mask.shape))]\n        return samples\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/discrete/uniform/#bionemo.moco.distributions.prior.discrete.uniform.DiscreteUniformPrior.__init__","title":"<code>__init__(num_classes=10)</code>","text":"<p>Initializes a discrete uniform prior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>The number of classes in the discrete uniform distribution. Defaults to 10.</p> <code>10</code> Source code in <code>bionemo/moco/distributions/prior/discrete/uniform.py</code> <pre><code>def __init__(self, num_classes: int = 10) -&gt; None:\n    \"\"\"Initializes a discrete uniform prior distribution.\n\n    Args:\n        num_classes (int): The number of classes in the discrete uniform distribution. Defaults to 10.\n    \"\"\"\n    prior_dist = torch.ones((num_classes)) * 1 / num_classes\n    super().__init__(num_classes, prior_dist)\n    if torch.sum(self.prior_dist).item() - 1.0 &gt; 1e-5:\n        raise ValueError(\"Prior distribution probabilities do not sum up to 1.0\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/prior/discrete/uniform/#bionemo.moco.distributions.prior.discrete.uniform.DiscreteUniformPrior.sample","title":"<code>sample(shape, mask=None, device='cpu', rng_generator=None)</code>","text":"<p>Generates a specified number of samples.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>Tuple</code> <p>The shape of the samples to generate.</p> required <code>device</code> <code>str</code> <p>cpu or gpu.</p> <code>'cpu'</code> <code>mask</code> <code>Optional[Tensor]</code> <p>An optional mask to apply to the samples. Defaults to None.</p> <code>None</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Float</code> <code>Tensor</code> <p>A tensor of samples.</p> Source code in <code>bionemo/moco/distributions/prior/discrete/uniform.py</code> <pre><code>def sample(\n    self,\n    shape: Tuple,\n    mask: Optional[Tensor] = None,\n    device: Union[str, torch.device] = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n) -&gt; Tensor:\n    \"\"\"Generates a specified number of samples.\n\n    Args:\n        shape (Tuple): The shape of the samples to generate.\n        device (str): cpu or gpu.\n        mask (Optional[Tensor]): An optional mask to apply to the samples. Defaults to None.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n    Returns:\n        Float: A tensor of samples.\n    \"\"\"\n    samples = torch.randint(0, self.num_classes, shape, device=device, generator=rng_generator)\n    if mask is not None:\n        samples = samples * mask[(...,) + (None,) * (len(samples.shape) - len(mask.shape))]\n    return samples\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/time/beta/","title":"Beta","text":""},{"location":"main/references/API_reference/bionemo/moco/distributions/time/beta/#bionemo.moco.distributions.time.beta.BetaTimeDistribution","title":"<code>BetaTimeDistribution</code>","text":"<p>               Bases: <code>TimeDistribution</code></p> <p>A class representing a beta time distribution.</p> Source code in <code>bionemo/moco/distributions/time/beta.py</code> <pre><code>class BetaTimeDistribution(TimeDistribution):\n    \"\"\"A class representing a beta time distribution.\"\"\"\n\n    def __init__(\n        self,\n        p1: Float = 2.0,\n        p2: Float = 1.0,\n        min_t: Float = 0.0,\n        max_t: Float = 1.0,\n        discrete_time: Bool = False,\n        nsteps: Optional[int] = None,\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Initializes a BetaTimeDistribution object.\n\n        Args:\n            p1 (Float): The first shape parameter of the beta distribution.\n            p2 (Float): The second shape parameter of the beta distribution.\n            min_t (Float): The minimum time value.\n            max_t (Float): The maximum time value.\n            discrete_time (Bool): Whether the time is discrete.\n            nsteps (Optional[int]): Number of nsteps for discretization.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n        \"\"\"\n        super().__init__(discrete_time, nsteps, min_t, max_t, rng_generator)\n        self.dist = torch.distributions.Beta(p1, p2)\n\n    def sample(\n        self,\n        n_samples: Union[int, Tuple[int, ...], torch.Size],\n        device: Union[str, torch.device] = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Generates a specified number of samples from the uniform time distribution.\n\n        Args:\n            n_samples (int): The number of samples to generate.\n            device (str): cpu or gpu.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n        Returns:\n            A tensor of samples.\n        \"\"\"\n        if rng_generator is None:\n            rng_generator = self.rng_generator\n        if isinstance(n_samples, int):\n            n_samples = torch.Size([n_samples])\n        elif isinstance(n_samples, tuple):\n            n_samples = torch.Size(n_samples)\n        time_step = self.dist.sample(n_samples).to(device=device)\n        if self.min_t and self.max_t and self.min_t &gt; 0:\n            time_step = time_step * (self.max_t - self.min_t) + self.min_t\n        if self.discrete_time:\n            if self.nsteps is None:\n                raise ValueError(\"nsteps cannot be None for discrete time sampling\")\n            time_step = float_time_to_index(time_step, self.nsteps)\n        return time_step\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/time/beta/#bionemo.moco.distributions.time.beta.BetaTimeDistribution.__init__","title":"<code>__init__(p1=2.0, p2=1.0, min_t=0.0, max_t=1.0, discrete_time=False, nsteps=None, rng_generator=None)</code>","text":"<p>Initializes a BetaTimeDistribution object.</p> <p>Parameters:</p> Name Type Description Default <code>p1</code> <code>Float</code> <p>The first shape parameter of the beta distribution.</p> <code>2.0</code> <code>p2</code> <code>Float</code> <p>The second shape parameter of the beta distribution.</p> <code>1.0</code> <code>min_t</code> <code>Float</code> <p>The minimum time value.</p> <code>0.0</code> <code>max_t</code> <code>Float</code> <p>The maximum time value.</p> <code>1.0</code> <code>discrete_time</code> <code>Bool</code> <p>Whether the time is discrete.</p> <code>False</code> <code>nsteps</code> <code>Optional[int]</code> <p>Number of nsteps for discretization.</p> <code>None</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/moco/distributions/time/beta.py</code> <pre><code>def __init__(\n    self,\n    p1: Float = 2.0,\n    p2: Float = 1.0,\n    min_t: Float = 0.0,\n    max_t: Float = 1.0,\n    discrete_time: Bool = False,\n    nsteps: Optional[int] = None,\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Initializes a BetaTimeDistribution object.\n\n    Args:\n        p1 (Float): The first shape parameter of the beta distribution.\n        p2 (Float): The second shape parameter of the beta distribution.\n        min_t (Float): The minimum time value.\n        max_t (Float): The maximum time value.\n        discrete_time (Bool): Whether the time is discrete.\n        nsteps (Optional[int]): Number of nsteps for discretization.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n    \"\"\"\n    super().__init__(discrete_time, nsteps, min_t, max_t, rng_generator)\n    self.dist = torch.distributions.Beta(p1, p2)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/time/beta/#bionemo.moco.distributions.time.beta.BetaTimeDistribution.sample","title":"<code>sample(n_samples, device='cpu', rng_generator=None)</code>","text":"<p>Generates a specified number of samples from the uniform time distribution.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>The number of samples to generate.</p> required <code>device</code> <code>str</code> <p>cpu or gpu.</p> <code>'cpu'</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>A tensor of samples.</p> Source code in <code>bionemo/moco/distributions/time/beta.py</code> <pre><code>def sample(\n    self,\n    n_samples: Union[int, Tuple[int, ...], torch.Size],\n    device: Union[str, torch.device] = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Generates a specified number of samples from the uniform time distribution.\n\n    Args:\n        n_samples (int): The number of samples to generate.\n        device (str): cpu or gpu.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n    Returns:\n        A tensor of samples.\n    \"\"\"\n    if rng_generator is None:\n        rng_generator = self.rng_generator\n    if isinstance(n_samples, int):\n        n_samples = torch.Size([n_samples])\n    elif isinstance(n_samples, tuple):\n        n_samples = torch.Size(n_samples)\n    time_step = self.dist.sample(n_samples).to(device=device)\n    if self.min_t and self.max_t and self.min_t &gt; 0:\n        time_step = time_step * (self.max_t - self.min_t) + self.min_t\n    if self.discrete_time:\n        if self.nsteps is None:\n            raise ValueError(\"nsteps cannot be None for discrete time sampling\")\n        time_step = float_time_to_index(time_step, self.nsteps)\n    return time_step\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/time/distribution/","title":"Distribution","text":""},{"location":"main/references/API_reference/bionemo/moco/distributions/time/distribution/#bionemo.moco.distributions.time.distribution.MixTimeDistribution","title":"<code>MixTimeDistribution</code>","text":"<p>An abstract base class representing a mixed time distribution.</p> <p>uniform_dist = UniformTimeDistribution(min_t=0.0, max_t=1.0, discrete_time=False) beta_dist = BetaTimeDistribution(min_t=0.0, max_t=1.0, discrete_time=False, p1=2.0, p2=1.0) mix_dist = MixTimeDistribution(uniform_dist, beta_dist, mix_fraction=0.5)</p> Source code in <code>bionemo/moco/distributions/time/distribution.py</code> <pre><code>class MixTimeDistribution:\n    \"\"\"An abstract base class representing a mixed time distribution.\n\n    uniform_dist = UniformTimeDistribution(min_t=0.0, max_t=1.0, discrete_time=False)\n    beta_dist = BetaTimeDistribution(min_t=0.0, max_t=1.0, discrete_time=False, p1=2.0, p2=1.0)\n    mix_dist = MixTimeDistribution(uniform_dist, beta_dist, mix_fraction=0.5)\n    \"\"\"\n\n    def __init__(self, dist1: TimeDistribution, dist2: TimeDistribution, mix_fraction: Float):\n        \"\"\"Initializes a MixTimeDistribution object.\n\n        Args:\n            dist1 (TimeDistribution): The first time distribution.\n            dist2 (TimeDistribution): The second time distribution.\n            mix_fraction (Float): The fraction of samples to draw from dist1. Must be between 0 and 1.\n        \"\"\"\n        if not 0 &lt;= mix_fraction &lt;= 1:\n            raise ValueError(\"mix_fraction must be between 0 and 1\")\n        self.dist1 = dist1\n        self.dist2 = dist2\n        self.mix_fraction = mix_fraction\n\n    def sample(\n        self, n_samples: int, device: Union[str, torch.device] = \"cpu\", rng_generator: Optional[torch.Generator] = None\n    ) -&gt; Float:\n        \"\"\"Generates a specified number of samples from the mixed time distribution.\n\n        Args:\n            n_samples (int): The number of samples to generate.\n            device (str): cpu or gpu.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n        Returns:\n            Float: A list or array of samples.\n        \"\"\"\n        samples_dist1 = self.dist1.sample(n_samples, device)\n        samples_dist2 = self.dist2.sample(n_samples, device)\n        mix = torch.rand(n_samples, device=device, generator=rng_generator)\n        return torch.where(mix &lt; self.mix_fraction, samples_dist1, samples_dist2)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/time/distribution/#bionemo.moco.distributions.time.distribution.MixTimeDistribution.__init__","title":"<code>__init__(dist1, dist2, mix_fraction)</code>","text":"<p>Initializes a MixTimeDistribution object.</p> <p>Parameters:</p> Name Type Description Default <code>dist1</code> <code>TimeDistribution</code> <p>The first time distribution.</p> required <code>dist2</code> <code>TimeDistribution</code> <p>The second time distribution.</p> required <code>mix_fraction</code> <code>Float</code> <p>The fraction of samples to draw from dist1. Must be between 0 and 1.</p> required Source code in <code>bionemo/moco/distributions/time/distribution.py</code> <pre><code>def __init__(self, dist1: TimeDistribution, dist2: TimeDistribution, mix_fraction: Float):\n    \"\"\"Initializes a MixTimeDistribution object.\n\n    Args:\n        dist1 (TimeDistribution): The first time distribution.\n        dist2 (TimeDistribution): The second time distribution.\n        mix_fraction (Float): The fraction of samples to draw from dist1. Must be between 0 and 1.\n    \"\"\"\n    if not 0 &lt;= mix_fraction &lt;= 1:\n        raise ValueError(\"mix_fraction must be between 0 and 1\")\n    self.dist1 = dist1\n    self.dist2 = dist2\n    self.mix_fraction = mix_fraction\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/time/distribution/#bionemo.moco.distributions.time.distribution.MixTimeDistribution.sample","title":"<code>sample(n_samples, device='cpu', rng_generator=None)</code>","text":"<p>Generates a specified number of samples from the mixed time distribution.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>The number of samples to generate.</p> required <code>device</code> <code>str</code> <p>cpu or gpu.</p> <code>'cpu'</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Float</code> <code>Float</code> <p>A list or array of samples.</p> Source code in <code>bionemo/moco/distributions/time/distribution.py</code> <pre><code>def sample(\n    self, n_samples: int, device: Union[str, torch.device] = \"cpu\", rng_generator: Optional[torch.Generator] = None\n) -&gt; Float:\n    \"\"\"Generates a specified number of samples from the mixed time distribution.\n\n    Args:\n        n_samples (int): The number of samples to generate.\n        device (str): cpu or gpu.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n    Returns:\n        Float: A list or array of samples.\n    \"\"\"\n    samples_dist1 = self.dist1.sample(n_samples, device)\n    samples_dist2 = self.dist2.sample(n_samples, device)\n    mix = torch.rand(n_samples, device=device, generator=rng_generator)\n    return torch.where(mix &lt; self.mix_fraction, samples_dist1, samples_dist2)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/time/distribution/#bionemo.moco.distributions.time.distribution.TimeDistribution","title":"<code>TimeDistribution</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract base class representing a time distribution.</p> <p>Parameters:</p> Name Type Description Default <code>discrete_time</code> <code>Bool</code> <p>Whether the time is discrete.</p> <code>False</code> <code>nsteps</code> <code>Optional[int]</code> <p>Number of nsteps for discretization.</p> <code>None</code> <code>min_t</code> <code>Optional[Float]</code> <p>Min continuous time.</p> <code>None</code> <code>max_t</code> <code>Optional[Float]</code> <p>Max continuous time.</p> <code>None</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/moco/distributions/time/distribution.py</code> <pre><code>class TimeDistribution(ABC):\n    \"\"\"An abstract base class representing a time distribution.\n\n    Args:\n        discrete_time (Bool): Whether the time is discrete.\n        nsteps (Optional[int]): Number of nsteps for discretization.\n        min_t (Optional[Float]): Min continuous time.\n        max_t (Optional[Float]): Max continuous time.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n    \"\"\"\n\n    def __init__(\n        self,\n        discrete_time: Bool = False,\n        nsteps: Optional[int] = None,\n        min_t: Optional[Float] = None,\n        max_t: Optional[Float] = None,\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Initializes a TimeDistribution object.\"\"\"\n        self.discrete_time = discrete_time\n        self.nsteps = nsteps\n        self.rng_generator = rng_generator\n        if discrete_time:\n            min_t = 0.0\n            max_t = 1.0\n            if nsteps is None:\n                raise ValueError(\"nsteps must not be None and must be specified for discrete time\")\n        if min_t is not None and isinstance(min_t, float):\n            if not 0 &lt;= min_t &lt; 1.0:\n                raise ValueError(\"min_t must be greater than or equal to 0 and less than 1.0\")\n        self.min_t = min_t\n        if max_t is not None and isinstance(max_t, float):\n            if not 0 &lt; max_t &lt;= 1.0:\n                raise ValueError(\"max_t must be greater than 0 and less than or equal to 1.0\")\n        self.max_t = max_t\n        if (\n            self.min_t is not None\n            and self.max_t is not None\n            and isinstance(self.min_t, float)\n            and isinstance(self.max_t, float)\n        ):\n            if self.min_t &gt;= self.max_t:\n                raise ValueError(\"min_t must be less than max_t\")\n\n    @abstractmethod\n    def sample(\n        self,\n        n_samples: Union[int, Tuple[int, ...], torch.Size],\n        device: Union[str, torch.device] = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n    ) -&gt; Float:\n        \"\"\"Generates a specified number of samples from the time distribution.\n\n        Args:\n        n_samples (int): The number of samples to generate.\n        device (str): cpu or gpu.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n        Returns:\n            Float: A list or array of samples.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/time/distribution/#bionemo.moco.distributions.time.distribution.TimeDistribution.__init__","title":"<code>__init__(discrete_time=False, nsteps=None, min_t=None, max_t=None, rng_generator=None)</code>","text":"<p>Initializes a TimeDistribution object.</p> Source code in <code>bionemo/moco/distributions/time/distribution.py</code> <pre><code>def __init__(\n    self,\n    discrete_time: Bool = False,\n    nsteps: Optional[int] = None,\n    min_t: Optional[Float] = None,\n    max_t: Optional[Float] = None,\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Initializes a TimeDistribution object.\"\"\"\n    self.discrete_time = discrete_time\n    self.nsteps = nsteps\n    self.rng_generator = rng_generator\n    if discrete_time:\n        min_t = 0.0\n        max_t = 1.0\n        if nsteps is None:\n            raise ValueError(\"nsteps must not be None and must be specified for discrete time\")\n    if min_t is not None and isinstance(min_t, float):\n        if not 0 &lt;= min_t &lt; 1.0:\n            raise ValueError(\"min_t must be greater than or equal to 0 and less than 1.0\")\n    self.min_t = min_t\n    if max_t is not None and isinstance(max_t, float):\n        if not 0 &lt; max_t &lt;= 1.0:\n            raise ValueError(\"max_t must be greater than 0 and less than or equal to 1.0\")\n    self.max_t = max_t\n    if (\n        self.min_t is not None\n        and self.max_t is not None\n        and isinstance(self.min_t, float)\n        and isinstance(self.max_t, float)\n    ):\n        if self.min_t &gt;= self.max_t:\n            raise ValueError(\"min_t must be less than max_t\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/time/distribution/#bionemo.moco.distributions.time.distribution.TimeDistribution.sample","title":"<code>sample(n_samples, device='cpu', rng_generator=None)</code>  <code>abstractmethod</code>","text":"<p>Generates a specified number of samples from the time distribution.</p> <p>Args: n_samples (int): The number of samples to generate. device (str): cpu or gpu. rng_generator: An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <p>Returns:</p> Name Type Description <code>Float</code> <code>Float</code> <p>A list or array of samples.</p> Source code in <code>bionemo/moco/distributions/time/distribution.py</code> <pre><code>@abstractmethod\ndef sample(\n    self,\n    n_samples: Union[int, Tuple[int, ...], torch.Size],\n    device: Union[str, torch.device] = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n) -&gt; Float:\n    \"\"\"Generates a specified number of samples from the time distribution.\n\n    Args:\n    n_samples (int): The number of samples to generate.\n    device (str): cpu or gpu.\n    rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n    Returns:\n        Float: A list or array of samples.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/time/logit_normal/","title":"Logit normal","text":""},{"location":"main/references/API_reference/bionemo/moco/distributions/time/logit_normal/#bionemo.moco.distributions.time.logit_normal.LogitNormalTimeDistribution","title":"<code>LogitNormalTimeDistribution</code>","text":"<p>               Bases: <code>TimeDistribution</code></p> <p>A class representing a logit normal time distribution.</p> Source code in <code>bionemo/moco/distributions/time/logit_normal.py</code> <pre><code>class LogitNormalTimeDistribution(TimeDistribution):\n    \"\"\"A class representing a logit normal time distribution.\"\"\"\n\n    def __init__(\n        self,\n        p1: Float = 0.0,\n        p2: Float = 1.0,\n        min_t: Float = 0.0,\n        max_t: Float = 1.0,\n        discrete_time: Bool = False,\n        nsteps: Optional[int] = None,\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Initializes a BetaTimeDistribution object.\n\n        Args:\n            p1 (Float): The first shape parameter of the logit normal distribution i.e. the mean.\n            p2 (Float): The second shape parameter of the logit normal distribution i.e. the std.\n            min_t (Float): The minimum time value.\n            max_t (Float): The maximum time value.\n            discrete_time (Bool): Whether the time is discrete.\n            nsteps (Optional[int]): Number of nsteps for discretization.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n        \"\"\"\n        super().__init__(discrete_time, nsteps, min_t, max_t, rng_generator)\n        self.p1 = p1\n        self.p2 = p2\n\n    def sample(\n        self,\n        n_samples: Union[int, Tuple[int, ...], torch.Size],\n        device: Union[str, torch.device] = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Generates a specified number of samples from the uniform time distribution.\n\n        Args:\n            n_samples (int): The number of samples to generate.\n            device (str): cpu or gpu.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n        Returns:\n            A tensor of samples.\n        \"\"\"\n        if rng_generator is None:\n            rng_generator = self.rng_generator\n        time_step = torch.randn(n_samples, device=device, generator=rng_generator) * self.p2 + self.p1\n        time_step = torch.nn.functional.sigmoid(time_step)\n        if self.min_t and self.max_t and (self.min_t &gt; 0 or self.max_t &lt; 1):\n            time_step = time_step * (self.max_t - self.min_t) + self.min_t\n        if self.discrete_time:\n            if self.nsteps is None:\n                raise ValueError(\"nsteps cannot be None for discrete time sampling\")\n            time_step = float_time_to_index(time_step, self.nsteps)\n        return time_step\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/time/logit_normal/#bionemo.moco.distributions.time.logit_normal.LogitNormalTimeDistribution.__init__","title":"<code>__init__(p1=0.0, p2=1.0, min_t=0.0, max_t=1.0, discrete_time=False, nsteps=None, rng_generator=None)</code>","text":"<p>Initializes a BetaTimeDistribution object.</p> <p>Parameters:</p> Name Type Description Default <code>p1</code> <code>Float</code> <p>The first shape parameter of the logit normal distribution i.e. the mean.</p> <code>0.0</code> <code>p2</code> <code>Float</code> <p>The second shape parameter of the logit normal distribution i.e. the std.</p> <code>1.0</code> <code>min_t</code> <code>Float</code> <p>The minimum time value.</p> <code>0.0</code> <code>max_t</code> <code>Float</code> <p>The maximum time value.</p> <code>1.0</code> <code>discrete_time</code> <code>Bool</code> <p>Whether the time is discrete.</p> <code>False</code> <code>nsteps</code> <code>Optional[int]</code> <p>Number of nsteps for discretization.</p> <code>None</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/moco/distributions/time/logit_normal.py</code> <pre><code>def __init__(\n    self,\n    p1: Float = 0.0,\n    p2: Float = 1.0,\n    min_t: Float = 0.0,\n    max_t: Float = 1.0,\n    discrete_time: Bool = False,\n    nsteps: Optional[int] = None,\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Initializes a BetaTimeDistribution object.\n\n    Args:\n        p1 (Float): The first shape parameter of the logit normal distribution i.e. the mean.\n        p2 (Float): The second shape parameter of the logit normal distribution i.e. the std.\n        min_t (Float): The minimum time value.\n        max_t (Float): The maximum time value.\n        discrete_time (Bool): Whether the time is discrete.\n        nsteps (Optional[int]): Number of nsteps for discretization.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n    \"\"\"\n    super().__init__(discrete_time, nsteps, min_t, max_t, rng_generator)\n    self.p1 = p1\n    self.p2 = p2\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/time/logit_normal/#bionemo.moco.distributions.time.logit_normal.LogitNormalTimeDistribution.sample","title":"<code>sample(n_samples, device='cpu', rng_generator=None)</code>","text":"<p>Generates a specified number of samples from the uniform time distribution.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>The number of samples to generate.</p> required <code>device</code> <code>str</code> <p>cpu or gpu.</p> <code>'cpu'</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>A tensor of samples.</p> Source code in <code>bionemo/moco/distributions/time/logit_normal.py</code> <pre><code>def sample(\n    self,\n    n_samples: Union[int, Tuple[int, ...], torch.Size],\n    device: Union[str, torch.device] = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Generates a specified number of samples from the uniform time distribution.\n\n    Args:\n        n_samples (int): The number of samples to generate.\n        device (str): cpu or gpu.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n    Returns:\n        A tensor of samples.\n    \"\"\"\n    if rng_generator is None:\n        rng_generator = self.rng_generator\n    time_step = torch.randn(n_samples, device=device, generator=rng_generator) * self.p2 + self.p1\n    time_step = torch.nn.functional.sigmoid(time_step)\n    if self.min_t and self.max_t and (self.min_t &gt; 0 or self.max_t &lt; 1):\n        time_step = time_step * (self.max_t - self.min_t) + self.min_t\n    if self.discrete_time:\n        if self.nsteps is None:\n            raise ValueError(\"nsteps cannot be None for discrete time sampling\")\n        time_step = float_time_to_index(time_step, self.nsteps)\n    return time_step\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/time/uniform/","title":"Uniform","text":""},{"location":"main/references/API_reference/bionemo/moco/distributions/time/uniform/#bionemo.moco.distributions.time.uniform.SymmetricUniformTimeDistribution","title":"<code>SymmetricUniformTimeDistribution</code>","text":"<p>               Bases: <code>TimeDistribution</code></p> <p>A class representing a uniform time distribution.</p> Source code in <code>bionemo/moco/distributions/time/uniform.py</code> <pre><code>class SymmetricUniformTimeDistribution(TimeDistribution):\n    \"\"\"A class representing a uniform time distribution.\"\"\"\n\n    def __init__(\n        self,\n        min_t: Float = 0.0,\n        max_t: Float = 1.0,\n        discrete_time: Bool = False,\n        nsteps: Optional[int] = None,\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Initializes a UniformTimeDistribution object.\n\n        Args:\n            min_t (Float): The minimum time value.\n            max_t (Float): The maximum time value.\n            discrete_time (Bool): Whether the time is discrete.\n            nsteps (Optional[int]): Number of nsteps for discretization.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n        \"\"\"\n        super().__init__(discrete_time, nsteps, min_t, max_t, rng_generator)\n\n    def sample(\n        self,\n        n_samples: Union[int, Tuple[int, ...], torch.Size],\n        device: Union[str, torch.device] = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Generates a specified number of samples from the uniform time distribution.\n\n        Args:\n            n_samples (int): The number of samples to generate.\n            device (str): cpu or gpu.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n        Returns:\n            A tensor of samples.\n        \"\"\"\n        if rng_generator is None:\n            rng_generator = self.rng_generator\n        if not isinstance(n_samples, int):\n            n_samples = n_samples[0]\n        if self.discrete_time:\n            if self.nsteps is None:\n                raise ValueError(\"nsteps cannot be None for discrete time sampling\")\n            time_step = torch.randint(\n                0, self.nsteps, size=(n_samples // 2 + 1,), device=device, generator=rng_generator\n            )\n            time_step = torch.cat([time_step, self.nsteps - time_step - 1], dim=0)[:n_samples]\n        else:\n            time_step = torch.rand(n_samples // 2 + 1, device=device, generator=rng_generator)\n            time_step = torch.cat([time_step, 1 - time_step], dim=0)[:n_samples]\n            if self.min_t and self.max_t and self.min_t &gt; 0:\n                time_step = time_step * (self.max_t - self.min_t) + self.min_t\n        return time_step\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/time/uniform/#bionemo.moco.distributions.time.uniform.SymmetricUniformTimeDistribution.__init__","title":"<code>__init__(min_t=0.0, max_t=1.0, discrete_time=False, nsteps=None, rng_generator=None)</code>","text":"<p>Initializes a UniformTimeDistribution object.</p> <p>Parameters:</p> Name Type Description Default <code>min_t</code> <code>Float</code> <p>The minimum time value.</p> <code>0.0</code> <code>max_t</code> <code>Float</code> <p>The maximum time value.</p> <code>1.0</code> <code>discrete_time</code> <code>Bool</code> <p>Whether the time is discrete.</p> <code>False</code> <code>nsteps</code> <code>Optional[int]</code> <p>Number of nsteps for discretization.</p> <code>None</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/moco/distributions/time/uniform.py</code> <pre><code>def __init__(\n    self,\n    min_t: Float = 0.0,\n    max_t: Float = 1.0,\n    discrete_time: Bool = False,\n    nsteps: Optional[int] = None,\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Initializes a UniformTimeDistribution object.\n\n    Args:\n        min_t (Float): The minimum time value.\n        max_t (Float): The maximum time value.\n        discrete_time (Bool): Whether the time is discrete.\n        nsteps (Optional[int]): Number of nsteps for discretization.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n    \"\"\"\n    super().__init__(discrete_time, nsteps, min_t, max_t, rng_generator)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/time/uniform/#bionemo.moco.distributions.time.uniform.SymmetricUniformTimeDistribution.sample","title":"<code>sample(n_samples, device='cpu', rng_generator=None)</code>","text":"<p>Generates a specified number of samples from the uniform time distribution.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>The number of samples to generate.</p> required <code>device</code> <code>str</code> <p>cpu or gpu.</p> <code>'cpu'</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>A tensor of samples.</p> Source code in <code>bionemo/moco/distributions/time/uniform.py</code> <pre><code>def sample(\n    self,\n    n_samples: Union[int, Tuple[int, ...], torch.Size],\n    device: Union[str, torch.device] = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Generates a specified number of samples from the uniform time distribution.\n\n    Args:\n        n_samples (int): The number of samples to generate.\n        device (str): cpu or gpu.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n    Returns:\n        A tensor of samples.\n    \"\"\"\n    if rng_generator is None:\n        rng_generator = self.rng_generator\n    if not isinstance(n_samples, int):\n        n_samples = n_samples[0]\n    if self.discrete_time:\n        if self.nsteps is None:\n            raise ValueError(\"nsteps cannot be None for discrete time sampling\")\n        time_step = torch.randint(\n            0, self.nsteps, size=(n_samples // 2 + 1,), device=device, generator=rng_generator\n        )\n        time_step = torch.cat([time_step, self.nsteps - time_step - 1], dim=0)[:n_samples]\n    else:\n        time_step = torch.rand(n_samples // 2 + 1, device=device, generator=rng_generator)\n        time_step = torch.cat([time_step, 1 - time_step], dim=0)[:n_samples]\n        if self.min_t and self.max_t and self.min_t &gt; 0:\n            time_step = time_step * (self.max_t - self.min_t) + self.min_t\n    return time_step\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/time/uniform/#bionemo.moco.distributions.time.uniform.UniformTimeDistribution","title":"<code>UniformTimeDistribution</code>","text":"<p>               Bases: <code>TimeDistribution</code></p> <p>A class representing a uniform time distribution.</p> Source code in <code>bionemo/moco/distributions/time/uniform.py</code> <pre><code>class UniformTimeDistribution(TimeDistribution):\n    \"\"\"A class representing a uniform time distribution.\"\"\"\n\n    def __init__(\n        self,\n        min_t: Float = 0.0,\n        max_t: Float = 1.0,\n        discrete_time: Bool = False,\n        nsteps: Optional[int] = None,\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Initializes a UniformTimeDistribution object.\n\n        Args:\n            min_t (Float): The minimum time value.\n            max_t (Float): The maximum time value.\n            discrete_time (Bool): Whether the time is discrete.\n            nsteps (Optional[int]): Number of nsteps for discretization.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n        \"\"\"\n        super().__init__(discrete_time, nsteps, min_t, max_t, rng_generator)\n\n    def sample(\n        self,\n        n_samples: Union[int, Tuple[int, ...], torch.Size],\n        device: Union[str, torch.device] = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Generates a specified number of samples from the uniform time distribution.\n\n        Args:\n            n_samples (int): The number of samples to generate.\n            device (str): cpu or gpu.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n        Returns:\n            A tensor of samples.\n        \"\"\"\n        if rng_generator is None:\n            rng_generator = self.rng_generator\n        if self.discrete_time:\n            if self.nsteps is None:\n                raise ValueError(\"nsteps cannot be None for discrete time sampling\")\n            time_step = torch.randint(\n                0,\n                self.nsteps,\n                size=(n_samples,) if isinstance(n_samples, int) else n_samples,\n                device=device,\n                generator=rng_generator,\n            )\n        else:\n            time_step = torch.rand(n_samples, device=device, generator=rng_generator)\n            if self.min_t and self.max_t and self.min_t &gt; 0:\n                time_step = time_step * (self.max_t - self.min_t) + self.min_t\n        return time_step\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/time/uniform/#bionemo.moco.distributions.time.uniform.UniformTimeDistribution.__init__","title":"<code>__init__(min_t=0.0, max_t=1.0, discrete_time=False, nsteps=None, rng_generator=None)</code>","text":"<p>Initializes a UniformTimeDistribution object.</p> <p>Parameters:</p> Name Type Description Default <code>min_t</code> <code>Float</code> <p>The minimum time value.</p> <code>0.0</code> <code>max_t</code> <code>Float</code> <p>The maximum time value.</p> <code>1.0</code> <code>discrete_time</code> <code>Bool</code> <p>Whether the time is discrete.</p> <code>False</code> <code>nsteps</code> <code>Optional[int]</code> <p>Number of nsteps for discretization.</p> <code>None</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/moco/distributions/time/uniform.py</code> <pre><code>def __init__(\n    self,\n    min_t: Float = 0.0,\n    max_t: Float = 1.0,\n    discrete_time: Bool = False,\n    nsteps: Optional[int] = None,\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Initializes a UniformTimeDistribution object.\n\n    Args:\n        min_t (Float): The minimum time value.\n        max_t (Float): The maximum time value.\n        discrete_time (Bool): Whether the time is discrete.\n        nsteps (Optional[int]): Number of nsteps for discretization.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n    \"\"\"\n    super().__init__(discrete_time, nsteps, min_t, max_t, rng_generator)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/time/uniform/#bionemo.moco.distributions.time.uniform.UniformTimeDistribution.sample","title":"<code>sample(n_samples, device='cpu', rng_generator=None)</code>","text":"<p>Generates a specified number of samples from the uniform time distribution.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>The number of samples to generate.</p> required <code>device</code> <code>str</code> <p>cpu or gpu.</p> <code>'cpu'</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>A tensor of samples.</p> Source code in <code>bionemo/moco/distributions/time/uniform.py</code> <pre><code>def sample(\n    self,\n    n_samples: Union[int, Tuple[int, ...], torch.Size],\n    device: Union[str, torch.device] = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Generates a specified number of samples from the uniform time distribution.\n\n    Args:\n        n_samples (int): The number of samples to generate.\n        device (str): cpu or gpu.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n    Returns:\n        A tensor of samples.\n    \"\"\"\n    if rng_generator is None:\n        rng_generator = self.rng_generator\n    if self.discrete_time:\n        if self.nsteps is None:\n            raise ValueError(\"nsteps cannot be None for discrete time sampling\")\n        time_step = torch.randint(\n            0,\n            self.nsteps,\n            size=(n_samples,) if isinstance(n_samples, int) else n_samples,\n            device=device,\n            generator=rng_generator,\n        )\n    else:\n        time_step = torch.rand(n_samples, device=device, generator=rng_generator)\n        if self.min_t and self.max_t and self.min_t &gt; 0:\n            time_step = time_step * (self.max_t - self.min_t) + self.min_t\n    return time_step\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/distributions/time/utils/","title":"Utils","text":""},{"location":"main/references/API_reference/bionemo/moco/distributions/time/utils/#bionemo.moco.distributions.time.utils.float_time_to_index","title":"<code>float_time_to_index(time, num_time_steps)</code>","text":"<p>Convert a float time value to a time index.</p> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>Tensor</code> <p>A tensor of float time values in the range [0, 1].</p> required <code>num_time_steps</code> <code>int</code> <p>The number of discrete time steps.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor of time indices corresponding to the input float time values.</p> Source code in <code>bionemo/moco/distributions/time/utils.py</code> <pre><code>def float_time_to_index(time: torch.Tensor, num_time_steps: int) -&gt; torch.Tensor:\n    \"\"\"Convert a float time value to a time index.\n\n    Args:\n        time (torch.Tensor): A tensor of float time values in the range [0, 1].\n        num_time_steps (int): The number of discrete time steps.\n\n    Returns:\n        torch.Tensor: A tensor of time indices corresponding to the input float time values.\n    \"\"\"\n    # Ensure time values are in the range [0, 1]\n    time = torch.clamp(time, 0.0, 1.0)\n\n    # Scale to the index range and round\n    indices = torch.round(time * (num_time_steps - 1)).to(torch.int64)\n\n    return indices\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/base_interpolant/","title":"Base interpolant","text":""},{"location":"main/references/API_reference/bionemo/moco/interpolants/base_interpolant/#bionemo.moco.interpolants.base_interpolant.Interpolant","title":"<code>Interpolant</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract base class representing an Interpolant.</p> <p>This class serves as a foundation for creating interpolants that can be used in various applications, providing a basic structure and interface for interpolation-related operations.</p> Source code in <code>bionemo/moco/interpolants/base_interpolant.py</code> <pre><code>class Interpolant(ABC):\n    \"\"\"An abstract base class representing an Interpolant.\n\n    This class serves as a foundation for creating interpolants that can be used\n    in various applications, providing a basic structure and interface for\n    interpolation-related operations.\n    \"\"\"\n\n    def __init__(\n        self,\n        time_distribution: TimeDistribution,\n        prior_distribution: PriorDistribution,\n        device: Union[str, torch.device] = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Initializes the Interpolant class.\n\n        Args:\n            time_distribution (TimeDistribution): The distribution of time steps.\n            prior_distribution (PriorDistribution): The prior distribution of the variable.\n            device (Union[str, torch.device], optional): The device on which to operate. Defaults to \"cpu\".\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n        \"\"\"\n        self.time_distribution = time_distribution\n        self.prior_distribution = prior_distribution\n        self.device = device\n        self.rng_generator = rng_generator\n\n    @abstractmethod\n    def interpolate(self, *args, **kwargs) -&gt; Tensor:\n        \"\"\"Get x(t) with given time t from noise and data.\n\n        Interpolate between x0 and x1 at the given time t.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def step(self, *args, **kwargs) -&gt; Tensor:\n        \"\"\"Do one step integration.\"\"\"\n        pass\n\n    def general_step(self, method_name: str, kwargs: dict):\n        \"\"\"Calls a step method of the class by its name, passing the provided keyword arguments.\n\n        Args:\n            method_name (str): The name of the step method to call.\n            kwargs (dict): Keyword arguments to pass to the step method.\n\n        Returns:\n            The result of the step method call.\n\n        Raises:\n            ValueError: If the provided method name does not start with 'step'.\n            Exception: If the step method call fails. The error message includes a list of available step methods.\n\n        Note:\n            This method allows for dynamic invocation of step methods, providing flexibility in the class's usage.\n        \"\"\"\n        if not method_name.startswith(\"step\"):\n            raise ValueError(f\"Method name '{method_name}' does not start with 'step'\")\n\n        try:\n            # Get the step method by its name\n            func = getattr(self, method_name)\n            # Call the step method with the provided keyword arguments\n            return func(**kwargs)\n        except Exception as e:\n            # Get a list of available step methods\n            available_methods = \"\\n\".join([f\"  - {attr}\" for attr in dir(self) if attr.startswith(\"step\")])\n            # Create a detailed error message\n            error_message = f\"Error calling method '{method_name}': {e}\\nAvailable step methods:\\n{available_methods}\"\n            # Re-raise the exception with the detailed error message\n            raise type(e)(error_message)\n\n    def sample_prior(self, *args, **kwargs) -&gt; Tensor:\n        \"\"\"Sample from prior distribution.\n\n        This method generates a sample from the prior distribution specified by the\n        `prior_distribution` attribute.\n\n        Returns:\n            Tensor: The generated sample from the prior distribution.\n        \"\"\"\n        # Ensure the device is specified, default to self.device if not provided\n        if \"device\" not in kwargs:\n            kwargs[\"device\"] = self.device\n        kwargs[\"rng_generator\"] = self.rng_generator\n        # Sample from the prior distribution\n        return self.prior_distribution.sample(*args, **kwargs)\n\n    def sample_time(self, *args, **kwargs) -&gt; Tensor:\n        \"\"\"Sample from time distribution.\"\"\"\n        # Ensure the device is specified, default to self.device if not provided\n        if \"device\" not in kwargs:\n            kwargs[\"device\"] = self.device\n        kwargs[\"rng_generator\"] = self.rng_generator\n        # Sample from the time distribution\n        return self.time_distribution.sample(*args, **kwargs)\n\n    def to_device(self, device: str):\n        \"\"\"Moves all internal tensors to the specified device and updates the `self.device` attribute.\n\n        Args:\n            device (str): The device to move the tensors to (e.g. \"cpu\", \"cuda:0\").\n\n        Note:\n            This method is used to transfer the internal state of the DDPM interpolant to a different device.\n            It updates the `self.device` attribute to reflect the new device and moves all internal tensors to the specified device.\n        \"\"\"\n        self.device = device\n        for attr_name in dir(self):\n            if attr_name.startswith(\"_\") and isinstance(getattr(self, attr_name), torch.Tensor):\n                setattr(self, attr_name, getattr(self, attr_name).to(device))\n        return self\n\n    def clean_mask_center(self, data: Tensor, mask: Optional[Tensor] = None, center: Bool = False) -&gt; Tensor:\n        \"\"\"Returns a clean tensor that has been masked and/or centered based on the function arguments.\n\n        Args:\n            data: The input data with shape (..., nodes, features).\n            mask: An optional mask to apply to the data with shape (..., nodes). If provided, it is used to calculate the CoM. Defaults to None.\n            center: A boolean indicating whether to center the data around the calculated CoM. Defaults to False.\n\n        Returns:\n            The data with shape (..., nodes, features) either centered around the CoM if `center` is True or unchanged if `center` is False.\n        \"\"\"\n        if mask is not None:\n            data = data * mask.unsqueeze(-1)\n        if not center:\n            return data\n        if mask is None:\n            num_nodes = torch.tensor(data.shape[1], device=data.device)\n        else:\n            num_nodes = torch.clamp(mask.sum(dim=-1), min=1)  # clamp used to prevent divide by 0\n        com = data.sum(dim=-2) / num_nodes.unsqueeze(-1)\n        return data - com.unsqueeze(-2)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/base_interpolant/#bionemo.moco.interpolants.base_interpolant.Interpolant.__init__","title":"<code>__init__(time_distribution, prior_distribution, device='cpu', rng_generator=None)</code>","text":"<p>Initializes the Interpolant class.</p> <p>Parameters:</p> Name Type Description Default <code>time_distribution</code> <code>TimeDistribution</code> <p>The distribution of time steps.</p> required <code>prior_distribution</code> <code>PriorDistribution</code> <p>The prior distribution of the variable.</p> required <code>device</code> <code>Union[str, device]</code> <p>The device on which to operate. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/moco/interpolants/base_interpolant.py</code> <pre><code>def __init__(\n    self,\n    time_distribution: TimeDistribution,\n    prior_distribution: PriorDistribution,\n    device: Union[str, torch.device] = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Initializes the Interpolant class.\n\n    Args:\n        time_distribution (TimeDistribution): The distribution of time steps.\n        prior_distribution (PriorDistribution): The prior distribution of the variable.\n        device (Union[str, torch.device], optional): The device on which to operate. Defaults to \"cpu\".\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n    \"\"\"\n    self.time_distribution = time_distribution\n    self.prior_distribution = prior_distribution\n    self.device = device\n    self.rng_generator = rng_generator\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/base_interpolant/#bionemo.moco.interpolants.base_interpolant.Interpolant.clean_mask_center","title":"<code>clean_mask_center(data, mask=None, center=False)</code>","text":"<p>Returns a clean tensor that has been masked and/or centered based on the function arguments.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>The input data with shape (..., nodes, features).</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>An optional mask to apply to the data with shape (..., nodes). If provided, it is used to calculate the CoM. Defaults to None.</p> <code>None</code> <code>center</code> <code>Bool</code> <p>A boolean indicating whether to center the data around the calculated CoM. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The data with shape (..., nodes, features) either centered around the CoM if <code>center</code> is True or unchanged if <code>center</code> is False.</p> Source code in <code>bionemo/moco/interpolants/base_interpolant.py</code> <pre><code>def clean_mask_center(self, data: Tensor, mask: Optional[Tensor] = None, center: Bool = False) -&gt; Tensor:\n    \"\"\"Returns a clean tensor that has been masked and/or centered based on the function arguments.\n\n    Args:\n        data: The input data with shape (..., nodes, features).\n        mask: An optional mask to apply to the data with shape (..., nodes). If provided, it is used to calculate the CoM. Defaults to None.\n        center: A boolean indicating whether to center the data around the calculated CoM. Defaults to False.\n\n    Returns:\n        The data with shape (..., nodes, features) either centered around the CoM if `center` is True or unchanged if `center` is False.\n    \"\"\"\n    if mask is not None:\n        data = data * mask.unsqueeze(-1)\n    if not center:\n        return data\n    if mask is None:\n        num_nodes = torch.tensor(data.shape[1], device=data.device)\n    else:\n        num_nodes = torch.clamp(mask.sum(dim=-1), min=1)  # clamp used to prevent divide by 0\n    com = data.sum(dim=-2) / num_nodes.unsqueeze(-1)\n    return data - com.unsqueeze(-2)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/base_interpolant/#bionemo.moco.interpolants.base_interpolant.Interpolant.general_step","title":"<code>general_step(method_name, kwargs)</code>","text":"<p>Calls a step method of the class by its name, passing the provided keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>method_name</code> <code>str</code> <p>The name of the step method to call.</p> required <code>kwargs</code> <code>dict</code> <p>Keyword arguments to pass to the step method.</p> required <p>Returns:</p> Type Description <p>The result of the step method call.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided method name does not start with 'step'.</p> <code>Exception</code> <p>If the step method call fails. The error message includes a list of available step methods.</p> Note <p>This method allows for dynamic invocation of step methods, providing flexibility in the class's usage.</p> Source code in <code>bionemo/moco/interpolants/base_interpolant.py</code> <pre><code>def general_step(self, method_name: str, kwargs: dict):\n    \"\"\"Calls a step method of the class by its name, passing the provided keyword arguments.\n\n    Args:\n        method_name (str): The name of the step method to call.\n        kwargs (dict): Keyword arguments to pass to the step method.\n\n    Returns:\n        The result of the step method call.\n\n    Raises:\n        ValueError: If the provided method name does not start with 'step'.\n        Exception: If the step method call fails. The error message includes a list of available step methods.\n\n    Note:\n        This method allows for dynamic invocation of step methods, providing flexibility in the class's usage.\n    \"\"\"\n    if not method_name.startswith(\"step\"):\n        raise ValueError(f\"Method name '{method_name}' does not start with 'step'\")\n\n    try:\n        # Get the step method by its name\n        func = getattr(self, method_name)\n        # Call the step method with the provided keyword arguments\n        return func(**kwargs)\n    except Exception as e:\n        # Get a list of available step methods\n        available_methods = \"\\n\".join([f\"  - {attr}\" for attr in dir(self) if attr.startswith(\"step\")])\n        # Create a detailed error message\n        error_message = f\"Error calling method '{method_name}': {e}\\nAvailable step methods:\\n{available_methods}\"\n        # Re-raise the exception with the detailed error message\n        raise type(e)(error_message)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/base_interpolant/#bionemo.moco.interpolants.base_interpolant.Interpolant.interpolate","title":"<code>interpolate(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Get x(t) with given time t from noise and data.</p> <p>Interpolate between x0 and x1 at the given time t.</p> Source code in <code>bionemo/moco/interpolants/base_interpolant.py</code> <pre><code>@abstractmethod\ndef interpolate(self, *args, **kwargs) -&gt; Tensor:\n    \"\"\"Get x(t) with given time t from noise and data.\n\n    Interpolate between x0 and x1 at the given time t.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/base_interpolant/#bionemo.moco.interpolants.base_interpolant.Interpolant.sample_prior","title":"<code>sample_prior(*args, **kwargs)</code>","text":"<p>Sample from prior distribution.</p> <p>This method generates a sample from the prior distribution specified by the <code>prior_distribution</code> attribute.</p> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The generated sample from the prior distribution.</p> Source code in <code>bionemo/moco/interpolants/base_interpolant.py</code> <pre><code>def sample_prior(self, *args, **kwargs) -&gt; Tensor:\n    \"\"\"Sample from prior distribution.\n\n    This method generates a sample from the prior distribution specified by the\n    `prior_distribution` attribute.\n\n    Returns:\n        Tensor: The generated sample from the prior distribution.\n    \"\"\"\n    # Ensure the device is specified, default to self.device if not provided\n    if \"device\" not in kwargs:\n        kwargs[\"device\"] = self.device\n    kwargs[\"rng_generator\"] = self.rng_generator\n    # Sample from the prior distribution\n    return self.prior_distribution.sample(*args, **kwargs)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/base_interpolant/#bionemo.moco.interpolants.base_interpolant.Interpolant.sample_time","title":"<code>sample_time(*args, **kwargs)</code>","text":"<p>Sample from time distribution.</p> Source code in <code>bionemo/moco/interpolants/base_interpolant.py</code> <pre><code>def sample_time(self, *args, **kwargs) -&gt; Tensor:\n    \"\"\"Sample from time distribution.\"\"\"\n    # Ensure the device is specified, default to self.device if not provided\n    if \"device\" not in kwargs:\n        kwargs[\"device\"] = self.device\n    kwargs[\"rng_generator\"] = self.rng_generator\n    # Sample from the time distribution\n    return self.time_distribution.sample(*args, **kwargs)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/base_interpolant/#bionemo.moco.interpolants.base_interpolant.Interpolant.step","title":"<code>step(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Do one step integration.</p> Source code in <code>bionemo/moco/interpolants/base_interpolant.py</code> <pre><code>@abstractmethod\ndef step(self, *args, **kwargs) -&gt; Tensor:\n    \"\"\"Do one step integration.\"\"\"\n    pass\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/base_interpolant/#bionemo.moco.interpolants.base_interpolant.Interpolant.to_device","title":"<code>to_device(device)</code>","text":"<p>Moves all internal tensors to the specified device and updates the <code>self.device</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>The device to move the tensors to (e.g. \"cpu\", \"cuda:0\").</p> required Note <p>This method is used to transfer the internal state of the DDPM interpolant to a different device. It updates the <code>self.device</code> attribute to reflect the new device and moves all internal tensors to the specified device.</p> Source code in <code>bionemo/moco/interpolants/base_interpolant.py</code> <pre><code>def to_device(self, device: str):\n    \"\"\"Moves all internal tensors to the specified device and updates the `self.device` attribute.\n\n    Args:\n        device (str): The device to move the tensors to (e.g. \"cpu\", \"cuda:0\").\n\n    Note:\n        This method is used to transfer the internal state of the DDPM interpolant to a different device.\n        It updates the `self.device` attribute to reflect the new device and moves all internal tensors to the specified device.\n    \"\"\"\n    self.device = device\n    for attr_name in dir(self):\n        if attr_name.startswith(\"_\") and isinstance(getattr(self, attr_name), torch.Tensor):\n            setattr(self, attr_name, getattr(self, attr_name).to(device))\n    return self\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/base_interpolant/#bionemo.moco.interpolants.base_interpolant.PredictionType","title":"<code>PredictionType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>An enumeration representing the type of prediction a Denoising Diffusion Probabilistic Model (DDPM) can be used for.</p> <p>DDPMs are versatile models that can be utilized for various prediction tasks, including:</p> <ul> <li>Data: Predicting the original data distribution from a noisy input.</li> <li>Noise: Predicting the noise that was added to the original data to obtain the input.</li> <li>Velocity: Predicting the velocity or rate of change of the data, particularly useful for modeling temporal dynamics.</li> </ul> <p>These prediction types can be used to train neural networks for specific tasks, such as denoising, image synthesis, or time-series forecasting.</p> Source code in <code>bionemo/moco/interpolants/base_interpolant.py</code> <pre><code>class PredictionType(Enum):\n    \"\"\"An enumeration representing the type of prediction a Denoising Diffusion Probabilistic Model (DDPM) can be used for.\n\n    DDPMs are versatile models that can be utilized for various prediction tasks, including:\n\n    - **Data**: Predicting the original data distribution from a noisy input.\n    - **Noise**: Predicting the noise that was added to the original data to obtain the input.\n    - **Velocity**: Predicting the velocity or rate of change of the data, particularly useful for modeling temporal dynamics.\n\n    These prediction types can be used to train neural networks for specific tasks, such as denoising, image synthesis, or time-series forecasting.\n    \"\"\"\n\n    DATA = \"data\"\n    NOISE = \"noise\"\n    VELOCITY = \"velocity\"\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/base_interpolant/#bionemo.moco.interpolants.base_interpolant.pad_like","title":"<code>pad_like(source, target)</code>","text":"<p>Pads the dimensions of the source tensor to match the dimensions of the target tensor.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Tensor</code> <p>The tensor to be padded.</p> required <code>target</code> <code>Tensor</code> <p>The tensor that the source tensor should match in dimensions.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The padded source tensor.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the source tensor has more dimensions than the target tensor.</p> Example <p>source = torch.tensor([1, 2, 3])  # shape: (3,) target = torch.tensor([[1, 2], [4, 5], [7, 8]])  # shape: (3, 2) padded_source = pad_like(source, target)  # shape: (3, 1)</p> Source code in <code>bionemo/moco/interpolants/base_interpolant.py</code> <pre><code>def pad_like(source: Tensor, target: Tensor) -&gt; Tensor:\n    \"\"\"Pads the dimensions of the source tensor to match the dimensions of the target tensor.\n\n    Args:\n        source (Tensor): The tensor to be padded.\n        target (Tensor): The tensor that the source tensor should match in dimensions.\n\n    Returns:\n        Tensor: The padded source tensor.\n\n    Raises:\n        ValueError: If the source tensor has more dimensions than the target tensor.\n\n    Example:\n        &gt;&gt;&gt; source = torch.tensor([1, 2, 3])  # shape: (3,)\n        &gt;&gt;&gt; target = torch.tensor([[1, 2], [4, 5], [7, 8]])  # shape: (3, 2)\n        &gt;&gt;&gt; padded_source = pad_like(source, target)  # shape: (3, 1)\n    \"\"\"\n    if source.ndim == target.ndim:\n        return source\n    elif source.ndim &gt; target.ndim:\n        raise ValueError(f\"Cannot pad {source.shape} to {target.shape}\")\n    return source.view(list(source.shape) + [1] * (target.ndim - source.ndim))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/base_interpolant/#bionemo.moco.interpolants.base_interpolant.string_to_enum","title":"<code>string_to_enum(value, enum_type)</code>","text":"<p>Converts a string to an enum value of the specified type. If the input is already an enum instance, it is returned as-is.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, E]</code> <p>The string to convert or an existing enum instance.</p> required <code>enum_type</code> <code>Type[E]</code> <p>The enum type to convert to.</p> required <p>Returns:</p> Name Type Description <code>E</code> <code>AnyEnum</code> <p>The corresponding enum value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the string does not correspond to any enum member.</p> Source code in <code>bionemo/moco/interpolants/base_interpolant.py</code> <pre><code>def string_to_enum(value: Union[str, AnyEnum], enum_type: Type[AnyEnum]) -&gt; AnyEnum:\n    \"\"\"Converts a string to an enum value of the specified type. If the input is already an enum instance, it is returned as-is.\n\n    Args:\n        value (Union[str, E]): The string to convert or an existing enum instance.\n        enum_type (Type[E]): The enum type to convert to.\n\n    Returns:\n        E: The corresponding enum value.\n\n    Raises:\n        ValueError: If the string does not correspond to any enum member.\n    \"\"\"\n    if isinstance(value, enum_type):\n        # If the value is already an enum, return it\n        return value\n\n    try:\n        # Match the value to the Enum, case-insensitively\n        return enum_type(value)\n    except ValueError:\n        # Raise a helpful error if the value is invalid\n        valid_values = [e.value for e in enum_type]\n        raise ValueError(f\"Invalid value '{value}'. Expected one of {valid_values}.\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/batch_augmentation/","title":"Batch augmentation","text":""},{"location":"main/references/API_reference/bionemo/moco/interpolants/batch_augmentation/#bionemo.moco.interpolants.batch_augmentation.BatchDataAugmentation","title":"<code>BatchDataAugmentation</code>","text":"<p>Facilitates the creation of batch augmentation objects based on specified optimal transport types.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>The device to use for computations (e.g., 'cpu', 'cuda').</p> required <code>num_threads</code> <code>int</code> <p>The number of threads to utilize.</p> required Source code in <code>bionemo/moco/interpolants/batch_augmentation.py</code> <pre><code>class BatchDataAugmentation:\n    \"\"\"Facilitates the creation of batch augmentation objects based on specified optimal transport types.\n\n    Args:\n        device (str): The device to use for computations (e.g., 'cpu', 'cuda').\n        num_threads (int): The number of threads to utilize.\n    \"\"\"\n\n    def __init__(self, device, num_threads):\n        \"\"\"Initializes a BatchAugmentation instance.\n\n        Args:\n            device (str): Device for computation.\n            num_threads (int): Number of threads to use.\n        \"\"\"\n        self.device = device\n        self.num_threads = num_threads\n\n    def create(self, method_type: AugmentationType):\n        \"\"\"Creates a batch augmentation object of the specified type.\n\n        Args:\n            method_type (AugmentationType): The type of optimal transport method.\n\n        Returns:\n            The augmentation object if the type is supported, otherwise **None**.\n        \"\"\"\n        if method_type == AugmentationType.EXACT_OT:\n            augmentation = OTSampler(method=\"exact\", device=self.device, num_threads=self.num_threads)\n        elif method_type == AugmentationType.KABSCH:\n            augmentation = KabschAugmentation()\n        elif method_type == AugmentationType.EQUIVARIANT_OT:\n            augmentation = EquivariantOTSampler(method=\"exact\", device=self.device, num_threads=self.num_threads)\n        else:\n            return None\n        return augmentation\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/batch_augmentation/#bionemo.moco.interpolants.batch_augmentation.BatchDataAugmentation.__init__","title":"<code>__init__(device, num_threads)</code>","text":"<p>Initializes a BatchAugmentation instance.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>Device for computation.</p> required <code>num_threads</code> <code>int</code> <p>Number of threads to use.</p> required Source code in <code>bionemo/moco/interpolants/batch_augmentation.py</code> <pre><code>def __init__(self, device, num_threads):\n    \"\"\"Initializes a BatchAugmentation instance.\n\n    Args:\n        device (str): Device for computation.\n        num_threads (int): Number of threads to use.\n    \"\"\"\n    self.device = device\n    self.num_threads = num_threads\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/batch_augmentation/#bionemo.moco.interpolants.batch_augmentation.BatchDataAugmentation.create","title":"<code>create(method_type)</code>","text":"<p>Creates a batch augmentation object of the specified type.</p> <p>Parameters:</p> Name Type Description Default <code>method_type</code> <code>AugmentationType</code> <p>The type of optimal transport method.</p> required <p>Returns:</p> Type Description <p>The augmentation object if the type is supported, otherwise None.</p> Source code in <code>bionemo/moco/interpolants/batch_augmentation.py</code> <pre><code>def create(self, method_type: AugmentationType):\n    \"\"\"Creates a batch augmentation object of the specified type.\n\n    Args:\n        method_type (AugmentationType): The type of optimal transport method.\n\n    Returns:\n        The augmentation object if the type is supported, otherwise **None**.\n    \"\"\"\n    if method_type == AugmentationType.EXACT_OT:\n        augmentation = OTSampler(method=\"exact\", device=self.device, num_threads=self.num_threads)\n    elif method_type == AugmentationType.KABSCH:\n        augmentation = KabschAugmentation()\n    elif method_type == AugmentationType.EQUIVARIANT_OT:\n        augmentation = EquivariantOTSampler(method=\"exact\", device=self.device, num_threads=self.num_threads)\n    else:\n        return None\n    return augmentation\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/","title":"Continuous flow matching","text":""},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher","title":"<code>ContinuousFlowMatcher</code>","text":"<p>               Bases: <code>Interpolant</code></p> <p>A Continuous Flow Matching interpolant.</p> <p>Examples: <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.moco.distributions.prior.continuous.gaussian import GaussianPrior\n&gt;&gt;&gt; from bionemo.moco.distributions.time.uniform import UniformTimeDistribution\n&gt;&gt;&gt; from bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching import ContinuousFlowMatcher\n&gt;&gt;&gt; from bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule\n\nflow_matcher = ContinuousFlowMatcher(\n    time_distribution = UniformTimeDistribution(...),\n    prior_distribution = GaussianPrior(...),\n    )\nmodel = Model(...)\n\n# Training\nfor epoch in range(1000):\n    data = data_loader.get(...)\n    time = flow_matcher.sample_time(batch_size)\n    noise = flow_matcher.sample_prior(data.shape)\n    data, time, noise = flow_matcher.apply_augmentation(noise, data) # Optional, only for OT\n    xt = flow_matcher.interpolate(data, time, noise)\n    flow = flow_matcher.calculate_target(data, noise)\n\n    u_pred = model(xt, time)\n    loss = flow_matcher.loss(u_pred, flow)\n    loss.backward()\n\n# Generation\nx_pred = flow_matcher.sample_prior(data.shape)\ninference_sched = LinearInferenceSchedule(...)\nfor t in inference_sched.generate_schedule():\n    time = inference_sched.pad_time(x_pred.shape[0], t)\n    u_hat = model(x_pred, time)\n    x_pred = flow_matcher.step(u_hat, x_pred, time)\nreturn x_pred\n</code></pre></p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>class ContinuousFlowMatcher(Interpolant):\n    \"\"\"A Continuous Flow Matching interpolant.\n\n     -------\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from bionemo.moco.distributions.prior.continuous.gaussian import GaussianPrior\n    &gt;&gt;&gt; from bionemo.moco.distributions.time.uniform import UniformTimeDistribution\n    &gt;&gt;&gt; from bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching import ContinuousFlowMatcher\n    &gt;&gt;&gt; from bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule\n\n    flow_matcher = ContinuousFlowMatcher(\n        time_distribution = UniformTimeDistribution(...),\n        prior_distribution = GaussianPrior(...),\n        )\n    model = Model(...)\n\n    # Training\n    for epoch in range(1000):\n        data = data_loader.get(...)\n        time = flow_matcher.sample_time(batch_size)\n        noise = flow_matcher.sample_prior(data.shape)\n        data, time, noise = flow_matcher.apply_augmentation(noise, data) # Optional, only for OT\n        xt = flow_matcher.interpolate(data, time, noise)\n        flow = flow_matcher.calculate_target(data, noise)\n\n        u_pred = model(xt, time)\n        loss = flow_matcher.loss(u_pred, flow)\n        loss.backward()\n\n    # Generation\n    x_pred = flow_matcher.sample_prior(data.shape)\n    inference_sched = LinearInferenceSchedule(...)\n    for t in inference_sched.generate_schedule():\n        time = inference_sched.pad_time(x_pred.shape[0], t)\n        u_hat = model(x_pred, time)\n        x_pred = flow_matcher.step(u_hat, x_pred, time)\n    return x_pred\n\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        time_distribution: TimeDistribution,\n        prior_distribution: PriorDistribution,\n        prediction_type: Union[PredictionType, str] = PredictionType.DATA,\n        sigma: Float = 0,\n        augmentation_type: Optional[Union[AugmentationType, str]] = None,\n        augmentation_num_threads: int = 1,\n        data_scale: Float = 1.0,\n        device: Union[str, torch.device] = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n        eps: Float = 1e-5,\n    ):\n        \"\"\"Initializes the Continuous Flow Matching interpolant.\n\n        Args:\n            time_distribution (TimeDistribution): The distribution of time steps, used to sample time points for the diffusion process.\n            prior_distribution (PriorDistribution): The prior distribution of the variable, used as the starting point for the diffusion process.\n            prediction_type (PredictionType, optional): The type of prediction, either \"flow\" or another type. Defaults to PredictionType.DATA.\n            sigma (Float, optional): The standard deviation of the Gaussian noise added to the interpolated data. Defaults to 0.\n            augmentation_type (Optional[Union[AugmentationType, str]], optional): The type of optimal transport, if applicable. Defaults to None.\n            augmentation_num_threads:  Number of threads to use for OT solver. If \"max\", uses the maximum number of threads. Default is 1.\n            data_scale (Float, optional): The scale factor for the data. Defaults to 1.0.\n            device (Union[str, torch.device], optional): The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n            eps: Small float to prevent divide by zero\n        \"\"\"\n        super().__init__(time_distribution, prior_distribution, device, rng_generator)\n        self.prediction_type = string_to_enum(prediction_type, PredictionType)\n        if self.prediction_type == PredictionType.NOISE:\n            raise ValueError(\"Prediction type cannot be NOISE for Continuous Flow Matching\")\n        self.sigma = sigma\n        self.augmentation_type = augmentation_type\n        self.data_scale = data_scale\n        self.eps = eps\n        if data_scale &lt;= 0:\n            raise ValueError(\"Data Scale must be &gt; 0\")\n        if augmentation_type is not None:\n            self.augmentation_type = augmentation_type = string_to_enum(augmentation_type, AugmentationType)\n            self.augmentation_sampler = self._build_augmentation_sampler(\n                method_type=augmentation_type, num_threads=augmentation_num_threads\n            )\n        self._loss_function = nn.MSELoss(reduction=\"none\")\n\n    def _build_augmentation_sampler(self, method_type: AugmentationType, num_threads: int = 1):\n        \"\"\"Build the optimal transport sampler for the given optimal transport type.\n\n        Args:\n            method_type (augmentation_type): The type of augmentation.\n            num_threads (int): The number of threads to use for the OT sampler, default to 1.\n\n        Returns:\n            The augmentation object.\n        \"\"\"\n        return BatchDataAugmentation(self.device, num_threads).create(method_type)\n\n    def apply_augmentation(self, x0: Tensor, x1: Tensor, mask: Optional[Tensor] = None, **kwargs) -&gt; tuple:\n        \"\"\"Sample and apply the optimal transport plan between batched (and masked) x0 and x1.\n\n        Args:\n            x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n            x1 (Tensor): shape (bs, *dim), data from source minibatch.\n            mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n            **kwargs: Additional keyword arguments to be passed to self.augmentation_sampler.apply_augmentation or handled within this method.\n\n\n        Returns:\n            Tuple: tuple of 2 tensors, represents the noise and data samples following OT plan pi.\n        \"\"\"\n        if self.augmentation_sampler is None:\n            raise ValueError(\"Optimal Transport Sampler is not defined\")\n        return self.augmentation_sampler.apply_augmentation(x0, x1, mask=mask, **kwargs)\n\n    def undo_scale_data(self, data: Tensor) -&gt; Tensor:\n        \"\"\"Downscale the input data by the data scale factor.\n\n        Args:\n            data (Tensor): The input data to downscale.\n\n        Returns:\n            The downscaled data.\n        \"\"\"\n        return 1 / self.data_scale * data\n\n    def scale_data(self, data: Tensor) -&gt; Tensor:\n        \"\"\"Upscale the input data by the data scale factor.\n\n        Args:\n            data (Tensor): The input data to upscale.\n\n        Returns:\n            The upscaled data.\n        \"\"\"\n        return self.data_scale * data\n\n    def interpolate(self, data: Tensor, t: Tensor, noise: Tensor) -&gt; Tensor:\n        \"\"\"Get x_t with given time t from noise (x_0) and data (x_1).\n\n        Currently, we use the linear interpolation as defined in:\n            1. Rectified flow: https://arxiv.org/abs/2209.03003.\n            2. Conditional flow matching: https://arxiv.org/abs/2210.02747 (called conditional optimal transport).\n\n        Args:\n            noise (Tensor): noise from prior(), shape (batchsize, nodes, features)\n            t (Tensor): time, shape (batchsize)\n            data (Tensor): target, shape (batchsize, nodes, features)\n        \"\"\"\n        # Expand t to the same shape as noise: ones([b,n,f]) * t([b,1,1])\n        t = pad_like(t, data)\n        # Calculate x_t as the linear interpolation between noise and data\n        x_t = data * t + noise * (1.0 - t)\n        # Add Gaussian Noise\n        if self.sigma &gt; 0:\n            x_t += self.sigma * torch.randn(x_t.shape, device=x_t.device, generator=self.rng_generator)\n        return x_t\n\n    def calculate_target(self, data: Tensor, noise: Tensor, mask: Optional[Tensor] = None) -&gt; Tensor:\n        \"\"\"Get the target vector field at time t.\n\n        Args:\n            noise (Tensor): noise from prior(), shape (batchsize, nodes, features)\n            data (Tensor): target, shape (batchsize, nodes, features)\n            mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n\n        Returns:\n            Tensor: The target vector field at time t.\n        \"\"\"\n        # Calculate the target vector field u_t(x_t|x_1) as the difference between data and noise because t~[0,1]\n        if self.prediction_type == PredictionType.VELOCITY:\n            u_t = data - noise\n        elif self.prediction_type == PredictionType.DATA:\n            u_t = data\n        else:\n            raise ValueError(\n                f\"Given prediction_type {self.prediction_type} is not supproted for Continuous Flow Matching.\"\n            )\n        if mask is not None:\n            u_t = u_t * mask.unsqueeze(-1)\n        return u_t\n\n    def process_vector_field_prediction(\n        self,\n        model_output: Tensor,\n        xt: Optional[Tensor] = None,\n        t: Optional[Tensor] = None,\n        mask: Optional[Tensor] = None,\n    ):\n        \"\"\"Process the model output based on the prediction type to calculate vecotr field.\n\n        Args:\n            model_output (Tensor): The output of the model.\n            xt (Tensor): The input sample.\n            t (Tensor): The time step.\n            mask (Optional[Tensor], optional): An optional mask to apply to the model output. Defaults to None.\n\n        Returns:\n            The vector field prediction based on the prediction type.\n\n        Raises:\n            ValueError: If the prediction type is not \"flow\" or \"data\".\n        \"\"\"\n        if self.prediction_type == PredictionType.VELOCITY:\n            pred_vector_field = model_output\n        elif self.prediction_type == PredictionType.DATA:\n            if xt is None or t is None:\n                raise ValueError(\"Xt and Time cannpt be None for vector field conversion\")\n            t = pad_like(t, model_output)\n            pred_vector_field = (model_output - xt) / (1 - t + self.eps)\n        else:\n            raise ValueError(\n                f\"prediction_type given as {self.prediction_type} must be `flow` or `data` \"\n                \"for Continuous Flow Matching.\"\n            )\n        if mask is not None:\n            pred_vector_field = pred_vector_field * mask.unsqueeze(-1)\n        return pred_vector_field\n\n    def process_data_prediction(\n        self,\n        model_output: Tensor,\n        xt: Optional[Tensor] = None,\n        t: Optional[Tensor] = None,\n        mask: Optional[Tensor] = None,\n    ):\n        \"\"\"Process the model output based on the prediction type to generate clean data.\n\n        Args:\n            model_output (Tensor): The output of the model.\n            xt (Tensor): The input sample.\n            t (Tensor): The time step.\n            mask (Optional[Tensor], optional): An optional mask to apply to the model output. Defaults to None.\n\n        Returns:\n            The data prediction based on the prediction type.\n\n        Raises:\n            ValueError: If the prediction type is not \"flow\".\n        \"\"\"\n        if self.prediction_type == PredictionType.VELOCITY:\n            if xt is None or t is None:\n                raise ValueError(\"Xt and time cannot be None\")\n            t = pad_like(t, model_output)\n            pred_data = xt + (1 - t) * model_output\n        elif self.prediction_type == PredictionType.DATA:\n            pred_data = model_output\n        else:\n            raise ValueError(\n                f\"prediction_type given as {self.prediction_type} must be `flow` for Continuous Flow Matching.\"\n            )\n        if mask is not None:\n            pred_data = pred_data * mask.unsqueeze(-1)\n        return pred_data\n\n    def step(\n        self,\n        model_out: Tensor,\n        xt: Tensor,\n        dt: Tensor,\n        t: Optional[Tensor] = None,\n        mask: Optional[Tensor] = None,\n        center: Bool = False,\n    ):\n        \"\"\"Perform a single ODE step integration using Euler method.\n\n        Args:\n            model_out (Tensor): The output of the model at the current time step.\n            xt (Tensor): The current intermediate state.\n            dt (Tensor): The time step size.\n            t (Tensor, optional): The current time. Defaults to None.\n            mask (Optional[Tensor], optional): A mask to apply to the model output. Defaults to None.\n            center (Bool, optional): Whether to center the output. Defaults to False.\n\n        Returns:\n            x_next (Tensor): The updated state of the system after the single step, x_(t+dt).\n\n        Notes:\n        - If a mask is provided, it is applied element-wise to the model output before scaling.\n        - The `clean` method is called on the updated state before it is returned.\n        \"\"\"\n        if mask is not None:\n            model_out = model_out * mask.unsqueeze(-1)\n        v_t = self.process_vector_field_prediction(model_out, xt, t, mask)\n        dt = pad_like(dt, model_out)\n        delta_x = v_t * dt\n        x_next = xt + delta_x\n        x_next = self.clean_mask_center(x_next, mask, center)\n        return x_next\n\n    def step_score_stochastic(\n        self,\n        model_out: Tensor,\n        xt: Tensor,\n        dt: Tensor,\n        t: Tensor,\n        mask: Optional[Tensor] = None,\n        gt_mode: str = \"tan\",\n        gt_p: Float = 1.0,\n        gt_clamp: Optional[Float] = None,\n        score_temperature: Float = 1.0,\n        noise_temperature: Float = 1.0,\n        t_lim_ode: Float = 0.99,\n        center: Bool = False,\n    ):\n        r\"\"\"Perform a single SDE step integration using a score-based Langevin update.\n\n        d x_t = [v(x_t, t) + g(t) * s(x_t, t) * score_temperature] dt + \\sqrt{2 * g(t) * noise_temperature} dw_t.\n\n        Args:\n            model_out (Tensor): The output of the model at the current time step.\n            xt (Tensor): The current intermediate state.\n            dt (Tensor): The time step size.\n            t (Tensor, optional): The current time. Defaults to None.\n            mask (Optional[Tensor], optional): A mask to apply to the model output. Defaults to None.\n            gt_mode (str, optional): The mode for the gt function. Defaults to \"tan\".\n            gt_p (Float, optional): The parameter for the gt function. Defaults to 1.0.\n            gt_clamp: (Float, optional): Upper limit of gt term. Defaults to None.\n            score_temperature (Float, optional): The temperature for the score part of the step. Defaults to 1.0.\n            noise_temperature (Float, optional): The temperature for the stochastic part of the step. Defaults to 1.0.\n            t_lim_ode (Float, optional): The time limit for the ODE step. Defaults to 0.99.\n            center (Bool, optional): Whether to center the output. Defaults to False.\n\n        Returns:\n            x_next (Tensor): The updated state of the system after the single step, x_(t+dt).\n\n        Notes:\n            - If a mask is provided, it is applied element-wise to the model output before scaling.\n            - The `clean` method is called on the updated state before it is returned.\n        \"\"\"\n        if self.augmentation_type is not None:\n            raise ValueError(\"Optimal Transport violates the vector field to score conversion\")\n        if not isinstance(self.prior_distribution, GaussianPrior):\n            raise ValueError(\n                \"Prior distribution must be an instance of GaussianPrior to learn a proper score function\"\n            )\n        if t.min() &gt;= t_lim_ode:\n            return self.step(model_out, xt, dt, t, mask, center)\n        if mask is not None:\n            model_out = model_out * mask.unsqueeze(-1)\n        v_t = self.process_vector_field_prediction(model_out, xt, t, mask)\n        dt = pad_like(dt, model_out)\n        t = pad_like(t, model_out)\n        score = self.vf_to_score(xt, v_t, t)\n        gt = self.get_gt(t, gt_mode, gt_p, gt_clamp)\n        eps = torch.randn(xt.shape, dtype=xt.dtype, device=xt.device, generator=self.rng_generator)\n        std_eps = torch.sqrt(2 * gt * noise_temperature * dt)\n        delta_x = (v_t + gt * score * score_temperature) * dt + std_eps * eps\n        x_next = xt + delta_x\n        x_next = self.clean_mask_center(x_next, mask, center)\n        return x_next\n\n    def loss(\n        self,\n        model_pred: Tensor,\n        target: Tensor,\n        t: Optional[Tensor] = None,\n        xt: Optional[Tensor] = None,\n        mask: Optional[Tensor] = None,\n        target_type: Union[PredictionType, str] = PredictionType.DATA,\n    ):\n        \"\"\"Calculate the loss given the model prediction, data sample, time, and mask.\n\n        If target_type is FLOW loss = ||v_hat - (x1-x0)||**2\n        If target_type is DATA loss = ||x1_hat - x1||**2 * 1 / (1 - t)**2 as the target vector field = x1 - x0 = (1/(1-t)) * x1 - xt where xt = tx1 - (1-t)x0.\n        This functions supports any cominbation of prediction_type and target_type in {DATA, FLOW}.\n\n        Args:\n            model_pred (Tensor): The predicted output from the model.\n            target (Tensor): The target output for the model prediction.\n            t (Optional[Tensor], optional): The time for the model prediction. Defaults to None.\n            xt (Optional[Tensor], optional): The interpolated data. Defaults to None.\n            mask (Optional[Tensor], optional): The mask for the data point. Defaults to None.\n            target_type (PredictionType, optional): The type of the target output. Defaults to PredictionType.DATA.\n\n        Returns:\n            Tensor: The calculated loss batch tensor.\n        \"\"\"\n        target_type = string_to_enum(target_type, PredictionType)\n        if target_type == PredictionType.DATA:\n            model_pred = self.process_data_prediction(model_pred, xt, t, mask)\n        else:\n            model_pred = self.process_vector_field_prediction(model_pred, xt, t, mask)\n        raw_loss = self._loss_function(model_pred, target)\n\n        if mask is not None:\n            loss = raw_loss * mask.unsqueeze(-1)\n            n_elem = torch.sum(mask, dim=-1)\n            loss = torch.sum(loss, dim=tuple(range(1, raw_loss.ndim))) / n_elem\n        else:\n            loss = torch.sum(raw_loss, dim=tuple(range(1, raw_loss.ndim))) / model_pred.size(1)\n        if target_type == PredictionType.DATA:\n            if t is None:\n                raise ValueError(\"Time cannot be None when using a time-based weighting\")\n            loss_weight = 1.0 / ((1.0 - t) ** 2 + self.eps)\n            loss = loss_weight * loss\n        return loss\n\n    def vf_to_score(\n        self,\n        x_t: Tensor,\n        v: Tensor,\n        t: Tensor,\n    ) -&gt; Tensor:\n        \"\"\"From Geffner et al. Computes score of noisy density given the vector field learned by flow matching.\n\n        With our interpolation scheme these are related by\n\n        v(x_t, t) = (1 / t) (x_t + scale_ref ** 2 * (1 - t) * s(x_t, t)),\n\n        or equivalently,\n\n        s(x_t, t) = (t * v(x_t, t) - x_t) / (scale_ref ** 2 * (1 - t)).\n\n        with scale_ref = 1\n\n        Args:\n            x_t: Noisy sample, shape [*, dim]\n            v: Vector field, shape [*, dim]\n            t: Interpolation time, shape [*] (must be &lt; 1)\n\n        Returns:\n            Score of intermediate density, shape [*, dim].\n        \"\"\"\n        assert torch.all(t &lt; 1.0), \"vf_to_score requires t &lt; 1 (strict)\"\n        t = pad_like(t, v)\n        num = t * v - x_t  # [*, dim]\n        den = 1.0 - t  # [*, 1]\n        score = num / den\n        return score  # [*, dim]\n\n    def get_gt(\n        self,\n        t: Tensor,\n        mode: str = \"tan\",\n        param: float = 1.0,\n        clamp_val: Optional[float] = None,\n        eps: float = 1e-2,\n    ) -&gt; Tensor:\n        \"\"\"From Geffner et al. Computes gt for different modes.\n\n        Args:\n            t: times where we'll evaluate, covers [0, 1), shape [nsteps]\n            mode: \"us\" or \"tan\"\n            param: parameterized transformation\n            clamp_val: value to clamp gt, no clamping if None\n            eps: small value leave as it is\n        \"\"\"\n\n        # Function to get variants for some gt mode\n        def transform_gt(gt, f_pow=1.0):\n            # 1.0 means no transformation\n            if f_pow == 1.0:\n                return gt\n\n            # First we somewhat normalize between 0 and 1\n            log_gt = torch.log(gt)\n            mean_log_gt = torch.mean(log_gt)\n            log_gt_centered = log_gt - mean_log_gt\n            normalized = torch.nn.functional.sigmoid(log_gt_centered)\n            # Transformation here\n            normalized = normalized**f_pow\n            # Undo normalization with the transformed variable\n            log_gt_centered_rec = torch.logit(normalized, eps=1e-6)\n            log_gt_rec = log_gt_centered_rec + mean_log_gt\n            gt_rec = torch.exp(log_gt_rec)\n            return gt_rec\n\n        # Numerical reasons for some schedule\n        t = torch.clamp(t, 0, 1 - self.eps)\n\n        if mode == \"us\":\n            num = 1.0 - t\n            den = t\n            gt = num / (den + eps)\n        elif mode == \"tan\":\n            num = torch.sin((1.0 - t) * torch.pi / 2.0)\n            den = torch.cos((1.0 - t) * torch.pi / 2.0)\n            gt = (torch.pi / 2.0) * num / (den + eps)\n        elif mode == \"1/t\":\n            num = 1.0\n            den = t\n            gt = num / (den + eps)\n        elif mode == \"1/t2\":\n            num = 1.0\n            den = t**2\n            gt = num / (den + eps)\n        elif mode == \"1/t1p5\":\n            num = 1.0\n            den = t**1.5\n            gt = num / (den + eps)\n        elif mode == \"2/t\":\n            num = 2.0\n            den = t\n            gt = num / (den + eps)\n        elif mode == \"2/t2\":\n            num = 2.0\n            den = t**2\n            gt = num / (den + eps)\n        elif mode == \"2/t1p5\":\n            num = 2.0\n            den = t**1.5\n            gt = num / (den + eps)\n        elif mode == \"1mt\":\n            gt = 1 - t\n        elif mode == \"t\":\n            gt = t\n        elif mode == \"ones\":\n            gt = 0 * t + 1\n        else:\n            raise NotImplementedError(f\"gt not implemented {mode}\")\n        gt = transform_gt(gt, f_pow=param)\n        gt = torch.clamp(gt, 0, clamp_val)  # If None no clamping\n        return gt  # [s]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher.__init__","title":"<code>__init__(time_distribution, prior_distribution, prediction_type=PredictionType.DATA, sigma=0, augmentation_type=None, augmentation_num_threads=1, data_scale=1.0, device='cpu', rng_generator=None, eps=1e-05)</code>","text":"<p>Initializes the Continuous Flow Matching interpolant.</p> <p>Parameters:</p> Name Type Description Default <code>time_distribution</code> <code>TimeDistribution</code> <p>The distribution of time steps, used to sample time points for the diffusion process.</p> required <code>prior_distribution</code> <code>PriorDistribution</code> <p>The prior distribution of the variable, used as the starting point for the diffusion process.</p> required <code>prediction_type</code> <code>PredictionType</code> <p>The type of prediction, either \"flow\" or another type. Defaults to PredictionType.DATA.</p> <code>DATA</code> <code>sigma</code> <code>Float</code> <p>The standard deviation of the Gaussian noise added to the interpolated data. Defaults to 0.</p> <code>0</code> <code>augmentation_type</code> <code>Optional[Union[AugmentationType, str]]</code> <p>The type of optimal transport, if applicable. Defaults to None.</p> <code>None</code> <code>augmentation_num_threads</code> <code>int</code> <p>Number of threads to use for OT solver. If \"max\", uses the maximum number of threads. Default is 1.</p> <code>1</code> <code>data_scale</code> <code>Float</code> <p>The scale factor for the data. Defaults to 1.0.</p> <code>1.0</code> <code>device</code> <code>Union[str, device]</code> <p>The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".</p> <code>'cpu'</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> <code>eps</code> <code>Float</code> <p>Small float to prevent divide by zero</p> <code>1e-05</code> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def __init__(\n    self,\n    time_distribution: TimeDistribution,\n    prior_distribution: PriorDistribution,\n    prediction_type: Union[PredictionType, str] = PredictionType.DATA,\n    sigma: Float = 0,\n    augmentation_type: Optional[Union[AugmentationType, str]] = None,\n    augmentation_num_threads: int = 1,\n    data_scale: Float = 1.0,\n    device: Union[str, torch.device] = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n    eps: Float = 1e-5,\n):\n    \"\"\"Initializes the Continuous Flow Matching interpolant.\n\n    Args:\n        time_distribution (TimeDistribution): The distribution of time steps, used to sample time points for the diffusion process.\n        prior_distribution (PriorDistribution): The prior distribution of the variable, used as the starting point for the diffusion process.\n        prediction_type (PredictionType, optional): The type of prediction, either \"flow\" or another type. Defaults to PredictionType.DATA.\n        sigma (Float, optional): The standard deviation of the Gaussian noise added to the interpolated data. Defaults to 0.\n        augmentation_type (Optional[Union[AugmentationType, str]], optional): The type of optimal transport, if applicable. Defaults to None.\n        augmentation_num_threads:  Number of threads to use for OT solver. If \"max\", uses the maximum number of threads. Default is 1.\n        data_scale (Float, optional): The scale factor for the data. Defaults to 1.0.\n        device (Union[str, torch.device], optional): The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n        eps: Small float to prevent divide by zero\n    \"\"\"\n    super().__init__(time_distribution, prior_distribution, device, rng_generator)\n    self.prediction_type = string_to_enum(prediction_type, PredictionType)\n    if self.prediction_type == PredictionType.NOISE:\n        raise ValueError(\"Prediction type cannot be NOISE for Continuous Flow Matching\")\n    self.sigma = sigma\n    self.augmentation_type = augmentation_type\n    self.data_scale = data_scale\n    self.eps = eps\n    if data_scale &lt;= 0:\n        raise ValueError(\"Data Scale must be &gt; 0\")\n    if augmentation_type is not None:\n        self.augmentation_type = augmentation_type = string_to_enum(augmentation_type, AugmentationType)\n        self.augmentation_sampler = self._build_augmentation_sampler(\n            method_type=augmentation_type, num_threads=augmentation_num_threads\n        )\n    self._loss_function = nn.MSELoss(reduction=\"none\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher.apply_augmentation","title":"<code>apply_augmentation(x0, x1, mask=None, **kwargs)</code>","text":"<p>Sample and apply the optimal transport plan between batched (and masked) x0 and x1.</p> <p>Parameters:</p> Name Type Description Default <code>x0</code> <code>Tensor</code> <p>shape (bs, *dim), noise from source minibatch.</p> required <code>x1</code> <code>Tensor</code> <p>shape (bs, *dim), data from source minibatch.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to be passed to self.augmentation_sampler.apply_augmentation or handled within this method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>tuple</code> <p>tuple of 2 tensors, represents the noise and data samples following OT plan pi.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def apply_augmentation(self, x0: Tensor, x1: Tensor, mask: Optional[Tensor] = None, **kwargs) -&gt; tuple:\n    \"\"\"Sample and apply the optimal transport plan between batched (and masked) x0 and x1.\n\n    Args:\n        x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n        x1 (Tensor): shape (bs, *dim), data from source minibatch.\n        mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n        **kwargs: Additional keyword arguments to be passed to self.augmentation_sampler.apply_augmentation or handled within this method.\n\n\n    Returns:\n        Tuple: tuple of 2 tensors, represents the noise and data samples following OT plan pi.\n    \"\"\"\n    if self.augmentation_sampler is None:\n        raise ValueError(\"Optimal Transport Sampler is not defined\")\n    return self.augmentation_sampler.apply_augmentation(x0, x1, mask=mask, **kwargs)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher.calculate_target","title":"<code>calculate_target(data, noise, mask=None)</code>","text":"<p>Get the target vector field at time t.</p> <p>Parameters:</p> Name Type Description Default <code>noise</code> <code>Tensor</code> <p>noise from prior(), shape (batchsize, nodes, features)</p> required <code>data</code> <code>Tensor</code> <p>target, shape (batchsize, nodes, features)</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The target vector field at time t.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def calculate_target(self, data: Tensor, noise: Tensor, mask: Optional[Tensor] = None) -&gt; Tensor:\n    \"\"\"Get the target vector field at time t.\n\n    Args:\n        noise (Tensor): noise from prior(), shape (batchsize, nodes, features)\n        data (Tensor): target, shape (batchsize, nodes, features)\n        mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n\n    Returns:\n        Tensor: The target vector field at time t.\n    \"\"\"\n    # Calculate the target vector field u_t(x_t|x_1) as the difference between data and noise because t~[0,1]\n    if self.prediction_type == PredictionType.VELOCITY:\n        u_t = data - noise\n    elif self.prediction_type == PredictionType.DATA:\n        u_t = data\n    else:\n        raise ValueError(\n            f\"Given prediction_type {self.prediction_type} is not supproted for Continuous Flow Matching.\"\n        )\n    if mask is not None:\n        u_t = u_t * mask.unsqueeze(-1)\n    return u_t\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher.get_gt","title":"<code>get_gt(t, mode='tan', param=1.0, clamp_val=None, eps=0.01)</code>","text":"<p>From Geffner et al. Computes gt for different modes.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>times where we'll evaluate, covers [0, 1), shape [nsteps]</p> required <code>mode</code> <code>str</code> <p>\"us\" or \"tan\"</p> <code>'tan'</code> <code>param</code> <code>float</code> <p>parameterized transformation</p> <code>1.0</code> <code>clamp_val</code> <code>Optional[float]</code> <p>value to clamp gt, no clamping if None</p> <code>None</code> <code>eps</code> <code>float</code> <p>small value leave as it is</p> <code>0.01</code> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def get_gt(\n    self,\n    t: Tensor,\n    mode: str = \"tan\",\n    param: float = 1.0,\n    clamp_val: Optional[float] = None,\n    eps: float = 1e-2,\n) -&gt; Tensor:\n    \"\"\"From Geffner et al. Computes gt for different modes.\n\n    Args:\n        t: times where we'll evaluate, covers [0, 1), shape [nsteps]\n        mode: \"us\" or \"tan\"\n        param: parameterized transformation\n        clamp_val: value to clamp gt, no clamping if None\n        eps: small value leave as it is\n    \"\"\"\n\n    # Function to get variants for some gt mode\n    def transform_gt(gt, f_pow=1.0):\n        # 1.0 means no transformation\n        if f_pow == 1.0:\n            return gt\n\n        # First we somewhat normalize between 0 and 1\n        log_gt = torch.log(gt)\n        mean_log_gt = torch.mean(log_gt)\n        log_gt_centered = log_gt - mean_log_gt\n        normalized = torch.nn.functional.sigmoid(log_gt_centered)\n        # Transformation here\n        normalized = normalized**f_pow\n        # Undo normalization with the transformed variable\n        log_gt_centered_rec = torch.logit(normalized, eps=1e-6)\n        log_gt_rec = log_gt_centered_rec + mean_log_gt\n        gt_rec = torch.exp(log_gt_rec)\n        return gt_rec\n\n    # Numerical reasons for some schedule\n    t = torch.clamp(t, 0, 1 - self.eps)\n\n    if mode == \"us\":\n        num = 1.0 - t\n        den = t\n        gt = num / (den + eps)\n    elif mode == \"tan\":\n        num = torch.sin((1.0 - t) * torch.pi / 2.0)\n        den = torch.cos((1.0 - t) * torch.pi / 2.0)\n        gt = (torch.pi / 2.0) * num / (den + eps)\n    elif mode == \"1/t\":\n        num = 1.0\n        den = t\n        gt = num / (den + eps)\n    elif mode == \"1/t2\":\n        num = 1.0\n        den = t**2\n        gt = num / (den + eps)\n    elif mode == \"1/t1p5\":\n        num = 1.0\n        den = t**1.5\n        gt = num / (den + eps)\n    elif mode == \"2/t\":\n        num = 2.0\n        den = t\n        gt = num / (den + eps)\n    elif mode == \"2/t2\":\n        num = 2.0\n        den = t**2\n        gt = num / (den + eps)\n    elif mode == \"2/t1p5\":\n        num = 2.0\n        den = t**1.5\n        gt = num / (den + eps)\n    elif mode == \"1mt\":\n        gt = 1 - t\n    elif mode == \"t\":\n        gt = t\n    elif mode == \"ones\":\n        gt = 0 * t + 1\n    else:\n        raise NotImplementedError(f\"gt not implemented {mode}\")\n    gt = transform_gt(gt, f_pow=param)\n    gt = torch.clamp(gt, 0, clamp_val)  # If None no clamping\n    return gt  # [s]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher.interpolate","title":"<code>interpolate(data, t, noise)</code>","text":"<p>Get x_t with given time t from noise (x_0) and data (x_1).</p> <p>Currently, we use the linear interpolation as defined in:     1. Rectified flow: https://arxiv.org/abs/2209.03003.     2. Conditional flow matching: https://arxiv.org/abs/2210.02747 (called conditional optimal transport).</p> <p>Parameters:</p> Name Type Description Default <code>noise</code> <code>Tensor</code> <p>noise from prior(), shape (batchsize, nodes, features)</p> required <code>t</code> <code>Tensor</code> <p>time, shape (batchsize)</p> required <code>data</code> <code>Tensor</code> <p>target, shape (batchsize, nodes, features)</p> required Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def interpolate(self, data: Tensor, t: Tensor, noise: Tensor) -&gt; Tensor:\n    \"\"\"Get x_t with given time t from noise (x_0) and data (x_1).\n\n    Currently, we use the linear interpolation as defined in:\n        1. Rectified flow: https://arxiv.org/abs/2209.03003.\n        2. Conditional flow matching: https://arxiv.org/abs/2210.02747 (called conditional optimal transport).\n\n    Args:\n        noise (Tensor): noise from prior(), shape (batchsize, nodes, features)\n        t (Tensor): time, shape (batchsize)\n        data (Tensor): target, shape (batchsize, nodes, features)\n    \"\"\"\n    # Expand t to the same shape as noise: ones([b,n,f]) * t([b,1,1])\n    t = pad_like(t, data)\n    # Calculate x_t as the linear interpolation between noise and data\n    x_t = data * t + noise * (1.0 - t)\n    # Add Gaussian Noise\n    if self.sigma &gt; 0:\n        x_t += self.sigma * torch.randn(x_t.shape, device=x_t.device, generator=self.rng_generator)\n    return x_t\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher.loss","title":"<code>loss(model_pred, target, t=None, xt=None, mask=None, target_type=PredictionType.DATA)</code>","text":"<p>Calculate the loss given the model prediction, data sample, time, and mask.</p> <p>If target_type is FLOW loss = ||v_hat - (x1-x0)||2 If target_type is DATA loss = ||x1_hat - x1||2 * 1 / (1 - t)**2 as the target vector field = x1 - x0 = (1/(1-t)) * x1 - xt where xt = tx1 - (1-t)x0. This functions supports any cominbation of prediction_type and target_type in {DATA, FLOW}.</p> <p>Parameters:</p> Name Type Description Default <code>model_pred</code> <code>Tensor</code> <p>The predicted output from the model.</p> required <code>target</code> <code>Tensor</code> <p>The target output for the model prediction.</p> required <code>t</code> <code>Optional[Tensor]</code> <p>The time for the model prediction. Defaults to None.</p> <code>None</code> <code>xt</code> <code>Optional[Tensor]</code> <p>The interpolated data. Defaults to None.</p> <code>None</code> <code>mask</code> <code>Optional[Tensor]</code> <p>The mask for the data point. Defaults to None.</p> <code>None</code> <code>target_type</code> <code>PredictionType</code> <p>The type of the target output. Defaults to PredictionType.DATA.</p> <code>DATA</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The calculated loss batch tensor.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def loss(\n    self,\n    model_pred: Tensor,\n    target: Tensor,\n    t: Optional[Tensor] = None,\n    xt: Optional[Tensor] = None,\n    mask: Optional[Tensor] = None,\n    target_type: Union[PredictionType, str] = PredictionType.DATA,\n):\n    \"\"\"Calculate the loss given the model prediction, data sample, time, and mask.\n\n    If target_type is FLOW loss = ||v_hat - (x1-x0)||**2\n    If target_type is DATA loss = ||x1_hat - x1||**2 * 1 / (1 - t)**2 as the target vector field = x1 - x0 = (1/(1-t)) * x1 - xt where xt = tx1 - (1-t)x0.\n    This functions supports any cominbation of prediction_type and target_type in {DATA, FLOW}.\n\n    Args:\n        model_pred (Tensor): The predicted output from the model.\n        target (Tensor): The target output for the model prediction.\n        t (Optional[Tensor], optional): The time for the model prediction. Defaults to None.\n        xt (Optional[Tensor], optional): The interpolated data. Defaults to None.\n        mask (Optional[Tensor], optional): The mask for the data point. Defaults to None.\n        target_type (PredictionType, optional): The type of the target output. Defaults to PredictionType.DATA.\n\n    Returns:\n        Tensor: The calculated loss batch tensor.\n    \"\"\"\n    target_type = string_to_enum(target_type, PredictionType)\n    if target_type == PredictionType.DATA:\n        model_pred = self.process_data_prediction(model_pred, xt, t, mask)\n    else:\n        model_pred = self.process_vector_field_prediction(model_pred, xt, t, mask)\n    raw_loss = self._loss_function(model_pred, target)\n\n    if mask is not None:\n        loss = raw_loss * mask.unsqueeze(-1)\n        n_elem = torch.sum(mask, dim=-1)\n        loss = torch.sum(loss, dim=tuple(range(1, raw_loss.ndim))) / n_elem\n    else:\n        loss = torch.sum(raw_loss, dim=tuple(range(1, raw_loss.ndim))) / model_pred.size(1)\n    if target_type == PredictionType.DATA:\n        if t is None:\n            raise ValueError(\"Time cannot be None when using a time-based weighting\")\n        loss_weight = 1.0 / ((1.0 - t) ** 2 + self.eps)\n        loss = loss_weight * loss\n    return loss\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher.process_data_prediction","title":"<code>process_data_prediction(model_output, xt=None, t=None, mask=None)</code>","text":"<p>Process the model output based on the prediction type to generate clean data.</p> <p>Parameters:</p> Name Type Description Default <code>model_output</code> <code>Tensor</code> <p>The output of the model.</p> required <code>xt</code> <code>Tensor</code> <p>The input sample.</p> <code>None</code> <code>t</code> <code>Tensor</code> <p>The time step.</p> <code>None</code> <code>mask</code> <code>Optional[Tensor]</code> <p>An optional mask to apply to the model output. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>The data prediction based on the prediction type.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the prediction type is not \"flow\".</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def process_data_prediction(\n    self,\n    model_output: Tensor,\n    xt: Optional[Tensor] = None,\n    t: Optional[Tensor] = None,\n    mask: Optional[Tensor] = None,\n):\n    \"\"\"Process the model output based on the prediction type to generate clean data.\n\n    Args:\n        model_output (Tensor): The output of the model.\n        xt (Tensor): The input sample.\n        t (Tensor): The time step.\n        mask (Optional[Tensor], optional): An optional mask to apply to the model output. Defaults to None.\n\n    Returns:\n        The data prediction based on the prediction type.\n\n    Raises:\n        ValueError: If the prediction type is not \"flow\".\n    \"\"\"\n    if self.prediction_type == PredictionType.VELOCITY:\n        if xt is None or t is None:\n            raise ValueError(\"Xt and time cannot be None\")\n        t = pad_like(t, model_output)\n        pred_data = xt + (1 - t) * model_output\n    elif self.prediction_type == PredictionType.DATA:\n        pred_data = model_output\n    else:\n        raise ValueError(\n            f\"prediction_type given as {self.prediction_type} must be `flow` for Continuous Flow Matching.\"\n        )\n    if mask is not None:\n        pred_data = pred_data * mask.unsqueeze(-1)\n    return pred_data\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher.process_vector_field_prediction","title":"<code>process_vector_field_prediction(model_output, xt=None, t=None, mask=None)</code>","text":"<p>Process the model output based on the prediction type to calculate vecotr field.</p> <p>Parameters:</p> Name Type Description Default <code>model_output</code> <code>Tensor</code> <p>The output of the model.</p> required <code>xt</code> <code>Tensor</code> <p>The input sample.</p> <code>None</code> <code>t</code> <code>Tensor</code> <p>The time step.</p> <code>None</code> <code>mask</code> <code>Optional[Tensor]</code> <p>An optional mask to apply to the model output. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>The vector field prediction based on the prediction type.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the prediction type is not \"flow\" or \"data\".</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def process_vector_field_prediction(\n    self,\n    model_output: Tensor,\n    xt: Optional[Tensor] = None,\n    t: Optional[Tensor] = None,\n    mask: Optional[Tensor] = None,\n):\n    \"\"\"Process the model output based on the prediction type to calculate vecotr field.\n\n    Args:\n        model_output (Tensor): The output of the model.\n        xt (Tensor): The input sample.\n        t (Tensor): The time step.\n        mask (Optional[Tensor], optional): An optional mask to apply to the model output. Defaults to None.\n\n    Returns:\n        The vector field prediction based on the prediction type.\n\n    Raises:\n        ValueError: If the prediction type is not \"flow\" or \"data\".\n    \"\"\"\n    if self.prediction_type == PredictionType.VELOCITY:\n        pred_vector_field = model_output\n    elif self.prediction_type == PredictionType.DATA:\n        if xt is None or t is None:\n            raise ValueError(\"Xt and Time cannpt be None for vector field conversion\")\n        t = pad_like(t, model_output)\n        pred_vector_field = (model_output - xt) / (1 - t + self.eps)\n    else:\n        raise ValueError(\n            f\"prediction_type given as {self.prediction_type} must be `flow` or `data` \"\n            \"for Continuous Flow Matching.\"\n        )\n    if mask is not None:\n        pred_vector_field = pred_vector_field * mask.unsqueeze(-1)\n    return pred_vector_field\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher.scale_data","title":"<code>scale_data(data)</code>","text":"<p>Upscale the input data by the data scale factor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>The input data to upscale.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The upscaled data.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def scale_data(self, data: Tensor) -&gt; Tensor:\n    \"\"\"Upscale the input data by the data scale factor.\n\n    Args:\n        data (Tensor): The input data to upscale.\n\n    Returns:\n        The upscaled data.\n    \"\"\"\n    return self.data_scale * data\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher.step","title":"<code>step(model_out, xt, dt, t=None, mask=None, center=False)</code>","text":"<p>Perform a single ODE step integration using Euler method.</p> <p>Parameters:</p> Name Type Description Default <code>model_out</code> <code>Tensor</code> <p>The output of the model at the current time step.</p> required <code>xt</code> <code>Tensor</code> <p>The current intermediate state.</p> required <code>dt</code> <code>Tensor</code> <p>The time step size.</p> required <code>t</code> <code>Tensor</code> <p>The current time. Defaults to None.</p> <code>None</code> <code>mask</code> <code>Optional[Tensor]</code> <p>A mask to apply to the model output. Defaults to None.</p> <code>None</code> <code>center</code> <code>Bool</code> <p>Whether to center the output. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>x_next</code> <code>Tensor</code> <p>The updated state of the system after the single step, x_(t+dt).</p> <p>Notes: - If a mask is provided, it is applied element-wise to the model output before scaling. - The <code>clean</code> method is called on the updated state before it is returned.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def step(\n    self,\n    model_out: Tensor,\n    xt: Tensor,\n    dt: Tensor,\n    t: Optional[Tensor] = None,\n    mask: Optional[Tensor] = None,\n    center: Bool = False,\n):\n    \"\"\"Perform a single ODE step integration using Euler method.\n\n    Args:\n        model_out (Tensor): The output of the model at the current time step.\n        xt (Tensor): The current intermediate state.\n        dt (Tensor): The time step size.\n        t (Tensor, optional): The current time. Defaults to None.\n        mask (Optional[Tensor], optional): A mask to apply to the model output. Defaults to None.\n        center (Bool, optional): Whether to center the output. Defaults to False.\n\n    Returns:\n        x_next (Tensor): The updated state of the system after the single step, x_(t+dt).\n\n    Notes:\n    - If a mask is provided, it is applied element-wise to the model output before scaling.\n    - The `clean` method is called on the updated state before it is returned.\n    \"\"\"\n    if mask is not None:\n        model_out = model_out * mask.unsqueeze(-1)\n    v_t = self.process_vector_field_prediction(model_out, xt, t, mask)\n    dt = pad_like(dt, model_out)\n    delta_x = v_t * dt\n    x_next = xt + delta_x\n    x_next = self.clean_mask_center(x_next, mask, center)\n    return x_next\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher.step_score_stochastic","title":"<code>step_score_stochastic(model_out, xt, dt, t, mask=None, gt_mode='tan', gt_p=1.0, gt_clamp=None, score_temperature=1.0, noise_temperature=1.0, t_lim_ode=0.99, center=False)</code>","text":"<p>Perform a single SDE step integration using a score-based Langevin update.</p> <p>d x_t = [v(x_t, t) + g(t) * s(x_t, t) * score_temperature] dt + \\sqrt{2 * g(t) * noise_temperature} dw_t.</p> <p>Parameters:</p> Name Type Description Default <code>model_out</code> <code>Tensor</code> <p>The output of the model at the current time step.</p> required <code>xt</code> <code>Tensor</code> <p>The current intermediate state.</p> required <code>dt</code> <code>Tensor</code> <p>The time step size.</p> required <code>t</code> <code>Tensor</code> <p>The current time. Defaults to None.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>A mask to apply to the model output. Defaults to None.</p> <code>None</code> <code>gt_mode</code> <code>str</code> <p>The mode for the gt function. Defaults to \"tan\".</p> <code>'tan'</code> <code>gt_p</code> <code>Float</code> <p>The parameter for the gt function. Defaults to 1.0.</p> <code>1.0</code> <code>gt_clamp</code> <code>Optional[Float]</code> <p>(Float, optional): Upper limit of gt term. Defaults to None.</p> <code>None</code> <code>score_temperature</code> <code>Float</code> <p>The temperature for the score part of the step. Defaults to 1.0.</p> <code>1.0</code> <code>noise_temperature</code> <code>Float</code> <p>The temperature for the stochastic part of the step. Defaults to 1.0.</p> <code>1.0</code> <code>t_lim_ode</code> <code>Float</code> <p>The time limit for the ODE step. Defaults to 0.99.</p> <code>0.99</code> <code>center</code> <code>Bool</code> <p>Whether to center the output. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>x_next</code> <code>Tensor</code> <p>The updated state of the system after the single step, x_(t+dt).</p> Notes <ul> <li>If a mask is provided, it is applied element-wise to the model output before scaling.</li> <li>The <code>clean</code> method is called on the updated state before it is returned.</li> </ul> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def step_score_stochastic(\n    self,\n    model_out: Tensor,\n    xt: Tensor,\n    dt: Tensor,\n    t: Tensor,\n    mask: Optional[Tensor] = None,\n    gt_mode: str = \"tan\",\n    gt_p: Float = 1.0,\n    gt_clamp: Optional[Float] = None,\n    score_temperature: Float = 1.0,\n    noise_temperature: Float = 1.0,\n    t_lim_ode: Float = 0.99,\n    center: Bool = False,\n):\n    r\"\"\"Perform a single SDE step integration using a score-based Langevin update.\n\n    d x_t = [v(x_t, t) + g(t) * s(x_t, t) * score_temperature] dt + \\sqrt{2 * g(t) * noise_temperature} dw_t.\n\n    Args:\n        model_out (Tensor): The output of the model at the current time step.\n        xt (Tensor): The current intermediate state.\n        dt (Tensor): The time step size.\n        t (Tensor, optional): The current time. Defaults to None.\n        mask (Optional[Tensor], optional): A mask to apply to the model output. Defaults to None.\n        gt_mode (str, optional): The mode for the gt function. Defaults to \"tan\".\n        gt_p (Float, optional): The parameter for the gt function. Defaults to 1.0.\n        gt_clamp: (Float, optional): Upper limit of gt term. Defaults to None.\n        score_temperature (Float, optional): The temperature for the score part of the step. Defaults to 1.0.\n        noise_temperature (Float, optional): The temperature for the stochastic part of the step. Defaults to 1.0.\n        t_lim_ode (Float, optional): The time limit for the ODE step. Defaults to 0.99.\n        center (Bool, optional): Whether to center the output. Defaults to False.\n\n    Returns:\n        x_next (Tensor): The updated state of the system after the single step, x_(t+dt).\n\n    Notes:\n        - If a mask is provided, it is applied element-wise to the model output before scaling.\n        - The `clean` method is called on the updated state before it is returned.\n    \"\"\"\n    if self.augmentation_type is not None:\n        raise ValueError(\"Optimal Transport violates the vector field to score conversion\")\n    if not isinstance(self.prior_distribution, GaussianPrior):\n        raise ValueError(\n            \"Prior distribution must be an instance of GaussianPrior to learn a proper score function\"\n        )\n    if t.min() &gt;= t_lim_ode:\n        return self.step(model_out, xt, dt, t, mask, center)\n    if mask is not None:\n        model_out = model_out * mask.unsqueeze(-1)\n    v_t = self.process_vector_field_prediction(model_out, xt, t, mask)\n    dt = pad_like(dt, model_out)\n    t = pad_like(t, model_out)\n    score = self.vf_to_score(xt, v_t, t)\n    gt = self.get_gt(t, gt_mode, gt_p, gt_clamp)\n    eps = torch.randn(xt.shape, dtype=xt.dtype, device=xt.device, generator=self.rng_generator)\n    std_eps = torch.sqrt(2 * gt * noise_temperature * dt)\n    delta_x = (v_t + gt * score * score_temperature) * dt + std_eps * eps\n    x_next = xt + delta_x\n    x_next = self.clean_mask_center(x_next, mask, center)\n    return x_next\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher.undo_scale_data","title":"<code>undo_scale_data(data)</code>","text":"<p>Downscale the input data by the data scale factor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>The input data to downscale.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The downscaled data.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def undo_scale_data(self, data: Tensor) -&gt; Tensor:\n    \"\"\"Downscale the input data by the data scale factor.\n\n    Args:\n        data (Tensor): The input data to downscale.\n\n    Returns:\n        The downscaled data.\n    \"\"\"\n    return 1 / self.data_scale * data\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher.vf_to_score","title":"<code>vf_to_score(x_t, v, t)</code>","text":"<p>From Geffner et al. Computes score of noisy density given the vector field learned by flow matching.</p> <p>With our interpolation scheme these are related by</p> <p>v(x_t, t) = (1 / t) (x_t + scale_ref ** 2 * (1 - t) * s(x_t, t)),</p> <p>or equivalently,</p> <p>s(x_t, t) = (t * v(x_t, t) - x_t) / (scale_ref ** 2 * (1 - t)).</p> <p>with scale_ref = 1</p> <p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Tensor</code> <p>Noisy sample, shape [*, dim]</p> required <code>v</code> <code>Tensor</code> <p>Vector field, shape [*, dim]</p> required <code>t</code> <code>Tensor</code> <p>Interpolation time, shape [*] (must be &lt; 1)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Score of intermediate density, shape [*, dim].</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def vf_to_score(\n    self,\n    x_t: Tensor,\n    v: Tensor,\n    t: Tensor,\n) -&gt; Tensor:\n    \"\"\"From Geffner et al. Computes score of noisy density given the vector field learned by flow matching.\n\n    With our interpolation scheme these are related by\n\n    v(x_t, t) = (1 / t) (x_t + scale_ref ** 2 * (1 - t) * s(x_t, t)),\n\n    or equivalently,\n\n    s(x_t, t) = (t * v(x_t, t) - x_t) / (scale_ref ** 2 * (1 - t)).\n\n    with scale_ref = 1\n\n    Args:\n        x_t: Noisy sample, shape [*, dim]\n        v: Vector field, shape [*, dim]\n        t: Interpolation time, shape [*] (must be &lt; 1)\n\n    Returns:\n        Score of intermediate density, shape [*, dim].\n    \"\"\"\n    assert torch.all(t &lt; 1.0), \"vf_to_score requires t &lt; 1 (strict)\"\n    t = pad_like(t, v)\n    num = t * v - x_t  # [*, dim]\n    den = 1.0 - t  # [*, 1]\n    score = num / den\n    return score  # [*, dim]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/","title":"Vdm","text":""},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM","title":"<code>VDM</code>","text":"<p>               Bases: <code>Interpolant</code></p> <p>A Variational Diffusion Models (VDM) interpolant.</p> <p>Examples: <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.moco.distributions.prior.continuous.gaussian import GaussianPrior\n&gt;&gt;&gt; from bionemo.moco.distributions.time.uniform import UniformTimeDistribution\n&gt;&gt;&gt; from bionemo.moco.interpolants.discrete_time.continuous.vdm import VDM\n&gt;&gt;&gt; from bionemo.moco.schedules.noise.continuous_snr_transforms import CosineSNRTransform\n&gt;&gt;&gt; from bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule\n\n\nvdm = VDM(\n    time_distribution = UniformTimeDistribution(...),\n    prior_distribution = GaussianPrior(...),\n    noise_schedule = CosineSNRTransform(...),\n    )\nmodel = Model(...)\n\n# Training\nfor epoch in range(1000):\n    data = data_loader.get(...)\n    time = vdm.sample_time(batch_size)\n    noise = vdm.sample_prior(data.shape)\n    xt = vdm.interpolate(data, noise, time)\n\n    x_pred = model(xt, time)\n    loss = vdm.loss(x_pred, data, time)\n    loss.backward()\n\n# Generation\nx_pred = vdm.sample_prior(data.shape)\nfor t in LinearInferenceSchedule(...).generate_schedule():\n    time = torch.full((batch_size,), t)\n    x_hat = model(x_pred, time)\n    x_pred = vdm.step(x_hat, time, x_pred)\nreturn x_pred\n</code></pre></p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>class VDM(Interpolant):\n    \"\"\"A Variational Diffusion Models (VDM) interpolant.\n\n     -------\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from bionemo.moco.distributions.prior.continuous.gaussian import GaussianPrior\n    &gt;&gt;&gt; from bionemo.moco.distributions.time.uniform import UniformTimeDistribution\n    &gt;&gt;&gt; from bionemo.moco.interpolants.discrete_time.continuous.vdm import VDM\n    &gt;&gt;&gt; from bionemo.moco.schedules.noise.continuous_snr_transforms import CosineSNRTransform\n    &gt;&gt;&gt; from bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule\n\n\n    vdm = VDM(\n        time_distribution = UniformTimeDistribution(...),\n        prior_distribution = GaussianPrior(...),\n        noise_schedule = CosineSNRTransform(...),\n        )\n    model = Model(...)\n\n    # Training\n    for epoch in range(1000):\n        data = data_loader.get(...)\n        time = vdm.sample_time(batch_size)\n        noise = vdm.sample_prior(data.shape)\n        xt = vdm.interpolate(data, noise, time)\n\n        x_pred = model(xt, time)\n        loss = vdm.loss(x_pred, data, time)\n        loss.backward()\n\n    # Generation\n    x_pred = vdm.sample_prior(data.shape)\n    for t in LinearInferenceSchedule(...).generate_schedule():\n        time = torch.full((batch_size,), t)\n        x_hat = model(x_pred, time)\n        x_pred = vdm.step(x_hat, time, x_pred)\n    return x_pred\n\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        time_distribution: TimeDistribution,\n        prior_distribution: PriorDistribution,\n        noise_schedule: ContinuousSNRTransform,\n        prediction_type: Union[PredictionType, str] = PredictionType.DATA,\n        device: Union[str, torch.device] = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Initializes the DDPM interpolant.\n\n        Args:\n            time_distribution (TimeDistribution): The distribution of time steps, used to sample time points for the diffusion process.\n            prior_distribution (PriorDistribution): The prior distribution of the variable, used as the starting point for the diffusion process.\n            noise_schedule (ContinuousSNRTransform): The schedule of noise, defining the amount of noise added at each time step.\n            prediction_type (PredictionType, optional): The type of prediction, either \"data\" or another type. Defaults to \"data\".\n            device (str, optional): The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n        \"\"\"\n        super().__init__(time_distribution, prior_distribution, device, rng_generator)\n        if not isinstance(prior_distribution, GaussianPrior):\n            warnings.warn(\"Prior distribution is not a GaussianPrior, unexpected behavior may occur\")\n        self.noise_schedule = noise_schedule\n        self.prediction_type = string_to_enum(prediction_type, PredictionType)\n        self._loss_function = nn.MSELoss(reduction=\"none\")\n\n    def interpolate(self, data: Tensor, t: Tensor, noise: Tensor):\n        \"\"\"Get x(t) with given time t from noise and data.\n\n        Args:\n            data (Tensor): target\n            t (Tensor): time\n            noise (Tensor): noise from prior()\n        \"\"\"\n        log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n        psi, omega = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n        psi = pad_like(psi, data)\n        omega = pad_like(omega, data)\n        x_t = data * psi + noise * omega\n        return x_t\n\n    def forward_process(self, data: Tensor, t: Tensor, noise: Optional[Tensor] = None):\n        \"\"\"Get x(t) with given time t from noise and data.\n\n        Args:\n            data (Tensor): target\n            t (Tensor): time\n            noise (Tensor, optional): noise from prior(). Defaults to None\n        \"\"\"\n        if noise is None:\n            noise = self.sample_prior(data.shape)\n        return self.interpolate(data, t, noise)\n\n    def process_data_prediction(self, model_output: Tensor, sample, t):\n        \"\"\"Converts the model output to a data prediction based on the prediction type.\n\n        This conversion stems from the Progressive Distillation for Fast Sampling of Diffusion Models https://arxiv.org/pdf/2202.00512.\n        Given the model output and the sample, we convert the output to a data prediction based on the prediction type.\n        The conversion formulas are as follows:\n        - For \"noise\" prediction type: `pred_data = (sample - noise_scale * model_output) / data_scale`\n        - For \"data\" prediction type: `pred_data = model_output`\n        - For \"v_prediction\" prediction type: `pred_data = data_scale * sample - noise_scale * model_output`\n\n        Args:\n            model_output (Tensor): The output of the model.\n            sample (Tensor): The input sample.\n            t (Tensor): The time step.\n\n        Returns:\n            The data prediction based on the prediction type.\n\n        Raises:\n            ValueError: If the prediction type is not one of \"noise\", \"data\", or \"v_prediction\".\n        \"\"\"\n        log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n        data_scale, noise_scale = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n        data_scale = pad_like(data_scale, model_output)\n        noise_scale = pad_like(noise_scale, model_output)\n        if self.prediction_type == PredictionType.NOISE:\n            pred_data = (sample - noise_scale * model_output) / data_scale\n        elif self.prediction_type == PredictionType.DATA:\n            pred_data = model_output\n        elif self.prediction_type == PredictionType.VELOCITY:\n            pred_data = data_scale * sample - noise_scale * model_output\n        else:\n            raise ValueError(\n                f\"prediction_type given as {self.prediction_type} must be one of PredictionType.NOISE, PredictionType.DATA or\"\n                f\" PredictionType.VELOCITY for vdm.\"\n            )\n        return pred_data\n\n    def process_noise_prediction(self, model_output: Tensor, sample: Tensor, t: Tensor):\n        \"\"\"Do the same as process_data_prediction but take the model output and convert to nosie.\n\n        Args:\n            model_output (Tensor): The output of the model.\n            sample (Tensor): The input sample.\n            t (Tensor): The time step.\n\n        Returns:\n            The input as noise if the prediction type is \"noise\".\n\n        Raises:\n            ValueError: If the prediction type is not \"noise\".\n        \"\"\"\n        log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n        data_scale, noise_scale = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n        data_scale = pad_like(data_scale, model_output)\n        noise_scale = pad_like(noise_scale, model_output)\n        if self.prediction_type == PredictionType.NOISE:\n            pred_noise = model_output\n        elif self.prediction_type == PredictionType.DATA:\n            pred_noise = (sample - data_scale * model_output) / noise_scale\n        elif self.prediction_type == PredictionType.VELOCITY:\n            pred_data = data_scale * sample - noise_scale * model_output\n            pred_noise = (sample - data_scale * pred_data) / noise_scale\n        else:\n            raise ValueError(\n                f\"prediction_type given as {self.prediction_type} must be one of `noise`, `data` or\"\n                \" `v_prediction`  for vdm.\"\n            )\n        return pred_noise\n\n    def step(\n        self,\n        model_out: Tensor,\n        t: Tensor,\n        xt: Tensor,\n        dt: Tensor,\n        mask: Optional[Tensor] = None,\n        center: Bool = False,\n        temperature: Float = 1.0,\n    ):\n        \"\"\"Do one step integration.\n\n        Args:\n            model_out (Tensor): The output of the model.\n            xt (Tensor): The current data point.\n            t (Tensor): The current time step.\n            dt (Tensor): The time step increment.\n            mask (Optional[Tensor], optional): An optional mask to apply to the data. Defaults to None.\n            center (bool): Whether to center the data. Defaults to False.\n            temperature (Float): The temperature parameter for low temperature sampling. Defaults to 1.0.\n\n        Note:\n            The temperature parameter controls the trade off between diversity and sample quality.\n            Decreasing the temperature sharpens the sampling distribtion to focus on more likely samples.\n            The impact of low temperature sampling must be ablated analytically.\n        \"\"\"\n        if mask is not None:\n            model_out = model_out * mask.unsqueeze(-1)\n        x_hat = self.process_data_prediction(model_out, xt, t)\n\n        log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n        alpha_t, sigma_t = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n\n        if (t - dt &lt; 0).any():\n            raise ValueError(\n                \"Error in inference schedule: t - dt &lt; 0. Please ensure that your inference time schedule has shape T with the final t = dt to make s = 0\"\n            )\n\n        log_snr_s = self.noise_schedule.calculate_log_snr(t - dt, device=t.device)\n        alpha_s, sigma_s = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr_s)\n        sigma_s_2 = sigma_s * sigma_s\n        sigma_t_2 = sigma_t * sigma_t\n        alpha_t_s = alpha_t / alpha_s\n        sigma_2_t_s = -torch.expm1(F.softplus(-log_snr_s) - F.softplus(-log_snr))  # Equation 63\n\n        omega_r = alpha_t_s * sigma_s_2 / sigma_t_2  # Equation 28\n        psi_r = alpha_s * sigma_2_t_s / sigma_t_2\n        std = sigma_2_t_s.sqrt() * sigma_s / sigma_t\n        nonzero_mask = (\n            t &gt; 0\n        ).float()  # based on the time this is always just ones. can leave for now to see if ever want to take extra step and only grab mean\n\n        psi_r = pad_like(psi_r, x_hat)\n        omega_r = pad_like(omega_r, x_hat)\n        std = pad_like(std, x_hat)\n        nonzero_mask = pad_like(nonzero_mask, x_hat)\n\n        mean = psi_r * x_hat + omega_r * xt\n        eps = torch.randn_like(mean).to(model_out.device)\n        x_next = mean + nonzero_mask * std * eps * temperature\n        x_next = self.clean_mask_center(x_next, mask, center)\n        return x_next\n\n    def score(self, x_hat: Tensor, xt: Tensor, t: Tensor):\n        \"\"\"Converts the data prediction to the estimated score function.\n\n        Args:\n            x_hat (tensor): The predicted data point.\n            xt (Tensor): The current data point.\n            t (Tensor): The time step.\n\n        Returns:\n            The estimated score function.\n        \"\"\"\n        log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n        psi, omega = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n        psi = pad_like(psi, x_hat)\n        omega = pad_like(omega, x_hat)\n        score = psi * x_hat - xt\n        score = score / (omega * omega)\n        return score\n\n    def step_ddim(\n        self,\n        model_out: Tensor,\n        t: Tensor,\n        xt: Tensor,\n        dt: Tensor,\n        mask: Optional[Tensor] = None,\n        eta: Float = 0.0,\n        center: Bool = False,\n    ):\n        \"\"\"Do one step of DDIM sampling.\n\n        From the ddpm equations alpha_bar = alpha**2 and  1 - alpha**2 = sigma**2\n\n        Args:\n            model_out (Tensor): output of the model\n            t (Tensor): current time step\n            xt (Tensor): current data point\n            dt (Tensor): The time step increment.\n            mask (Optional[Tensor], optional): mask for the data point. Defaults to None.\n            eta (Float, optional): DDIM sampling parameter. Defaults to 0.0.\n            center (Bool, optional): whether to center the data point. Defaults to False.\n        \"\"\"\n        if mask is not None:\n            model_out = model_out * mask.unsqueeze(-1)\n        data_pred = self.process_data_prediction(model_out, xt, t)\n        noise_pred = self.process_noise_prediction(model_out, xt, t)\n        eps = torch.randn_like(data_pred).to(model_out.device)\n        log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n        squared_alpha = log_snr.sigmoid()\n        squared_sigma = (-log_snr).sigmoid()\n        log_snr_prev = self.noise_schedule.calculate_log_snr(t - dt, device=t.device)\n        squared_alpha_prev = log_snr_prev.sigmoid()\n        squared_sigma_prev = (-log_snr_prev).sigmoid()\n        sigma_t_2 = squared_sigma_prev / squared_sigma * (1 - squared_alpha / squared_alpha_prev)\n        psi_r = torch.sqrt(squared_alpha_prev)\n        omega_r = torch.sqrt(1 - squared_alpha_prev - eta * eta * sigma_t_2)\n\n        sigma_t_2 = pad_like(sigma_t_2, model_out)\n        psi_r = pad_like(psi_r, model_out)\n        omega_r = pad_like(omega_r, model_out)\n\n        mean = data_pred * psi_r + omega_r * noise_pred\n        x_next = mean + eta * sigma_t_2.sqrt() * eps\n        x_next = self.clean_mask_center(x_next, mask, center)\n        return x_next\n\n    def set_loss_weight_fn(self, fn: Callable):\n        \"\"\"Sets the loss_weight attribute of the instance to the given function.\n\n        Args:\n            fn: The function to set as the loss_weight attribute. This function should take three arguments: raw_loss, t, and weight_type.\n        \"\"\"\n        self.loss_weight = fn\n\n    def loss_weight(self, raw_loss: Tensor, t: Tensor, weight_type: str, dt: Float = 0.001) -&gt; Tensor:\n        \"\"\"Calculates the weight for the loss based on the given weight type.\n\n        This function computes the loss weight according to the specified `weight_type`.\n        The available weight types are:\n        - \"ones\": uniform weight of 1.0\n        - \"data_to_noise\": derived from Equation (9) of https://arxiv.org/pdf/2202.00512\n        - \"variational_objective_discrete\": based on the variational objective, see https://arxiv.org/pdf/2202.00512\n\n        Args:\n            raw_loss (Tensor): The raw loss calculated from the model prediction and target.\n            t (Tensor): The time step.\n            weight_type (str): The type of weight to use. Can be \"ones\", \"data_to_noise\", or \"variational_objective_discrete\".\n            dt (Float, optional): The time step increment. Defaults to 0.001.\n\n        Returns:\n            Tensor: The weight for the loss.\n\n        Raises:\n            ValueError: If the weight type is not recognized.\n        \"\"\"\n        if weight_type == \"ones\":\n            schedule = torch.ones_like(raw_loss).to(raw_loss.device)\n        elif weight_type == \"data_to_noise\":  #\n            log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n            psi, omega = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n            schedule = (psi**2) / (omega**2)\n            for _ in range(raw_loss.ndim - 1):\n                schedule = schedule.unsqueeze(-1)\n        elif weight_type == \"variational_objective_discrete\":\n            # Equation 14 https://arxiv.org/pdf/2107.00630\n            schedule = -torch.expm1(\n                self.noise_schedule.calculate_log_snr(t, device=t.device)\n                - self.noise_schedule.calculate_log_snr(t - dt, device=t.device)\n            )\n            for _ in range(raw_loss.ndim - 1):\n                schedule = schedule.unsqueeze(-1)\n        elif weight_type == \"variational_objective_continuous_noise\":\n            # equation 21 https://arxiv.org/pdf/2303.00848\n            t = t.requires_grad_()\n            log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n            gamma = log_snr\n            gamma_grad = -torch.autograd.grad(  # gamma_grad shape: (B, )\n                gamma,  # (B, )\n                t,  # (B, )\n                grad_outputs=torch.ones_like(gamma),\n                create_graph=True,\n                retain_graph=True,\n            )[0]\n            schedule = 0.5 * gamma_grad\n            for _ in range(raw_loss.ndim - 1):\n                schedule = schedule.unsqueeze(-1)\n        elif weight_type == \"variational_objective_continuous_data\":\n            t = t.requires_grad_()\n            log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n            snr = log_snr.exp()\n            snr_grad = torch.autograd.grad(  # gamma_grad shape: (B, )\n                snr,  # (B, )\n                t,  # (B, )\n                grad_outputs=torch.ones_like(snr),\n                create_graph=True,\n                retain_graph=True,\n            )[0]\n            schedule = -0.5 * snr_grad\n            for _ in range(raw_loss.ndim - 1):\n                schedule = schedule.unsqueeze(-1)\n        else:\n            raise ValueError(\"Invalid loss weight keyword\")\n        return schedule\n\n    def loss(\n        self,\n        model_pred: Tensor,\n        target: Tensor,\n        t: Tensor,\n        dt: Optional[Float] = 0.001,\n        mask: Optional[Tensor] = None,\n        weight_type: str = \"ones\",\n    ):\n        \"\"\"Calculates the loss given the model prediction, target, and time.\n\n        Args:\n            model_pred (Tensor): The predicted output from the model.\n            target (Tensor): The target output for the model prediction.\n            t (Tensor): The time at which the loss is calculated.\n            dt (Optional[Float], optional): The time step increment. Defaults to 0.001.\n            mask (Optional[Tensor], optional): The mask for the data point. Defaults to None.\n            weight_type (str, optional): The type of weight to use for the loss. Can be \"ones\", \"data_to_noise\", or \"variational_objective\". Defaults to \"ones\".\n\n        Returns:\n            Tensor: The calculated loss batch tensor.\n        \"\"\"\n        raw_loss = self._loss_function(model_pred, target)\n        update_weight = self.loss_weight(raw_loss, t, weight_type, dt)\n        loss = raw_loss * update_weight\n        if mask is not None:\n            loss = loss * mask.unsqueeze(-1)\n            n_elem = torch.sum(mask, dim=-1)\n            loss = torch.sum(loss, dim=tuple(range(1, raw_loss.ndim))) / n_elem\n        else:\n            loss = torch.sum(loss, dim=tuple(range(1, raw_loss.ndim))) / model_pred.size(1)\n        return loss\n\n    def step_hybrid_sde(\n        self,\n        model_out: Tensor,\n        t: Tensor,\n        xt: Tensor,\n        dt: Tensor,\n        mask: Optional[Tensor] = None,\n        center: Bool = False,\n        temperature: Float = 1.0,\n        equilibrium_rate: Float = 0.0,\n    ) -&gt; Tensor:\n        \"\"\"Do one step integration of Hybrid Langevin-Reverse Time SDE.\n\n        See section B.3 page 37 https://www.biorxiv.org/content/10.1101/2022.12.01.518682v1.full.pdf.\n        and https://github.com/generatebio/chroma/blob/929407c605013613941803c6113adefdccaad679/chroma/layers/structure/diffusion.py#L730\n\n        Args:\n            model_out (Tensor): The output of the model.\n            xt (Tensor): The current data point.\n            t (Tensor): The current time step.\n            dt (Tensor): The time step increment.\n            mask (Optional[Tensor], optional): An optional mask to apply to the data. Defaults to None.\n            center (bool, optional): Whether to center the data. Defaults to False.\n            temperature (Float, optional): The temperature parameter for low temperature sampling. Defaults to 1.0.\n            equilibrium_rate (Float, optional): The rate of Langevin equilibration.  Scales the amount of Langevin dynamics per unit time. Best values are in the range [1.0, 5.0]. Defaults to 0.0.\n\n        Note:\n        For all step functions that use the SDE formulation its important to note that we are moving backwards in time which corresponds to an apparent sign change.\n        A clear example can be seen in slide 29 https://ernestryu.com/courses/FM/diffusion1.pdf.\n        \"\"\"\n        if mask is not None:\n            model_out = model_out * mask.unsqueeze(-1)\n        x_hat = self.process_data_prediction(model_out, xt, t)\n        log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n        alpha, sigma = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n        # Schedule coeffiecients\n        beta = self.noise_schedule.calculate_beta(t)\n        inverse_temperature = 1 / temperature  # lambda_0\n        langevin_factor = equilibrium_rate\n        # Temperature coefficients\n        lambda_t = (\n            inverse_temperature * (sigma.pow(2) + alpha.pow(2)) / (inverse_temperature * sigma.pow(2) + alpha.pow(2))\n        )\n        # langevin_isothermal = True\n        lambda_langevin = inverse_temperature  # if langevin_isothermal else lambda_t\n\n        score_scale_t = lambda_t + lambda_langevin * langevin_factor / 2.0\n\n        eps = torch.randn_like(x_hat).to(model_out.device)\n        score = self.score(x_hat, xt, t)\n        beta = pad_like(beta, model_out)\n        score_scale_t = pad_like(score_scale_t, model_out)\n\n        gT = beta * ((-1 / 2) * xt - score_scale_t * score)\n        gW = torch.sqrt((1.0 + langevin_factor) * beta.abs()) * eps\n\n        x_next = xt + dt * gT + dt.sqrt() * gW\n        x_next = self.clean_mask_center(x_next, mask, center)\n        return x_next\n\n    def step_ode(\n        self,\n        model_out: Tensor,\n        t: Tensor,\n        xt: Tensor,\n        dt: Tensor,\n        mask: Optional[Tensor] = None,\n        center: Bool = False,\n        temperature: Float = 1.0,\n    ) -&gt; Tensor:\n        \"\"\"Do one step integration of ODE.\n\n        See section B page 36 https://www.biorxiv.org/content/10.1101/2022.12.01.518682v1.full.pdf.\n        and https://github.com/generatebio/chroma/blob/929407c605013613941803c6113adefdccaad679/chroma/layers/structure/diffusion.py#L730\n\n        Args:\n            model_out (Tensor): The output of the model.\n            xt (Tensor): The current data point.\n            t (Tensor): The current time step.\n            dt (Tensor): The time step increment.\n            mask (Optional[Tensor], optional): An optional mask to apply to the data. Defaults to None.\n            center (bool, optional): Whether to center the data. Defaults to False.\n            temperature (Float, optional): The temperature parameter for low temperature sampling. Defaults to 1.0.\n        \"\"\"\n        if mask is not None:\n            model_out = model_out * mask.unsqueeze(-1)\n        x_hat = self.process_data_prediction(model_out, xt, t)\n        log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n        alpha, sigma = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n        # Schedule coeffiecients\n        beta = self.noise_schedule.calculate_beta(t)\n        inverse_temperature = 1 / temperature\n        # Temperature coefficients\n        lambda_t = (\n            inverse_temperature * (sigma.pow(2) + alpha.pow(2)) / (inverse_temperature * sigma.pow(2) + alpha.pow(2))\n        )\n\n        score = self.score(x_hat, xt, t)\n        beta = pad_like(beta, model_out)\n        lambda_t = pad_like(lambda_t, model_out)\n\n        gT = (-1 / 2) * beta * (xt + lambda_t * score)\n\n        x_next = xt + gT * dt\n        x_next = self.clean_mask_center(x_next, mask, center)\n        return x_next\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM.__init__","title":"<code>__init__(time_distribution, prior_distribution, noise_schedule, prediction_type=PredictionType.DATA, device='cpu', rng_generator=None)</code>","text":"<p>Initializes the DDPM interpolant.</p> <p>Parameters:</p> Name Type Description Default <code>time_distribution</code> <code>TimeDistribution</code> <p>The distribution of time steps, used to sample time points for the diffusion process.</p> required <code>prior_distribution</code> <code>PriorDistribution</code> <p>The prior distribution of the variable, used as the starting point for the diffusion process.</p> required <code>noise_schedule</code> <code>ContinuousSNRTransform</code> <p>The schedule of noise, defining the amount of noise added at each time step.</p> required <code>prediction_type</code> <code>PredictionType</code> <p>The type of prediction, either \"data\" or another type. Defaults to \"data\".</p> <code>DATA</code> <code>device</code> <code>str</code> <p>The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".</p> <code>'cpu'</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>def __init__(\n    self,\n    time_distribution: TimeDistribution,\n    prior_distribution: PriorDistribution,\n    noise_schedule: ContinuousSNRTransform,\n    prediction_type: Union[PredictionType, str] = PredictionType.DATA,\n    device: Union[str, torch.device] = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Initializes the DDPM interpolant.\n\n    Args:\n        time_distribution (TimeDistribution): The distribution of time steps, used to sample time points for the diffusion process.\n        prior_distribution (PriorDistribution): The prior distribution of the variable, used as the starting point for the diffusion process.\n        noise_schedule (ContinuousSNRTransform): The schedule of noise, defining the amount of noise added at each time step.\n        prediction_type (PredictionType, optional): The type of prediction, either \"data\" or another type. Defaults to \"data\".\n        device (str, optional): The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n    \"\"\"\n    super().__init__(time_distribution, prior_distribution, device, rng_generator)\n    if not isinstance(prior_distribution, GaussianPrior):\n        warnings.warn(\"Prior distribution is not a GaussianPrior, unexpected behavior may occur\")\n    self.noise_schedule = noise_schedule\n    self.prediction_type = string_to_enum(prediction_type, PredictionType)\n    self._loss_function = nn.MSELoss(reduction=\"none\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM.forward_process","title":"<code>forward_process(data, t, noise=None)</code>","text":"<p>Get x(t) with given time t from noise and data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>target</p> required <code>t</code> <code>Tensor</code> <p>time</p> required <code>noise</code> <code>Tensor</code> <p>noise from prior(). Defaults to None</p> <code>None</code> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>def forward_process(self, data: Tensor, t: Tensor, noise: Optional[Tensor] = None):\n    \"\"\"Get x(t) with given time t from noise and data.\n\n    Args:\n        data (Tensor): target\n        t (Tensor): time\n        noise (Tensor, optional): noise from prior(). Defaults to None\n    \"\"\"\n    if noise is None:\n        noise = self.sample_prior(data.shape)\n    return self.interpolate(data, t, noise)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM.interpolate","title":"<code>interpolate(data, t, noise)</code>","text":"<p>Get x(t) with given time t from noise and data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>target</p> required <code>t</code> <code>Tensor</code> <p>time</p> required <code>noise</code> <code>Tensor</code> <p>noise from prior()</p> required Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>def interpolate(self, data: Tensor, t: Tensor, noise: Tensor):\n    \"\"\"Get x(t) with given time t from noise and data.\n\n    Args:\n        data (Tensor): target\n        t (Tensor): time\n        noise (Tensor): noise from prior()\n    \"\"\"\n    log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n    psi, omega = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n    psi = pad_like(psi, data)\n    omega = pad_like(omega, data)\n    x_t = data * psi + noise * omega\n    return x_t\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM.loss","title":"<code>loss(model_pred, target, t, dt=0.001, mask=None, weight_type='ones')</code>","text":"<p>Calculates the loss given the model prediction, target, and time.</p> <p>Parameters:</p> Name Type Description Default <code>model_pred</code> <code>Tensor</code> <p>The predicted output from the model.</p> required <code>target</code> <code>Tensor</code> <p>The target output for the model prediction.</p> required <code>t</code> <code>Tensor</code> <p>The time at which the loss is calculated.</p> required <code>dt</code> <code>Optional[Float]</code> <p>The time step increment. Defaults to 0.001.</p> <code>0.001</code> <code>mask</code> <code>Optional[Tensor]</code> <p>The mask for the data point. Defaults to None.</p> <code>None</code> <code>weight_type</code> <code>str</code> <p>The type of weight to use for the loss. Can be \"ones\", \"data_to_noise\", or \"variational_objective\". Defaults to \"ones\".</p> <code>'ones'</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The calculated loss batch tensor.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>def loss(\n    self,\n    model_pred: Tensor,\n    target: Tensor,\n    t: Tensor,\n    dt: Optional[Float] = 0.001,\n    mask: Optional[Tensor] = None,\n    weight_type: str = \"ones\",\n):\n    \"\"\"Calculates the loss given the model prediction, target, and time.\n\n    Args:\n        model_pred (Tensor): The predicted output from the model.\n        target (Tensor): The target output for the model prediction.\n        t (Tensor): The time at which the loss is calculated.\n        dt (Optional[Float], optional): The time step increment. Defaults to 0.001.\n        mask (Optional[Tensor], optional): The mask for the data point. Defaults to None.\n        weight_type (str, optional): The type of weight to use for the loss. Can be \"ones\", \"data_to_noise\", or \"variational_objective\". Defaults to \"ones\".\n\n    Returns:\n        Tensor: The calculated loss batch tensor.\n    \"\"\"\n    raw_loss = self._loss_function(model_pred, target)\n    update_weight = self.loss_weight(raw_loss, t, weight_type, dt)\n    loss = raw_loss * update_weight\n    if mask is not None:\n        loss = loss * mask.unsqueeze(-1)\n        n_elem = torch.sum(mask, dim=-1)\n        loss = torch.sum(loss, dim=tuple(range(1, raw_loss.ndim))) / n_elem\n    else:\n        loss = torch.sum(loss, dim=tuple(range(1, raw_loss.ndim))) / model_pred.size(1)\n    return loss\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM.loss_weight","title":"<code>loss_weight(raw_loss, t, weight_type, dt=0.001)</code>","text":"<p>Calculates the weight for the loss based on the given weight type.</p> <p>This function computes the loss weight according to the specified <code>weight_type</code>. The available weight types are: - \"ones\": uniform weight of 1.0 - \"data_to_noise\": derived from Equation (9) of https://arxiv.org/pdf/2202.00512 - \"variational_objective_discrete\": based on the variational objective, see https://arxiv.org/pdf/2202.00512</p> <p>Parameters:</p> Name Type Description Default <code>raw_loss</code> <code>Tensor</code> <p>The raw loss calculated from the model prediction and target.</p> required <code>t</code> <code>Tensor</code> <p>The time step.</p> required <code>weight_type</code> <code>str</code> <p>The type of weight to use. Can be \"ones\", \"data_to_noise\", or \"variational_objective_discrete\".</p> required <code>dt</code> <code>Float</code> <p>The time step increment. Defaults to 0.001.</p> <code>0.001</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The weight for the loss.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the weight type is not recognized.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>def loss_weight(self, raw_loss: Tensor, t: Tensor, weight_type: str, dt: Float = 0.001) -&gt; Tensor:\n    \"\"\"Calculates the weight for the loss based on the given weight type.\n\n    This function computes the loss weight according to the specified `weight_type`.\n    The available weight types are:\n    - \"ones\": uniform weight of 1.0\n    - \"data_to_noise\": derived from Equation (9) of https://arxiv.org/pdf/2202.00512\n    - \"variational_objective_discrete\": based on the variational objective, see https://arxiv.org/pdf/2202.00512\n\n    Args:\n        raw_loss (Tensor): The raw loss calculated from the model prediction and target.\n        t (Tensor): The time step.\n        weight_type (str): The type of weight to use. Can be \"ones\", \"data_to_noise\", or \"variational_objective_discrete\".\n        dt (Float, optional): The time step increment. Defaults to 0.001.\n\n    Returns:\n        Tensor: The weight for the loss.\n\n    Raises:\n        ValueError: If the weight type is not recognized.\n    \"\"\"\n    if weight_type == \"ones\":\n        schedule = torch.ones_like(raw_loss).to(raw_loss.device)\n    elif weight_type == \"data_to_noise\":  #\n        log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n        psi, omega = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n        schedule = (psi**2) / (omega**2)\n        for _ in range(raw_loss.ndim - 1):\n            schedule = schedule.unsqueeze(-1)\n    elif weight_type == \"variational_objective_discrete\":\n        # Equation 14 https://arxiv.org/pdf/2107.00630\n        schedule = -torch.expm1(\n            self.noise_schedule.calculate_log_snr(t, device=t.device)\n            - self.noise_schedule.calculate_log_snr(t - dt, device=t.device)\n        )\n        for _ in range(raw_loss.ndim - 1):\n            schedule = schedule.unsqueeze(-1)\n    elif weight_type == \"variational_objective_continuous_noise\":\n        # equation 21 https://arxiv.org/pdf/2303.00848\n        t = t.requires_grad_()\n        log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n        gamma = log_snr\n        gamma_grad = -torch.autograd.grad(  # gamma_grad shape: (B, )\n            gamma,  # (B, )\n            t,  # (B, )\n            grad_outputs=torch.ones_like(gamma),\n            create_graph=True,\n            retain_graph=True,\n        )[0]\n        schedule = 0.5 * gamma_grad\n        for _ in range(raw_loss.ndim - 1):\n            schedule = schedule.unsqueeze(-1)\n    elif weight_type == \"variational_objective_continuous_data\":\n        t = t.requires_grad_()\n        log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n        snr = log_snr.exp()\n        snr_grad = torch.autograd.grad(  # gamma_grad shape: (B, )\n            snr,  # (B, )\n            t,  # (B, )\n            grad_outputs=torch.ones_like(snr),\n            create_graph=True,\n            retain_graph=True,\n        )[0]\n        schedule = -0.5 * snr_grad\n        for _ in range(raw_loss.ndim - 1):\n            schedule = schedule.unsqueeze(-1)\n    else:\n        raise ValueError(\"Invalid loss weight keyword\")\n    return schedule\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM.process_data_prediction","title":"<code>process_data_prediction(model_output, sample, t)</code>","text":"<p>Converts the model output to a data prediction based on the prediction type.</p> <p>This conversion stems from the Progressive Distillation for Fast Sampling of Diffusion Models https://arxiv.org/pdf/2202.00512. Given the model output and the sample, we convert the output to a data prediction based on the prediction type. The conversion formulas are as follows: - For \"noise\" prediction type: <code>pred_data = (sample - noise_scale * model_output) / data_scale</code> - For \"data\" prediction type: <code>pred_data = model_output</code> - For \"v_prediction\" prediction type: <code>pred_data = data_scale * sample - noise_scale * model_output</code></p> <p>Parameters:</p> Name Type Description Default <code>model_output</code> <code>Tensor</code> <p>The output of the model.</p> required <code>sample</code> <code>Tensor</code> <p>The input sample.</p> required <code>t</code> <code>Tensor</code> <p>The time step.</p> required <p>Returns:</p> Type Description <p>The data prediction based on the prediction type.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the prediction type is not one of \"noise\", \"data\", or \"v_prediction\".</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>def process_data_prediction(self, model_output: Tensor, sample, t):\n    \"\"\"Converts the model output to a data prediction based on the prediction type.\n\n    This conversion stems from the Progressive Distillation for Fast Sampling of Diffusion Models https://arxiv.org/pdf/2202.00512.\n    Given the model output and the sample, we convert the output to a data prediction based on the prediction type.\n    The conversion formulas are as follows:\n    - For \"noise\" prediction type: `pred_data = (sample - noise_scale * model_output) / data_scale`\n    - For \"data\" prediction type: `pred_data = model_output`\n    - For \"v_prediction\" prediction type: `pred_data = data_scale * sample - noise_scale * model_output`\n\n    Args:\n        model_output (Tensor): The output of the model.\n        sample (Tensor): The input sample.\n        t (Tensor): The time step.\n\n    Returns:\n        The data prediction based on the prediction type.\n\n    Raises:\n        ValueError: If the prediction type is not one of \"noise\", \"data\", or \"v_prediction\".\n    \"\"\"\n    log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n    data_scale, noise_scale = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n    data_scale = pad_like(data_scale, model_output)\n    noise_scale = pad_like(noise_scale, model_output)\n    if self.prediction_type == PredictionType.NOISE:\n        pred_data = (sample - noise_scale * model_output) / data_scale\n    elif self.prediction_type == PredictionType.DATA:\n        pred_data = model_output\n    elif self.prediction_type == PredictionType.VELOCITY:\n        pred_data = data_scale * sample - noise_scale * model_output\n    else:\n        raise ValueError(\n            f\"prediction_type given as {self.prediction_type} must be one of PredictionType.NOISE, PredictionType.DATA or\"\n            f\" PredictionType.VELOCITY for vdm.\"\n        )\n    return pred_data\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM.process_noise_prediction","title":"<code>process_noise_prediction(model_output, sample, t)</code>","text":"<p>Do the same as process_data_prediction but take the model output and convert to nosie.</p> <p>Parameters:</p> Name Type Description Default <code>model_output</code> <code>Tensor</code> <p>The output of the model.</p> required <code>sample</code> <code>Tensor</code> <p>The input sample.</p> required <code>t</code> <code>Tensor</code> <p>The time step.</p> required <p>Returns:</p> Type Description <p>The input as noise if the prediction type is \"noise\".</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the prediction type is not \"noise\".</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>def process_noise_prediction(self, model_output: Tensor, sample: Tensor, t: Tensor):\n    \"\"\"Do the same as process_data_prediction but take the model output and convert to nosie.\n\n    Args:\n        model_output (Tensor): The output of the model.\n        sample (Tensor): The input sample.\n        t (Tensor): The time step.\n\n    Returns:\n        The input as noise if the prediction type is \"noise\".\n\n    Raises:\n        ValueError: If the prediction type is not \"noise\".\n    \"\"\"\n    log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n    data_scale, noise_scale = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n    data_scale = pad_like(data_scale, model_output)\n    noise_scale = pad_like(noise_scale, model_output)\n    if self.prediction_type == PredictionType.NOISE:\n        pred_noise = model_output\n    elif self.prediction_type == PredictionType.DATA:\n        pred_noise = (sample - data_scale * model_output) / noise_scale\n    elif self.prediction_type == PredictionType.VELOCITY:\n        pred_data = data_scale * sample - noise_scale * model_output\n        pred_noise = (sample - data_scale * pred_data) / noise_scale\n    else:\n        raise ValueError(\n            f\"prediction_type given as {self.prediction_type} must be one of `noise`, `data` or\"\n            \" `v_prediction`  for vdm.\"\n        )\n    return pred_noise\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM.score","title":"<code>score(x_hat, xt, t)</code>","text":"<p>Converts the data prediction to the estimated score function.</p> <p>Parameters:</p> Name Type Description Default <code>x_hat</code> <code>tensor</code> <p>The predicted data point.</p> required <code>xt</code> <code>Tensor</code> <p>The current data point.</p> required <code>t</code> <code>Tensor</code> <p>The time step.</p> required <p>Returns:</p> Type Description <p>The estimated score function.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>def score(self, x_hat: Tensor, xt: Tensor, t: Tensor):\n    \"\"\"Converts the data prediction to the estimated score function.\n\n    Args:\n        x_hat (tensor): The predicted data point.\n        xt (Tensor): The current data point.\n        t (Tensor): The time step.\n\n    Returns:\n        The estimated score function.\n    \"\"\"\n    log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n    psi, omega = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n    psi = pad_like(psi, x_hat)\n    omega = pad_like(omega, x_hat)\n    score = psi * x_hat - xt\n    score = score / (omega * omega)\n    return score\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM.set_loss_weight_fn","title":"<code>set_loss_weight_fn(fn)</code>","text":"<p>Sets the loss_weight attribute of the instance to the given function.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The function to set as the loss_weight attribute. This function should take three arguments: raw_loss, t, and weight_type.</p> required Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>def set_loss_weight_fn(self, fn: Callable):\n    \"\"\"Sets the loss_weight attribute of the instance to the given function.\n\n    Args:\n        fn: The function to set as the loss_weight attribute. This function should take three arguments: raw_loss, t, and weight_type.\n    \"\"\"\n    self.loss_weight = fn\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM.step","title":"<code>step(model_out, t, xt, dt, mask=None, center=False, temperature=1.0)</code>","text":"<p>Do one step integration.</p> <p>Parameters:</p> Name Type Description Default <code>model_out</code> <code>Tensor</code> <p>The output of the model.</p> required <code>xt</code> <code>Tensor</code> <p>The current data point.</p> required <code>t</code> <code>Tensor</code> <p>The current time step.</p> required <code>dt</code> <code>Tensor</code> <p>The time step increment.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>An optional mask to apply to the data. Defaults to None.</p> <code>None</code> <code>center</code> <code>bool</code> <p>Whether to center the data. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>Float</code> <p>The temperature parameter for low temperature sampling. Defaults to 1.0.</p> <code>1.0</code> Note <p>The temperature parameter controls the trade off between diversity and sample quality. Decreasing the temperature sharpens the sampling distribtion to focus on more likely samples. The impact of low temperature sampling must be ablated analytically.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>def step(\n    self,\n    model_out: Tensor,\n    t: Tensor,\n    xt: Tensor,\n    dt: Tensor,\n    mask: Optional[Tensor] = None,\n    center: Bool = False,\n    temperature: Float = 1.0,\n):\n    \"\"\"Do one step integration.\n\n    Args:\n        model_out (Tensor): The output of the model.\n        xt (Tensor): The current data point.\n        t (Tensor): The current time step.\n        dt (Tensor): The time step increment.\n        mask (Optional[Tensor], optional): An optional mask to apply to the data. Defaults to None.\n        center (bool): Whether to center the data. Defaults to False.\n        temperature (Float): The temperature parameter for low temperature sampling. Defaults to 1.0.\n\n    Note:\n        The temperature parameter controls the trade off between diversity and sample quality.\n        Decreasing the temperature sharpens the sampling distribtion to focus on more likely samples.\n        The impact of low temperature sampling must be ablated analytically.\n    \"\"\"\n    if mask is not None:\n        model_out = model_out * mask.unsqueeze(-1)\n    x_hat = self.process_data_prediction(model_out, xt, t)\n\n    log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n    alpha_t, sigma_t = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n\n    if (t - dt &lt; 0).any():\n        raise ValueError(\n            \"Error in inference schedule: t - dt &lt; 0. Please ensure that your inference time schedule has shape T with the final t = dt to make s = 0\"\n        )\n\n    log_snr_s = self.noise_schedule.calculate_log_snr(t - dt, device=t.device)\n    alpha_s, sigma_s = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr_s)\n    sigma_s_2 = sigma_s * sigma_s\n    sigma_t_2 = sigma_t * sigma_t\n    alpha_t_s = alpha_t / alpha_s\n    sigma_2_t_s = -torch.expm1(F.softplus(-log_snr_s) - F.softplus(-log_snr))  # Equation 63\n\n    omega_r = alpha_t_s * sigma_s_2 / sigma_t_2  # Equation 28\n    psi_r = alpha_s * sigma_2_t_s / sigma_t_2\n    std = sigma_2_t_s.sqrt() * sigma_s / sigma_t\n    nonzero_mask = (\n        t &gt; 0\n    ).float()  # based on the time this is always just ones. can leave for now to see if ever want to take extra step and only grab mean\n\n    psi_r = pad_like(psi_r, x_hat)\n    omega_r = pad_like(omega_r, x_hat)\n    std = pad_like(std, x_hat)\n    nonzero_mask = pad_like(nonzero_mask, x_hat)\n\n    mean = psi_r * x_hat + omega_r * xt\n    eps = torch.randn_like(mean).to(model_out.device)\n    x_next = mean + nonzero_mask * std * eps * temperature\n    x_next = self.clean_mask_center(x_next, mask, center)\n    return x_next\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM.step_ddim","title":"<code>step_ddim(model_out, t, xt, dt, mask=None, eta=0.0, center=False)</code>","text":"<p>Do one step of DDIM sampling.</p> <p>From the ddpm equations alpha_bar = alpha2 and  1 - alpha2 = sigma**2</p> <p>Parameters:</p> Name Type Description Default <code>model_out</code> <code>Tensor</code> <p>output of the model</p> required <code>t</code> <code>Tensor</code> <p>current time step</p> required <code>xt</code> <code>Tensor</code> <p>current data point</p> required <code>dt</code> <code>Tensor</code> <p>The time step increment.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>mask for the data point. Defaults to None.</p> <code>None</code> <code>eta</code> <code>Float</code> <p>DDIM sampling parameter. Defaults to 0.0.</p> <code>0.0</code> <code>center</code> <code>Bool</code> <p>whether to center the data point. Defaults to False.</p> <code>False</code> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>def step_ddim(\n    self,\n    model_out: Tensor,\n    t: Tensor,\n    xt: Tensor,\n    dt: Tensor,\n    mask: Optional[Tensor] = None,\n    eta: Float = 0.0,\n    center: Bool = False,\n):\n    \"\"\"Do one step of DDIM sampling.\n\n    From the ddpm equations alpha_bar = alpha**2 and  1 - alpha**2 = sigma**2\n\n    Args:\n        model_out (Tensor): output of the model\n        t (Tensor): current time step\n        xt (Tensor): current data point\n        dt (Tensor): The time step increment.\n        mask (Optional[Tensor], optional): mask for the data point. Defaults to None.\n        eta (Float, optional): DDIM sampling parameter. Defaults to 0.0.\n        center (Bool, optional): whether to center the data point. Defaults to False.\n    \"\"\"\n    if mask is not None:\n        model_out = model_out * mask.unsqueeze(-1)\n    data_pred = self.process_data_prediction(model_out, xt, t)\n    noise_pred = self.process_noise_prediction(model_out, xt, t)\n    eps = torch.randn_like(data_pred).to(model_out.device)\n    log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n    squared_alpha = log_snr.sigmoid()\n    squared_sigma = (-log_snr).sigmoid()\n    log_snr_prev = self.noise_schedule.calculate_log_snr(t - dt, device=t.device)\n    squared_alpha_prev = log_snr_prev.sigmoid()\n    squared_sigma_prev = (-log_snr_prev).sigmoid()\n    sigma_t_2 = squared_sigma_prev / squared_sigma * (1 - squared_alpha / squared_alpha_prev)\n    psi_r = torch.sqrt(squared_alpha_prev)\n    omega_r = torch.sqrt(1 - squared_alpha_prev - eta * eta * sigma_t_2)\n\n    sigma_t_2 = pad_like(sigma_t_2, model_out)\n    psi_r = pad_like(psi_r, model_out)\n    omega_r = pad_like(omega_r, model_out)\n\n    mean = data_pred * psi_r + omega_r * noise_pred\n    x_next = mean + eta * sigma_t_2.sqrt() * eps\n    x_next = self.clean_mask_center(x_next, mask, center)\n    return x_next\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM.step_hybrid_sde","title":"<code>step_hybrid_sde(model_out, t, xt, dt, mask=None, center=False, temperature=1.0, equilibrium_rate=0.0)</code>","text":"<p>Do one step integration of Hybrid Langevin-Reverse Time SDE.</p> <p>See section B.3 page 37 https://www.biorxiv.org/content/10.1101/2022.12.01.518682v1.full.pdf. and https://github.com/generatebio/chroma/blob/929407c605013613941803c6113adefdccaad679/chroma/layers/structure/diffusion.py#L730</p> <p>Parameters:</p> Name Type Description Default <code>model_out</code> <code>Tensor</code> <p>The output of the model.</p> required <code>xt</code> <code>Tensor</code> <p>The current data point.</p> required <code>t</code> <code>Tensor</code> <p>The current time step.</p> required <code>dt</code> <code>Tensor</code> <p>The time step increment.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>An optional mask to apply to the data. Defaults to None.</p> <code>None</code> <code>center</code> <code>bool</code> <p>Whether to center the data. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>Float</code> <p>The temperature parameter for low temperature sampling. Defaults to 1.0.</p> <code>1.0</code> <code>equilibrium_rate</code> <code>Float</code> <p>The rate of Langevin equilibration.  Scales the amount of Langevin dynamics per unit time. Best values are in the range [1.0, 5.0]. Defaults to 0.0.</p> <code>0.0</code> <p>Note: For all step functions that use the SDE formulation its important to note that we are moving backwards in time which corresponds to an apparent sign change. A clear example can be seen in slide 29 https://ernestryu.com/courses/FM/diffusion1.pdf.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>def step_hybrid_sde(\n    self,\n    model_out: Tensor,\n    t: Tensor,\n    xt: Tensor,\n    dt: Tensor,\n    mask: Optional[Tensor] = None,\n    center: Bool = False,\n    temperature: Float = 1.0,\n    equilibrium_rate: Float = 0.0,\n) -&gt; Tensor:\n    \"\"\"Do one step integration of Hybrid Langevin-Reverse Time SDE.\n\n    See section B.3 page 37 https://www.biorxiv.org/content/10.1101/2022.12.01.518682v1.full.pdf.\n    and https://github.com/generatebio/chroma/blob/929407c605013613941803c6113adefdccaad679/chroma/layers/structure/diffusion.py#L730\n\n    Args:\n        model_out (Tensor): The output of the model.\n        xt (Tensor): The current data point.\n        t (Tensor): The current time step.\n        dt (Tensor): The time step increment.\n        mask (Optional[Tensor], optional): An optional mask to apply to the data. Defaults to None.\n        center (bool, optional): Whether to center the data. Defaults to False.\n        temperature (Float, optional): The temperature parameter for low temperature sampling. Defaults to 1.0.\n        equilibrium_rate (Float, optional): The rate of Langevin equilibration.  Scales the amount of Langevin dynamics per unit time. Best values are in the range [1.0, 5.0]. Defaults to 0.0.\n\n    Note:\n    For all step functions that use the SDE formulation its important to note that we are moving backwards in time which corresponds to an apparent sign change.\n    A clear example can be seen in slide 29 https://ernestryu.com/courses/FM/diffusion1.pdf.\n    \"\"\"\n    if mask is not None:\n        model_out = model_out * mask.unsqueeze(-1)\n    x_hat = self.process_data_prediction(model_out, xt, t)\n    log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n    alpha, sigma = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n    # Schedule coeffiecients\n    beta = self.noise_schedule.calculate_beta(t)\n    inverse_temperature = 1 / temperature  # lambda_0\n    langevin_factor = equilibrium_rate\n    # Temperature coefficients\n    lambda_t = (\n        inverse_temperature * (sigma.pow(2) + alpha.pow(2)) / (inverse_temperature * sigma.pow(2) + alpha.pow(2))\n    )\n    # langevin_isothermal = True\n    lambda_langevin = inverse_temperature  # if langevin_isothermal else lambda_t\n\n    score_scale_t = lambda_t + lambda_langevin * langevin_factor / 2.0\n\n    eps = torch.randn_like(x_hat).to(model_out.device)\n    score = self.score(x_hat, xt, t)\n    beta = pad_like(beta, model_out)\n    score_scale_t = pad_like(score_scale_t, model_out)\n\n    gT = beta * ((-1 / 2) * xt - score_scale_t * score)\n    gW = torch.sqrt((1.0 + langevin_factor) * beta.abs()) * eps\n\n    x_next = xt + dt * gT + dt.sqrt() * gW\n    x_next = self.clean_mask_center(x_next, mask, center)\n    return x_next\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM.step_ode","title":"<code>step_ode(model_out, t, xt, dt, mask=None, center=False, temperature=1.0)</code>","text":"<p>Do one step integration of ODE.</p> <p>See section B page 36 https://www.biorxiv.org/content/10.1101/2022.12.01.518682v1.full.pdf. and https://github.com/generatebio/chroma/blob/929407c605013613941803c6113adefdccaad679/chroma/layers/structure/diffusion.py#L730</p> <p>Parameters:</p> Name Type Description Default <code>model_out</code> <code>Tensor</code> <p>The output of the model.</p> required <code>xt</code> <code>Tensor</code> <p>The current data point.</p> required <code>t</code> <code>Tensor</code> <p>The current time step.</p> required <code>dt</code> <code>Tensor</code> <p>The time step increment.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>An optional mask to apply to the data. Defaults to None.</p> <code>None</code> <code>center</code> <code>bool</code> <p>Whether to center the data. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>Float</code> <p>The temperature parameter for low temperature sampling. Defaults to 1.0.</p> <code>1.0</code> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>def step_ode(\n    self,\n    model_out: Tensor,\n    t: Tensor,\n    xt: Tensor,\n    dt: Tensor,\n    mask: Optional[Tensor] = None,\n    center: Bool = False,\n    temperature: Float = 1.0,\n) -&gt; Tensor:\n    \"\"\"Do one step integration of ODE.\n\n    See section B page 36 https://www.biorxiv.org/content/10.1101/2022.12.01.518682v1.full.pdf.\n    and https://github.com/generatebio/chroma/blob/929407c605013613941803c6113adefdccaad679/chroma/layers/structure/diffusion.py#L730\n\n    Args:\n        model_out (Tensor): The output of the model.\n        xt (Tensor): The current data point.\n        t (Tensor): The current time step.\n        dt (Tensor): The time step increment.\n        mask (Optional[Tensor], optional): An optional mask to apply to the data. Defaults to None.\n        center (bool, optional): Whether to center the data. Defaults to False.\n        temperature (Float, optional): The temperature parameter for low temperature sampling. Defaults to 1.0.\n    \"\"\"\n    if mask is not None:\n        model_out = model_out * mask.unsqueeze(-1)\n    x_hat = self.process_data_prediction(model_out, xt, t)\n    log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n    alpha, sigma = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n    # Schedule coeffiecients\n    beta = self.noise_schedule.calculate_beta(t)\n    inverse_temperature = 1 / temperature\n    # Temperature coefficients\n    lambda_t = (\n        inverse_temperature * (sigma.pow(2) + alpha.pow(2)) / (inverse_temperature * sigma.pow(2) + alpha.pow(2))\n    )\n\n    score = self.score(x_hat, xt, t)\n    beta = pad_like(beta, model_out)\n    lambda_t = pad_like(lambda_t, model_out)\n\n    gT = (-1 / 2) * beta * (xt + lambda_t * score)\n\n    x_next = xt + gT * dt\n    x_next = self.clean_mask_center(x_next, mask, center)\n    return x_next\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/augmentation_types/","title":"Augmentation types","text":""},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/augmentation_types/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.augmentation_types.AugmentationType","title":"<code>AugmentationType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>An enumeration representing the type ofOptimal Transport that can be used in Continuous Flow Matching.</p> <ul> <li>EXACT_OT: Standard mini batch optimal transport defined in  https://arxiv.org/pdf/2302.00482.</li> <li>EQUIVARIANT_OT: Adding roto/translation optimization to mini batch OT see https://arxiv.org/pdf/2306.15030  https://arxiv.org/pdf/2312.07168 4.2.</li> <li>KABSCH: Simple Kabsch alignment between each data and noise point, No permuation # https://arxiv.org/pdf/2410.22388 Sec 3.2</li> </ul> <p>These prediction types can be used to train neural networks for specific tasks, such as denoising, image synthesis, or time-series forecasting.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/augmentation_types.py</code> <pre><code>class AugmentationType(Enum):\n    \"\"\"An enumeration representing the type ofOptimal Transport that can be used in Continuous Flow Matching.\n\n    - **EXACT_OT**: Standard mini batch optimal transport defined in  https://arxiv.org/pdf/2302.00482.\n    - **EQUIVARIANT_OT**: Adding roto/translation optimization to mini batch OT see https://arxiv.org/pdf/2306.15030  https://arxiv.org/pdf/2312.07168 4.2.\n    - **KABSCH**: Simple Kabsch alignment between each data and noise point, No permuation # https://arxiv.org/pdf/2410.22388 Sec 3.2\n\n    These prediction types can be used to train neural networks for specific tasks, such as denoising, image synthesis, or time-series forecasting.\n    \"\"\"\n\n    EXACT_OT = \"exact_ot\"\n    EQUIVARIANT_OT = \"equivariant_ot\"\n    KABSCH = \"kabsch\"\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler/","title":"Equivariant ot sampler","text":""},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.equivariant_ot_sampler.EquivariantOTSampler","title":"<code>EquivariantOTSampler</code>","text":"<p>Sampler for Mini-batch Optimal Transport Plan with cost calculated after Kabsch alignment.</p> <p>EquivariantOTSampler implements sampling coordinates according to an OT plan (wrt squared Euclidean cost after Kabsch alignment) with different implementations of the plan calculation.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler.py</code> <pre><code>class EquivariantOTSampler:\n    \"\"\"Sampler for Mini-batch Optimal Transport Plan with cost calculated after Kabsch alignment.\n\n    EquivariantOTSampler implements sampling coordinates according to an OT plan\n    (wrt squared Euclidean cost after Kabsch alignment) with different implementations of the plan calculation.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        method: str = \"exact\",\n        device: Union[str, torch.device] = \"cpu\",\n        num_threads: int = 1,\n    ) -&gt; None:\n        \"\"\"Initialize the OTSampler class.\n\n        Args:\n            method (str): Choose which optimal transport solver you would like to use. Currently only support exact OT solvers (pot.emd).\n            device (Union[str, torch.device], optional): The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".\n            num_threads (Union[int, str], optional): Number of threads to use for OT solver. If \"max\", uses the maximum number of threads. Default is 1.\n\n        Raises:\n            ValueError: If the OT solver is not documented.\n            NotImplementedError: If the OT solver is not implemented.\n        \"\"\"\n        # ot_fn should take (a, b, M) as arguments where a, b are marginals and\n        # M is a cost matrix\n        if method == \"exact\":\n            self.ot_fn: Callable[..., torch.Tensor] = partial(pot.emd, numThreads=num_threads)  # type: ignore\n        elif method in {\"sinkhorn\", \"unbalanced\", \"partial\"}:\n            raise NotImplementedError(\"OT solver other than 'exact' is not implemented.\")\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n        self.device = device\n\n    def to_device(self, device: str):\n        \"\"\"Moves all internal tensors to the specified device and updates the `self.device` attribute.\n\n        Args:\n            device (str): The device to move the tensors to (e.g. \"cpu\", \"cuda:0\").\n\n        Note:\n            This method is used to transfer the internal state of the OTSampler to a different device.\n            It updates the `self.device` attribute to reflect the new device and moves all internal tensors to the specified device.\n        \"\"\"\n        self.device = device\n        for attr_name in dir(self):\n            if attr_name.startswith(\"_\") and isinstance(getattr(self, attr_name), torch.Tensor):\n                setattr(self, attr_name, getattr(self, attr_name).to(device))\n        return self\n\n    def sample_map(self, pi: Tensor, batch_size: int, replace: Bool = False) -&gt; Tuple[Tensor, Tensor]:\n        r\"\"\"Draw source and target samples from pi $(x,z) \\sim \\pi$.\n\n        Args:\n            pi (Tensor): shape (bs, bs), the OT matrix between noise and data in minibatch.\n            batch_size (int): The batch size of the minibatch.\n            replace (bool): sampling w/ or w/o replacement from the OT plan, default to False.\n\n        Returns:\n            Tuple: tuple of 2 tensors, represents the indices of noise and data samples from pi.\n        \"\"\"\n        if pi.shape[0] != batch_size or pi.shape[1] != batch_size:\n            raise ValueError(\"Shape mismatch: pi.shape = {}, batch_size = {}\".format(pi.shape, batch_size))\n        p = pi.flatten()\n        p = p / p.sum()\n        choices = torch.multinomial(p, batch_size, replacement=replace)\n        return torch.div(choices, pi.shape[1], rounding_mode=\"floor\"), choices % pi.shape[1]\n\n    def kabsch_align(self, target: Tensor, noise: Tensor) -&gt; Tensor:\n        \"\"\"Find the Rotation matrix (R) such that RMSD is minimized between target @ R.T and noise.\n\n        Args:\n            target (Tensor): shape (N, *dim), data from source minibatch.\n            noise (Tensor): shape (N, *dim), noise from source minibatch.\n\n        Returns:\n            R (Tensor): shape (*dim, *dim), the rotation matrix.\n        \"\"\"\n        dimension = target.shape[-1]\n        noise_centered = noise - noise.mean(dim=0)\n        target_centered = target - target.mean(dim=0)\n\n        # Compute the covariance matrix\n        covariance_matix = target_centered.T @ noise_centered\n\n        # Compute the SVD of the covariance matrix\n        U, S, Vt = torch.linalg.svd(covariance_matix)\n        d = torch.sign(torch.linalg.det(Vt.T @ U.T)).item()\n        d_mat = torch.tensor([1] * (dimension - 1) + [d], device=Vt.device, dtype=Vt.dtype)\n        R = Vt.T @ torch.diag(d_mat) @ U.T\n        return R\n\n    def _calculate_cost_matrix(self, x0: Tensor, x1: Tensor, mask: Optional[Tensor] = None) -&gt; Tuple[Tensor, Tensor]:\n        \"\"\"Compute the cost matrix between a source and a target minibatch.\n\n        The distance between noise and data is calculated after aligning them using Kabsch algorithm.\n\n        Args:\n            x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n            x1 (Tensor): shape (bs, *dim), data from source minibatch.\n            mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n\n        Returns:\n            M: shape (bs, bs), the cost matrix between noise and data in minibatch.\n            Rs: shape (bs, bs, *dim, *dim), the rotation matrix between noise and data in minibatch.\n        \"\"\"\n        if x0.shape[0] != x1.shape[0]:\n            raise ValueError(\"Shape mismatch: x0.shape = {}, x1.shape = {}\".format(x0.shape, x1.shape))\n        batchsize, maxlen, dimension = x0.shape[0], x0.shape[1], x0.shape[-1]\n        M = torch.zeros(batchsize, batchsize, device=x0.device)\n        Rs = torch.zeros(batchsize, batchsize, dimension, dimension, device=x0.device)\n        for i in range(batchsize):\n            for j in range(batchsize):\n                if mask is not None:\n                    x0i_mask = mask[i].bool()\n                else:\n                    x0i_mask = torch.ones(maxlen, device=x0.device).bool()\n                x0_masked, x1_masked = x0[i][x0i_mask], x1[j][x0i_mask]\n                # Rotate the data to align with the noise\n                R = self.kabsch_align(x1_masked, x0_masked)\n                x1_aligned = x1_masked @ R.T\n                # Here the cost only considered the rotational RMSD, not the translational RMSD\n                cost = torch.dist(x0_masked - x0_masked.mean(dim=0), x1_aligned - x1_aligned.mean(dim=0), p=2)\n                M[i, j] = cost\n                Rs[i, j] = R.T\n\n        return M, Rs\n\n    def get_ot_matrix(self, x0: Tensor, x1: Tensor, mask: Optional[Tensor] = None) -&gt; Tuple[Tensor, Tensor]:\n        \"\"\"Compute the OT matrix between a source and a target minibatch.\n\n        Args:\n            x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n            x1 (Tensor): shape (bs, *dim), data from source minibatch.\n            mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n\n        Returns:\n            p (Tensor): shape (bs, bs), the OT matrix between noise and data in minibatch.\n            Rs (Tensor): shape (bs, bs, *dim, *dim), the rotation matrix between noise and data in minibatch.\n        \"\"\"\n        # Compute the cost matrix\n        M, Rs = self._calculate_cost_matrix(x0, x1, mask)\n\n        # Set uniform weights for all samples in a minibatch\n        a, b = pot.unif(x0.shape[0], type_as=M), pot.unif(x1.shape[0], type_as=M)\n\n        # Compute the OT matrix using POT package\n        p = self.ot_fn(a, b, M)\n\n        # Handle Exceptions\n        if not torch.all(torch.isfinite(p)):\n            raise ValueError(\"OT plan map is not finite, cost mean, max: {}, {}\".format(M.mean(), M.max()))\n        if torch.abs(p.sum()) &lt; 1e-8:\n            warnings.warn(\"Numerical errors in OT matrix, reverting to uniform plan.\")\n            p = torch.ones_like(p) / p.numel()\n\n        return p, Rs\n\n    def apply_augmentation(\n        self,\n        x0: Tensor,\n        x1: Tensor,\n        mask: Optional[Tensor] = None,\n        replace: Bool = False,\n        sort: Optional[Literal[\"noise\", \"x0\", \"data\", \"x1\"]] = \"x0\",\n    ) -&gt; Tuple[Tensor, Tensor, Optional[Tensor]]:\n        r\"\"\"Sample indices for noise and data in minibatch according to OT plan.\n\n        Compute the OT plan $\\pi$ (wrt squared Euclidean cost after Kabsch alignment) between a source and a target\n        minibatch and draw source and target samples from pi $(x,z) \\sim \\pi$.\n\n        Args:\n            x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n            x1 (Tensor): shape (bs, *dim), data from source minibatch.\n            mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n            replace (bool): sampling w/ or w/o replacement from the OT plan, default to False.\n            sort (str): Optional Literal string to sort either x1 or x0 based on the input.\n\n        Returns:\n            Tuple: tuple of 2 tensors, represents the noise and data samples following OT plan pi.\n        \"\"\"\n        # Calculate the optimal transport\n        pi, Rs = self.get_ot_matrix(x0, x1, mask)\n\n        # Sample (x0, x1) mapping indices from the OT matrix\n        i, j = self.sample_map(pi, x0.shape[0], replace=replace)\n\n        if not replace and (sort == \"noise\" or sort == \"x0\"):\n            sort_idx = torch.argsort(i)\n            i = i[sort_idx]\n            j = j[sort_idx]\n\n            if not (i == torch.arange(x0.shape[0], device=i.device)).all():\n                raise ValueError(\"x0_idx should be a tensor from 0 to size - 1 when sort is 'noise' or 'x0\")\n        elif not replace and (sort == \"data\" or sort == \"x1\"):\n            sort_idx = torch.argsort(j)\n            i = i[sort_idx]\n            j = j[sort_idx]\n\n            if not (j == torch.arange(x1.shape[0], device=j.device)).all():\n                raise ValueError(\"x1_idx should be a tensor from 0 to size - 1 when sort is 'noise' or 'x0\")\n\n        # Get the corresponding rotation matrices\n        rotations = Rs[i, j, :, :]\n        noise = x0[i]\n        # Align the data samples using the rotation matrices\n        x1_aligned = torch.bmm(x1[j], rotations)\n        # Returns the true data that has been permuated and rotated. Translations are done either in preprocessing or after the fact.\n        data = x1_aligned\n\n        if mask is not None:\n            if mask.device != x0.device:\n                mask = mask.to(x0.device)\n            mask = mask[i]\n        # Output the permuted samples in the minibatch\n        return noise, data, mask\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.equivariant_ot_sampler.EquivariantOTSampler.__init__","title":"<code>__init__(method='exact', device='cpu', num_threads=1)</code>","text":"<p>Initialize the OTSampler class.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>Choose which optimal transport solver you would like to use. Currently only support exact OT solvers (pot.emd).</p> <code>'exact'</code> <code>device</code> <code>Union[str, device]</code> <p>The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".</p> <code>'cpu'</code> <code>num_threads</code> <code>Union[int, str]</code> <p>Number of threads to use for OT solver. If \"max\", uses the maximum number of threads. Default is 1.</p> <code>1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the OT solver is not documented.</p> <code>NotImplementedError</code> <p>If the OT solver is not implemented.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler.py</code> <pre><code>def __init__(\n    self,\n    method: str = \"exact\",\n    device: Union[str, torch.device] = \"cpu\",\n    num_threads: int = 1,\n) -&gt; None:\n    \"\"\"Initialize the OTSampler class.\n\n    Args:\n        method (str): Choose which optimal transport solver you would like to use. Currently only support exact OT solvers (pot.emd).\n        device (Union[str, torch.device], optional): The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".\n        num_threads (Union[int, str], optional): Number of threads to use for OT solver. If \"max\", uses the maximum number of threads. Default is 1.\n\n    Raises:\n        ValueError: If the OT solver is not documented.\n        NotImplementedError: If the OT solver is not implemented.\n    \"\"\"\n    # ot_fn should take (a, b, M) as arguments where a, b are marginals and\n    # M is a cost matrix\n    if method == \"exact\":\n        self.ot_fn: Callable[..., torch.Tensor] = partial(pot.emd, numThreads=num_threads)  # type: ignore\n    elif method in {\"sinkhorn\", \"unbalanced\", \"partial\"}:\n        raise NotImplementedError(\"OT solver other than 'exact' is not implemented.\")\n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n    self.device = device\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.equivariant_ot_sampler.EquivariantOTSampler.apply_augmentation","title":"<code>apply_augmentation(x0, x1, mask=None, replace=False, sort='x0')</code>","text":"<p>Sample indices for noise and data in minibatch according to OT plan.</p> <p>Compute the OT plan $\\pi$ (wrt squared Euclidean cost after Kabsch alignment) between a source and a target minibatch and draw source and target samples from pi $(x,z) \\sim \\pi$.</p> <p>Parameters:</p> Name Type Description Default <code>x0</code> <code>Tensor</code> <p>shape (bs, *dim), noise from source minibatch.</p> required <code>x1</code> <code>Tensor</code> <p>shape (bs, *dim), data from source minibatch.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.</p> <code>None</code> <code>replace</code> <code>bool</code> <p>sampling w/ or w/o replacement from the OT plan, default to False.</p> <code>False</code> <code>sort</code> <code>str</code> <p>Optional Literal string to sort either x1 or x0 based on the input.</p> <code>'x0'</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple[Tensor, Tensor, Optional[Tensor]]</code> <p>tuple of 2 tensors, represents the noise and data samples following OT plan pi.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler.py</code> <pre><code>def apply_augmentation(\n    self,\n    x0: Tensor,\n    x1: Tensor,\n    mask: Optional[Tensor] = None,\n    replace: Bool = False,\n    sort: Optional[Literal[\"noise\", \"x0\", \"data\", \"x1\"]] = \"x0\",\n) -&gt; Tuple[Tensor, Tensor, Optional[Tensor]]:\n    r\"\"\"Sample indices for noise and data in minibatch according to OT plan.\n\n    Compute the OT plan $\\pi$ (wrt squared Euclidean cost after Kabsch alignment) between a source and a target\n    minibatch and draw source and target samples from pi $(x,z) \\sim \\pi$.\n\n    Args:\n        x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n        x1 (Tensor): shape (bs, *dim), data from source minibatch.\n        mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n        replace (bool): sampling w/ or w/o replacement from the OT plan, default to False.\n        sort (str): Optional Literal string to sort either x1 or x0 based on the input.\n\n    Returns:\n        Tuple: tuple of 2 tensors, represents the noise and data samples following OT plan pi.\n    \"\"\"\n    # Calculate the optimal transport\n    pi, Rs = self.get_ot_matrix(x0, x1, mask)\n\n    # Sample (x0, x1) mapping indices from the OT matrix\n    i, j = self.sample_map(pi, x0.shape[0], replace=replace)\n\n    if not replace and (sort == \"noise\" or sort == \"x0\"):\n        sort_idx = torch.argsort(i)\n        i = i[sort_idx]\n        j = j[sort_idx]\n\n        if not (i == torch.arange(x0.shape[0], device=i.device)).all():\n            raise ValueError(\"x0_idx should be a tensor from 0 to size - 1 when sort is 'noise' or 'x0\")\n    elif not replace and (sort == \"data\" or sort == \"x1\"):\n        sort_idx = torch.argsort(j)\n        i = i[sort_idx]\n        j = j[sort_idx]\n\n        if not (j == torch.arange(x1.shape[0], device=j.device)).all():\n            raise ValueError(\"x1_idx should be a tensor from 0 to size - 1 when sort is 'noise' or 'x0\")\n\n    # Get the corresponding rotation matrices\n    rotations = Rs[i, j, :, :]\n    noise = x0[i]\n    # Align the data samples using the rotation matrices\n    x1_aligned = torch.bmm(x1[j], rotations)\n    # Returns the true data that has been permuated and rotated. Translations are done either in preprocessing or after the fact.\n    data = x1_aligned\n\n    if mask is not None:\n        if mask.device != x0.device:\n            mask = mask.to(x0.device)\n        mask = mask[i]\n    # Output the permuted samples in the minibatch\n    return noise, data, mask\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.equivariant_ot_sampler.EquivariantOTSampler.get_ot_matrix","title":"<code>get_ot_matrix(x0, x1, mask=None)</code>","text":"<p>Compute the OT matrix between a source and a target minibatch.</p> <p>Parameters:</p> Name Type Description Default <code>x0</code> <code>Tensor</code> <p>shape (bs, *dim), noise from source minibatch.</p> required <code>x1</code> <code>Tensor</code> <p>shape (bs, *dim), data from source minibatch.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>p</code> <code>Tensor</code> <p>shape (bs, bs), the OT matrix between noise and data in minibatch.</p> <code>Rs</code> <code>Tensor</code> <p>shape (bs, bs, dim, dim), the rotation matrix between noise and data in minibatch.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler.py</code> <pre><code>def get_ot_matrix(self, x0: Tensor, x1: Tensor, mask: Optional[Tensor] = None) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"Compute the OT matrix between a source and a target minibatch.\n\n    Args:\n        x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n        x1 (Tensor): shape (bs, *dim), data from source minibatch.\n        mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n\n    Returns:\n        p (Tensor): shape (bs, bs), the OT matrix between noise and data in minibatch.\n        Rs (Tensor): shape (bs, bs, *dim, *dim), the rotation matrix between noise and data in minibatch.\n    \"\"\"\n    # Compute the cost matrix\n    M, Rs = self._calculate_cost_matrix(x0, x1, mask)\n\n    # Set uniform weights for all samples in a minibatch\n    a, b = pot.unif(x0.shape[0], type_as=M), pot.unif(x1.shape[0], type_as=M)\n\n    # Compute the OT matrix using POT package\n    p = self.ot_fn(a, b, M)\n\n    # Handle Exceptions\n    if not torch.all(torch.isfinite(p)):\n        raise ValueError(\"OT plan map is not finite, cost mean, max: {}, {}\".format(M.mean(), M.max()))\n    if torch.abs(p.sum()) &lt; 1e-8:\n        warnings.warn(\"Numerical errors in OT matrix, reverting to uniform plan.\")\n        p = torch.ones_like(p) / p.numel()\n\n    return p, Rs\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.equivariant_ot_sampler.EquivariantOTSampler.kabsch_align","title":"<code>kabsch_align(target, noise)</code>","text":"<p>Find the Rotation matrix (R) such that RMSD is minimized between target @ R.T and noise.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Tensor</code> <p>shape (N, *dim), data from source minibatch.</p> required <code>noise</code> <code>Tensor</code> <p>shape (N, *dim), noise from source minibatch.</p> required <p>Returns:</p> Name Type Description <code>R</code> <code>Tensor</code> <p>shape (dim, dim), the rotation matrix.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler.py</code> <pre><code>def kabsch_align(self, target: Tensor, noise: Tensor) -&gt; Tensor:\n    \"\"\"Find the Rotation matrix (R) such that RMSD is minimized between target @ R.T and noise.\n\n    Args:\n        target (Tensor): shape (N, *dim), data from source minibatch.\n        noise (Tensor): shape (N, *dim), noise from source minibatch.\n\n    Returns:\n        R (Tensor): shape (*dim, *dim), the rotation matrix.\n    \"\"\"\n    dimension = target.shape[-1]\n    noise_centered = noise - noise.mean(dim=0)\n    target_centered = target - target.mean(dim=0)\n\n    # Compute the covariance matrix\n    covariance_matix = target_centered.T @ noise_centered\n\n    # Compute the SVD of the covariance matrix\n    U, S, Vt = torch.linalg.svd(covariance_matix)\n    d = torch.sign(torch.linalg.det(Vt.T @ U.T)).item()\n    d_mat = torch.tensor([1] * (dimension - 1) + [d], device=Vt.device, dtype=Vt.dtype)\n    R = Vt.T @ torch.diag(d_mat) @ U.T\n    return R\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.equivariant_ot_sampler.EquivariantOTSampler.sample_map","title":"<code>sample_map(pi, batch_size, replace=False)</code>","text":"<p>Draw source and target samples from pi $(x,z) \\sim \\pi$.</p> <p>Parameters:</p> Name Type Description Default <code>pi</code> <code>Tensor</code> <p>shape (bs, bs), the OT matrix between noise and data in minibatch.</p> required <code>batch_size</code> <code>int</code> <p>The batch size of the minibatch.</p> required <code>replace</code> <code>bool</code> <p>sampling w/ or w/o replacement from the OT plan, default to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple[Tensor, Tensor]</code> <p>tuple of 2 tensors, represents the indices of noise and data samples from pi.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler.py</code> <pre><code>def sample_map(self, pi: Tensor, batch_size: int, replace: Bool = False) -&gt; Tuple[Tensor, Tensor]:\n    r\"\"\"Draw source and target samples from pi $(x,z) \\sim \\pi$.\n\n    Args:\n        pi (Tensor): shape (bs, bs), the OT matrix between noise and data in minibatch.\n        batch_size (int): The batch size of the minibatch.\n        replace (bool): sampling w/ or w/o replacement from the OT plan, default to False.\n\n    Returns:\n        Tuple: tuple of 2 tensors, represents the indices of noise and data samples from pi.\n    \"\"\"\n    if pi.shape[0] != batch_size or pi.shape[1] != batch_size:\n        raise ValueError(\"Shape mismatch: pi.shape = {}, batch_size = {}\".format(pi.shape, batch_size))\n    p = pi.flatten()\n    p = p / p.sum()\n    choices = torch.multinomial(p, batch_size, replacement=replace)\n    return torch.div(choices, pi.shape[1], rounding_mode=\"floor\"), choices % pi.shape[1]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.equivariant_ot_sampler.EquivariantOTSampler.to_device","title":"<code>to_device(device)</code>","text":"<p>Moves all internal tensors to the specified device and updates the <code>self.device</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>The device to move the tensors to (e.g. \"cpu\", \"cuda:0\").</p> required Note <p>This method is used to transfer the internal state of the OTSampler to a different device. It updates the <code>self.device</code> attribute to reflect the new device and moves all internal tensors to the specified device.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler.py</code> <pre><code>def to_device(self, device: str):\n    \"\"\"Moves all internal tensors to the specified device and updates the `self.device` attribute.\n\n    Args:\n        device (str): The device to move the tensors to (e.g. \"cpu\", \"cuda:0\").\n\n    Note:\n        This method is used to transfer the internal state of the OTSampler to a different device.\n        It updates the `self.device` attribute to reflect the new device and moves all internal tensors to the specified device.\n    \"\"\"\n    self.device = device\n    for attr_name in dir(self):\n        if attr_name.startswith(\"_\") and isinstance(getattr(self, attr_name), torch.Tensor):\n            setattr(self, attr_name, getattr(self, attr_name).to(device))\n    return self\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/kabsch_augmentation/","title":"Kabsch augmentation","text":""},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/kabsch_augmentation/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.kabsch_augmentation.KabschAugmentation","title":"<code>KabschAugmentation</code>","text":"<p>Point-wise Kabsch alignment.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/kabsch_augmentation.py</code> <pre><code>class KabschAugmentation:\n    \"\"\"Point-wise Kabsch alignment.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the KabschAugmentation instance.\n\n        Notes:\n            - This implementation assumes no required initialization arguments.\n            - You can add instance variables (e.g., `self.variable_name`) as needed.\n        \"\"\"\n        pass  # No operations are performed when initializing with no args\n\n    def kabsch_align(self, target: Tensor, noise: Tensor):\n        \"\"\"Find the Rotation matrix (R) such that RMSD is minimized between target @ R.T and noise.\n\n        Args:\n            target (Tensor): shape (N, *dim), data from source minibatch.\n            noise (Tensor): shape (N, *dim), noise from source minibatch.\n\n        Returns:\n            R (Tensor): shape (*dim, *dim), the rotation matrix.\n            Aliged Target (Tensor): target tensor rotated and shifted to reduced RMSD with noise\n        \"\"\"\n        dimension = target.shape[-1]\n        noise_translation = noise.mean(dim=0)\n        noise_centered = noise - noise_translation\n        target_centered = target - target.mean(dim=0)\n\n        # Compute the covariance matrix\n        covariance_matix = target_centered.T @ noise_centered\n\n        # Compute the SVD of the covariance matrix\n        U, S, Vt = torch.linalg.svd(covariance_matix)\n        d = torch.sign(torch.linalg.det(Vt.T @ U.T)).item()\n        d_mat = torch.tensor([1] * (dimension - 1) + [d], device=Vt.device, dtype=Vt.dtype)\n        R = Vt.T @ torch.diag(d_mat) @ U.T\n\n        target_aligned = target_centered @ R.T + noise_translation\n\n        return R, target_aligned\n\n    def batch_kabsch_align(self, target: Tensor, noise: Tensor):\n        \"\"\"Find the Rotation matrix (R) such that RMSD is minimized between target @ R.T and noise.\n\n        Args:\n            target (Tensor): shape (B, N, *dim), data from source minibatch.\n            noise (Tensor): shape (B, N, *dim), noise from source minibatch.\n\n        Returns:\n            R (Tensor): shape (*dim, *dim), the rotation matrix.\n            Aliged Target (Tensor): target tensor rotated and shifted to reduced RMSD with noise\n        \"\"\"\n        # Corrected Batched Kabsch Alignment\n        batch_size, _, dimension = target.shape\n\n        # Center the target and noise tensors along the middle dimension (N) for each batch item\n        noise_translation = noise.mean(dim=1, keepdim=True)\n        noise_centered = noise - noise_translation\n        target_centered = target - target.mean(dim=1, keepdim=True)\n\n        # Compute the covariance matrix for each batch item\n        covariance_matrix = torch.matmul(target_centered.transpose(1, 2), noise_centered)\n\n        # Compute the SVD of the covariance matrix for each batch item\n        U, S, Vt = torch.linalg.svd(covariance_matrix)\n\n        # Adjust for proper rotation (determinant=1) for each batch item\n        d = torch.sign(torch.linalg.det(Vt @ U.transpose(-1, -2)))  # Keep as tensor for batch operations\n        d_mat = torch.diag_embed(\n            torch.cat(\n                [torch.ones(batch_size, dimension - 1, device=Vt.device, dtype=Vt.dtype), d.unsqueeze(-1)], dim=-1\n            )\n        )\n\n        R_batch = torch.matmul(torch.matmul(Vt.transpose(-1, -2), d_mat), U.transpose(-1, -2))\n\n        target_aligned = target_centered @ R_batch.transpose(-1, -2) + noise_translation\n        return R_batch, target_aligned\n\n    def apply_augmentation(\n        self,\n        x0: Tensor,\n        x1: Tensor,\n        mask: Optional[Tensor] = None,\n        align_noise_to_data=True,\n    ) -&gt; Tuple[Tensor, Tensor]:\n        r\"\"\"Sample indices for noise and data in minibatch according to OT plan.\n\n        Compute the OT plan $\\pi$ (wrt squared Euclidean cost after Kabsch alignment) between a source and a target\n        minibatch and draw source and target samples from pi $(x,z) \\sim \\pi$.\n\n        Args:\n            x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n            x1 (Tensor): shape (bs, *dim), data from source minibatch.\n            mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n            replace (bool): sampling w/ or w/o replacement from the OT plan, default to False.\n            align_noise_to_data (bool): Direction of alignment default is True meaning it augments Noise to reduce error to Data.\n\n        Returns:\n            Tuple: tuple of 2 tensors, represents the noise and data samples following OT plan pi.\n        \"\"\"\n        if x1.ndim &gt; 2:\n            align_func = self.batch_kabsch_align\n        else:\n            align_func = self.kabsch_align\n        if mask is not None:\n            mask = pad_like(mask, x1)\n            x1 = x1 * mask\n            x0 = x0 * mask\n        if align_noise_to_data:\n            # Compute the rotation matrix R that aligns x0 to x1\n            R, aligned_x0 = align_func(x0, x1)\n            noise = aligned_x0\n            data = x1\n        else:\n            # Compute the rotation matrix R that aligns x1 to x0\n            R, aligned_x1 = align_func(x1, x0)\n            noise = x0\n            data = aligned_x1\n        if mask is not None:\n            noise = noise * mask\n            data = data * mask\n        # Output the permuted samples in the minibatch\n        return noise, data\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/kabsch_augmentation/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.kabsch_augmentation.KabschAugmentation.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the KabschAugmentation instance.</p> Notes <ul> <li>This implementation assumes no required initialization arguments.</li> <li>You can add instance variables (e.g., <code>self.variable_name</code>) as needed.</li> </ul> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/kabsch_augmentation.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the KabschAugmentation instance.\n\n    Notes:\n        - This implementation assumes no required initialization arguments.\n        - You can add instance variables (e.g., `self.variable_name`) as needed.\n    \"\"\"\n    pass  # No operations are performed when initializing with no args\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/kabsch_augmentation/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.kabsch_augmentation.KabschAugmentation.apply_augmentation","title":"<code>apply_augmentation(x0, x1, mask=None, align_noise_to_data=True)</code>","text":"<p>Sample indices for noise and data in minibatch according to OT plan.</p> <p>Compute the OT plan $\\pi$ (wrt squared Euclidean cost after Kabsch alignment) between a source and a target minibatch and draw source and target samples from pi $(x,z) \\sim \\pi$.</p> <p>Parameters:</p> Name Type Description Default <code>x0</code> <code>Tensor</code> <p>shape (bs, *dim), noise from source minibatch.</p> required <code>x1</code> <code>Tensor</code> <p>shape (bs, *dim), data from source minibatch.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.</p> <code>None</code> <code>replace</code> <code>bool</code> <p>sampling w/ or w/o replacement from the OT plan, default to False.</p> required <code>align_noise_to_data</code> <code>bool</code> <p>Direction of alignment default is True meaning it augments Noise to reduce error to Data.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple[Tensor, Tensor]</code> <p>tuple of 2 tensors, represents the noise and data samples following OT plan pi.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/kabsch_augmentation.py</code> <pre><code>def apply_augmentation(\n    self,\n    x0: Tensor,\n    x1: Tensor,\n    mask: Optional[Tensor] = None,\n    align_noise_to_data=True,\n) -&gt; Tuple[Tensor, Tensor]:\n    r\"\"\"Sample indices for noise and data in minibatch according to OT plan.\n\n    Compute the OT plan $\\pi$ (wrt squared Euclidean cost after Kabsch alignment) between a source and a target\n    minibatch and draw source and target samples from pi $(x,z) \\sim \\pi$.\n\n    Args:\n        x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n        x1 (Tensor): shape (bs, *dim), data from source minibatch.\n        mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n        replace (bool): sampling w/ or w/o replacement from the OT plan, default to False.\n        align_noise_to_data (bool): Direction of alignment default is True meaning it augments Noise to reduce error to Data.\n\n    Returns:\n        Tuple: tuple of 2 tensors, represents the noise and data samples following OT plan pi.\n    \"\"\"\n    if x1.ndim &gt; 2:\n        align_func = self.batch_kabsch_align\n    else:\n        align_func = self.kabsch_align\n    if mask is not None:\n        mask = pad_like(mask, x1)\n        x1 = x1 * mask\n        x0 = x0 * mask\n    if align_noise_to_data:\n        # Compute the rotation matrix R that aligns x0 to x1\n        R, aligned_x0 = align_func(x0, x1)\n        noise = aligned_x0\n        data = x1\n    else:\n        # Compute the rotation matrix R that aligns x1 to x0\n        R, aligned_x1 = align_func(x1, x0)\n        noise = x0\n        data = aligned_x1\n    if mask is not None:\n        noise = noise * mask\n        data = data * mask\n    # Output the permuted samples in the minibatch\n    return noise, data\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/kabsch_augmentation/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.kabsch_augmentation.KabschAugmentation.batch_kabsch_align","title":"<code>batch_kabsch_align(target, noise)</code>","text":"<p>Find the Rotation matrix (R) such that RMSD is minimized between target @ R.T and noise.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Tensor</code> <p>shape (B, N, *dim), data from source minibatch.</p> required <code>noise</code> <code>Tensor</code> <p>shape (B, N, *dim), noise from source minibatch.</p> required <p>Returns:</p> Name Type Description <code>R</code> <code>Tensor</code> <p>shape (dim, dim), the rotation matrix.</p> <p>Aliged Target (Tensor): target tensor rotated and shifted to reduced RMSD with noise</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/kabsch_augmentation.py</code> <pre><code>def batch_kabsch_align(self, target: Tensor, noise: Tensor):\n    \"\"\"Find the Rotation matrix (R) such that RMSD is minimized between target @ R.T and noise.\n\n    Args:\n        target (Tensor): shape (B, N, *dim), data from source minibatch.\n        noise (Tensor): shape (B, N, *dim), noise from source minibatch.\n\n    Returns:\n        R (Tensor): shape (*dim, *dim), the rotation matrix.\n        Aliged Target (Tensor): target tensor rotated and shifted to reduced RMSD with noise\n    \"\"\"\n    # Corrected Batched Kabsch Alignment\n    batch_size, _, dimension = target.shape\n\n    # Center the target and noise tensors along the middle dimension (N) for each batch item\n    noise_translation = noise.mean(dim=1, keepdim=True)\n    noise_centered = noise - noise_translation\n    target_centered = target - target.mean(dim=1, keepdim=True)\n\n    # Compute the covariance matrix for each batch item\n    covariance_matrix = torch.matmul(target_centered.transpose(1, 2), noise_centered)\n\n    # Compute the SVD of the covariance matrix for each batch item\n    U, S, Vt = torch.linalg.svd(covariance_matrix)\n\n    # Adjust for proper rotation (determinant=1) for each batch item\n    d = torch.sign(torch.linalg.det(Vt @ U.transpose(-1, -2)))  # Keep as tensor for batch operations\n    d_mat = torch.diag_embed(\n        torch.cat(\n            [torch.ones(batch_size, dimension - 1, device=Vt.device, dtype=Vt.dtype), d.unsqueeze(-1)], dim=-1\n        )\n    )\n\n    R_batch = torch.matmul(torch.matmul(Vt.transpose(-1, -2), d_mat), U.transpose(-1, -2))\n\n    target_aligned = target_centered @ R_batch.transpose(-1, -2) + noise_translation\n    return R_batch, target_aligned\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/kabsch_augmentation/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.kabsch_augmentation.KabschAugmentation.kabsch_align","title":"<code>kabsch_align(target, noise)</code>","text":"<p>Find the Rotation matrix (R) such that RMSD is minimized between target @ R.T and noise.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Tensor</code> <p>shape (N, *dim), data from source minibatch.</p> required <code>noise</code> <code>Tensor</code> <p>shape (N, *dim), noise from source minibatch.</p> required <p>Returns:</p> Name Type Description <code>R</code> <code>Tensor</code> <p>shape (dim, dim), the rotation matrix.</p> <p>Aliged Target (Tensor): target tensor rotated and shifted to reduced RMSD with noise</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/kabsch_augmentation.py</code> <pre><code>def kabsch_align(self, target: Tensor, noise: Tensor):\n    \"\"\"Find the Rotation matrix (R) such that RMSD is minimized between target @ R.T and noise.\n\n    Args:\n        target (Tensor): shape (N, *dim), data from source minibatch.\n        noise (Tensor): shape (N, *dim), noise from source minibatch.\n\n    Returns:\n        R (Tensor): shape (*dim, *dim), the rotation matrix.\n        Aliged Target (Tensor): target tensor rotated and shifted to reduced RMSD with noise\n    \"\"\"\n    dimension = target.shape[-1]\n    noise_translation = noise.mean(dim=0)\n    noise_centered = noise - noise_translation\n    target_centered = target - target.mean(dim=0)\n\n    # Compute the covariance matrix\n    covariance_matix = target_centered.T @ noise_centered\n\n    # Compute the SVD of the covariance matrix\n    U, S, Vt = torch.linalg.svd(covariance_matix)\n    d = torch.sign(torch.linalg.det(Vt.T @ U.T)).item()\n    d_mat = torch.tensor([1] * (dimension - 1) + [d], device=Vt.device, dtype=Vt.dtype)\n    R = Vt.T @ torch.diag(d_mat) @ U.T\n\n    target_aligned = target_centered @ R.T + noise_translation\n\n    return R, target_aligned\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler/","title":"Ot sampler","text":""},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.ot_sampler.OTSampler","title":"<code>OTSampler</code>","text":"<p>Sampler for Exact Mini-batch Optimal Transport Plan.</p> <p>OTSampler implements sampling coordinates according to an OT plan (wrt squared Euclidean cost) with different implementations of the plan calculation. Code is adapted from https://github.com/atong01/conditional-flow-matching/blob/main/torchcfm/optimal_transport.py</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler.py</code> <pre><code>class OTSampler:\n    \"\"\"Sampler for Exact Mini-batch Optimal Transport Plan.\n\n    OTSampler implements sampling coordinates according to an OT plan (wrt squared Euclidean cost)\n    with different implementations of the plan calculation. Code is adapted from https://github.com/atong01/conditional-flow-matching/blob/main/torchcfm/optimal_transport.py\n\n    \"\"\"\n\n    def __init__(\n        self,\n        method: str = \"exact\",\n        device: Union[str, torch.device] = \"cpu\",\n        num_threads: int = 1,\n    ) -&gt; None:\n        \"\"\"Initialize the OTSampler class.\n\n        Args:\n            method (str): Choose which optimal transport solver you would like to use. Currently only support exact OT solvers (pot.emd).\n            device (Union[str, torch.device], optional): The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".\n            num_threads (Union[int, str], optional): Number of threads to use for OT solver. If \"max\", uses the maximum number of threads. Default is 1.\n\n        Raises:\n            ValueError: If the OT solver is not documented.\n            NotImplementedError: If the OT solver is not implemented.\n        \"\"\"\n        # ot_fn should take (a, b, M) as arguments where a, b are marginals and\n        # M is a cost matrix\n        if method == \"exact\":\n            self.ot_fn: Callable[..., torch.Tensor] = partial(pot.emd, numThreads=num_threads)  # type: ignore\n        elif method in {\"sinkhorn\", \"unbalanced\", \"partial\"}:\n            raise NotImplementedError(\"OT solver other than 'exact' is not implemented.\")\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n        self.device = device\n\n    def to_device(self, device: str):\n        \"\"\"Moves all internal tensors to the specified device and updates the `self.device` attribute.\n\n        Args:\n            device (str): The device to move the tensors to (e.g. \"cpu\", \"cuda:0\").\n\n        Note:\n            This method is used to transfer the internal state of the OTSampler to a different device.\n            It updates the `self.device` attribute to reflect the new device and moves all internal tensors to the specified device.\n        \"\"\"\n        self.device = device\n        for attr_name in dir(self):\n            if attr_name.startswith(\"_\") and isinstance(getattr(self, attr_name), torch.Tensor):\n                setattr(self, attr_name, getattr(self, attr_name).to(device))\n        return self\n\n    def sample_map(self, pi: Tensor, batch_size: int, replace: Bool = False) -&gt; Tuple[Tensor, Tensor]:\n        r\"\"\"Draw source and target samples from pi $(x,z) \\sim \\pi$.\n\n        Args:\n            pi (Tensor): shape (bs, bs), the OT matrix between noise and data in minibatch.\n            batch_size (int): The batch size of the minibatch.\n            replace (bool): sampling w/ or w/o replacement from the OT plan, default to False.\n\n        Returns:\n            Tuple: tuple of 2 tensors, represents the indices of noise and data samples from pi.\n        \"\"\"\n        if pi.shape[0] != batch_size or pi.shape[1] != batch_size:\n            raise ValueError(\"Shape mismatch: pi.shape = {}, batch_size = {}\".format(pi.shape, batch_size))\n        p = pi.flatten()\n        p = p / p.sum()\n        choices = torch.multinomial(p, batch_size, replacement=replace)\n        return torch.div(choices, pi.shape[1], rounding_mode=\"floor\"), choices % pi.shape[1]\n\n    def _calculate_cost_matrix(self, x0: Tensor, x1: Tensor, mask: Optional[Tensor] = None) -&gt; Tensor:\n        \"\"\"Compute the cost matrix between a source and a target minibatch.\n\n        Args:\n            x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n            x1 (Tensor): shape (bs, *dim), data from source minibatch.\n            mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n\n        Returns:\n            Tensor: shape (bs, bs), the cost matrix between noise and data in minibatch.\n        \"\"\"\n        if mask is None:\n            # Flatten the input tensors\n            x0, x1 = x0.reshape(x0.shape[0], -1), x1.reshape(x1.shape[0], -1)\n\n            # Compute the cost matrix. For exact OT, we use squared Euclidean distance.\n            M = torch.cdist(x0, x1) ** 2\n        else:\n            # Initialize the cost matrix\n            M = torch.zeros((x0.shape[0], x1.shape[0]))\n            # For each x0 sample, apply its mask to all x1 samples and calculate the cost\n            for i in range(x0.shape[0]):\n                x0i_mask = mask[i].unsqueeze(-1)\n                masked_x1 = x1 * x0i_mask\n                masked_x0 = x0[i] * x0i_mask\n                cost = torch.cdist(masked_x0.reshape(1, -1), masked_x1.reshape(x1.shape[0], -1)) ** 2\n                M[i] = cost\n        return M\n\n    def get_ot_matrix(self, x0: Tensor, x1: Tensor, mask: Optional[Tensor] = None) -&gt; Tensor:\n        \"\"\"Compute the OT matrix between a source and a target minibatch.\n\n        Args:\n            x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n            x1 (Tensor): shape (bs, *dim), data from source minibatch.\n            mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n\n        Returns:\n            p (Tensor): shape (bs, bs), the OT matrix between noise and data in minibatch.\n\n        \"\"\"\n        # Compute the cost matrix\n        M = self._calculate_cost_matrix(x0, x1, mask)\n        # Set uniform weights for all samples in a minibatch\n        a, b = pot.unif(x0.shape[0], type_as=M), pot.unif(x1.shape[0], type_as=M)\n\n        p = self.ot_fn(a, b, M)\n        # Handle exceptions\n        if not torch.all(torch.isfinite(p)):\n            raise ValueError(\"OT plan map is not finite, cost mean, max: {}, {}\".format(M.mean(), M.max()))\n        if torch.abs(p.sum()) &lt; 1e-8:\n            warnings.warn(\"Numerical errors in OT matrix, reverting to uniform plan.\")\n            p = torch.ones_like(p) / p.numel()\n\n        return p\n\n    def apply_augmentation(\n        self,\n        x0: Tensor,\n        x1: Tensor,\n        mask: Optional[Tensor] = None,\n        replace: Bool = False,\n        sort: Optional[Literal[\"noise\", \"x0\", \"data\", \"x1\"]] = \"x0\",\n    ) -&gt; Tuple[Tensor, Tensor, Optional[Tensor]]:\n        r\"\"\"Sample indices for noise and data in minibatch according to OT plan.\n\n        Compute the OT plan $\\pi$ (wrt squared Euclidean cost) between a source and a target\n        minibatch and draw source and target samples from pi $(x,z) \\sim \\pi$.\n\n        Args:\n            x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n            x1 (Tensor): shape (bs, *dim), data from source minibatch.\n            mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n            replace (bool): sampling w/ or w/o replacement from the OT plan, default to False.\n            sort (str): Optional Literal string to sort either x1 or x0 based on the input.\n\n        Returns:\n            Tuple: tuple of 2 tensors or 3 tensors if mask is used, represents the noise (plus mask) and data samples following OT plan pi.\n        \"\"\"\n        if replace and sort is not None:\n            raise ValueError(\"Cannot sample with replacement and sort\")\n        # Calculate the optimal transport\n        pi = self.get_ot_matrix(x0, x1, mask)\n\n        # Sample (x0, x1) mapping indices from the OT matrix\n        i, j = self.sample_map(pi, x0.shape[0], replace=replace)\n        if not replace and (sort == \"noise\" or sort == \"x0\"):\n            sort_idx = torch.argsort(i)\n            i = i[sort_idx]\n            j = j[sort_idx]\n\n            if not (i == torch.arange(x0.shape[0], device=i.device)).all():\n                raise ValueError(\"x0_idx should be a tensor from 0 to size - 1 when sort is 'noise' or 'x0\")\n            noise = x0\n            data = x1[j]\n        elif not replace and (sort == \"data\" or sort == \"x1\"):\n            sort_idx = torch.argsort(j)\n            i = i[sort_idx]\n            j = j[sort_idx]\n\n            if not (j == torch.arange(x1.shape[0], device=j.device)).all():\n                raise ValueError(\"x1_idx should be a tensor from 0 to size - 1 when sort is 'noise' or 'x0\")\n            noise = x0[i]\n            data = x1\n        else:\n            noise = x0[i]\n            data = x1[j]\n\n        # Output the permuted samples in the minibatch\n        if mask is not None:\n            if mask.device != x0.device:\n                mask = mask.to(x0.device)\n            mask = mask[i]\n        return noise, data, mask\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.ot_sampler.OTSampler.__init__","title":"<code>__init__(method='exact', device='cpu', num_threads=1)</code>","text":"<p>Initialize the OTSampler class.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>Choose which optimal transport solver you would like to use. Currently only support exact OT solvers (pot.emd).</p> <code>'exact'</code> <code>device</code> <code>Union[str, device]</code> <p>The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".</p> <code>'cpu'</code> <code>num_threads</code> <code>Union[int, str]</code> <p>Number of threads to use for OT solver. If \"max\", uses the maximum number of threads. Default is 1.</p> <code>1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the OT solver is not documented.</p> <code>NotImplementedError</code> <p>If the OT solver is not implemented.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler.py</code> <pre><code>def __init__(\n    self,\n    method: str = \"exact\",\n    device: Union[str, torch.device] = \"cpu\",\n    num_threads: int = 1,\n) -&gt; None:\n    \"\"\"Initialize the OTSampler class.\n\n    Args:\n        method (str): Choose which optimal transport solver you would like to use. Currently only support exact OT solvers (pot.emd).\n        device (Union[str, torch.device], optional): The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".\n        num_threads (Union[int, str], optional): Number of threads to use for OT solver. If \"max\", uses the maximum number of threads. Default is 1.\n\n    Raises:\n        ValueError: If the OT solver is not documented.\n        NotImplementedError: If the OT solver is not implemented.\n    \"\"\"\n    # ot_fn should take (a, b, M) as arguments where a, b are marginals and\n    # M is a cost matrix\n    if method == \"exact\":\n        self.ot_fn: Callable[..., torch.Tensor] = partial(pot.emd, numThreads=num_threads)  # type: ignore\n    elif method in {\"sinkhorn\", \"unbalanced\", \"partial\"}:\n        raise NotImplementedError(\"OT solver other than 'exact' is not implemented.\")\n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n    self.device = device\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.ot_sampler.OTSampler.apply_augmentation","title":"<code>apply_augmentation(x0, x1, mask=None, replace=False, sort='x0')</code>","text":"<p>Sample indices for noise and data in minibatch according to OT plan.</p> <p>Compute the OT plan $\\pi$ (wrt squared Euclidean cost) between a source and a target minibatch and draw source and target samples from pi $(x,z) \\sim \\pi$.</p> <p>Parameters:</p> Name Type Description Default <code>x0</code> <code>Tensor</code> <p>shape (bs, *dim), noise from source minibatch.</p> required <code>x1</code> <code>Tensor</code> <p>shape (bs, *dim), data from source minibatch.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.</p> <code>None</code> <code>replace</code> <code>bool</code> <p>sampling w/ or w/o replacement from the OT plan, default to False.</p> <code>False</code> <code>sort</code> <code>str</code> <p>Optional Literal string to sort either x1 or x0 based on the input.</p> <code>'x0'</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple[Tensor, Tensor, Optional[Tensor]]</code> <p>tuple of 2 tensors or 3 tensors if mask is used, represents the noise (plus mask) and data samples following OT plan pi.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler.py</code> <pre><code>def apply_augmentation(\n    self,\n    x0: Tensor,\n    x1: Tensor,\n    mask: Optional[Tensor] = None,\n    replace: Bool = False,\n    sort: Optional[Literal[\"noise\", \"x0\", \"data\", \"x1\"]] = \"x0\",\n) -&gt; Tuple[Tensor, Tensor, Optional[Tensor]]:\n    r\"\"\"Sample indices for noise and data in minibatch according to OT plan.\n\n    Compute the OT plan $\\pi$ (wrt squared Euclidean cost) between a source and a target\n    minibatch and draw source and target samples from pi $(x,z) \\sim \\pi$.\n\n    Args:\n        x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n        x1 (Tensor): shape (bs, *dim), data from source minibatch.\n        mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n        replace (bool): sampling w/ or w/o replacement from the OT plan, default to False.\n        sort (str): Optional Literal string to sort either x1 or x0 based on the input.\n\n    Returns:\n        Tuple: tuple of 2 tensors or 3 tensors if mask is used, represents the noise (plus mask) and data samples following OT plan pi.\n    \"\"\"\n    if replace and sort is not None:\n        raise ValueError(\"Cannot sample with replacement and sort\")\n    # Calculate the optimal transport\n    pi = self.get_ot_matrix(x0, x1, mask)\n\n    # Sample (x0, x1) mapping indices from the OT matrix\n    i, j = self.sample_map(pi, x0.shape[0], replace=replace)\n    if not replace and (sort == \"noise\" or sort == \"x0\"):\n        sort_idx = torch.argsort(i)\n        i = i[sort_idx]\n        j = j[sort_idx]\n\n        if not (i == torch.arange(x0.shape[0], device=i.device)).all():\n            raise ValueError(\"x0_idx should be a tensor from 0 to size - 1 when sort is 'noise' or 'x0\")\n        noise = x0\n        data = x1[j]\n    elif not replace and (sort == \"data\" or sort == \"x1\"):\n        sort_idx = torch.argsort(j)\n        i = i[sort_idx]\n        j = j[sort_idx]\n\n        if not (j == torch.arange(x1.shape[0], device=j.device)).all():\n            raise ValueError(\"x1_idx should be a tensor from 0 to size - 1 when sort is 'noise' or 'x0\")\n        noise = x0[i]\n        data = x1\n    else:\n        noise = x0[i]\n        data = x1[j]\n\n    # Output the permuted samples in the minibatch\n    if mask is not None:\n        if mask.device != x0.device:\n            mask = mask.to(x0.device)\n        mask = mask[i]\n    return noise, data, mask\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.ot_sampler.OTSampler.get_ot_matrix","title":"<code>get_ot_matrix(x0, x1, mask=None)</code>","text":"<p>Compute the OT matrix between a source and a target minibatch.</p> <p>Parameters:</p> Name Type Description Default <code>x0</code> <code>Tensor</code> <p>shape (bs, *dim), noise from source minibatch.</p> required <code>x1</code> <code>Tensor</code> <p>shape (bs, *dim), data from source minibatch.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>p</code> <code>Tensor</code> <p>shape (bs, bs), the OT matrix between noise and data in minibatch.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler.py</code> <pre><code>def get_ot_matrix(self, x0: Tensor, x1: Tensor, mask: Optional[Tensor] = None) -&gt; Tensor:\n    \"\"\"Compute the OT matrix between a source and a target minibatch.\n\n    Args:\n        x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n        x1 (Tensor): shape (bs, *dim), data from source minibatch.\n        mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n\n    Returns:\n        p (Tensor): shape (bs, bs), the OT matrix between noise and data in minibatch.\n\n    \"\"\"\n    # Compute the cost matrix\n    M = self._calculate_cost_matrix(x0, x1, mask)\n    # Set uniform weights for all samples in a minibatch\n    a, b = pot.unif(x0.shape[0], type_as=M), pot.unif(x1.shape[0], type_as=M)\n\n    p = self.ot_fn(a, b, M)\n    # Handle exceptions\n    if not torch.all(torch.isfinite(p)):\n        raise ValueError(\"OT plan map is not finite, cost mean, max: {}, {}\".format(M.mean(), M.max()))\n    if torch.abs(p.sum()) &lt; 1e-8:\n        warnings.warn(\"Numerical errors in OT matrix, reverting to uniform plan.\")\n        p = torch.ones_like(p) / p.numel()\n\n    return p\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.ot_sampler.OTSampler.sample_map","title":"<code>sample_map(pi, batch_size, replace=False)</code>","text":"<p>Draw source and target samples from pi $(x,z) \\sim \\pi$.</p> <p>Parameters:</p> Name Type Description Default <code>pi</code> <code>Tensor</code> <p>shape (bs, bs), the OT matrix between noise and data in minibatch.</p> required <code>batch_size</code> <code>int</code> <p>The batch size of the minibatch.</p> required <code>replace</code> <code>bool</code> <p>sampling w/ or w/o replacement from the OT plan, default to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple[Tensor, Tensor]</code> <p>tuple of 2 tensors, represents the indices of noise and data samples from pi.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler.py</code> <pre><code>def sample_map(self, pi: Tensor, batch_size: int, replace: Bool = False) -&gt; Tuple[Tensor, Tensor]:\n    r\"\"\"Draw source and target samples from pi $(x,z) \\sim \\pi$.\n\n    Args:\n        pi (Tensor): shape (bs, bs), the OT matrix between noise and data in minibatch.\n        batch_size (int): The batch size of the minibatch.\n        replace (bool): sampling w/ or w/o replacement from the OT plan, default to False.\n\n    Returns:\n        Tuple: tuple of 2 tensors, represents the indices of noise and data samples from pi.\n    \"\"\"\n    if pi.shape[0] != batch_size or pi.shape[1] != batch_size:\n        raise ValueError(\"Shape mismatch: pi.shape = {}, batch_size = {}\".format(pi.shape, batch_size))\n    p = pi.flatten()\n    p = p / p.sum()\n    choices = torch.multinomial(p, batch_size, replacement=replace)\n    return torch.div(choices, pi.shape[1], rounding_mode=\"floor\"), choices % pi.shape[1]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.ot_sampler.OTSampler.to_device","title":"<code>to_device(device)</code>","text":"<p>Moves all internal tensors to the specified device and updates the <code>self.device</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>The device to move the tensors to (e.g. \"cpu\", \"cuda:0\").</p> required Note <p>This method is used to transfer the internal state of the OTSampler to a different device. It updates the <code>self.device</code> attribute to reflect the new device and moves all internal tensors to the specified device.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler.py</code> <pre><code>def to_device(self, device: str):\n    \"\"\"Moves all internal tensors to the specified device and updates the `self.device` attribute.\n\n    Args:\n        device (str): The device to move the tensors to (e.g. \"cpu\", \"cuda:0\").\n\n    Note:\n        This method is used to transfer the internal state of the OTSampler to a different device.\n        It updates the `self.device` attribute to reflect the new device and moves all internal tensors to the specified device.\n    \"\"\"\n    self.device = device\n    for attr_name in dir(self):\n        if attr_name.startswith(\"_\") and isinstance(getattr(self, attr_name), torch.Tensor):\n            setattr(self, attr_name, getattr(self, attr_name).to(device))\n    return self\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching/","title":"Discrete flow matching","text":""},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching/#bionemo.moco.interpolants.continuous_time.discrete.discrete_flow_matching.DiscreteFlowMatcher","title":"<code>DiscreteFlowMatcher</code>","text":"<p>               Bases: <code>Interpolant</code></p> <p>A Discrete Flow Model (DFM) interpolant.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching.py</code> <pre><code>class DiscreteFlowMatcher(Interpolant):\n    \"\"\"A Discrete Flow Model (DFM) interpolant.\"\"\"\n\n    def __init__(\n        self,\n        time_distribution: TimeDistribution,\n        prior_distribution: DiscretePriorDistribution,\n        device: str = \"cpu\",\n        eps: Float = 1e-5,\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Initialize the DFM interpolant.\n\n        Args:\n            time_distribution (TimeDistribution): The time distribution for the diffusion process.\n            prior_distribution (DiscretePriorDistribution): The prior distribution for the discrete masked tokens.\n            device (str, optional): The device to use for computations. Defaults to \"cpu\".\n            eps: small Float to prevent dividing by zero.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n        \"\"\"\n        super().__init__(time_distribution, prior_distribution, device, rng_generator)\n        self.num_classes = prior_distribution.num_classes\n        self.eps = eps\n        self.use_mask = isinstance(self.prior_distribution, DiscreteMaskedPrior)\n        if self.use_mask:\n            self.mask_index = prior_distribution.mask_dim  # type: ignore\n        self._loss_function = nn.CrossEntropyLoss(reduction=\"none\")\n\n    def interpolate(self, data: Tensor, t: Tensor, noise: Tensor):\n        \"\"\"Get x(t) with given time t from noise and data.\n\n        Args:\n            data (Tensor): target discrete ids\n            t (Tensor): time\n            noise: tensor noise ids\n        \"\"\"\n        if data.dtype == torch.float and data.ndim &gt; 2:\n            x1 = data.argmax(-1)\n        else:\n            x1 = data\n        x0 = noise\n        t = pad_like(t, x1)\n        threshold = torch.rand_like(x1.float())\n        xt = torch.where((threshold &lt; 1 - t), x0, x1)\n        return xt\n\n    def loss(\n        self,\n        logits: Tensor,\n        target: Tensor,\n        time: Optional[Tensor] = None,\n        mask: Optional[Tensor] = None,\n        use_weight: Bool = False,\n    ):\n        \"\"\"Calculate the cross-entropy loss between the model prediction and the target output.\n\n        The loss is calculated between the batch x node x class logits and the target batch x node.\n        If using a masked prior please pass in the correct mask to calculate loss values on only masked states.\n        i.e. mask = data_mask * is_masked_state which is calculated with self.prior_dist.is_masked(xt))\n\n        If `use_weight` is True, the loss is weighted by 1/(1-t) defined in equation 24 in Appndix C. of https://arxiv.org/pdf/2402.04997\n\n        Args:\n            logits (Tensor): The predicted output from the model, with shape batch x node x class.\n            target (Tensor): The target output for the model prediction, with shape batch x node.\n            time (Tensor): The time at which the loss is calculated.\n            mask (Optional[Tensor], optional): The mask for the data point. Defaults to None.\n            use_weight (bool, optional): Whether to use the DFM time weight for the loss. Defaults to True.\n\n        Returns:\n            Tensor: The calculated loss batch tensor.\n        \"\"\"\n        assert target.ndim + 1 == logits.ndim\n        loss = self._loss_function(logits.transpose(-1, 1), target.long())\n        if mask is not None:\n            loss = loss * mask\n            num_non_masked_elements = torch.sum(mask, dim=-1)\n            num_non_masked_elements[num_non_masked_elements == 0] = (\n                1.0  #! prevents divide by zero since if the row is all zero the sum of loss = 0\n            )\n            loss = torch.sum(loss, dim=(-1)) / num_non_masked_elements\n        else:\n            loss = torch.sum(loss, dim=(-1)) / logits.size(1)\n        if use_weight:\n            if time is None:\n                raise ValueError(\"Time is required to compute the DFM liklehood weighting of 1/(1-t + self.eps)\")\n            loss = loss * 1 / (1 - time + self.eps)\n        return loss\n\n    def step(\n        self,\n        logits: Tensor,\n        t: Tensor,\n        xt: Tensor,\n        dt: Tensor | float,\n        temperature: Float = 1.0,\n        stochasticity: Float = 1.0,\n    ) -&gt; Tensor:\n        \"\"\"Perform a single step of DFM euler updates.\n\n        Args:\n            logits (Tensor): The input logits.\n            t (Tensor): The current time step.\n            xt (Tensor): The current state.\n            dt (Tensor | float): The time step increment.\n            temperature (Float, optional): The temperature for the softmax calculation. Defaults to 1.0.\n            stochasticity (Float, optional): The stochasticity value for the step calculation. Defaults to 1.0.\n\n        Returns:\n            Tensor: The updated state.\n        \"\"\"\n        x_1_pred_logits = logits\n        S = x_1_pred_logits.shape[-1]\n        t = pad_like(t, logits)\n        if isinstance(dt, float):\n            dt = torch.Tensor([dt] * t.shape[0]).to(self.device)\n        dt = pad_like(dt, logits)  # type: ignore\n\n        if self.use_mask:\n            if self.mask_index &gt;= S:\n                raise ValueError(\n                    \"If using a non inclusive DiscreteMaskedPrior initialization, please pad the logits input with DiscreteMaskedPrior.pad_sample(logits)\"\n                )\n\n            mask_one_hot = torch.zeros((S,), device=self.device)\n            mask_one_hot[self.mask_index] = 1.0\n            x_1_pred_logits[..., self.mask_index] = -1.0e9\n\n            x_1_pred_prob = F.softmax(x_1_pred_logits / temperature, dim=-1)\n\n            xt_is_mask = (xt == self.mask_index).unsqueeze(-1).float()  # b x n x 1\n            step_prob = (\n                dt * x_1_pred_prob * ((1 + stochasticity * t) / (1 - t)) * xt_is_mask\n                + dt\n                * (1 - xt_is_mask)\n                * mask_one_hot.view(1, 1, -1)\n                * stochasticity\n                * (\n                    t + dt &lt; 1\n                ).float()  # No remasking if on final step. NOTE should probably use step_argmax or step_sample instead\n            )  # (b, n, S)\n            step_prob = self._regularize_step_probs(step_prob, xt)\n        else:\n            x_1_pred_prob = torch.nn.functional.softmax(x_1_pred_logits / temperature, dim=-1)  # (b, n, S)\n\n            pt_x1_eq_xt_prob = torch.gather(x_1_pred_prob, dim=-1, index=xt.long().unsqueeze(-1))  # (b, n, 1)\n\n            step_prob = (\n                dt * x_1_pred_prob * ((1 + stochasticity + stochasticity * (S - 1) * t) / (1 - t))\n                + dt * pt_x1_eq_xt_prob * stochasticity\n            )\n            step_prob = self._regularize_step_probs(step_prob, xt)\n\n        x_next = torch.multinomial(step_prob.view(-1, S), num_samples=1, generator=self.rng_generator).view(xt.shape)\n        return x_next\n\n    def _regularize_step_probs(self, step_prob: Tensor, xt: Tensor) -&gt; Tensor:\n        \"\"\"Regularize the step probabilities to ensure that the probability of the current state xt is set to the remaining probability mass after clipping and scattering.\n\n        Args:\n            step_prob (Tensor): The input step probabilities with shape (batch, node, class).\n            xt (Tensor): The current state with shape (batch, node).\n\n        Returns:\n            Tensor: The regularized step probabilities with shape (batch, node, class).\n        \"\"\"\n        device = step_prob.device\n        # Clamp the step probabilities to ensure they are within the valid range [0.0, 1.0]\n        step_prob = torch.clamp(step_prob, min=0.0, max=1.0)\n        # Set the probability of the current state xt to 0\n        step_prob.scatter_(\n            dim=-1,\n            index=xt.unsqueeze(-1),\n            src=torch.zeros((*xt.shape, 1), dtype=torch.float, device=device),\n        )\n        # Set the probability of the current state xt to the remaining probability mass\n        step_prob.scatter_(\n            dim=-1,\n            index=xt[..., None],\n            src=1 - torch.sum(step_prob, dim=-1, keepdim=True),\n        )\n        step_prob = torch.clamp(step_prob, min=0.0, max=1.0)\n        # Clamp the step probabilities again to ensure they are within the valid range [0.0, 1.0]\n        return step_prob\n\n    def step_purity(\n        self,\n        logits: Tensor,\n        t: Tensor,\n        xt: Tensor,\n        dt: Tensor | float,\n        temperature: Float = 1.0,\n        stochasticity: Float = 1.0,\n    ) -&gt; Tensor:\n        \"\"\"Perform a single step of purity sampling.\n\n        https://github.com/jasonkyuyim/multiflow/blob/6278899970523bad29953047e7a42b32a41dc813/multiflow/data/interpolant.py#L346\n        Here's a high-level overview of what the function does:\n        TODO: check if the -1e9 and 1e-9 are small enough or using torch.inf would be better\n\n        1. Preprocessing:\n            Checks if dt is a float and converts it to a tensor if necessary.\n            Pads t and dt to match the shape of xt.\n            Checks if the mask_index is valid (i.e., within the range of possible discrete values).\n        2. Masking:\n            Sets the logits corresponding to the mask_index to a low value (-1e9) to effectively mask out those values.\n            Computes the softmax probabilities of the logits.\n            Sets the probability of the mask_index to a small value (1e-9) to avoid numerical issues.\n        3.Purity sampling:\n            Computes the maximum log probabilities of the softmax distribution.\n            Computes the indices of the top-number_to_unmask samples with the highest log probabilities.\n            Uses these indices to sample new values from the original distribution.\n        4. Unmasking and updating:\n            Creates a mask to select the top-number_to_unmask samples.\n            Uses this mask to update the current state xt with the new samples.\n        5. Re-masking:\n            Generates a new mask to randomly re-mask some of the updated samples.\n            Applies this mask to the updated state xt.\n\n        Args:\n            logits (Tensor): The input logits.\n            t (Tensor): The current time step.\n            xt (Tensor): The current state.\n            dt (Tensor): The time step increment.\n            temperature (Float, optional): The temperature for the softmax calculation. Defaults to 1.0.\n            stochasticity (Float, optional): The stochasticity value for the step calculation. Defaults to 1.0.\n\n        Returns:\n            Tensor: The updated state.\n        \"\"\"\n        if logits.ndim &gt; 3:\n            raise ValueError(\"Purity Sampling is only implmented for logits shape batch x sequence x state space.\")\n        if isinstance(dt, float):\n            dt = torch.Tensor([dt] * t.shape[0]).to(self.device)\n        x_1_pred_logits = logits\n        B, N, S = x_1_pred_logits.shape\n\n        if not self.use_mask:\n            raise ValueError(\"Purity Sampling only works with a DiscreteMaskPrior\")\n\n        if self.mask_index &gt;= S:\n            raise ValueError(\n                \"If using a non inclusive DiscreteMaskedPrior initialization, please pad the logits input with DiscreteMaskedPrior.pad_sample(logits)\"\n            )\n        x_1_pred_logits[..., self.mask_index] = -1.0e9\n        x_1_pred_prob = F.softmax(x_1_pred_logits / temperature, dim=-1)\n        x_1_pred_prob[..., self.mask_index] = 1e-9\n        max_logprob = torch.max(torch.log(x_1_pred_prob), dim=-1)[0]  # (b, n)\n        max_logprob = max_logprob - (xt != self.mask_index).float() * 1e9\n        sorted_max_logprobs_idcs = torch.argsort(max_logprob, dim=-1, descending=True)  # (b, n)\n        unmask_probs = (dt * (1 + stochasticity * t) / (1 - t)).clamp(max=1)\n        # For M mask tokens we have p chance to unmask so we try for each one and see how many to do\n        number_to_unmask = torch.binomial(\n            count=torch.count_nonzero(xt == self.mask_index, dim=-1).float(), prob=unmask_probs\n        )\n        unmasked_samples = torch.multinomial(x_1_pred_prob.view(-1, S), num_samples=1).view(xt.shape)\n\n        # Taken from MultiFlow\n        # Vectorized version of:\n        # for b in range(B):\n        #     for d in range(D):\n        #         if d &lt; number_to_unmask[b]:\n        #             aatypes_t[b, d] = unmasked_samples[b, sorted_max_logprobs_idcs[b, d]]\n\n        D_grid = torch.arange(N, device=self.device).view(1, -1).repeat(B, 1)\n        mask1 = (D_grid &lt; number_to_unmask.view(-1, 1)).float()\n        initial_val_max_logprob_idcs = sorted_max_logprobs_idcs[:, 0].view(-1, 1).repeat(1, N)\n        masked_sorted_max_logprobs_idcs = (\n            mask1 * sorted_max_logprobs_idcs + (1 - mask1) * initial_val_max_logprob_idcs\n        ).long()\n        mask2 = torch.zeros((B, N), dtype=torch.long, device=self.device)\n        mask2.scatter_(\n            dim=1,\n            index=masked_sorted_max_logprobs_idcs,\n            src=torch.ones((B, N), dtype=torch.long, device=self.device),\n        )\n        unmask_zero_row = (number_to_unmask == 0).view(-1, 1).repeat(1, N).long()\n        mask2 = mask2 * (1 - unmask_zero_row)\n        x_next = xt * (1 - mask2) + unmasked_samples * mask2\n\n        # re-mask\n        u = torch.rand((B, N), device=self.device, generator=self.rng_generator)\n        dt = pad_like(dt, u)  # type: ignore\n        re_mask_mask = (u &lt; dt * stochasticity).long()\n        x_next = x_next * (1 - re_mask_mask) + self.mask_index * re_mask_mask\n\n        return x_next\n\n    def step_argmax(self, model_out: Tensor):\n        \"\"\"Returns the index of the maximum value in the last dimension of the model output.\n\n        Args:\n            model_out (Tensor): The output of the model.\n\n        \"\"\"\n        if self.use_mask:\n            model_out[..., self.mask_index] = -1.0e9\n        return model_out.argmax(dim=-1)\n\n    def step_simple_sample(self, model_out: Tensor, temperature: float = 1.0, num_samples: int = 1):\n        \"\"\"Samples from the model output logits. Leads to more diversity than step_argmax.\n\n        Args:\n            model_out (Tensor): The output of the model.\n            temperature (Float, optional): The temperature for the softmax calculation. Defaults to 1.0.\n            num_samples (int): Number of samples to return\n\n        \"\"\"\n        if self.use_mask:\n            model_out[..., self.mask_index] = -1.0e9\n        samples = torch.multinomial(\n            torch.nn.functional.softmax(model_out / temperature, dim=-1).view(-1, self.num_classes),\n            num_samples=num_samples,\n            generator=self.rng_generator,\n        )  # batch * seq_len x num_samples\n        if num_samples == 1:\n            samples = samples.view(*model_out.shape[:-1])\n            # batch x seq_len\n        else:\n            samples = samples.view((*model_out.shape[:-1], num_samples))\n            # batch x seq_len x num_samples\n        return samples\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching/#bionemo.moco.interpolants.continuous_time.discrete.discrete_flow_matching.DiscreteFlowMatcher.__init__","title":"<code>__init__(time_distribution, prior_distribution, device='cpu', eps=1e-05, rng_generator=None)</code>","text":"<p>Initialize the DFM interpolant.</p> <p>Parameters:</p> Name Type Description Default <code>time_distribution</code> <code>TimeDistribution</code> <p>The time distribution for the diffusion process.</p> required <code>prior_distribution</code> <code>DiscretePriorDistribution</code> <p>The prior distribution for the discrete masked tokens.</p> required <code>device</code> <code>str</code> <p>The device to use for computations. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>eps</code> <code>Float</code> <p>small Float to prevent dividing by zero.</p> <code>1e-05</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching.py</code> <pre><code>def __init__(\n    self,\n    time_distribution: TimeDistribution,\n    prior_distribution: DiscretePriorDistribution,\n    device: str = \"cpu\",\n    eps: Float = 1e-5,\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Initialize the DFM interpolant.\n\n    Args:\n        time_distribution (TimeDistribution): The time distribution for the diffusion process.\n        prior_distribution (DiscretePriorDistribution): The prior distribution for the discrete masked tokens.\n        device (str, optional): The device to use for computations. Defaults to \"cpu\".\n        eps: small Float to prevent dividing by zero.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n    \"\"\"\n    super().__init__(time_distribution, prior_distribution, device, rng_generator)\n    self.num_classes = prior_distribution.num_classes\n    self.eps = eps\n    self.use_mask = isinstance(self.prior_distribution, DiscreteMaskedPrior)\n    if self.use_mask:\n        self.mask_index = prior_distribution.mask_dim  # type: ignore\n    self._loss_function = nn.CrossEntropyLoss(reduction=\"none\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching/#bionemo.moco.interpolants.continuous_time.discrete.discrete_flow_matching.DiscreteFlowMatcher.interpolate","title":"<code>interpolate(data, t, noise)</code>","text":"<p>Get x(t) with given time t from noise and data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>target discrete ids</p> required <code>t</code> <code>Tensor</code> <p>time</p> required <code>noise</code> <code>Tensor</code> <p>tensor noise ids</p> required Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching.py</code> <pre><code>def interpolate(self, data: Tensor, t: Tensor, noise: Tensor):\n    \"\"\"Get x(t) with given time t from noise and data.\n\n    Args:\n        data (Tensor): target discrete ids\n        t (Tensor): time\n        noise: tensor noise ids\n    \"\"\"\n    if data.dtype == torch.float and data.ndim &gt; 2:\n        x1 = data.argmax(-1)\n    else:\n        x1 = data\n    x0 = noise\n    t = pad_like(t, x1)\n    threshold = torch.rand_like(x1.float())\n    xt = torch.where((threshold &lt; 1 - t), x0, x1)\n    return xt\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching/#bionemo.moco.interpolants.continuous_time.discrete.discrete_flow_matching.DiscreteFlowMatcher.loss","title":"<code>loss(logits, target, time=None, mask=None, use_weight=False)</code>","text":"<p>Calculate the cross-entropy loss between the model prediction and the target output.</p> <p>The loss is calculated between the batch x node x class logits and the target batch x node. If using a masked prior please pass in the correct mask to calculate loss values on only masked states. i.e. mask = data_mask * is_masked_state which is calculated with self.prior_dist.is_masked(xt))</p> <p>If <code>use_weight</code> is True, the loss is weighted by 1/(1-t) defined in equation 24 in Appndix C. of https://arxiv.org/pdf/2402.04997</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>The predicted output from the model, with shape batch x node x class.</p> required <code>target</code> <code>Tensor</code> <p>The target output for the model prediction, with shape batch x node.</p> required <code>time</code> <code>Tensor</code> <p>The time at which the loss is calculated.</p> <code>None</code> <code>mask</code> <code>Optional[Tensor]</code> <p>The mask for the data point. Defaults to None.</p> <code>None</code> <code>use_weight</code> <code>bool</code> <p>Whether to use the DFM time weight for the loss. Defaults to True.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The calculated loss batch tensor.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching.py</code> <pre><code>def loss(\n    self,\n    logits: Tensor,\n    target: Tensor,\n    time: Optional[Tensor] = None,\n    mask: Optional[Tensor] = None,\n    use_weight: Bool = False,\n):\n    \"\"\"Calculate the cross-entropy loss between the model prediction and the target output.\n\n    The loss is calculated between the batch x node x class logits and the target batch x node.\n    If using a masked prior please pass in the correct mask to calculate loss values on only masked states.\n    i.e. mask = data_mask * is_masked_state which is calculated with self.prior_dist.is_masked(xt))\n\n    If `use_weight` is True, the loss is weighted by 1/(1-t) defined in equation 24 in Appndix C. of https://arxiv.org/pdf/2402.04997\n\n    Args:\n        logits (Tensor): The predicted output from the model, with shape batch x node x class.\n        target (Tensor): The target output for the model prediction, with shape batch x node.\n        time (Tensor): The time at which the loss is calculated.\n        mask (Optional[Tensor], optional): The mask for the data point. Defaults to None.\n        use_weight (bool, optional): Whether to use the DFM time weight for the loss. Defaults to True.\n\n    Returns:\n        Tensor: The calculated loss batch tensor.\n    \"\"\"\n    assert target.ndim + 1 == logits.ndim\n    loss = self._loss_function(logits.transpose(-1, 1), target.long())\n    if mask is not None:\n        loss = loss * mask\n        num_non_masked_elements = torch.sum(mask, dim=-1)\n        num_non_masked_elements[num_non_masked_elements == 0] = (\n            1.0  #! prevents divide by zero since if the row is all zero the sum of loss = 0\n        )\n        loss = torch.sum(loss, dim=(-1)) / num_non_masked_elements\n    else:\n        loss = torch.sum(loss, dim=(-1)) / logits.size(1)\n    if use_weight:\n        if time is None:\n            raise ValueError(\"Time is required to compute the DFM liklehood weighting of 1/(1-t + self.eps)\")\n        loss = loss * 1 / (1 - time + self.eps)\n    return loss\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching/#bionemo.moco.interpolants.continuous_time.discrete.discrete_flow_matching.DiscreteFlowMatcher.step","title":"<code>step(logits, t, xt, dt, temperature=1.0, stochasticity=1.0)</code>","text":"<p>Perform a single step of DFM euler updates.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>The input logits.</p> required <code>t</code> <code>Tensor</code> <p>The current time step.</p> required <code>xt</code> <code>Tensor</code> <p>The current state.</p> required <code>dt</code> <code>Tensor | float</code> <p>The time step increment.</p> required <code>temperature</code> <code>Float</code> <p>The temperature for the softmax calculation. Defaults to 1.0.</p> <code>1.0</code> <code>stochasticity</code> <code>Float</code> <p>The stochasticity value for the step calculation. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The updated state.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching.py</code> <pre><code>def step(\n    self,\n    logits: Tensor,\n    t: Tensor,\n    xt: Tensor,\n    dt: Tensor | float,\n    temperature: Float = 1.0,\n    stochasticity: Float = 1.0,\n) -&gt; Tensor:\n    \"\"\"Perform a single step of DFM euler updates.\n\n    Args:\n        logits (Tensor): The input logits.\n        t (Tensor): The current time step.\n        xt (Tensor): The current state.\n        dt (Tensor | float): The time step increment.\n        temperature (Float, optional): The temperature for the softmax calculation. Defaults to 1.0.\n        stochasticity (Float, optional): The stochasticity value for the step calculation. Defaults to 1.0.\n\n    Returns:\n        Tensor: The updated state.\n    \"\"\"\n    x_1_pred_logits = logits\n    S = x_1_pred_logits.shape[-1]\n    t = pad_like(t, logits)\n    if isinstance(dt, float):\n        dt = torch.Tensor([dt] * t.shape[0]).to(self.device)\n    dt = pad_like(dt, logits)  # type: ignore\n\n    if self.use_mask:\n        if self.mask_index &gt;= S:\n            raise ValueError(\n                \"If using a non inclusive DiscreteMaskedPrior initialization, please pad the logits input with DiscreteMaskedPrior.pad_sample(logits)\"\n            )\n\n        mask_one_hot = torch.zeros((S,), device=self.device)\n        mask_one_hot[self.mask_index] = 1.0\n        x_1_pred_logits[..., self.mask_index] = -1.0e9\n\n        x_1_pred_prob = F.softmax(x_1_pred_logits / temperature, dim=-1)\n\n        xt_is_mask = (xt == self.mask_index).unsqueeze(-1).float()  # b x n x 1\n        step_prob = (\n            dt * x_1_pred_prob * ((1 + stochasticity * t) / (1 - t)) * xt_is_mask\n            + dt\n            * (1 - xt_is_mask)\n            * mask_one_hot.view(1, 1, -1)\n            * stochasticity\n            * (\n                t + dt &lt; 1\n            ).float()  # No remasking if on final step. NOTE should probably use step_argmax or step_sample instead\n        )  # (b, n, S)\n        step_prob = self._regularize_step_probs(step_prob, xt)\n    else:\n        x_1_pred_prob = torch.nn.functional.softmax(x_1_pred_logits / temperature, dim=-1)  # (b, n, S)\n\n        pt_x1_eq_xt_prob = torch.gather(x_1_pred_prob, dim=-1, index=xt.long().unsqueeze(-1))  # (b, n, 1)\n\n        step_prob = (\n            dt * x_1_pred_prob * ((1 + stochasticity + stochasticity * (S - 1) * t) / (1 - t))\n            + dt * pt_x1_eq_xt_prob * stochasticity\n        )\n        step_prob = self._regularize_step_probs(step_prob, xt)\n\n    x_next = torch.multinomial(step_prob.view(-1, S), num_samples=1, generator=self.rng_generator).view(xt.shape)\n    return x_next\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching/#bionemo.moco.interpolants.continuous_time.discrete.discrete_flow_matching.DiscreteFlowMatcher.step_argmax","title":"<code>step_argmax(model_out)</code>","text":"<p>Returns the index of the maximum value in the last dimension of the model output.</p> <p>Parameters:</p> Name Type Description Default <code>model_out</code> <code>Tensor</code> <p>The output of the model.</p> required Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching.py</code> <pre><code>def step_argmax(self, model_out: Tensor):\n    \"\"\"Returns the index of the maximum value in the last dimension of the model output.\n\n    Args:\n        model_out (Tensor): The output of the model.\n\n    \"\"\"\n    if self.use_mask:\n        model_out[..., self.mask_index] = -1.0e9\n    return model_out.argmax(dim=-1)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching/#bionemo.moco.interpolants.continuous_time.discrete.discrete_flow_matching.DiscreteFlowMatcher.step_purity","title":"<code>step_purity(logits, t, xt, dt, temperature=1.0, stochasticity=1.0)</code>","text":"<p>Perform a single step of purity sampling.</p> <p>https://github.com/jasonkyuyim/multiflow/blob/6278899970523bad29953047e7a42b32a41dc813/multiflow/data/interpolant.py#L346 Here's a high-level overview of what the function does: TODO: check if the -1e9 and 1e-9 are small enough or using torch.inf would be better</p> <ol> <li>Preprocessing:     Checks if dt is a float and converts it to a tensor if necessary.     Pads t and dt to match the shape of xt.     Checks if the mask_index is valid (i.e., within the range of possible discrete values).</li> <li>Masking:     Sets the logits corresponding to the mask_index to a low value (-1e9) to effectively mask out those values.     Computes the softmax probabilities of the logits.     Sets the probability of the mask_index to a small value (1e-9) to avoid numerical issues. 3.Purity sampling:     Computes the maximum log probabilities of the softmax distribution.     Computes the indices of the top-number_to_unmask samples with the highest log probabilities.     Uses these indices to sample new values from the original distribution.</li> <li>Unmasking and updating:     Creates a mask to select the top-number_to_unmask samples.     Uses this mask to update the current state xt with the new samples.</li> <li>Re-masking:     Generates a new mask to randomly re-mask some of the updated samples.     Applies this mask to the updated state xt.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>The input logits.</p> required <code>t</code> <code>Tensor</code> <p>The current time step.</p> required <code>xt</code> <code>Tensor</code> <p>The current state.</p> required <code>dt</code> <code>Tensor</code> <p>The time step increment.</p> required <code>temperature</code> <code>Float</code> <p>The temperature for the softmax calculation. Defaults to 1.0.</p> <code>1.0</code> <code>stochasticity</code> <code>Float</code> <p>The stochasticity value for the step calculation. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The updated state.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching.py</code> <pre><code>def step_purity(\n    self,\n    logits: Tensor,\n    t: Tensor,\n    xt: Tensor,\n    dt: Tensor | float,\n    temperature: Float = 1.0,\n    stochasticity: Float = 1.0,\n) -&gt; Tensor:\n    \"\"\"Perform a single step of purity sampling.\n\n    https://github.com/jasonkyuyim/multiflow/blob/6278899970523bad29953047e7a42b32a41dc813/multiflow/data/interpolant.py#L346\n    Here's a high-level overview of what the function does:\n    TODO: check if the -1e9 and 1e-9 are small enough or using torch.inf would be better\n\n    1. Preprocessing:\n        Checks if dt is a float and converts it to a tensor if necessary.\n        Pads t and dt to match the shape of xt.\n        Checks if the mask_index is valid (i.e., within the range of possible discrete values).\n    2. Masking:\n        Sets the logits corresponding to the mask_index to a low value (-1e9) to effectively mask out those values.\n        Computes the softmax probabilities of the logits.\n        Sets the probability of the mask_index to a small value (1e-9) to avoid numerical issues.\n    3.Purity sampling:\n        Computes the maximum log probabilities of the softmax distribution.\n        Computes the indices of the top-number_to_unmask samples with the highest log probabilities.\n        Uses these indices to sample new values from the original distribution.\n    4. Unmasking and updating:\n        Creates a mask to select the top-number_to_unmask samples.\n        Uses this mask to update the current state xt with the new samples.\n    5. Re-masking:\n        Generates a new mask to randomly re-mask some of the updated samples.\n        Applies this mask to the updated state xt.\n\n    Args:\n        logits (Tensor): The input logits.\n        t (Tensor): The current time step.\n        xt (Tensor): The current state.\n        dt (Tensor): The time step increment.\n        temperature (Float, optional): The temperature for the softmax calculation. Defaults to 1.0.\n        stochasticity (Float, optional): The stochasticity value for the step calculation. Defaults to 1.0.\n\n    Returns:\n        Tensor: The updated state.\n    \"\"\"\n    if logits.ndim &gt; 3:\n        raise ValueError(\"Purity Sampling is only implmented for logits shape batch x sequence x state space.\")\n    if isinstance(dt, float):\n        dt = torch.Tensor([dt] * t.shape[0]).to(self.device)\n    x_1_pred_logits = logits\n    B, N, S = x_1_pred_logits.shape\n\n    if not self.use_mask:\n        raise ValueError(\"Purity Sampling only works with a DiscreteMaskPrior\")\n\n    if self.mask_index &gt;= S:\n        raise ValueError(\n            \"If using a non inclusive DiscreteMaskedPrior initialization, please pad the logits input with DiscreteMaskedPrior.pad_sample(logits)\"\n        )\n    x_1_pred_logits[..., self.mask_index] = -1.0e9\n    x_1_pred_prob = F.softmax(x_1_pred_logits / temperature, dim=-1)\n    x_1_pred_prob[..., self.mask_index] = 1e-9\n    max_logprob = torch.max(torch.log(x_1_pred_prob), dim=-1)[0]  # (b, n)\n    max_logprob = max_logprob - (xt != self.mask_index).float() * 1e9\n    sorted_max_logprobs_idcs = torch.argsort(max_logprob, dim=-1, descending=True)  # (b, n)\n    unmask_probs = (dt * (1 + stochasticity * t) / (1 - t)).clamp(max=1)\n    # For M mask tokens we have p chance to unmask so we try for each one and see how many to do\n    number_to_unmask = torch.binomial(\n        count=torch.count_nonzero(xt == self.mask_index, dim=-1).float(), prob=unmask_probs\n    )\n    unmasked_samples = torch.multinomial(x_1_pred_prob.view(-1, S), num_samples=1).view(xt.shape)\n\n    # Taken from MultiFlow\n    # Vectorized version of:\n    # for b in range(B):\n    #     for d in range(D):\n    #         if d &lt; number_to_unmask[b]:\n    #             aatypes_t[b, d] = unmasked_samples[b, sorted_max_logprobs_idcs[b, d]]\n\n    D_grid = torch.arange(N, device=self.device).view(1, -1).repeat(B, 1)\n    mask1 = (D_grid &lt; number_to_unmask.view(-1, 1)).float()\n    initial_val_max_logprob_idcs = sorted_max_logprobs_idcs[:, 0].view(-1, 1).repeat(1, N)\n    masked_sorted_max_logprobs_idcs = (\n        mask1 * sorted_max_logprobs_idcs + (1 - mask1) * initial_val_max_logprob_idcs\n    ).long()\n    mask2 = torch.zeros((B, N), dtype=torch.long, device=self.device)\n    mask2.scatter_(\n        dim=1,\n        index=masked_sorted_max_logprobs_idcs,\n        src=torch.ones((B, N), dtype=torch.long, device=self.device),\n    )\n    unmask_zero_row = (number_to_unmask == 0).view(-1, 1).repeat(1, N).long()\n    mask2 = mask2 * (1 - unmask_zero_row)\n    x_next = xt * (1 - mask2) + unmasked_samples * mask2\n\n    # re-mask\n    u = torch.rand((B, N), device=self.device, generator=self.rng_generator)\n    dt = pad_like(dt, u)  # type: ignore\n    re_mask_mask = (u &lt; dt * stochasticity).long()\n    x_next = x_next * (1 - re_mask_mask) + self.mask_index * re_mask_mask\n\n    return x_next\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching/#bionemo.moco.interpolants.continuous_time.discrete.discrete_flow_matching.DiscreteFlowMatcher.step_simple_sample","title":"<code>step_simple_sample(model_out, temperature=1.0, num_samples=1)</code>","text":"<p>Samples from the model output logits. Leads to more diversity than step_argmax.</p> <p>Parameters:</p> Name Type Description Default <code>model_out</code> <code>Tensor</code> <p>The output of the model.</p> required <code>temperature</code> <code>Float</code> <p>The temperature for the softmax calculation. Defaults to 1.0.</p> <code>1.0</code> <code>num_samples</code> <code>int</code> <p>Number of samples to return</p> <code>1</code> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching.py</code> <pre><code>def step_simple_sample(self, model_out: Tensor, temperature: float = 1.0, num_samples: int = 1):\n    \"\"\"Samples from the model output logits. Leads to more diversity than step_argmax.\n\n    Args:\n        model_out (Tensor): The output of the model.\n        temperature (Float, optional): The temperature for the softmax calculation. Defaults to 1.0.\n        num_samples (int): Number of samples to return\n\n    \"\"\"\n    if self.use_mask:\n        model_out[..., self.mask_index] = -1.0e9\n    samples = torch.multinomial(\n        torch.nn.functional.softmax(model_out / temperature, dim=-1).view(-1, self.num_classes),\n        num_samples=num_samples,\n        generator=self.rng_generator,\n    )  # batch * seq_len x num_samples\n    if num_samples == 1:\n        samples = samples.view(*model_out.shape[:-1])\n        # batch x seq_len\n    else:\n        samples = samples.view((*model_out.shape[:-1], num_samples))\n        # batch x seq_len x num_samples\n    return samples\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/","title":"Mdlm","text":""},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM","title":"<code>MDLM</code>","text":"<p>               Bases: <code>Interpolant</code></p> <p>A Masked discrete Diffusion Language Model (MDLM) interpolant.</p> <p>Examples: <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.moco.distributions.prior.discrete.mask import DiscreteMaskedPrior\n&gt;&gt;&gt; from bionemo.moco.distributions.time.uniform import UniformTimeDistribution\n&gt;&gt;&gt; from bionemo.moco.interpolants.continuous_time.discrete.mdlm import MDLM\n&gt;&gt;&gt; from bionemo.moco.schedules.noise.continuous_noise_transforms import CosineExpNoiseTransform\n&gt;&gt;&gt; from bionemo.moco.schedules.inference_time_schedules import LinearTimeSchedule\n\n\nmdlm = MDLM(\n    time_distribution = UniformTimeDistribution(discrete_time = False,...),\n    prior_distribution = DiscreteMaskedPrior(...),\n    noise_schedule = CosineExpNoiseTransform(...),\n    )\nmodel = Model(...)\n\n# Training\nfor epoch in range(1000):\n    data = data_loader.get(...)\n    time = mdlm.sample_time(batch_size)\n    xt = mdlm.interpolate(data, time)\n\n    logits = model(xt, time)\n    loss = mdlm.loss(logits, data, xt, time)\n    loss.backward()\n\n# Generation\nx_pred = mdlm.sample_prior(data.shape)\nschedule = LinearTimeSchedule(...)\ninference_time = schedule.generate_schedule()\ndts = schedue.discreteize()\nfor t, dt in zip(inference_time, dts):\n    time = torch.full((batch_size,), t)\n    logits = model(x_pred, time)\n    x_pred = mdlm.step(logits, time, x_pred, dt)\nreturn x_pred\n</code></pre></p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>class MDLM(Interpolant):\n    \"\"\"A Masked discrete Diffusion Language Model (MDLM) interpolant.\n\n     -------\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from bionemo.moco.distributions.prior.discrete.mask import DiscreteMaskedPrior\n    &gt;&gt;&gt; from bionemo.moco.distributions.time.uniform import UniformTimeDistribution\n    &gt;&gt;&gt; from bionemo.moco.interpolants.continuous_time.discrete.mdlm import MDLM\n    &gt;&gt;&gt; from bionemo.moco.schedules.noise.continuous_noise_transforms import CosineExpNoiseTransform\n    &gt;&gt;&gt; from bionemo.moco.schedules.inference_time_schedules import LinearTimeSchedule\n\n\n    mdlm = MDLM(\n        time_distribution = UniformTimeDistribution(discrete_time = False,...),\n        prior_distribution = DiscreteMaskedPrior(...),\n        noise_schedule = CosineExpNoiseTransform(...),\n        )\n    model = Model(...)\n\n    # Training\n    for epoch in range(1000):\n        data = data_loader.get(...)\n        time = mdlm.sample_time(batch_size)\n        xt = mdlm.interpolate(data, time)\n\n        logits = model(xt, time)\n        loss = mdlm.loss(logits, data, xt, time)\n        loss.backward()\n\n    # Generation\n    x_pred = mdlm.sample_prior(data.shape)\n    schedule = LinearTimeSchedule(...)\n    inference_time = schedule.generate_schedule()\n    dts = schedue.discreteize()\n    for t, dt in zip(inference_time, dts):\n        time = torch.full((batch_size,), t)\n        logits = model(x_pred, time)\n        x_pred = mdlm.step(logits, time, x_pred, dt)\n    return x_pred\n\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        time_distribution: TimeDistribution,\n        prior_distribution: DiscreteMaskedPrior,\n        noise_schedule: ContinuousExpNoiseTransform,\n        device: str = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Initialize the Masked Discrete Language Model (MDLM) interpolant.\n\n        Args:\n            time_distribution (TimeDistribution): The distribution governing the time variable in the diffusion process.\n            prior_distribution (DiscreteMaskedPrior): The prior distribution over the discrete token space, including masked tokens.\n            noise_schedule (ContinuousExpNoiseTransform): The noise schedule defining the noise intensity as a function of time.\n            device (str, optional): The device to use for computations. Defaults to \"cpu\".\n            rng_generator (Optional[torch.Generator], optional): The random number generator for reproducibility. Defaults to None.\n        \"\"\"\n        super().__init__(time_distribution, prior_distribution, device, rng_generator)\n        if not isinstance(prior_distribution, DiscreteMaskedPrior):\n            raise ValueError(\"DiscreteMaskedPrior required for MDLM\")\n        if not isinstance(noise_schedule, ContinuousExpNoiseTransform):\n            raise ValueError(\"ContinuousExpNoiseTransform required for MDLM\")\n        self.noise_schedule = noise_schedule\n        self.num_classes = prior_distribution.num_classes\n        self.mask_index = prior_distribution.mask_dim\n        # Gumbel used for confidence sampling. Note rng_generator not compatible with torch.Distribution.\n        # self.gumbel_dist = torch.distributions.Gumbel(torch.tensor(0.0), torch.tensor(1.0))\n\n    def interpolate(self, data: Tensor, t: Tensor):\n        \"\"\"Get x(t) with given time t from noise and data.\n\n        Args:\n            data (Tensor): target discrete ids\n            t (Tensor): time\n        \"\"\"\n        if data.dtype == torch.float and data.ndim &gt; 2:\n            x0 = data.argmax(-1)\n        else:\n            x0 = data\n        sigma = self.noise_schedule.calculate_sigma(t, data.device)\n        alpha = self.noise_schedule.sigma_to_alpha(sigma)\n        p_mask = 1 - alpha\n        p_mask = pad_like(p_mask, x0)\n        mask_indices = torch.rand(*x0.shape, device=x0.device, generator=self.rng_generator) &lt; p_mask\n        xt = torch.where(mask_indices, self.mask_index, x0)\n        return xt\n\n    def forward_process(self, data: Tensor, t: Tensor) -&gt; Tensor:\n        \"\"\"Apply the forward process to the data at time t.\n\n        Args:\n            data (Tensor): target discrete ids\n            t (Tensor): time\n\n        Returns:\n            Tensor: x(t) after applying the forward process\n        \"\"\"\n        return self.interpolate(data, t)\n\n    def loss(\n        self,\n        logits: Tensor,\n        target: Tensor,\n        xt: Tensor,\n        time: Tensor,\n        mask: Optional[Tensor] = None,\n        use_weight=True,\n        global_mean: bool = False,\n    ):\n        \"\"\"Calculate the cross-entropy loss between the model prediction and the target output.\n\n        The loss is calculated between the batch x node x class logits and the target batch x node,\n        considering the current state of the discrete sequence `xt` at time `time`.\n\n        If `use_weight` is True, the loss is weighted by the reduced form of the MDLM time weight for continuous NELBO,\n        as specified in equation 11 of https://arxiv.org/pdf/2406.07524. This weight is proportional to the derivative\n        of the noise schedule with respect to time, and is used to emphasize the importance of accurate predictions at\n        certain times in the diffusion process.\n\n        Args:\n            logits (Tensor): The predicted output from the model, with shape batch x node x class.\n            target (Tensor): The target output for the model prediction, with shape batch x node.\n            xt (Tensor): The current state of the discrete sequence, with shape batch x node.\n            time (Tensor): The time at which the loss is calculated.\n            mask (Optional[Tensor], optional): The mask for the data point. Defaults to None.\n            use_weight (bool, optional): Whether to use the MDLM time weight for the loss. Defaults to True.\n            global_mean (bool, optional): All token losses are summed and divided by total token count. Examples with more tokens (longer sequences) implicitly contribute more to the loss. Defaults to False.\n\n        Returns:\n            Tensor: The calculated loss batch tensor.\n        \"\"\"\n        logprobs = self._subs_parameterization(logits, xt)\n        log_p_theta = torch.gather(input=logprobs, dim=-1, index=target[..., None]).squeeze(-1)\n\n        sigma = self.noise_schedule.calculate_sigma(time, target.device)\n        dsigma = self.noise_schedule.d_dt_sigma(time, target.device)  # type: ignore\n        loss = -log_p_theta\n        if use_weight:\n            loss = loss * (dsigma / torch.expm1(sigma))[:, None]\n\n        if global_mean:\n            if mask is not None:\n                loss = loss * mask\n                loss = loss.sum() / mask.sum()\n            else:\n                loss = loss.sum() / logits.size(1)\n        else:\n            if mask is not None:\n                loss = loss * mask\n                num_non_masked_elements = torch.sum(mask, dim=-1)\n                loss = torch.sum(loss, dim=(-1)) / num_non_masked_elements\n            else:\n                loss = torch.sum(loss, dim=(-1)) / logits.size(1)\n        return loss\n\n    def _subs_parameterization(self, logits: Tensor, xt: Tensor) -&gt; Tensor:\n        \"\"\"Apply subsititution parameterization to the logits.\n\n        This function enforces that the model can never predict a mask token by lowering the mask logits.\n        Then, for all unmasked tokens, it copies over from xt to enable carry over unmasked.\n        Once a token is unmasked, it stays the same.\n        See Sec. 3.2.3 https://arxiv.org/pdf/2406.07524.\n\n        Note that recent work has shown that allowing the model to rethink\n        carry over unmasking is beneficial https://arxiv.org/abs/2410.06264.\n\n        Args:\n            logits (Tensor): The logits tensor with shape batch x node x class.\n            xt (Tensor): The tensor of unmasked tokens with shape batch x node.\n\n        Returns:\n            Tensor: The modified logits tensor with substitution parameterization applied.\n        \"\"\"\n        logits[..., self.mask_index] += -1000000.0  # clean input is never masked\n        logprobs = logits - torch.logsumexp(logits, dim=-1, keepdim=True)  # normalize\n        unmasked_indices = xt != self.mask_index\n        logprobs[unmasked_indices] = -1000000.0\n        logprobs[unmasked_indices, xt[unmasked_indices]] = 0  # Unmasked token remains unchanged\n        return logprobs\n\n    def step(self, logits: Tensor, t: Tensor, xt: Tensor, dt: Tensor, temperature: float = 1.0) -&gt; Tensor:\n        \"\"\"Perform a single step of MDLM DDPM step.\n\n        Parameters:\n        logits (Tensor): The input logits.\n        t (Tensor): The current time step.\n        xt (Tensor): The current state.\n        dt (Tensor): The time step increment.\n        temperature (float): Softmax temperature defaults to 1.0.\n\n        Returns:\n        Tensor: The updated state.\n        \"\"\"\n        sigma_t = self.noise_schedule.calculate_sigma(t, logits.device)\n        sigma_s = self.noise_schedule.calculate_sigma(t - dt, logits.device)\n        alpha_t = torch.exp(-sigma_t)\n        alpha_s = torch.exp(-sigma_s)\n        p_mask_s = 1 - alpha_s\n        alpha_t = pad_like(alpha_t, logits)\n        alpha_s = pad_like(alpha_s, logits)\n        p_mask_s = pad_like(p_mask_s, logits)\n        # Apply subs parameterization\n        log_p_x0 = self._subs_parameterization(logits, xt) / temperature\n        if p_mask_s.ndim != log_p_x0.ndim:\n            raise ValueError(f\"Dimension Mistmatch {p_mask_s.shape} {log_p_x0.shape}\")\n        # Equation 7 from MDLM\n        prob_s_given_t = log_p_x0.exp() * (\n            alpha_s - alpha_t\n        )  # righthand side (alpha_s - alpha_t)*x = (1 - alpha_t - (1 - alpha_s)) * x\n        prob_s_given_t[..., self.mask_index] = p_mask_s[..., 0]  # lefthand side (1 - alpha_s)*M\n        sampled_x = self._sample_categorical(prob_s_given_t)\n        carry_over_unmask = (xt != self.mask_index).to(xt.dtype)\n        return carry_over_unmask * xt + (1 - carry_over_unmask) * sampled_x\n\n    def _sample_categorical(self, categorical_probs: Tensor) -&gt; Tensor:\n        \"\"\"Sample from a categorical distribution using the Gumbel trick.\n\n        Args:\n            categorical_probs (Tensor): The probabilities of each category, shape batch x node x class.\n\n        Returns:\n            Tensor: The sampled category indices, shape batch x node.\n        \"\"\"\n        gumbel_norm = (\n            1e-10\n            - (\n                torch.rand(*categorical_probs.shape, device=categorical_probs.device, generator=self.rng_generator)\n                + 1e-10\n            ).log()\n        )\n        scaled_proability = categorical_probs / gumbel_norm\n        return scaled_proability.argmax(dim=-1)\n\n    def get_num_steps_confidence(self, xt: Tensor, num_tokens_unmask: int = 1):\n        \"\"\"Calculate the maximum number of steps with confidence.\n\n        This method computes the maximum count of occurrences where the input tensor `xt` matches the `mask_index`\n        along the last dimension (-1). The result is returned as a single float value.\n\n        Args:\n            xt (Tensor): Input tensor to evaluate against the mask index.\n            num_tokens_unmask (int): number of tokens to unamsk at each step.\n\n        Returns:\n            float: The maximum number of steps with confidence (i.e., matching the mask index).\n        \"\"\"\n        nsteps = (xt == self.mask_index).sum(-1).max().item()\n        if num_tokens_unmask == 1:\n            return int(nsteps)\n        else:\n            return int(max(math.ceil(nsteps // num_tokens_unmask), 1))\n\n    def step_auto_regressive(\n        self,\n        logits: Tensor,\n        xt: Tensor,\n        logit_temperature: float = 1.0,\n    ):\n        \"\"\"Auto-regressive sampling from MDLM.\n\n        This method samples from the predicted logits and replaces the next token in the sequence.\n        The next token is chosen based on the predicted logits and the current state of the sequence.\n        \"\"\"\n        xt = xt.clone()\n        log_p_x0 = self._subs_parameterization(logits, xt)\n        # sample the code from the softmax prediction\n        probs = torch.softmax(log_p_x0 / logit_temperature, dim=-1)\n        preds = torch.distributions.Categorical(probs=probs).sample()\n\n        # do not predict on already predicted tokens\n        mask = xt == self.mask_index\n\n        next_idx = torch.where(mask)[1][0]\n        to_replace = torch.zeros_like(xt)\n        to_replace[:, next_idx] = 1\n        to_replace = (mask.float() * to_replace.float()).bool()\n\n        xt[to_replace] = preds[to_replace]\n        return xt\n\n    def step_confidence(\n        self,\n        logits: Tensor,\n        xt: Tensor,\n        curr_step: int,\n        num_steps: int,\n        logit_temperature: float = 1.0,\n        randomness: float = 1.0,\n        confidence_temperature: float = 1.0,\n        num_tokens_unmask: int = 1,\n    ) -&gt; Tensor:\n        \"\"\"Update the input sequence xt by sampling from the predicted logits and adding Gumbel noise.\n\n        Method taken from GenMol Lee et al. https://arxiv.org/abs/2501.06158\n\n        Args:\n            logits: Predicted logits\n            xt: Input sequence\n            curr_step: Current step\n            num_steps: Total number of steps\n            logit_temperature: Temperature for softmax over logits\n            randomness: Scale for Gumbel noise\n            confidence_temperature: Temperature for Gumbel confidence\n            num_tokens_unmask: number of tokens to unmask each step\n\n        Returns:\n            Updated input sequence xt unmasking num_tokens_unmask token each step.\n        \"\"\"\n        if xt.ndim &gt; 3:\n            raise NotImplementedError(\n                \"step_confidence is implemented for Batch x Sequence x State Space shaped tensors.\"\n            )\n        if curr_step &lt; 0 or num_steps &lt; 1 or num_tokens_unmask &lt; 1:\n            raise ValueError(\"Invalid input values for curr_step, num_steps, or num_tokens_unmask.\")\n        xt = xt.clone()\n        log_p_x0 = self._subs_parameterization(logits, xt)\n        # sample the code from the softmax prediction\n        probs = torch.softmax(log_p_x0 / logit_temperature, dim=-1)\n        preds = torch.distributions.Categorical(probs=probs).sample()\n\n        confidence = probs.gather(-1, preds.unsqueeze(-1)).squeeze(-1)\n        # add Gumbel noise decreasing over the sampling process\n        ratio = curr_step / (num_steps - 1)\n        # Using manual definition of 0,1 Gumbel to pass in generator, manually specifying the device is faster than transfer\n        gumbel_sample = -torch.log(\n            -torch.log(torch.rand(xt.shape, device=logits.device, generator=self.rng_generator))\n        )\n        # gumbel_sample = self.gumbel_dist.sample(xt.shape).to(logits.device)\n        gumbel_noise = gumbel_sample * randomness * (1 - ratio)  # type: ignore\n        confidence = (\n            (torch.log(confidence) + gumbel_noise) / confidence_temperature\n        )  # stems from tau of https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#gumbel_softmax\n\n        # do not predict on already predicted tokens\n        mask = xt == self.mask_index\n        confidence[~mask] = -torch.inf\n\n        # choose the predicted token with the highest confidence\n        confidence_threshold, idx_mask = torch.topk(confidence, k=num_tokens_unmask, dim=-1)\n        confidence_threshold = confidence_threshold[:, -1].unsqueeze(-1)\n        # Rather than take all tokens with confidence above the threshold, we use the topk indices to replace the chosen tokens\n        # replace the chosen tokens\n        to_replace = torch.zeros_like(confidence)\n        to_replace.scatter_(1, idx_mask, 1)\n        to_replace = to_replace.bool() &amp; mask.bool()\n        # to_replace = confidence &gt;= confidence_threshold\n        # to_replace = (mask.float() * to_replace.float()).bool()\n        xt[to_replace] = preds[to_replace]\n        return xt\n\n    def step_confidence_margin(\n        self,\n        logits: Tensor,\n        xt: Tensor,\n        curr_step: int,\n        num_steps: int,\n        logit_temperature: float = 1.0,\n        randomness: float = 1.0,\n        confidence_temperature: float = 1.0,\n        num_tokens_unmask: int = 1,\n    ) -&gt; Tensor:\n        \"\"\"Kim et al., Train for the Worst, Plan for the Best: Understanding Token Ordering in Masked Diffusions, ICML, 2025.\n\n        This method is similar to step_confidence, but it uses the margin of the confidence scores to replace the tokens.\n        The margin is the difference between the confidence score and the next highest confidence score.\n        The tokens with the highest margin are replaced.\n\n        Args:\n            logits: Predicted logits\n            xt: Input sequence\n            curr_step: Current step\n            num_steps: Total number of steps\n            logit_temperature: Temperature for softmax over logits\n            randomness: Scale for Gumbel noise\n            confidence_temperature: Temperature for Gumbel confidence\n            num_tokens_unmask: number of tokens to unmask each step\n\n        Returns:\n            Updated input sequence xt unmasking num_tokens_unmask token each step.\n        \"\"\"\n        if xt.ndim &gt; 3:\n            raise NotImplementedError(\n                \"step_confidence is implemented for Batch x Sequence x State Space shaped tensors.\"\n            )\n        if curr_step &lt; 0 or num_steps &lt; 1 or num_tokens_unmask &lt; 1:\n            raise ValueError(\"Invalid input values for curr_step, num_steps, or num_tokens_unmask.\")\n        xt = xt.clone()\n        log_p_x0 = self._subs_parameterization(logits, xt)\n        # sample the code from the softmax prediction\n        probs = torch.softmax(log_p_x0 / logit_temperature, dim=-1)\n        preds = torch.stack([torch.multinomial(prob, num_samples=2, replacement=False) for prob in probs])\n        confidence_first = probs.gather(-1, preds[:, :, 0].unsqueeze(-1)).squeeze(-1)\n        confidence_second = probs.gather(-1, preds[:, :, 1].unsqueeze(-1)).squeeze(-1)\n        confidence = confidence_first - confidence_second\n        preds = preds[:, :, 0]\n        # add Gumbel noise decreasing over the sampling process\n        ratio = curr_step / (num_steps - 1)\n        # Using manual definition of 0,1 Gumbel to pass in generator, manually specifying the device is faster than transfer\n        gumbel_sample = -torch.log(\n            -torch.log(torch.rand(xt.shape, device=logits.device, generator=self.rng_generator))\n        )\n        # gumbel_sample = self.gumbel_dist.sample(xt.shape).to(logits.device)\n        gumbel_noise = gumbel_sample * randomness * (1 - ratio)  # type: ignore\n        confidence = (\n            (torch.log(confidence) + gumbel_noise) / confidence_temperature\n        )  # stems from tau of https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#gumbel_softmax\n\n        # do not predict on already predicted tokens\n        mask = xt == self.mask_index\n        confidence[~mask] = -torch.inf\n\n        # choose the predicted token with the highest confidence\n        confidence_threshold, idx_mask = torch.topk(confidence, k=num_tokens_unmask, dim=-1)\n        confidence_threshold = confidence_threshold[:, -1].unsqueeze(-1)\n        # Rather than take all tokens with confidence above the threshold, we use the topk indices to replace the chosen tokens\n        # replace the chosen tokens\n        to_replace = torch.zeros_like(confidence)\n        to_replace.scatter_(1, idx_mask, 1)\n        to_replace = to_replace.bool() &amp; mask.bool()\n        # to_replace = confidence &gt;= confidence_threshold\n        # to_replace = (mask.float() * to_replace.float()).bool()\n        xt[to_replace] = preds[to_replace]\n        return xt\n\n    def step_argmax(self, model_out: Tensor):\n        \"\"\"Returns the index of the maximum value in the last dimension of the model output.\n\n        Args:\n            model_out (Tensor): The output of the model.\n\n        Returns:\n            Tensor: The index of the maximum value in the last dimension of the model output.\n        \"\"\"\n        return model_out.argmax(dim=-1)\n\n    def calculate_score(self, logits, x, t):\n        \"\"\"Returns score of the given sample x at time t with the corresponding model output logits.\n\n        Args:\n            logits (Tensor): The output of the model.\n            x (Tensor): The current data point.\n            t (Tensor): The current time.\n\n        Returns:\n            Tensor: The score defined in Appendix C.3 Equation 76 of MDLM.\n        \"\"\"\n        sigma_t = self.noise_schedule.calculate_sigma(t, logits.device)\n        log_ratio = -torch.log(\n            torch.expm1(sigma_t)\n        )  # log ( exp(-sigma) / (1 - exp(-sigma))) = log(1/ (exp(sigma) - 1))\n\n        # Create masked and unmasked log scores\n        masked_log_score = logits + pad_like(log_ratio, logits)  # xt is masked and prediction is not\n        masked_log_score[..., self.mask_index] = 0  # xt and prediction are mask\n\n        unmasked_log_score = torch.full_like(logits, -1000000.0)\n        unmasked_log_score.scatter_(-1, x[..., None], 0)  # place zeros where current predictions are\n        unmasked_log_score[..., self.mask_index] = -pad_like(log_ratio, logits[..., 0])\n\n        # Combine masked and unmasked log scores\n        masked_indices = (x == self.mask_index).to(logits.dtype)[..., None]\n        log_score = masked_log_score * masked_indices + unmasked_log_score * (1 - masked_indices)\n\n        return log_score.exp()\n\n    def step_self_path_planning(\n        self,\n        logits: Tensor,\n        xt: Tensor,\n        t: Tensor,\n        curr_step: int,\n        num_steps: int,\n        logit_temperature: float = 1.0,\n        randomness: float = 1.0,\n        confidence_temperature: float = 1.0,\n        score_type: Literal[\"confidence\", \"random\"] = \"confidence\",\n        fix_mask: Optional[Tensor] = None,\n    ) -&gt; Tensor:\n        \"\"\"Self Path Planning (P2) Sampling from Peng et al. https://arxiv.org/html/2502.03540v1.\n\n        Args:\n            logits (Tensor): Predicted logits for sampling.\n            xt (Tensor): Input sequence to be updated.\n            t (Tensor): Time tensor (e.g., time steps or temporal info).\n            curr_step (int): Current iteration in the planning process.\n            num_steps (int): Total number of planning steps.\n            logit_temperature (float): Temperature for logits (default: 1.0).\n            randomness (float): Introduced randomness level (default: 1.0).\n            confidence_temperature (float): Temperature for confidence scoring (default: 1.0).\n            score_type (Literal[\"confidence\", \"random\"]): Sampling score type (default: \"confidence\").\n            fix_mask (Optional[Tensor]): inital mask where True when not a mask tokens (default: None).\n\n        Returns:\n            Tensor: Updated input sequence xt after iterative unmasking.\n        \"\"\"\n        if xt.ndim &gt; 3:\n            raise NotImplementedError(\n                \"step_confidence is implemented for Batch x Sequence x State Space shaped tensors.\"\n            )\n        if curr_step &lt; 0 or num_steps &lt; 1:\n            raise ValueError(\"Invalid input values for curr_step, num_steps.\")\n        xt = xt.clone()\n        if fix_mask is None:\n            fix_mask = torch.zeros_like(xt).bool()  #! if any sequenes are fixed from the start of trajectory\n        last_mask = xt == self.mask_index\n        unmask_candidates = (\n            last_mask &amp; ~fix_mask\n        )  #! I want to consider tokens to un mask that are currently masked and not fixed. This fixes a typo in pseudo code\n        x1_pred, logp = self.stochastic_sample_from_categorical(\n            logits, temperature=logit_temperature, noise_scale=confidence_temperature\n        )\n        if curr_step == num_steps - 1:\n            xt[last_mask] = x1_pred[last_mask]\n        else:\n            if score_type == \"confidence\":\n                score = logp\n            elif score_type == \"random\":\n                score = torch.rand_like(logp).log()\n\n            score = score.masked_fill(fix_mask.squeeze(-1), float(\"inf\"))\n            score[unmask_candidates.squeeze(-1)] *= randomness\n            num_to_mask = torch.clamp(\n                ((~fix_mask).sum(dim=1, keepdim=True).float() * t.unsqueeze(-1)).long(), max=xt.shape[-1] - 1\n            )  #! here is is t since diffusion time is 1 to 0. Clamp is to set it to 0 N-1 since topk uses it as indices\n            mask = self.topk_lowest_masking(score, num_to_mask)\n            xt[mask] = self.mask_index\n            mask_to_x1 = last_mask &amp; ~mask\n            xt[mask_to_x1] = x1_pred[mask_to_x1]\n        return xt\n\n    def topk_lowest_masking(self, scores: Tensor, cutoff_len: Tensor):\n        \"\"\"Generates a mask for the lowest scoring elements up to a specified cutoff length.\n\n        Args:\n            scores (Tensor): Input scores tensor with shape (... , num_elements)\n            cutoff_len (Tensor): Number of lowest-scoring elements to mask (per batch element)\n\n        Returns:\n            Tensor: Boolean mask tensor with same shape as `scores`, where `True` indicates\n                    the corresponding element is among the `cutoff_len` lowest scores.\n\n        Example:\n            &gt;&gt;&gt; scores = torch.tensor([[0.9, 0.8, 0.1, 0.05], [0.7, 0.4, 0.3, 0.2]])\n            &gt;&gt;&gt; cutoff_len = 2\n            &gt;&gt;&gt; mask = topk_lowest_masking(scores, cutoff_len)\n            &gt;&gt;&gt; print(mask)\n            tensor([[False, False, True, True],\n                    [False, True, True, False]])\n        \"\"\"\n        sorted_scores, _ = scores.sort(dim=-1)\n        threshold = sorted_scores.gather(dim=-1, index=cutoff_len)\n        return scores &lt; threshold\n\n    def stochastic_sample_from_categorical(self, logits: Tensor, temperature: float = 1.0, noise_scale: float = 1.0):\n        \"\"\"Stochastically samples from a categorical distribution defined by input logits, with optional temperature and noise scaling for diverse sampling.\n\n        Args:\n            logits (Tensor): Input logits tensor with shape (... , num_categories)\n            temperature (float, optional): Softmax temperature. Higher values produce more uniform samples. Defaults to 1.0.\n            noise_scale (float, optional): Scale for Gumbel noise. Higher values produce more diverse samples. Defaults to 1.0.\n\n        Returns:\n            tuple:\n                - **tokens** (LongTensor): Sampling result (category indices) with shape (... , )\n                - **scores** (Tensor): Corresponding log-softmax scores for the sampled tokens, with shape (... , )\n        \"\"\"\n        if temperature &gt; 0:\n            gumbel = -torch.log(\n                -torch.log(torch.rand(logits.shape, device=logits.device, generator=self.rng_generator) + 1e-8) + 1e-8\n            )  #! avoid device transfers\n            logits = logits / temperature + noise_scale * gumbel\n        scores, tokens = logits.log_softmax(dim=-1).max(dim=-1)\n        return tokens, scores\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM.__init__","title":"<code>__init__(time_distribution, prior_distribution, noise_schedule, device='cpu', rng_generator=None)</code>","text":"<p>Initialize the Masked Discrete Language Model (MDLM) interpolant.</p> <p>Parameters:</p> Name Type Description Default <code>time_distribution</code> <code>TimeDistribution</code> <p>The distribution governing the time variable in the diffusion process.</p> required <code>prior_distribution</code> <code>DiscreteMaskedPrior</code> <p>The prior distribution over the discrete token space, including masked tokens.</p> required <code>noise_schedule</code> <code>ContinuousExpNoiseTransform</code> <p>The noise schedule defining the noise intensity as a function of time.</p> required <code>device</code> <code>str</code> <p>The device to use for computations. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>The random number generator for reproducibility. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def __init__(\n    self,\n    time_distribution: TimeDistribution,\n    prior_distribution: DiscreteMaskedPrior,\n    noise_schedule: ContinuousExpNoiseTransform,\n    device: str = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Initialize the Masked Discrete Language Model (MDLM) interpolant.\n\n    Args:\n        time_distribution (TimeDistribution): The distribution governing the time variable in the diffusion process.\n        prior_distribution (DiscreteMaskedPrior): The prior distribution over the discrete token space, including masked tokens.\n        noise_schedule (ContinuousExpNoiseTransform): The noise schedule defining the noise intensity as a function of time.\n        device (str, optional): The device to use for computations. Defaults to \"cpu\".\n        rng_generator (Optional[torch.Generator], optional): The random number generator for reproducibility. Defaults to None.\n    \"\"\"\n    super().__init__(time_distribution, prior_distribution, device, rng_generator)\n    if not isinstance(prior_distribution, DiscreteMaskedPrior):\n        raise ValueError(\"DiscreteMaskedPrior required for MDLM\")\n    if not isinstance(noise_schedule, ContinuousExpNoiseTransform):\n        raise ValueError(\"ContinuousExpNoiseTransform required for MDLM\")\n    self.noise_schedule = noise_schedule\n    self.num_classes = prior_distribution.num_classes\n    self.mask_index = prior_distribution.mask_dim\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM.calculate_score","title":"<code>calculate_score(logits, x, t)</code>","text":"<p>Returns score of the given sample x at time t with the corresponding model output logits.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>The output of the model.</p> required <code>x</code> <code>Tensor</code> <p>The current data point.</p> required <code>t</code> <code>Tensor</code> <p>The current time.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The score defined in Appendix C.3 Equation 76 of MDLM.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def calculate_score(self, logits, x, t):\n    \"\"\"Returns score of the given sample x at time t with the corresponding model output logits.\n\n    Args:\n        logits (Tensor): The output of the model.\n        x (Tensor): The current data point.\n        t (Tensor): The current time.\n\n    Returns:\n        Tensor: The score defined in Appendix C.3 Equation 76 of MDLM.\n    \"\"\"\n    sigma_t = self.noise_schedule.calculate_sigma(t, logits.device)\n    log_ratio = -torch.log(\n        torch.expm1(sigma_t)\n    )  # log ( exp(-sigma) / (1 - exp(-sigma))) = log(1/ (exp(sigma) - 1))\n\n    # Create masked and unmasked log scores\n    masked_log_score = logits + pad_like(log_ratio, logits)  # xt is masked and prediction is not\n    masked_log_score[..., self.mask_index] = 0  # xt and prediction are mask\n\n    unmasked_log_score = torch.full_like(logits, -1000000.0)\n    unmasked_log_score.scatter_(-1, x[..., None], 0)  # place zeros where current predictions are\n    unmasked_log_score[..., self.mask_index] = -pad_like(log_ratio, logits[..., 0])\n\n    # Combine masked and unmasked log scores\n    masked_indices = (x == self.mask_index).to(logits.dtype)[..., None]\n    log_score = masked_log_score * masked_indices + unmasked_log_score * (1 - masked_indices)\n\n    return log_score.exp()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM.forward_process","title":"<code>forward_process(data, t)</code>","text":"<p>Apply the forward process to the data at time t.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>target discrete ids</p> required <code>t</code> <code>Tensor</code> <p>time</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>x(t) after applying the forward process</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def forward_process(self, data: Tensor, t: Tensor) -&gt; Tensor:\n    \"\"\"Apply the forward process to the data at time t.\n\n    Args:\n        data (Tensor): target discrete ids\n        t (Tensor): time\n\n    Returns:\n        Tensor: x(t) after applying the forward process\n    \"\"\"\n    return self.interpolate(data, t)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM.get_num_steps_confidence","title":"<code>get_num_steps_confidence(xt, num_tokens_unmask=1)</code>","text":"<p>Calculate the maximum number of steps with confidence.</p> <p>This method computes the maximum count of occurrences where the input tensor <code>xt</code> matches the <code>mask_index</code> along the last dimension (-1). The result is returned as a single float value.</p> <p>Parameters:</p> Name Type Description Default <code>xt</code> <code>Tensor</code> <p>Input tensor to evaluate against the mask index.</p> required <code>num_tokens_unmask</code> <code>int</code> <p>number of tokens to unamsk at each step.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>float</code> <p>The maximum number of steps with confidence (i.e., matching the mask index).</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def get_num_steps_confidence(self, xt: Tensor, num_tokens_unmask: int = 1):\n    \"\"\"Calculate the maximum number of steps with confidence.\n\n    This method computes the maximum count of occurrences where the input tensor `xt` matches the `mask_index`\n    along the last dimension (-1). The result is returned as a single float value.\n\n    Args:\n        xt (Tensor): Input tensor to evaluate against the mask index.\n        num_tokens_unmask (int): number of tokens to unamsk at each step.\n\n    Returns:\n        float: The maximum number of steps with confidence (i.e., matching the mask index).\n    \"\"\"\n    nsteps = (xt == self.mask_index).sum(-1).max().item()\n    if num_tokens_unmask == 1:\n        return int(nsteps)\n    else:\n        return int(max(math.ceil(nsteps // num_tokens_unmask), 1))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM.interpolate","title":"<code>interpolate(data, t)</code>","text":"<p>Get x(t) with given time t from noise and data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>target discrete ids</p> required <code>t</code> <code>Tensor</code> <p>time</p> required Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def interpolate(self, data: Tensor, t: Tensor):\n    \"\"\"Get x(t) with given time t from noise and data.\n\n    Args:\n        data (Tensor): target discrete ids\n        t (Tensor): time\n    \"\"\"\n    if data.dtype == torch.float and data.ndim &gt; 2:\n        x0 = data.argmax(-1)\n    else:\n        x0 = data\n    sigma = self.noise_schedule.calculate_sigma(t, data.device)\n    alpha = self.noise_schedule.sigma_to_alpha(sigma)\n    p_mask = 1 - alpha\n    p_mask = pad_like(p_mask, x0)\n    mask_indices = torch.rand(*x0.shape, device=x0.device, generator=self.rng_generator) &lt; p_mask\n    xt = torch.where(mask_indices, self.mask_index, x0)\n    return xt\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM.loss","title":"<code>loss(logits, target, xt, time, mask=None, use_weight=True, global_mean=False)</code>","text":"<p>Calculate the cross-entropy loss between the model prediction and the target output.</p> <p>The loss is calculated between the batch x node x class logits and the target batch x node, considering the current state of the discrete sequence <code>xt</code> at time <code>time</code>.</p> <p>If <code>use_weight</code> is True, the loss is weighted by the reduced form of the MDLM time weight for continuous NELBO, as specified in equation 11 of https://arxiv.org/pdf/2406.07524. This weight is proportional to the derivative of the noise schedule with respect to time, and is used to emphasize the importance of accurate predictions at certain times in the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>The predicted output from the model, with shape batch x node x class.</p> required <code>target</code> <code>Tensor</code> <p>The target output for the model prediction, with shape batch x node.</p> required <code>xt</code> <code>Tensor</code> <p>The current state of the discrete sequence, with shape batch x node.</p> required <code>time</code> <code>Tensor</code> <p>The time at which the loss is calculated.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>The mask for the data point. Defaults to None.</p> <code>None</code> <code>use_weight</code> <code>bool</code> <p>Whether to use the MDLM time weight for the loss. Defaults to True.</p> <code>True</code> <code>global_mean</code> <code>bool</code> <p>All token losses are summed and divided by total token count. Examples with more tokens (longer sequences) implicitly contribute more to the loss. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The calculated loss batch tensor.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def loss(\n    self,\n    logits: Tensor,\n    target: Tensor,\n    xt: Tensor,\n    time: Tensor,\n    mask: Optional[Tensor] = None,\n    use_weight=True,\n    global_mean: bool = False,\n):\n    \"\"\"Calculate the cross-entropy loss between the model prediction and the target output.\n\n    The loss is calculated between the batch x node x class logits and the target batch x node,\n    considering the current state of the discrete sequence `xt` at time `time`.\n\n    If `use_weight` is True, the loss is weighted by the reduced form of the MDLM time weight for continuous NELBO,\n    as specified in equation 11 of https://arxiv.org/pdf/2406.07524. This weight is proportional to the derivative\n    of the noise schedule with respect to time, and is used to emphasize the importance of accurate predictions at\n    certain times in the diffusion process.\n\n    Args:\n        logits (Tensor): The predicted output from the model, with shape batch x node x class.\n        target (Tensor): The target output for the model prediction, with shape batch x node.\n        xt (Tensor): The current state of the discrete sequence, with shape batch x node.\n        time (Tensor): The time at which the loss is calculated.\n        mask (Optional[Tensor], optional): The mask for the data point. Defaults to None.\n        use_weight (bool, optional): Whether to use the MDLM time weight for the loss. Defaults to True.\n        global_mean (bool, optional): All token losses are summed and divided by total token count. Examples with more tokens (longer sequences) implicitly contribute more to the loss. Defaults to False.\n\n    Returns:\n        Tensor: The calculated loss batch tensor.\n    \"\"\"\n    logprobs = self._subs_parameterization(logits, xt)\n    log_p_theta = torch.gather(input=logprobs, dim=-1, index=target[..., None]).squeeze(-1)\n\n    sigma = self.noise_schedule.calculate_sigma(time, target.device)\n    dsigma = self.noise_schedule.d_dt_sigma(time, target.device)  # type: ignore\n    loss = -log_p_theta\n    if use_weight:\n        loss = loss * (dsigma / torch.expm1(sigma))[:, None]\n\n    if global_mean:\n        if mask is not None:\n            loss = loss * mask\n            loss = loss.sum() / mask.sum()\n        else:\n            loss = loss.sum() / logits.size(1)\n    else:\n        if mask is not None:\n            loss = loss * mask\n            num_non_masked_elements = torch.sum(mask, dim=-1)\n            loss = torch.sum(loss, dim=(-1)) / num_non_masked_elements\n        else:\n            loss = torch.sum(loss, dim=(-1)) / logits.size(1)\n    return loss\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM.step","title":"<code>step(logits, t, xt, dt, temperature=1.0)</code>","text":"<p>Perform a single step of MDLM DDPM step.</p> <p>Parameters: logits (Tensor): The input logits. t (Tensor): The current time step. xt (Tensor): The current state. dt (Tensor): The time step increment. temperature (float): Softmax temperature defaults to 1.0.</p> <p>Returns: Tensor: The updated state.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def step(self, logits: Tensor, t: Tensor, xt: Tensor, dt: Tensor, temperature: float = 1.0) -&gt; Tensor:\n    \"\"\"Perform a single step of MDLM DDPM step.\n\n    Parameters:\n    logits (Tensor): The input logits.\n    t (Tensor): The current time step.\n    xt (Tensor): The current state.\n    dt (Tensor): The time step increment.\n    temperature (float): Softmax temperature defaults to 1.0.\n\n    Returns:\n    Tensor: The updated state.\n    \"\"\"\n    sigma_t = self.noise_schedule.calculate_sigma(t, logits.device)\n    sigma_s = self.noise_schedule.calculate_sigma(t - dt, logits.device)\n    alpha_t = torch.exp(-sigma_t)\n    alpha_s = torch.exp(-sigma_s)\n    p_mask_s = 1 - alpha_s\n    alpha_t = pad_like(alpha_t, logits)\n    alpha_s = pad_like(alpha_s, logits)\n    p_mask_s = pad_like(p_mask_s, logits)\n    # Apply subs parameterization\n    log_p_x0 = self._subs_parameterization(logits, xt) / temperature\n    if p_mask_s.ndim != log_p_x0.ndim:\n        raise ValueError(f\"Dimension Mistmatch {p_mask_s.shape} {log_p_x0.shape}\")\n    # Equation 7 from MDLM\n    prob_s_given_t = log_p_x0.exp() * (\n        alpha_s - alpha_t\n    )  # righthand side (alpha_s - alpha_t)*x = (1 - alpha_t - (1 - alpha_s)) * x\n    prob_s_given_t[..., self.mask_index] = p_mask_s[..., 0]  # lefthand side (1 - alpha_s)*M\n    sampled_x = self._sample_categorical(prob_s_given_t)\n    carry_over_unmask = (xt != self.mask_index).to(xt.dtype)\n    return carry_over_unmask * xt + (1 - carry_over_unmask) * sampled_x\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM.step_argmax","title":"<code>step_argmax(model_out)</code>","text":"<p>Returns the index of the maximum value in the last dimension of the model output.</p> <p>Parameters:</p> Name Type Description Default <code>model_out</code> <code>Tensor</code> <p>The output of the model.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The index of the maximum value in the last dimension of the model output.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def step_argmax(self, model_out: Tensor):\n    \"\"\"Returns the index of the maximum value in the last dimension of the model output.\n\n    Args:\n        model_out (Tensor): The output of the model.\n\n    Returns:\n        Tensor: The index of the maximum value in the last dimension of the model output.\n    \"\"\"\n    return model_out.argmax(dim=-1)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM.step_auto_regressive","title":"<code>step_auto_regressive(logits, xt, logit_temperature=1.0)</code>","text":"<p>Auto-regressive sampling from MDLM.</p> <p>This method samples from the predicted logits and replaces the next token in the sequence. The next token is chosen based on the predicted logits and the current state of the sequence.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def step_auto_regressive(\n    self,\n    logits: Tensor,\n    xt: Tensor,\n    logit_temperature: float = 1.0,\n):\n    \"\"\"Auto-regressive sampling from MDLM.\n\n    This method samples from the predicted logits and replaces the next token in the sequence.\n    The next token is chosen based on the predicted logits and the current state of the sequence.\n    \"\"\"\n    xt = xt.clone()\n    log_p_x0 = self._subs_parameterization(logits, xt)\n    # sample the code from the softmax prediction\n    probs = torch.softmax(log_p_x0 / logit_temperature, dim=-1)\n    preds = torch.distributions.Categorical(probs=probs).sample()\n\n    # do not predict on already predicted tokens\n    mask = xt == self.mask_index\n\n    next_idx = torch.where(mask)[1][0]\n    to_replace = torch.zeros_like(xt)\n    to_replace[:, next_idx] = 1\n    to_replace = (mask.float() * to_replace.float()).bool()\n\n    xt[to_replace] = preds[to_replace]\n    return xt\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM.step_confidence","title":"<code>step_confidence(logits, xt, curr_step, num_steps, logit_temperature=1.0, randomness=1.0, confidence_temperature=1.0, num_tokens_unmask=1)</code>","text":"<p>Update the input sequence xt by sampling from the predicted logits and adding Gumbel noise.</p> <p>Method taken from GenMol Lee et al. https://arxiv.org/abs/2501.06158</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Predicted logits</p> required <code>xt</code> <code>Tensor</code> <p>Input sequence</p> required <code>curr_step</code> <code>int</code> <p>Current step</p> required <code>num_steps</code> <code>int</code> <p>Total number of steps</p> required <code>logit_temperature</code> <code>float</code> <p>Temperature for softmax over logits</p> <code>1.0</code> <code>randomness</code> <code>float</code> <p>Scale for Gumbel noise</p> <code>1.0</code> <code>confidence_temperature</code> <code>float</code> <p>Temperature for Gumbel confidence</p> <code>1.0</code> <code>num_tokens_unmask</code> <code>int</code> <p>number of tokens to unmask each step</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Updated input sequence xt unmasking num_tokens_unmask token each step.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def step_confidence(\n    self,\n    logits: Tensor,\n    xt: Tensor,\n    curr_step: int,\n    num_steps: int,\n    logit_temperature: float = 1.0,\n    randomness: float = 1.0,\n    confidence_temperature: float = 1.0,\n    num_tokens_unmask: int = 1,\n) -&gt; Tensor:\n    \"\"\"Update the input sequence xt by sampling from the predicted logits and adding Gumbel noise.\n\n    Method taken from GenMol Lee et al. https://arxiv.org/abs/2501.06158\n\n    Args:\n        logits: Predicted logits\n        xt: Input sequence\n        curr_step: Current step\n        num_steps: Total number of steps\n        logit_temperature: Temperature for softmax over logits\n        randomness: Scale for Gumbel noise\n        confidence_temperature: Temperature for Gumbel confidence\n        num_tokens_unmask: number of tokens to unmask each step\n\n    Returns:\n        Updated input sequence xt unmasking num_tokens_unmask token each step.\n    \"\"\"\n    if xt.ndim &gt; 3:\n        raise NotImplementedError(\n            \"step_confidence is implemented for Batch x Sequence x State Space shaped tensors.\"\n        )\n    if curr_step &lt; 0 or num_steps &lt; 1 or num_tokens_unmask &lt; 1:\n        raise ValueError(\"Invalid input values for curr_step, num_steps, or num_tokens_unmask.\")\n    xt = xt.clone()\n    log_p_x0 = self._subs_parameterization(logits, xt)\n    # sample the code from the softmax prediction\n    probs = torch.softmax(log_p_x0 / logit_temperature, dim=-1)\n    preds = torch.distributions.Categorical(probs=probs).sample()\n\n    confidence = probs.gather(-1, preds.unsqueeze(-1)).squeeze(-1)\n    # add Gumbel noise decreasing over the sampling process\n    ratio = curr_step / (num_steps - 1)\n    # Using manual definition of 0,1 Gumbel to pass in generator, manually specifying the device is faster than transfer\n    gumbel_sample = -torch.log(\n        -torch.log(torch.rand(xt.shape, device=logits.device, generator=self.rng_generator))\n    )\n    # gumbel_sample = self.gumbel_dist.sample(xt.shape).to(logits.device)\n    gumbel_noise = gumbel_sample * randomness * (1 - ratio)  # type: ignore\n    confidence = (\n        (torch.log(confidence) + gumbel_noise) / confidence_temperature\n    )  # stems from tau of https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#gumbel_softmax\n\n    # do not predict on already predicted tokens\n    mask = xt == self.mask_index\n    confidence[~mask] = -torch.inf\n\n    # choose the predicted token with the highest confidence\n    confidence_threshold, idx_mask = torch.topk(confidence, k=num_tokens_unmask, dim=-1)\n    confidence_threshold = confidence_threshold[:, -1].unsqueeze(-1)\n    # Rather than take all tokens with confidence above the threshold, we use the topk indices to replace the chosen tokens\n    # replace the chosen tokens\n    to_replace = torch.zeros_like(confidence)\n    to_replace.scatter_(1, idx_mask, 1)\n    to_replace = to_replace.bool() &amp; mask.bool()\n    # to_replace = confidence &gt;= confidence_threshold\n    # to_replace = (mask.float() * to_replace.float()).bool()\n    xt[to_replace] = preds[to_replace]\n    return xt\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM.step_confidence_margin","title":"<code>step_confidence_margin(logits, xt, curr_step, num_steps, logit_temperature=1.0, randomness=1.0, confidence_temperature=1.0, num_tokens_unmask=1)</code>","text":"<p>Kim et al., Train for the Worst, Plan for the Best: Understanding Token Ordering in Masked Diffusions, ICML, 2025.</p> <p>This method is similar to step_confidence, but it uses the margin of the confidence scores to replace the tokens. The margin is the difference between the confidence score and the next highest confidence score. The tokens with the highest margin are replaced.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Predicted logits</p> required <code>xt</code> <code>Tensor</code> <p>Input sequence</p> required <code>curr_step</code> <code>int</code> <p>Current step</p> required <code>num_steps</code> <code>int</code> <p>Total number of steps</p> required <code>logit_temperature</code> <code>float</code> <p>Temperature for softmax over logits</p> <code>1.0</code> <code>randomness</code> <code>float</code> <p>Scale for Gumbel noise</p> <code>1.0</code> <code>confidence_temperature</code> <code>float</code> <p>Temperature for Gumbel confidence</p> <code>1.0</code> <code>num_tokens_unmask</code> <code>int</code> <p>number of tokens to unmask each step</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Updated input sequence xt unmasking num_tokens_unmask token each step.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def step_confidence_margin(\n    self,\n    logits: Tensor,\n    xt: Tensor,\n    curr_step: int,\n    num_steps: int,\n    logit_temperature: float = 1.0,\n    randomness: float = 1.0,\n    confidence_temperature: float = 1.0,\n    num_tokens_unmask: int = 1,\n) -&gt; Tensor:\n    \"\"\"Kim et al., Train for the Worst, Plan for the Best: Understanding Token Ordering in Masked Diffusions, ICML, 2025.\n\n    This method is similar to step_confidence, but it uses the margin of the confidence scores to replace the tokens.\n    The margin is the difference between the confidence score and the next highest confidence score.\n    The tokens with the highest margin are replaced.\n\n    Args:\n        logits: Predicted logits\n        xt: Input sequence\n        curr_step: Current step\n        num_steps: Total number of steps\n        logit_temperature: Temperature for softmax over logits\n        randomness: Scale for Gumbel noise\n        confidence_temperature: Temperature for Gumbel confidence\n        num_tokens_unmask: number of tokens to unmask each step\n\n    Returns:\n        Updated input sequence xt unmasking num_tokens_unmask token each step.\n    \"\"\"\n    if xt.ndim &gt; 3:\n        raise NotImplementedError(\n            \"step_confidence is implemented for Batch x Sequence x State Space shaped tensors.\"\n        )\n    if curr_step &lt; 0 or num_steps &lt; 1 or num_tokens_unmask &lt; 1:\n        raise ValueError(\"Invalid input values for curr_step, num_steps, or num_tokens_unmask.\")\n    xt = xt.clone()\n    log_p_x0 = self._subs_parameterization(logits, xt)\n    # sample the code from the softmax prediction\n    probs = torch.softmax(log_p_x0 / logit_temperature, dim=-1)\n    preds = torch.stack([torch.multinomial(prob, num_samples=2, replacement=False) for prob in probs])\n    confidence_first = probs.gather(-1, preds[:, :, 0].unsqueeze(-1)).squeeze(-1)\n    confidence_second = probs.gather(-1, preds[:, :, 1].unsqueeze(-1)).squeeze(-1)\n    confidence = confidence_first - confidence_second\n    preds = preds[:, :, 0]\n    # add Gumbel noise decreasing over the sampling process\n    ratio = curr_step / (num_steps - 1)\n    # Using manual definition of 0,1 Gumbel to pass in generator, manually specifying the device is faster than transfer\n    gumbel_sample = -torch.log(\n        -torch.log(torch.rand(xt.shape, device=logits.device, generator=self.rng_generator))\n    )\n    # gumbel_sample = self.gumbel_dist.sample(xt.shape).to(logits.device)\n    gumbel_noise = gumbel_sample * randomness * (1 - ratio)  # type: ignore\n    confidence = (\n        (torch.log(confidence) + gumbel_noise) / confidence_temperature\n    )  # stems from tau of https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#gumbel_softmax\n\n    # do not predict on already predicted tokens\n    mask = xt == self.mask_index\n    confidence[~mask] = -torch.inf\n\n    # choose the predicted token with the highest confidence\n    confidence_threshold, idx_mask = torch.topk(confidence, k=num_tokens_unmask, dim=-1)\n    confidence_threshold = confidence_threshold[:, -1].unsqueeze(-1)\n    # Rather than take all tokens with confidence above the threshold, we use the topk indices to replace the chosen tokens\n    # replace the chosen tokens\n    to_replace = torch.zeros_like(confidence)\n    to_replace.scatter_(1, idx_mask, 1)\n    to_replace = to_replace.bool() &amp; mask.bool()\n    # to_replace = confidence &gt;= confidence_threshold\n    # to_replace = (mask.float() * to_replace.float()).bool()\n    xt[to_replace] = preds[to_replace]\n    return xt\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM.step_self_path_planning","title":"<code>step_self_path_planning(logits, xt, t, curr_step, num_steps, logit_temperature=1.0, randomness=1.0, confidence_temperature=1.0, score_type='confidence', fix_mask=None)</code>","text":"<p>Self Path Planning (P2) Sampling from Peng et al. https://arxiv.org/html/2502.03540v1.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Predicted logits for sampling.</p> required <code>xt</code> <code>Tensor</code> <p>Input sequence to be updated.</p> required <code>t</code> <code>Tensor</code> <p>Time tensor (e.g., time steps or temporal info).</p> required <code>curr_step</code> <code>int</code> <p>Current iteration in the planning process.</p> required <code>num_steps</code> <code>int</code> <p>Total number of planning steps.</p> required <code>logit_temperature</code> <code>float</code> <p>Temperature for logits (default: 1.0).</p> <code>1.0</code> <code>randomness</code> <code>float</code> <p>Introduced randomness level (default: 1.0).</p> <code>1.0</code> <code>confidence_temperature</code> <code>float</code> <p>Temperature for confidence scoring (default: 1.0).</p> <code>1.0</code> <code>score_type</code> <code>Literal['confidence', 'random']</code> <p>Sampling score type (default: \"confidence\").</p> <code>'confidence'</code> <code>fix_mask</code> <code>Optional[Tensor]</code> <p>inital mask where True when not a mask tokens (default: None).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Updated input sequence xt after iterative unmasking.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def step_self_path_planning(\n    self,\n    logits: Tensor,\n    xt: Tensor,\n    t: Tensor,\n    curr_step: int,\n    num_steps: int,\n    logit_temperature: float = 1.0,\n    randomness: float = 1.0,\n    confidence_temperature: float = 1.0,\n    score_type: Literal[\"confidence\", \"random\"] = \"confidence\",\n    fix_mask: Optional[Tensor] = None,\n) -&gt; Tensor:\n    \"\"\"Self Path Planning (P2) Sampling from Peng et al. https://arxiv.org/html/2502.03540v1.\n\n    Args:\n        logits (Tensor): Predicted logits for sampling.\n        xt (Tensor): Input sequence to be updated.\n        t (Tensor): Time tensor (e.g., time steps or temporal info).\n        curr_step (int): Current iteration in the planning process.\n        num_steps (int): Total number of planning steps.\n        logit_temperature (float): Temperature for logits (default: 1.0).\n        randomness (float): Introduced randomness level (default: 1.0).\n        confidence_temperature (float): Temperature for confidence scoring (default: 1.0).\n        score_type (Literal[\"confidence\", \"random\"]): Sampling score type (default: \"confidence\").\n        fix_mask (Optional[Tensor]): inital mask where True when not a mask tokens (default: None).\n\n    Returns:\n        Tensor: Updated input sequence xt after iterative unmasking.\n    \"\"\"\n    if xt.ndim &gt; 3:\n        raise NotImplementedError(\n            \"step_confidence is implemented for Batch x Sequence x State Space shaped tensors.\"\n        )\n    if curr_step &lt; 0 or num_steps &lt; 1:\n        raise ValueError(\"Invalid input values for curr_step, num_steps.\")\n    xt = xt.clone()\n    if fix_mask is None:\n        fix_mask = torch.zeros_like(xt).bool()  #! if any sequenes are fixed from the start of trajectory\n    last_mask = xt == self.mask_index\n    unmask_candidates = (\n        last_mask &amp; ~fix_mask\n    )  #! I want to consider tokens to un mask that are currently masked and not fixed. This fixes a typo in pseudo code\n    x1_pred, logp = self.stochastic_sample_from_categorical(\n        logits, temperature=logit_temperature, noise_scale=confidence_temperature\n    )\n    if curr_step == num_steps - 1:\n        xt[last_mask] = x1_pred[last_mask]\n    else:\n        if score_type == \"confidence\":\n            score = logp\n        elif score_type == \"random\":\n            score = torch.rand_like(logp).log()\n\n        score = score.masked_fill(fix_mask.squeeze(-1), float(\"inf\"))\n        score[unmask_candidates.squeeze(-1)] *= randomness\n        num_to_mask = torch.clamp(\n            ((~fix_mask).sum(dim=1, keepdim=True).float() * t.unsqueeze(-1)).long(), max=xt.shape[-1] - 1\n        )  #! here is is t since diffusion time is 1 to 0. Clamp is to set it to 0 N-1 since topk uses it as indices\n        mask = self.topk_lowest_masking(score, num_to_mask)\n        xt[mask] = self.mask_index\n        mask_to_x1 = last_mask &amp; ~mask\n        xt[mask_to_x1] = x1_pred[mask_to_x1]\n    return xt\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM.stochastic_sample_from_categorical","title":"<code>stochastic_sample_from_categorical(logits, temperature=1.0, noise_scale=1.0)</code>","text":"<p>Stochastically samples from a categorical distribution defined by input logits, with optional temperature and noise scaling for diverse sampling.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Input logits tensor with shape (... , num_categories)</p> required <code>temperature</code> <code>float</code> <p>Softmax temperature. Higher values produce more uniform samples. Defaults to 1.0.</p> <code>1.0</code> <code>noise_scale</code> <code>float</code> <p>Scale for Gumbel noise. Higher values produce more diverse samples. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>tuple</code> <ul> <li>tokens (LongTensor): Sampling result (category indices) with shape (... , )</li> <li>scores (Tensor): Corresponding log-softmax scores for the sampled tokens, with shape (... , )</li> </ul> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def stochastic_sample_from_categorical(self, logits: Tensor, temperature: float = 1.0, noise_scale: float = 1.0):\n    \"\"\"Stochastically samples from a categorical distribution defined by input logits, with optional temperature and noise scaling for diverse sampling.\n\n    Args:\n        logits (Tensor): Input logits tensor with shape (... , num_categories)\n        temperature (float, optional): Softmax temperature. Higher values produce more uniform samples. Defaults to 1.0.\n        noise_scale (float, optional): Scale for Gumbel noise. Higher values produce more diverse samples. Defaults to 1.0.\n\n    Returns:\n        tuple:\n            - **tokens** (LongTensor): Sampling result (category indices) with shape (... , )\n            - **scores** (Tensor): Corresponding log-softmax scores for the sampled tokens, with shape (... , )\n    \"\"\"\n    if temperature &gt; 0:\n        gumbel = -torch.log(\n            -torch.log(torch.rand(logits.shape, device=logits.device, generator=self.rng_generator) + 1e-8) + 1e-8\n        )  #! avoid device transfers\n        logits = logits / temperature + noise_scale * gumbel\n    scores, tokens = logits.log_softmax(dim=-1).max(dim=-1)\n    return tokens, scores\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM.topk_lowest_masking","title":"<code>topk_lowest_masking(scores, cutoff_len)</code>","text":"<p>Generates a mask for the lowest scoring elements up to a specified cutoff length.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>Tensor</code> <p>Input scores tensor with shape (... , num_elements)</p> required <code>cutoff_len</code> <code>Tensor</code> <p>Number of lowest-scoring elements to mask (per batch element)</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <p>Boolean mask tensor with same shape as <code>scores</code>, where <code>True</code> indicates     the corresponding element is among the <code>cutoff_len</code> lowest scores.</p> Example <p>scores = torch.tensor([[0.9, 0.8, 0.1, 0.05], [0.7, 0.4, 0.3, 0.2]]) cutoff_len = 2 mask = topk_lowest_masking(scores, cutoff_len) print(mask) tensor([[False, False, True, True],         [False, True, True, False]])</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def topk_lowest_masking(self, scores: Tensor, cutoff_len: Tensor):\n    \"\"\"Generates a mask for the lowest scoring elements up to a specified cutoff length.\n\n    Args:\n        scores (Tensor): Input scores tensor with shape (... , num_elements)\n        cutoff_len (Tensor): Number of lowest-scoring elements to mask (per batch element)\n\n    Returns:\n        Tensor: Boolean mask tensor with same shape as `scores`, where `True` indicates\n                the corresponding element is among the `cutoff_len` lowest scores.\n\n    Example:\n        &gt;&gt;&gt; scores = torch.tensor([[0.9, 0.8, 0.1, 0.05], [0.7, 0.4, 0.3, 0.2]])\n        &gt;&gt;&gt; cutoff_len = 2\n        &gt;&gt;&gt; mask = topk_lowest_masking(scores, cutoff_len)\n        &gt;&gt;&gt; print(mask)\n        tensor([[False, False, True, True],\n                [False, True, True, False]])\n    \"\"\"\n    sorted_scores, _ = scores.sort(dim=-1)\n    threshold = sorted_scores.gather(dim=-1, index=cutoff_len)\n    return scores &lt; threshold\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/utils/","title":"Utils","text":""},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/utils/#bionemo.moco.interpolants.discrete_time.utils.safe_index","title":"<code>safe_index(tensor, index, device)</code>","text":"<p>Safely indexes a tensor using a given index and returns the result on a specified device.</p> <p>Note can implement forcing with  return tensor[index.to(tensor.device)].to(device) but has costly migration.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to be indexed.</p> required <code>index</code> <code>Tensor</code> <p>The index to use for indexing the tensor.</p> required <code>device</code> <code>device</code> <p>The device on which the result should be returned.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The indexed tensor on the specified device.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tensor, index are not all on the same device.</p> Source code in <code>bionemo/moco/interpolants/discrete_time/utils.py</code> <pre><code>def safe_index(tensor: Tensor, index: Tensor, device: Optional[torch.device]):\n    \"\"\"Safely indexes a tensor using a given index and returns the result on a specified device.\n\n    Note can implement forcing with  return tensor[index.to(tensor.device)].to(device) but has costly migration.\n\n    Args:\n        tensor (Tensor): The tensor to be indexed.\n        index (Tensor): The index to use for indexing the tensor.\n        device (torch.device): The device on which the result should be returned.\n\n    Returns:\n        Tensor: The indexed tensor on the specified device.\n\n    Raises:\n        ValueError: If tensor, index are not all on the same device.\n    \"\"\"\n    if not (tensor.device == index.device):\n        raise ValueError(\n            f\"Tensor, index, and device must all be on the same device. \"\n            f\"Got tensor.device={tensor.device}, index.device={index.device}, and device={device}.\"\n        )\n\n    return tensor[index].to(device)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/","title":"Ddpm","text":""},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM","title":"<code>DDPM</code>","text":"<p>               Bases: <code>Interpolant</code></p> <p>A Denoising Diffusion Probabilistic Model (DDPM) interpolant.</p> <p>Examples: <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.moco.distributions.prior.continuous.gaussian import GaussianPrior\n&gt;&gt;&gt; from bionemo.moco.distributions.time.uniform import UniformTimeDistribution\n&gt;&gt;&gt; from bionemo.moco.interpolants.discrete_time.continuous.ddpm import DDPM\n&gt;&gt;&gt; from bionemo.moco.schedules.noise.discrete_noise_schedules import DiscreteCosineNoiseSchedule\n&gt;&gt;&gt; from bionemo.moco.schedules.inference_time_schedules import DiscreteLinearInferenceSchedule\n\n\nddpm = DDPM(\n    time_distribution = UniformTimeDistribution(discrete_time = True,...),\n    prior_distribution = GaussianPrior(...),\n    noise_schedule = DiscreteCosineNoiseSchedule(...),\n    )\nmodel = Model(...)\n\n# Training\nfor epoch in range(1000):\n    data = data_loader.get(...)\n    time = ddpm.sample_time(batch_size)\n    noise = ddpm.sample_prior(data.shape)\n    xt = ddpm.interpolate(data, noise, time)\n\n    x_pred = model(xt, time)\n    loss = ddpm.loss(x_pred, data, time)\n    loss.backward()\n\n# Generation\nx_pred = ddpm.sample_prior(data.shape)\nfor t in DiscreteLinearTimeSchedule(...).generate_schedule():\n    time = torch.full((batch_size,), t)\n    x_hat = model(x_pred, time)\n    x_pred = ddpm.step(x_hat, time, x_pred)\nreturn x_pred\n</code></pre></p> Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>class DDPM(Interpolant):\n    \"\"\"A Denoising Diffusion Probabilistic Model (DDPM) interpolant.\n\n     -------\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from bionemo.moco.distributions.prior.continuous.gaussian import GaussianPrior\n    &gt;&gt;&gt; from bionemo.moco.distributions.time.uniform import UniformTimeDistribution\n    &gt;&gt;&gt; from bionemo.moco.interpolants.discrete_time.continuous.ddpm import DDPM\n    &gt;&gt;&gt; from bionemo.moco.schedules.noise.discrete_noise_schedules import DiscreteCosineNoiseSchedule\n    &gt;&gt;&gt; from bionemo.moco.schedules.inference_time_schedules import DiscreteLinearInferenceSchedule\n\n\n    ddpm = DDPM(\n        time_distribution = UniformTimeDistribution(discrete_time = True,...),\n        prior_distribution = GaussianPrior(...),\n        noise_schedule = DiscreteCosineNoiseSchedule(...),\n        )\n    model = Model(...)\n\n    # Training\n    for epoch in range(1000):\n        data = data_loader.get(...)\n        time = ddpm.sample_time(batch_size)\n        noise = ddpm.sample_prior(data.shape)\n        xt = ddpm.interpolate(data, noise, time)\n\n        x_pred = model(xt, time)\n        loss = ddpm.loss(x_pred, data, time)\n        loss.backward()\n\n    # Generation\n    x_pred = ddpm.sample_prior(data.shape)\n    for t in DiscreteLinearTimeSchedule(...).generate_schedule():\n        time = torch.full((batch_size,), t)\n        x_hat = model(x_pred, time)\n        x_pred = ddpm.step(x_hat, time, x_pred)\n    return x_pred\n\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        time_distribution: TimeDistribution,\n        prior_distribution: PriorDistribution,\n        noise_schedule: DiscreteNoiseSchedule,\n        prediction_type: Union[PredictionType, str] = PredictionType.DATA,\n        device: Union[str, torch.device] = \"cpu\",\n        last_time_idx: int = 0,\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Initializes the DDPM interpolant.\n\n        Args:\n            time_distribution (TimeDistribution): The distribution of time steps, used to sample time points for the diffusion process.\n            prior_distribution (PriorDistribution): The prior distribution of the variable, used as the starting point for the diffusion process.\n            noise_schedule (DiscreteNoiseSchedule): The schedule of noise, defining the amount of noise added at each time step.\n            prediction_type (PredictionType): The type of prediction, either \"data\" or another type. Defaults to \"data\".\n            device (str): The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".\n            last_time_idx (int, optional): The last time index for discrete time. Set to 0 if discrete time is T-1, ..., 0 or 1 if T, ..., 1. Defaults to 0.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n        \"\"\"\n        super().__init__(time_distribution, prior_distribution, device, rng_generator)\n        if not isinstance(prior_distribution, GaussianPrior):\n            warnings.warn(\"Prior distribution is not a GaussianPrior, unexpected behavior may occur\")\n        self.noise_schedule = noise_schedule\n        self._initialize_schedules(device)\n        self.prediction_type = string_to_enum(prediction_type, PredictionType)\n        self._loss_function = nn.MSELoss(reduction=\"none\")\n        self.last_time_idx = last_time_idx\n\n    def _initialize_schedules(self, device: Union[str, torch.device] = \"cpu\"):\n        \"\"\"Sets up the Denoising Diffusion Probabilistic Model (DDPM) equations.\n\n        This method initializes the schedules for the forward and reverse processes of the DDPM. It calculates the\n        alphas, betas, and log variances required for the diffusion process.\n\n        Specifically, it computes:\n\n        * `alpha_bar`: the cumulative product of `alpha_t`\n        * `alpha_bar_prev`: the previous cumulative product of `alpha_t`\n        * `posterior_variance`: the variance of the posterior distribution\n        * `posterior_mean_c0_coef` and `posterior_mean_ct_coef`: the coefficients for the posterior mean\n        * `log_var`: the log variance of the posterior distribution\n\n        These values are then used to set up the forward and reverse schedules for the DDPM.\n        Specifically this is equation (6) (7) from https://arxiv.org/pdf/2006.11239\n        \"\"\"\n        if self.noise_schedule is None:\n            raise ValueError(\"noise_schedule cannot be None for DDPM\")\n        alphas = self.noise_schedule.generate_schedule(device=device)\n        betas = 1 - alphas\n        log_alpha = torch.log(alphas)\n        log_alpha_bar = torch.cumsum(log_alpha, dim=0)\n        alpha_bar = alphas_cumprod = torch.exp(log_alpha_bar)\n        alpha_bar_prev = alphas_cumprod_prev = torch.nn.functional.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n        posterior_variance = betas * (1.0 - alpha_bar_prev) / (1.0 - alpha_bar)\n        posterior_mean_c0_coef = betas * torch.sqrt(alphas_cumprod_prev) / (1.0 - alpha_bar)\n        posterior_mean_ct_coef = (1.0 - alpha_bar_prev) * torch.sqrt(alphas) / (1.0 - alpha_bar)\n        # log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        posterior_logvar = torch.log(\n            torch.nn.functional.pad(posterior_variance[:-1], (1, 0), value=posterior_variance[0].item())\n        )\n        self._forward_data_schedule = torch.sqrt(alpha_bar)\n        self._forward_noise_schedule = torch.sqrt(1 - alpha_bar)\n        self._reverse_data_schedule = posterior_mean_c0_coef\n        self._reverse_noise_schedule = posterior_mean_ct_coef\n        self._log_var = posterior_logvar\n        self._alpha_bar = alpha_bar\n        self._alpha_bar_prev = alpha_bar_prev\n        self._betas = betas\n        self._posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n\n    @property\n    def forward_data_schedule(self) -&gt; torch.Tensor:\n        \"\"\"Returns the forward data schedule.\"\"\"\n        return self._forward_data_schedule\n\n    @property\n    def forward_noise_schedule(self) -&gt; torch.Tensor:\n        \"\"\"Returns the forward noise schedule.\"\"\"\n        return self._forward_noise_schedule\n\n    @property\n    def reverse_data_schedule(self) -&gt; torch.Tensor:\n        \"\"\"Returns the reverse data schedule.\"\"\"\n        return self._reverse_data_schedule\n\n    @property\n    def reverse_noise_schedule(self) -&gt; torch.Tensor:\n        \"\"\"Returns the reverse noise schedule.\"\"\"\n        return self._reverse_noise_schedule\n\n    @property\n    def log_var(self) -&gt; torch.Tensor:\n        \"\"\"Returns the log variance.\"\"\"\n        return self._log_var\n\n    @property\n    def alpha_bar(self) -&gt; torch.Tensor:\n        \"\"\"Returns the alpha bar values.\"\"\"\n        return self._alpha_bar\n\n    @property\n    def alpha_bar_prev(self) -&gt; torch.Tensor:\n        \"\"\"Returns the previous alpha bar values.\"\"\"\n        return self._alpha_bar_prev\n\n    def interpolate(self, data: Tensor, t: Tensor, noise: Tensor):\n        \"\"\"Get x(t) with given time t from noise and data.\n\n        Args:\n            data (Tensor): target\n            t (Tensor): time\n            noise (Tensor): noise from prior()\n        \"\"\"\n        psi = safe_index(self._forward_data_schedule, t - self.last_time_idx, data.device)\n        omega = safe_index(self._forward_noise_schedule, t - self.last_time_idx, data.device)\n        psi = pad_like(psi, data)\n        omega = pad_like(omega, data)\n        x_t = data * psi + noise * omega\n        return x_t\n\n    def forward_process(self, data: Tensor, t: Tensor, noise: Optional[Tensor] = None):\n        \"\"\"Get x(t) with given time t from noise and data.\n\n        Args:\n            data (Tensor): target\n            t (Tensor): time\n            noise (Tensor, optional): noise from prior(). Defaults to None.\n        \"\"\"\n        if noise is None:\n            noise = self.sample_prior(data.shape)\n        return self.interpolate(data, t, noise)\n\n    def process_data_prediction(self, model_output: Tensor, sample: Tensor, t: Tensor):\n        \"\"\"Converts the model output to a data prediction based on the prediction type.\n\n        This conversion stems from the Progressive Distillation for Fast Sampling of Diffusion Models https://arxiv.org/pdf/2202.00512.\n        Given the model output and the sample, we convert the output to a data prediction based on the prediction type.\n        The conversion formulas are as follows:\n        - For \"noise\" prediction type: `pred_data = (sample - noise_scale * model_output) / data_scale`\n        - For \"data\" prediction type: `pred_data = model_output`\n        - For \"v_prediction\" prediction type: `pred_data = data_scale * sample - noise_scale * model_output`\n\n        Args:\n            model_output (Tensor): The output of the model.\n            sample (Tensor): The input sample.\n            t (Tensor): The time step.\n\n        Returns:\n            The data prediction based on the prediction type.\n\n        Raises:\n            ValueError: If the prediction type is not one of \"noise\", \"data\", or \"v_prediction\".\n        \"\"\"\n        data_scale = safe_index(self._forward_data_schedule, t - self.last_time_idx, model_output.device)\n        noise_scale = safe_index(self._forward_noise_schedule, t - self.last_time_idx, model_output.device)\n        data_scale = pad_like(data_scale, model_output)\n        noise_scale = pad_like(noise_scale, model_output)\n        if self.prediction_type == PredictionType.NOISE:\n            pred_data = (sample - noise_scale * model_output) / data_scale\n        elif self.prediction_type == PredictionType.DATA:\n            pred_data = model_output\n        elif self.prediction_type == PredictionType.VELOCITY:\n            pred_data = data_scale * sample - noise_scale * model_output\n        else:\n            raise ValueError(\n                f\"prediction_type given as {self.prediction_type} must be one of PredictionType.NOISE, PredictionType.DATA or\"\n                f\" PredictionType.VELOCITY for DDPM.\"\n            )\n        return pred_data\n\n    def process_noise_prediction(self, model_output, sample, t):\n        \"\"\"Do the same as process_data_prediction but take the model output and convert to nosie.\n\n        Args:\n            model_output: The output of the model.\n            sample: The input sample.\n            t: The time step.\n\n        Returns:\n            The input as noise if the prediction type is \"noise\".\n\n        Raises:\n            ValueError: If the prediction type is not \"noise\".\n        \"\"\"\n        data_scale = safe_index(self._forward_data_schedule, t - self.last_time_idx, model_output.device)\n        noise_scale = safe_index(self._forward_noise_schedule, t - self.last_time_idx, model_output.device)\n        data_scale = pad_like(data_scale, model_output)\n        noise_scale = pad_like(noise_scale, model_output)\n        if self.prediction_type == PredictionType.NOISE:\n            pred_noise = model_output\n        elif self.prediction_type == PredictionType.DATA:\n            pred_noise = (sample - data_scale * model_output) / noise_scale\n        elif self.prediction_type == PredictionType.VELOCITY:\n            pred_data = data_scale * sample - noise_scale * model_output\n            pred_noise = (sample - data_scale * pred_data) / noise_scale\n        else:\n            raise ValueError(\n                f\"prediction_type given as {self.prediction_type} must be one of `noise`, `data` or\"\n                \" `v_prediction`  for DDPM.\"\n            )\n        return pred_noise\n\n    def calculate_velocity(self, data: Tensor, t: Tensor, noise: Tensor) -&gt; Tensor:\n        \"\"\"Calculate the velocity term given the data, time step, and noise.\n\n        Args:\n            data (Tensor): The input data.\n            t (Tensor): The current time step.\n            noise (Tensor): The noise term.\n\n        Returns:\n            Tensor: The calculated velocity term.\n        \"\"\"\n        data_scale = safe_index(self._forward_data_schedule, t - self.last_time_idx, data.device)\n        noise_scale = safe_index(self._forward_noise_schedule, t - self.last_time_idx, data.device)\n        data_scale = pad_like(data_scale, data)\n        noise_scale = pad_like(noise_scale, data)\n        v = data_scale * noise - noise_scale * data\n        return v\n\n    @torch.no_grad()\n    def step(\n        self,\n        model_out: Tensor,\n        t: Tensor,\n        xt: Tensor,\n        mask: Optional[Tensor] = None,\n        center: Bool = False,\n        temperature: Float = 1.0,\n    ):\n        \"\"\"Do one step integration.\n\n        Args:\n        model_out (Tensor): The output of the model.\n        t (Tensor): The current time step.\n        xt (Tensor): The current data point.\n        mask (Optional[Tensor], optional): An optional mask to apply to the data. Defaults to None.\n        center (bool, optional): Whether to center the data. Defaults to False.\n        temperature (Float, optional): The temperature parameter for low temperature sampling. Defaults to 1.0.\n\n        Note:\n        The temperature parameter controls the level of randomness in the sampling process. A temperature of 1.0 corresponds to standard diffusion sampling, while lower temperatures (e.g. 0.5, 0.2) result in less random and more deterministic samples. This can be useful for tasks that require more control over the generation process.\n\n        Note for discrete time we sample from [T-1, ..., 1, 0] for T steps so we sample t = 0 hence the mask.\n        For continuous time we start from [1, 1 -dt, ..., dt] for T steps where s = t - 1 when t = 0 i.e dt is then 0\n\n        \"\"\"\n        if mask is not None:\n            model_out = model_out * mask.unsqueeze(-1)\n        x_hat = self.process_data_prediction(model_out, xt, t)\n        psi_r = safe_index(self._reverse_data_schedule, t - self.last_time_idx, x_hat.device)\n        omega_r = safe_index(self._reverse_noise_schedule, t - self.last_time_idx, x_hat.device)\n        log_var = safe_index(self._log_var, t - self.last_time_idx, x_hat.device)  # self._log_var[t.long()]\n        nonzero_mask = (t &gt; self.last_time_idx).float()\n        psi_r = pad_like(psi_r, x_hat)\n        omega_r = pad_like(omega_r, x_hat)\n        log_var = pad_like(log_var, x_hat)\n        nonzero_mask = pad_like(nonzero_mask, x_hat)\n\n        mean = psi_r * x_hat + omega_r * xt\n        eps = torch.randn_like(mean).to(model_out.device)\n\n        x_next = mean + nonzero_mask * (0.5 * log_var).exp() * eps * temperature\n        x_next = self.clean_mask_center(x_next, mask, center)\n        return x_next\n\n    def step_noise(\n        self,\n        model_out: Tensor,\n        t: Tensor,\n        xt: Tensor,\n        mask: Optional[Tensor] = None,\n        center: Bool = False,\n        temperature: Float = 1.0,\n    ):\n        \"\"\"Do one step integration.\n\n        Args:\n        model_out (Tensor): The output of the model.\n        t (Tensor): The current time step.\n        xt (Tensor): The current data point.\n        mask (Optional[Tensor], optional): An optional mask to apply to the data. Defaults to None.\n        center (bool, optional): Whether to center the data. Defaults to False.\n        temperature (Float, optional): The temperature parameter for low temperature sampling. Defaults to 1.0.\n\n        Note:\n        The temperature parameter controls the level of randomness in the sampling process.\n        A temperature of 1.0 corresponds to standard diffusion sampling, while lower temperatures (e.g. 0.5, 0.2)\n        result in less random and more deterministic samples. This can be useful for tasks\n        that require more control over the generation process.\n\n        Note for discrete time we sample from [T-1, ..., 1, 0] for T steps so we sample t = 0 hence the mask.\n        For continuous time we start from [1, 1 -dt, ..., dt] for T steps where s = t - 1 when t = 0 i.e dt is then 0\n\n        \"\"\"\n        if mask is not None:\n            model_out = model_out * mask.unsqueeze(-1)\n        eps_hat = self.process_noise_prediction(model_out, xt, t)\n        beta_t = safe_index(self._betas, t - self.last_time_idx, model_out.device)\n        recip_sqrt_alpha_t = torch.sqrt(1 / (1 - beta_t))\n        eps_factor = (\n            safe_index(self._betas, t - self.last_time_idx, model_out.device)\n            / (1 - safe_index(self._alpha_bar, t - self.last_time_idx, model_out.device)).sqrt()\n        )\n        var = safe_index(self._posterior_variance, t - self.last_time_idx, model_out.device)  # self._log_var[t.long()]\n\n        nonzero_mask = (t &gt; self.last_time_idx).float()\n        nonzero_mask = pad_like(nonzero_mask, model_out)\n        eps_factor = pad_like(eps_factor, xt)\n        recip_sqrt_alpha_t = pad_like(recip_sqrt_alpha_t, xt)\n        var = pad_like(var, xt)\n\n        x_next = (\n            recip_sqrt_alpha_t * (xt - eps_factor * eps_hat)\n            + nonzero_mask * var.sqrt() * torch.randn_like(eps_hat).to(model_out.device) * temperature\n        )\n        x_next = self.clean_mask_center(x_next, mask, center)\n        return x_next\n\n    def score(self, x_hat: Tensor, xt: Tensor, t: Tensor):\n        \"\"\"Converts the data prediction to the estimated score function.\n\n        Args:\n            x_hat (Tensor): The predicted data point.\n            xt (Tensor): The current data point.\n            t (Tensor): The time step.\n\n        Returns:\n            The estimated score function.\n        \"\"\"\n        alpha = safe_index(self._forward_data_schedule, t - self.last_time_idx, x_hat.device)\n        beta = safe_index(self._forward_noise_schedule, t - self.last_time_idx, x_hat.device)\n        alpha = pad_like(alpha, x_hat)\n        beta = pad_like(beta, x_hat)\n        score = alpha * x_hat - xt\n        score = score / (beta * beta)\n        return score\n\n    def step_ddim(\n        self,\n        model_out: Tensor,\n        t: Tensor,\n        xt: Tensor,\n        mask: Optional[Tensor] = None,\n        eta: Float = 0.0,\n        center: Bool = False,\n    ):\n        \"\"\"Do one step of DDIM sampling.\n\n        Args:\n            model_out (Tensor): output of the model\n            t (Tensor): current time step\n            xt (Tensor): current data point\n            mask (Optional[Tensor], optional): mask for the data point. Defaults to None.\n            eta (Float, optional): DDIM sampling parameter. Defaults to 0.0.\n            center (Bool, optional): whether to center the data point. Defaults to False.\n        \"\"\"\n        if mask is not None:\n            model_out = model_out * mask.unsqueeze(-1)\n        data_pred = self.process_data_prediction(model_out, xt, t)\n        noise_pred = self.process_noise_prediction(model_out, xt, t)\n        eps = torch.randn_like(data_pred).to(model_out.device)\n        sigma = (\n            eta\n            * torch.sqrt((1 - self._alpha_bar_prev) / (1 - self._alpha_bar))\n            * torch.sqrt(1 - self._alpha_bar / self._alpha_bar_prev)\n        )\n        sigma_t = safe_index(sigma, t - self.last_time_idx, model_out.device)\n        psi_r = safe_index(torch.sqrt(self._alpha_bar_prev), t - self.last_time_idx, model_out.device)\n        omega_r = safe_index(torch.sqrt(1 - self._alpha_bar_prev - sigma**2), t - self.last_time_idx, model_out.device)\n        sigma_t = pad_like(sigma_t, model_out)\n        psi_r = pad_like(psi_r, model_out)\n        omega_r = pad_like(omega_r, model_out)\n        mean = data_pred * psi_r + omega_r * noise_pred\n        x_next = mean + sigma_t * eps\n        x_next = self.clean_mask_center(x_next, mask, center)\n        return x_next\n\n    def set_loss_weight_fn(self, fn):\n        \"\"\"Sets the loss_weight attribute of the instance to the given function.\n\n        Args:\n            fn: The function to set as the loss_weight attribute. This function should take three arguments: raw_loss, t, and weight_type.\n        \"\"\"\n        self.loss_weight = fn\n\n    def loss_weight(self, raw_loss: Tensor, t: Optional[Tensor], weight_type: str) -&gt; Tensor:\n        \"\"\"Calculates the weight for the loss based on the given weight type.\n\n        These data_to_noise loss weights is derived in Equation (9) of https://arxiv.org/pdf/2202.00512.\n\n        Args:\n            raw_loss (Tensor): The raw loss calculated from the model prediction and target.\n            t (Tensor): The time step.\n            weight_type (str): The type of weight to use. Can be \"ones\" or \"data_to_noise\" or \"noise_to_data\".\n\n        Returns:\n            Tensor: The weight for the loss.\n\n        Raises:\n            ValueError: If the weight type is not recognized.\n        \"\"\"\n        if weight_type == \"ones\":\n            schedule = torch.ones_like(raw_loss).to(raw_loss.device)\n        elif weight_type == \"data_to_noise\":\n            if t is None:\n                raise ValueError(\"Time cannot be None when using the data_to_noise loss weight\")\n            schedule = (safe_index(self._forward_data_schedule, t - self.last_time_idx, raw_loss.device) ** 2) / (\n                safe_index(self._forward_noise_schedule, t - self.last_time_idx, raw_loss.device) ** 2\n            )\n            schedule = pad_like(schedule, raw_loss)\n        elif weight_type == \"noise_to_data\":\n            if t is None:\n                raise ValueError(\"Time cannot be None when using the data_to_noise loss weight\")\n            schedule = (safe_index(self._forward_noise_schedule, t - self.last_time_idx, raw_loss.device) ** 2) / (\n                safe_index(self._forward_data_schedule, t - self.last_time_idx, raw_loss.device) ** 2\n            )\n            schedule = pad_like(schedule, raw_loss)\n        else:\n            raise ValueError(\"Invalid loss weight keyword\")\n        return schedule\n\n    def loss(\n        self,\n        model_pred: Tensor,\n        target: Tensor,\n        t: Optional[Tensor] = None,\n        mask: Optional[Tensor] = None,\n        weight_type: Literal[\"ones\", \"data_to_noise\", \"noise_to_data\"] = \"ones\",\n    ):\n        \"\"\"Calculate the loss given the model prediction, data sample, and time.\n\n        The default weight_type is \"ones\" meaning no change / multiplying by all ones.\n        data_to_noise is available to scale the data MSE loss into the appropriate loss that is theoretically equivalent\n        to noise prediction. noise_to_data is provided for a similar reason for completeness.\n\n        Args:\n            model_pred (Tensor): The predicted output from the model.\n            target (Tensor): The target output for the model prediction.\n            t (Tensor): The time at which the loss is calculated.\n            mask (Optional[Tensor], optional): The mask for the data point. Defaults to None.\n            weight_type (Literal[\"ones\", \"data_to_noise\", \"noise_to_data\"]): The type of weight to use for the loss. Defaults to \"ones\".\n\n        Returns:\n            Tensor: The calculated loss batch tensor.\n        \"\"\"\n        raw_loss = self._loss_function(model_pred, target)\n        if weight_type != \"ones\":\n            update_weight = self.loss_weight(raw_loss, t, weight_type)\n            loss = raw_loss * update_weight\n        else:\n            loss = raw_loss\n        if mask is not None:\n            loss = loss * mask.unsqueeze(-1)\n            n_elem = torch.sum(mask, dim=-1)\n            loss = torch.sum(loss, dim=tuple(range(1, raw_loss.ndim))) / n_elem\n        else:\n            loss = torch.sum(loss, dim=tuple(range(1, raw_loss.ndim))) / model_pred.size(1)\n        return loss\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.alpha_bar","title":"<code>alpha_bar</code>  <code>property</code>","text":"<p>Returns the alpha bar values.</p>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.alpha_bar_prev","title":"<code>alpha_bar_prev</code>  <code>property</code>","text":"<p>Returns the previous alpha bar values.</p>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.forward_data_schedule","title":"<code>forward_data_schedule</code>  <code>property</code>","text":"<p>Returns the forward data schedule.</p>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.forward_noise_schedule","title":"<code>forward_noise_schedule</code>  <code>property</code>","text":"<p>Returns the forward noise schedule.</p>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.log_var","title":"<code>log_var</code>  <code>property</code>","text":"<p>Returns the log variance.</p>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.reverse_data_schedule","title":"<code>reverse_data_schedule</code>  <code>property</code>","text":"<p>Returns the reverse data schedule.</p>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.reverse_noise_schedule","title":"<code>reverse_noise_schedule</code>  <code>property</code>","text":"<p>Returns the reverse noise schedule.</p>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.__init__","title":"<code>__init__(time_distribution, prior_distribution, noise_schedule, prediction_type=PredictionType.DATA, device='cpu', last_time_idx=0, rng_generator=None)</code>","text":"<p>Initializes the DDPM interpolant.</p> <p>Parameters:</p> Name Type Description Default <code>time_distribution</code> <code>TimeDistribution</code> <p>The distribution of time steps, used to sample time points for the diffusion process.</p> required <code>prior_distribution</code> <code>PriorDistribution</code> <p>The prior distribution of the variable, used as the starting point for the diffusion process.</p> required <code>noise_schedule</code> <code>DiscreteNoiseSchedule</code> <p>The schedule of noise, defining the amount of noise added at each time step.</p> required <code>prediction_type</code> <code>PredictionType</code> <p>The type of prediction, either \"data\" or another type. Defaults to \"data\".</p> <code>DATA</code> <code>device</code> <code>str</code> <p>The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".</p> <code>'cpu'</code> <code>last_time_idx</code> <code>int</code> <p>The last time index for discrete time. Set to 0 if discrete time is T-1, ..., 0 or 1 if T, ..., 1. Defaults to 0.</p> <code>0</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>def __init__(\n    self,\n    time_distribution: TimeDistribution,\n    prior_distribution: PriorDistribution,\n    noise_schedule: DiscreteNoiseSchedule,\n    prediction_type: Union[PredictionType, str] = PredictionType.DATA,\n    device: Union[str, torch.device] = \"cpu\",\n    last_time_idx: int = 0,\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Initializes the DDPM interpolant.\n\n    Args:\n        time_distribution (TimeDistribution): The distribution of time steps, used to sample time points for the diffusion process.\n        prior_distribution (PriorDistribution): The prior distribution of the variable, used as the starting point for the diffusion process.\n        noise_schedule (DiscreteNoiseSchedule): The schedule of noise, defining the amount of noise added at each time step.\n        prediction_type (PredictionType): The type of prediction, either \"data\" or another type. Defaults to \"data\".\n        device (str): The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".\n        last_time_idx (int, optional): The last time index for discrete time. Set to 0 if discrete time is T-1, ..., 0 or 1 if T, ..., 1. Defaults to 0.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n    \"\"\"\n    super().__init__(time_distribution, prior_distribution, device, rng_generator)\n    if not isinstance(prior_distribution, GaussianPrior):\n        warnings.warn(\"Prior distribution is not a GaussianPrior, unexpected behavior may occur\")\n    self.noise_schedule = noise_schedule\n    self._initialize_schedules(device)\n    self.prediction_type = string_to_enum(prediction_type, PredictionType)\n    self._loss_function = nn.MSELoss(reduction=\"none\")\n    self.last_time_idx = last_time_idx\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.calculate_velocity","title":"<code>calculate_velocity(data, t, noise)</code>","text":"<p>Calculate the velocity term given the data, time step, and noise.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>The input data.</p> required <code>t</code> <code>Tensor</code> <p>The current time step.</p> required <code>noise</code> <code>Tensor</code> <p>The noise term.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The calculated velocity term.</p> Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>def calculate_velocity(self, data: Tensor, t: Tensor, noise: Tensor) -&gt; Tensor:\n    \"\"\"Calculate the velocity term given the data, time step, and noise.\n\n    Args:\n        data (Tensor): The input data.\n        t (Tensor): The current time step.\n        noise (Tensor): The noise term.\n\n    Returns:\n        Tensor: The calculated velocity term.\n    \"\"\"\n    data_scale = safe_index(self._forward_data_schedule, t - self.last_time_idx, data.device)\n    noise_scale = safe_index(self._forward_noise_schedule, t - self.last_time_idx, data.device)\n    data_scale = pad_like(data_scale, data)\n    noise_scale = pad_like(noise_scale, data)\n    v = data_scale * noise - noise_scale * data\n    return v\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.forward_process","title":"<code>forward_process(data, t, noise=None)</code>","text":"<p>Get x(t) with given time t from noise and data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>target</p> required <code>t</code> <code>Tensor</code> <p>time</p> required <code>noise</code> <code>Tensor</code> <p>noise from prior(). Defaults to None.</p> <code>None</code> Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>def forward_process(self, data: Tensor, t: Tensor, noise: Optional[Tensor] = None):\n    \"\"\"Get x(t) with given time t from noise and data.\n\n    Args:\n        data (Tensor): target\n        t (Tensor): time\n        noise (Tensor, optional): noise from prior(). Defaults to None.\n    \"\"\"\n    if noise is None:\n        noise = self.sample_prior(data.shape)\n    return self.interpolate(data, t, noise)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.interpolate","title":"<code>interpolate(data, t, noise)</code>","text":"<p>Get x(t) with given time t from noise and data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>target</p> required <code>t</code> <code>Tensor</code> <p>time</p> required <code>noise</code> <code>Tensor</code> <p>noise from prior()</p> required Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>def interpolate(self, data: Tensor, t: Tensor, noise: Tensor):\n    \"\"\"Get x(t) with given time t from noise and data.\n\n    Args:\n        data (Tensor): target\n        t (Tensor): time\n        noise (Tensor): noise from prior()\n    \"\"\"\n    psi = safe_index(self._forward_data_schedule, t - self.last_time_idx, data.device)\n    omega = safe_index(self._forward_noise_schedule, t - self.last_time_idx, data.device)\n    psi = pad_like(psi, data)\n    omega = pad_like(omega, data)\n    x_t = data * psi + noise * omega\n    return x_t\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.loss","title":"<code>loss(model_pred, target, t=None, mask=None, weight_type='ones')</code>","text":"<p>Calculate the loss given the model prediction, data sample, and time.</p> <p>The default weight_type is \"ones\" meaning no change / multiplying by all ones. data_to_noise is available to scale the data MSE loss into the appropriate loss that is theoretically equivalent to noise prediction. noise_to_data is provided for a similar reason for completeness.</p> <p>Parameters:</p> Name Type Description Default <code>model_pred</code> <code>Tensor</code> <p>The predicted output from the model.</p> required <code>target</code> <code>Tensor</code> <p>The target output for the model prediction.</p> required <code>t</code> <code>Tensor</code> <p>The time at which the loss is calculated.</p> <code>None</code> <code>mask</code> <code>Optional[Tensor]</code> <p>The mask for the data point. Defaults to None.</p> <code>None</code> <code>weight_type</code> <code>Literal['ones', 'data_to_noise', 'noise_to_data']</code> <p>The type of weight to use for the loss. Defaults to \"ones\".</p> <code>'ones'</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The calculated loss batch tensor.</p> Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>def loss(\n    self,\n    model_pred: Tensor,\n    target: Tensor,\n    t: Optional[Tensor] = None,\n    mask: Optional[Tensor] = None,\n    weight_type: Literal[\"ones\", \"data_to_noise\", \"noise_to_data\"] = \"ones\",\n):\n    \"\"\"Calculate the loss given the model prediction, data sample, and time.\n\n    The default weight_type is \"ones\" meaning no change / multiplying by all ones.\n    data_to_noise is available to scale the data MSE loss into the appropriate loss that is theoretically equivalent\n    to noise prediction. noise_to_data is provided for a similar reason for completeness.\n\n    Args:\n        model_pred (Tensor): The predicted output from the model.\n        target (Tensor): The target output for the model prediction.\n        t (Tensor): The time at which the loss is calculated.\n        mask (Optional[Tensor], optional): The mask for the data point. Defaults to None.\n        weight_type (Literal[\"ones\", \"data_to_noise\", \"noise_to_data\"]): The type of weight to use for the loss. Defaults to \"ones\".\n\n    Returns:\n        Tensor: The calculated loss batch tensor.\n    \"\"\"\n    raw_loss = self._loss_function(model_pred, target)\n    if weight_type != \"ones\":\n        update_weight = self.loss_weight(raw_loss, t, weight_type)\n        loss = raw_loss * update_weight\n    else:\n        loss = raw_loss\n    if mask is not None:\n        loss = loss * mask.unsqueeze(-1)\n        n_elem = torch.sum(mask, dim=-1)\n        loss = torch.sum(loss, dim=tuple(range(1, raw_loss.ndim))) / n_elem\n    else:\n        loss = torch.sum(loss, dim=tuple(range(1, raw_loss.ndim))) / model_pred.size(1)\n    return loss\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.loss_weight","title":"<code>loss_weight(raw_loss, t, weight_type)</code>","text":"<p>Calculates the weight for the loss based on the given weight type.</p> <p>These data_to_noise loss weights is derived in Equation (9) of https://arxiv.org/pdf/2202.00512.</p> <p>Parameters:</p> Name Type Description Default <code>raw_loss</code> <code>Tensor</code> <p>The raw loss calculated from the model prediction and target.</p> required <code>t</code> <code>Tensor</code> <p>The time step.</p> required <code>weight_type</code> <code>str</code> <p>The type of weight to use. Can be \"ones\" or \"data_to_noise\" or \"noise_to_data\".</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The weight for the loss.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the weight type is not recognized.</p> Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>def loss_weight(self, raw_loss: Tensor, t: Optional[Tensor], weight_type: str) -&gt; Tensor:\n    \"\"\"Calculates the weight for the loss based on the given weight type.\n\n    These data_to_noise loss weights is derived in Equation (9) of https://arxiv.org/pdf/2202.00512.\n\n    Args:\n        raw_loss (Tensor): The raw loss calculated from the model prediction and target.\n        t (Tensor): The time step.\n        weight_type (str): The type of weight to use. Can be \"ones\" or \"data_to_noise\" or \"noise_to_data\".\n\n    Returns:\n        Tensor: The weight for the loss.\n\n    Raises:\n        ValueError: If the weight type is not recognized.\n    \"\"\"\n    if weight_type == \"ones\":\n        schedule = torch.ones_like(raw_loss).to(raw_loss.device)\n    elif weight_type == \"data_to_noise\":\n        if t is None:\n            raise ValueError(\"Time cannot be None when using the data_to_noise loss weight\")\n        schedule = (safe_index(self._forward_data_schedule, t - self.last_time_idx, raw_loss.device) ** 2) / (\n            safe_index(self._forward_noise_schedule, t - self.last_time_idx, raw_loss.device) ** 2\n        )\n        schedule = pad_like(schedule, raw_loss)\n    elif weight_type == \"noise_to_data\":\n        if t is None:\n            raise ValueError(\"Time cannot be None when using the data_to_noise loss weight\")\n        schedule = (safe_index(self._forward_noise_schedule, t - self.last_time_idx, raw_loss.device) ** 2) / (\n            safe_index(self._forward_data_schedule, t - self.last_time_idx, raw_loss.device) ** 2\n        )\n        schedule = pad_like(schedule, raw_loss)\n    else:\n        raise ValueError(\"Invalid loss weight keyword\")\n    return schedule\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.process_data_prediction","title":"<code>process_data_prediction(model_output, sample, t)</code>","text":"<p>Converts the model output to a data prediction based on the prediction type.</p> <p>This conversion stems from the Progressive Distillation for Fast Sampling of Diffusion Models https://arxiv.org/pdf/2202.00512. Given the model output and the sample, we convert the output to a data prediction based on the prediction type. The conversion formulas are as follows: - For \"noise\" prediction type: <code>pred_data = (sample - noise_scale * model_output) / data_scale</code> - For \"data\" prediction type: <code>pred_data = model_output</code> - For \"v_prediction\" prediction type: <code>pred_data = data_scale * sample - noise_scale * model_output</code></p> <p>Parameters:</p> Name Type Description Default <code>model_output</code> <code>Tensor</code> <p>The output of the model.</p> required <code>sample</code> <code>Tensor</code> <p>The input sample.</p> required <code>t</code> <code>Tensor</code> <p>The time step.</p> required <p>Returns:</p> Type Description <p>The data prediction based on the prediction type.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the prediction type is not one of \"noise\", \"data\", or \"v_prediction\".</p> Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>def process_data_prediction(self, model_output: Tensor, sample: Tensor, t: Tensor):\n    \"\"\"Converts the model output to a data prediction based on the prediction type.\n\n    This conversion stems from the Progressive Distillation for Fast Sampling of Diffusion Models https://arxiv.org/pdf/2202.00512.\n    Given the model output and the sample, we convert the output to a data prediction based on the prediction type.\n    The conversion formulas are as follows:\n    - For \"noise\" prediction type: `pred_data = (sample - noise_scale * model_output) / data_scale`\n    - For \"data\" prediction type: `pred_data = model_output`\n    - For \"v_prediction\" prediction type: `pred_data = data_scale * sample - noise_scale * model_output`\n\n    Args:\n        model_output (Tensor): The output of the model.\n        sample (Tensor): The input sample.\n        t (Tensor): The time step.\n\n    Returns:\n        The data prediction based on the prediction type.\n\n    Raises:\n        ValueError: If the prediction type is not one of \"noise\", \"data\", or \"v_prediction\".\n    \"\"\"\n    data_scale = safe_index(self._forward_data_schedule, t - self.last_time_idx, model_output.device)\n    noise_scale = safe_index(self._forward_noise_schedule, t - self.last_time_idx, model_output.device)\n    data_scale = pad_like(data_scale, model_output)\n    noise_scale = pad_like(noise_scale, model_output)\n    if self.prediction_type == PredictionType.NOISE:\n        pred_data = (sample - noise_scale * model_output) / data_scale\n    elif self.prediction_type == PredictionType.DATA:\n        pred_data = model_output\n    elif self.prediction_type == PredictionType.VELOCITY:\n        pred_data = data_scale * sample - noise_scale * model_output\n    else:\n        raise ValueError(\n            f\"prediction_type given as {self.prediction_type} must be one of PredictionType.NOISE, PredictionType.DATA or\"\n            f\" PredictionType.VELOCITY for DDPM.\"\n        )\n    return pred_data\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.process_noise_prediction","title":"<code>process_noise_prediction(model_output, sample, t)</code>","text":"<p>Do the same as process_data_prediction but take the model output and convert to nosie.</p> <p>Parameters:</p> Name Type Description Default <code>model_output</code> <p>The output of the model.</p> required <code>sample</code> <p>The input sample.</p> required <code>t</code> <p>The time step.</p> required <p>Returns:</p> Type Description <p>The input as noise if the prediction type is \"noise\".</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the prediction type is not \"noise\".</p> Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>def process_noise_prediction(self, model_output, sample, t):\n    \"\"\"Do the same as process_data_prediction but take the model output and convert to nosie.\n\n    Args:\n        model_output: The output of the model.\n        sample: The input sample.\n        t: The time step.\n\n    Returns:\n        The input as noise if the prediction type is \"noise\".\n\n    Raises:\n        ValueError: If the prediction type is not \"noise\".\n    \"\"\"\n    data_scale = safe_index(self._forward_data_schedule, t - self.last_time_idx, model_output.device)\n    noise_scale = safe_index(self._forward_noise_schedule, t - self.last_time_idx, model_output.device)\n    data_scale = pad_like(data_scale, model_output)\n    noise_scale = pad_like(noise_scale, model_output)\n    if self.prediction_type == PredictionType.NOISE:\n        pred_noise = model_output\n    elif self.prediction_type == PredictionType.DATA:\n        pred_noise = (sample - data_scale * model_output) / noise_scale\n    elif self.prediction_type == PredictionType.VELOCITY:\n        pred_data = data_scale * sample - noise_scale * model_output\n        pred_noise = (sample - data_scale * pred_data) / noise_scale\n    else:\n        raise ValueError(\n            f\"prediction_type given as {self.prediction_type} must be one of `noise`, `data` or\"\n            \" `v_prediction`  for DDPM.\"\n        )\n    return pred_noise\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.score","title":"<code>score(x_hat, xt, t)</code>","text":"<p>Converts the data prediction to the estimated score function.</p> <p>Parameters:</p> Name Type Description Default <code>x_hat</code> <code>Tensor</code> <p>The predicted data point.</p> required <code>xt</code> <code>Tensor</code> <p>The current data point.</p> required <code>t</code> <code>Tensor</code> <p>The time step.</p> required <p>Returns:</p> Type Description <p>The estimated score function.</p> Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>def score(self, x_hat: Tensor, xt: Tensor, t: Tensor):\n    \"\"\"Converts the data prediction to the estimated score function.\n\n    Args:\n        x_hat (Tensor): The predicted data point.\n        xt (Tensor): The current data point.\n        t (Tensor): The time step.\n\n    Returns:\n        The estimated score function.\n    \"\"\"\n    alpha = safe_index(self._forward_data_schedule, t - self.last_time_idx, x_hat.device)\n    beta = safe_index(self._forward_noise_schedule, t - self.last_time_idx, x_hat.device)\n    alpha = pad_like(alpha, x_hat)\n    beta = pad_like(beta, x_hat)\n    score = alpha * x_hat - xt\n    score = score / (beta * beta)\n    return score\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.set_loss_weight_fn","title":"<code>set_loss_weight_fn(fn)</code>","text":"<p>Sets the loss_weight attribute of the instance to the given function.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <p>The function to set as the loss_weight attribute. This function should take three arguments: raw_loss, t, and weight_type.</p> required Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>def set_loss_weight_fn(self, fn):\n    \"\"\"Sets the loss_weight attribute of the instance to the given function.\n\n    Args:\n        fn: The function to set as the loss_weight attribute. This function should take three arguments: raw_loss, t, and weight_type.\n    \"\"\"\n    self.loss_weight = fn\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.step","title":"<code>step(model_out, t, xt, mask=None, center=False, temperature=1.0)</code>","text":"<p>Do one step integration.</p> <p>Args: model_out (Tensor): The output of the model. t (Tensor): The current time step. xt (Tensor): The current data point. mask (Optional[Tensor], optional): An optional mask to apply to the data. Defaults to None. center (bool, optional): Whether to center the data. Defaults to False. temperature (Float, optional): The temperature parameter for low temperature sampling. Defaults to 1.0.</p> <p>Note: The temperature parameter controls the level of randomness in the sampling process. A temperature of 1.0 corresponds to standard diffusion sampling, while lower temperatures (e.g. 0.5, 0.2) result in less random and more deterministic samples. This can be useful for tasks that require more control over the generation process.</p> <p>Note for discrete time we sample from [T-1, ..., 1, 0] for T steps so we sample t = 0 hence the mask. For continuous time we start from [1, 1 -dt, ..., dt] for T steps where s = t - 1 when t = 0 i.e dt is then 0</p> Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>@torch.no_grad()\ndef step(\n    self,\n    model_out: Tensor,\n    t: Tensor,\n    xt: Tensor,\n    mask: Optional[Tensor] = None,\n    center: Bool = False,\n    temperature: Float = 1.0,\n):\n    \"\"\"Do one step integration.\n\n    Args:\n    model_out (Tensor): The output of the model.\n    t (Tensor): The current time step.\n    xt (Tensor): The current data point.\n    mask (Optional[Tensor], optional): An optional mask to apply to the data. Defaults to None.\n    center (bool, optional): Whether to center the data. Defaults to False.\n    temperature (Float, optional): The temperature parameter for low temperature sampling. Defaults to 1.0.\n\n    Note:\n    The temperature parameter controls the level of randomness in the sampling process. A temperature of 1.0 corresponds to standard diffusion sampling, while lower temperatures (e.g. 0.5, 0.2) result in less random and more deterministic samples. This can be useful for tasks that require more control over the generation process.\n\n    Note for discrete time we sample from [T-1, ..., 1, 0] for T steps so we sample t = 0 hence the mask.\n    For continuous time we start from [1, 1 -dt, ..., dt] for T steps where s = t - 1 when t = 0 i.e dt is then 0\n\n    \"\"\"\n    if mask is not None:\n        model_out = model_out * mask.unsqueeze(-1)\n    x_hat = self.process_data_prediction(model_out, xt, t)\n    psi_r = safe_index(self._reverse_data_schedule, t - self.last_time_idx, x_hat.device)\n    omega_r = safe_index(self._reverse_noise_schedule, t - self.last_time_idx, x_hat.device)\n    log_var = safe_index(self._log_var, t - self.last_time_idx, x_hat.device)  # self._log_var[t.long()]\n    nonzero_mask = (t &gt; self.last_time_idx).float()\n    psi_r = pad_like(psi_r, x_hat)\n    omega_r = pad_like(omega_r, x_hat)\n    log_var = pad_like(log_var, x_hat)\n    nonzero_mask = pad_like(nonzero_mask, x_hat)\n\n    mean = psi_r * x_hat + omega_r * xt\n    eps = torch.randn_like(mean).to(model_out.device)\n\n    x_next = mean + nonzero_mask * (0.5 * log_var).exp() * eps * temperature\n    x_next = self.clean_mask_center(x_next, mask, center)\n    return x_next\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.step_ddim","title":"<code>step_ddim(model_out, t, xt, mask=None, eta=0.0, center=False)</code>","text":"<p>Do one step of DDIM sampling.</p> <p>Parameters:</p> Name Type Description Default <code>model_out</code> <code>Tensor</code> <p>output of the model</p> required <code>t</code> <code>Tensor</code> <p>current time step</p> required <code>xt</code> <code>Tensor</code> <p>current data point</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>mask for the data point. Defaults to None.</p> <code>None</code> <code>eta</code> <code>Float</code> <p>DDIM sampling parameter. Defaults to 0.0.</p> <code>0.0</code> <code>center</code> <code>Bool</code> <p>whether to center the data point. Defaults to False.</p> <code>False</code> Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>def step_ddim(\n    self,\n    model_out: Tensor,\n    t: Tensor,\n    xt: Tensor,\n    mask: Optional[Tensor] = None,\n    eta: Float = 0.0,\n    center: Bool = False,\n):\n    \"\"\"Do one step of DDIM sampling.\n\n    Args:\n        model_out (Tensor): output of the model\n        t (Tensor): current time step\n        xt (Tensor): current data point\n        mask (Optional[Tensor], optional): mask for the data point. Defaults to None.\n        eta (Float, optional): DDIM sampling parameter. Defaults to 0.0.\n        center (Bool, optional): whether to center the data point. Defaults to False.\n    \"\"\"\n    if mask is not None:\n        model_out = model_out * mask.unsqueeze(-1)\n    data_pred = self.process_data_prediction(model_out, xt, t)\n    noise_pred = self.process_noise_prediction(model_out, xt, t)\n    eps = torch.randn_like(data_pred).to(model_out.device)\n    sigma = (\n        eta\n        * torch.sqrt((1 - self._alpha_bar_prev) / (1 - self._alpha_bar))\n        * torch.sqrt(1 - self._alpha_bar / self._alpha_bar_prev)\n    )\n    sigma_t = safe_index(sigma, t - self.last_time_idx, model_out.device)\n    psi_r = safe_index(torch.sqrt(self._alpha_bar_prev), t - self.last_time_idx, model_out.device)\n    omega_r = safe_index(torch.sqrt(1 - self._alpha_bar_prev - sigma**2), t - self.last_time_idx, model_out.device)\n    sigma_t = pad_like(sigma_t, model_out)\n    psi_r = pad_like(psi_r, model_out)\n    omega_r = pad_like(omega_r, model_out)\n    mean = data_pred * psi_r + omega_r * noise_pred\n    x_next = mean + sigma_t * eps\n    x_next = self.clean_mask_center(x_next, mask, center)\n    return x_next\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.step_noise","title":"<code>step_noise(model_out, t, xt, mask=None, center=False, temperature=1.0)</code>","text":"<p>Do one step integration.</p> <p>Args: model_out (Tensor): The output of the model. t (Tensor): The current time step. xt (Tensor): The current data point. mask (Optional[Tensor], optional): An optional mask to apply to the data. Defaults to None. center (bool, optional): Whether to center the data. Defaults to False. temperature (Float, optional): The temperature parameter for low temperature sampling. Defaults to 1.0.</p> <p>Note: The temperature parameter controls the level of randomness in the sampling process. A temperature of 1.0 corresponds to standard diffusion sampling, while lower temperatures (e.g. 0.5, 0.2) result in less random and more deterministic samples. This can be useful for tasks that require more control over the generation process.</p> <p>Note for discrete time we sample from [T-1, ..., 1, 0] for T steps so we sample t = 0 hence the mask. For continuous time we start from [1, 1 -dt, ..., dt] for T steps where s = t - 1 when t = 0 i.e dt is then 0</p> Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>def step_noise(\n    self,\n    model_out: Tensor,\n    t: Tensor,\n    xt: Tensor,\n    mask: Optional[Tensor] = None,\n    center: Bool = False,\n    temperature: Float = 1.0,\n):\n    \"\"\"Do one step integration.\n\n    Args:\n    model_out (Tensor): The output of the model.\n    t (Tensor): The current time step.\n    xt (Tensor): The current data point.\n    mask (Optional[Tensor], optional): An optional mask to apply to the data. Defaults to None.\n    center (bool, optional): Whether to center the data. Defaults to False.\n    temperature (Float, optional): The temperature parameter for low temperature sampling. Defaults to 1.0.\n\n    Note:\n    The temperature parameter controls the level of randomness in the sampling process.\n    A temperature of 1.0 corresponds to standard diffusion sampling, while lower temperatures (e.g. 0.5, 0.2)\n    result in less random and more deterministic samples. This can be useful for tasks\n    that require more control over the generation process.\n\n    Note for discrete time we sample from [T-1, ..., 1, 0] for T steps so we sample t = 0 hence the mask.\n    For continuous time we start from [1, 1 -dt, ..., dt] for T steps where s = t - 1 when t = 0 i.e dt is then 0\n\n    \"\"\"\n    if mask is not None:\n        model_out = model_out * mask.unsqueeze(-1)\n    eps_hat = self.process_noise_prediction(model_out, xt, t)\n    beta_t = safe_index(self._betas, t - self.last_time_idx, model_out.device)\n    recip_sqrt_alpha_t = torch.sqrt(1 / (1 - beta_t))\n    eps_factor = (\n        safe_index(self._betas, t - self.last_time_idx, model_out.device)\n        / (1 - safe_index(self._alpha_bar, t - self.last_time_idx, model_out.device)).sqrt()\n    )\n    var = safe_index(self._posterior_variance, t - self.last_time_idx, model_out.device)  # self._log_var[t.long()]\n\n    nonzero_mask = (t &gt; self.last_time_idx).float()\n    nonzero_mask = pad_like(nonzero_mask, model_out)\n    eps_factor = pad_like(eps_factor, xt)\n    recip_sqrt_alpha_t = pad_like(recip_sqrt_alpha_t, xt)\n    var = pad_like(var, xt)\n\n    x_next = (\n        recip_sqrt_alpha_t * (xt - eps_factor * eps_hat)\n        + nonzero_mask * var.sqrt() * torch.randn_like(eps_hat).to(model_out.device) * temperature\n    )\n    x_next = self.clean_mask_center(x_next, mask, center)\n    return x_next\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/discrete/d3pm/","title":"D3pm","text":""},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/discrete/d3pm/#bionemo.moco.interpolants.discrete_time.discrete.d3pm.D3PM","title":"<code>D3PM</code>","text":"<p>               Bases: <code>Interpolant</code></p> <p>A Discrete Denoising Diffusion Probabilistic Model (D3PM) interpolant.</p> Source code in <code>bionemo/moco/interpolants/discrete_time/discrete/d3pm.py</code> <pre><code>class D3PM(Interpolant):\n    \"\"\"A Discrete Denoising Diffusion Probabilistic Model (D3PM) interpolant.\"\"\"\n\n    def __init__(\n        self,\n        time_distribution: TimeDistribution,\n        prior_distribution: DiscretePriorDistribution,\n        noise_schedule: DiscreteNoiseSchedule,\n        device: str = \"cpu\",\n        last_time_idx: int = 0,\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Initializes the D3PM interpolant.\n\n        Args:\n            time_distribution (TimeDistribution): The distribution of time steps, used to sample time points for the diffusion process.\n            prior_distribution (PriorDistribution): The prior distribution of the variable, used as the starting point for the diffusion process.\n            noise_schedule (DiscreteNoiseSchedule): The schedule of noise, defining the amount of noise added at each time step.\n            device (str, optional): The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".\n            last_time_idx (int, optional): The last time index to consider in the interpolation process. Defaults to 0.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n        \"\"\"\n        # We initialize with CPU due to numerical precision issues on A100 that are not observed on A6000\n        super().__init__(time_distribution, prior_distribution, \"cpu\", rng_generator)\n        self.noise_schedule = noise_schedule\n        self._loss_function = nn.CrossEntropyLoss(reduction=\"none\")\n        self.timesteps = noise_schedule.nsteps\n        self.num_classes = prior_distribution.num_classes\n        self.terminal_distribution = prior_distribution.prior_dist.to(self.device)\n        self._initialize_schedules(self.device)\n        self.last_time_idx = last_time_idx\n        self.to_device(device)\n\n    def _get_Qt(self, alphas: Tensor) -&gt; Tensor:\n        \"\"\"Calculate the transition matrix Qt based on the terminal distribution.\n\n        The transition matrix Qt represents the probabilities of transitioning from one state to another at a given time step.\n        It is calculated based on the terminal distribution, which can be either uniform, a mask, or a custom distribution.\n        See Appendix A.2 D3PM https://arxiv.org/pdf/2107.03006 which shows what happens for various prior distributions.\n\n        The terminal distribution can be:\n        - Uniform: a uniform distribution over all states.\n        - Mask: a mask where the last dimension is 1 and the rest are 0.\n        - Custom: a custom distribution provided by the user.\n\n        Args:\n            alphas (Tensor): A tensor of probabilities, where each alpha represents the probability of staying in a state at a given time step.\n\n        Returns:\n            Tensor: The transition matrix Qt.\n        \"\"\"\n        QT = []\n        for alpha_t in alphas:\n            stay_prob = torch.eye(len(self.terminal_distribution), device=self.device) * alpha_t\n            diffuse_prob = (1.0 - alpha_t) * (\n                torch.ones(1, len(self.terminal_distribution), device=self.device)\n                * (self.terminal_distribution.unsqueeze(0))\n            )\n            QT.append(stay_prob + diffuse_prob)\n        return torch.stack(QT, dim=0)\n\n    def _calculate_transition_matrix(self, alphas: Tensor) -&gt; Tuple[Tensor, Tensor, Tensor]:\n        \"\"\"Calculates the rate transition matrix `Qt`, its cumulative variant `Qt_bar`, and the cumulative variant of the previous time step `Qt_bar_prev`.\n\n        Args:\n            alphas (Tensor): A tensor of probabilities, where each alpha represents the probability of staying in a state at a given time step.\n\n        Returns:\n            Tuple[Tensor, Tensor, Tensor]: A tuple containing the rate transition matrix `Qt`, its cumulative variant `Qt_bar`, and the cumulative variant of the previous time step `Qt_bar_prev`.\n        \"\"\"\n        Qt = self._get_Qt(alphas)\n        Qt_prev = torch.eye(self.num_classes, device=self.device)\n        Qt_bar = []\n        for i in range(len(alphas)):\n            Qtb = Qt_prev @ Qt[i]\n            if torch.any((Qtb.sum(-1) - 1.0).abs() &gt; 1e-4):\n                raise ValueError(f\"Invalid Distribution for Qt_bar at step {i}\")\n            Qt_bar.append(Qtb)\n            Qt_prev = Qtb\n        Qt_bar = torch.stack(Qt_bar)\n        Qt_bar_prev = Qt_bar[:-1]\n        Qt_prev_pad = torch.eye(self.num_classes, device=self.device)\n        Qt_bar_prev = torch.concat([Qt_prev_pad.unsqueeze(0), Qt_bar_prev], dim=0)\n        return Qt, Qt_bar, Qt_bar_prev\n\n    def _initialize_schedules(self, device):\n        \"\"\"Initializes the transition matrices for the discrete diffusion process.\n\n        This method computes the rate transition matrix `Qt` and its cumulative variants `Qt_bar` and `Qt_prev_bar`\n        based on the provided noise schedule.\n\n        Note:\n            `Qt` represents the rate transition matrix, where `Qt[t]` is the transition matrix at time step `t`.\n            `Qt_bar` and `Qt_prev_bar` are the cumulative variants of `Qt`, where `Qt_bar[t]` represents the cumulative\n            transition matrix from time step `0` to `t`, and `Qt_prev_bar[t]` represents the cumulative transition matrix\n            from time step `0` to `t-1`.\n\n        Args:\n            device (str): The device on which to compute the transition matrices.\n        \"\"\"\n        if self.noise_schedule is None:\n            raise ValueError(\"noise_schedule cannot be None for D3PM\")\n        alphas = self.noise_schedule.generate_schedule(device=device)\n        log_alpha = torch.log(alphas)\n        log_alpha_bar = torch.cumsum(log_alpha, dim=0)\n        self._alpha_bar = torch.exp(log_alpha_bar)\n        #! Note to users that the tranditional cosine schedule is a very quick convergence of alpha. Pay close attention to the scheduler here\n        Qt, Qt_bar, Qt_prev_bar = self._calculate_transition_matrix(alphas)\n        self._Qt = Qt[-self.timesteps :]\n        self._Qt_transposed = self._Qt.transpose(1, 2)\n        self._Qt_bar = Qt_bar[-self.timesteps :]\n        self._Qt_prev_bar = Qt_prev_bar[-self.timesteps :]\n\n    def interpolate(self, data: Tensor, t: Tensor):\n        \"\"\"Interpolate using discrete interpolation method.\n\n        This method implements Equation 2 from the D3PM paper (https://arxiv.org/pdf/2107.03006), which\n        calculates the interpolated discrete state `xt` at time `t` given the input data and noise\n        via q(xt|x0) = Cat(xt; p = x0*Qt_bar).\n\n        Args:\n            data (Tensor): The input data to be interpolated.\n            t (Tensor): The time step at which to interpolate.\n\n        Returns:\n            Tensor: The interpolated discrete state `xt` at time `t`.\n        \"\"\"\n        if not _is_one_hot(data, self.num_classes):\n            x1_hot = F.one_hot(data, self.num_classes)\n        else:\n            x1_hot = data\n        ford = safe_index(self._Qt_bar, t - self.last_time_idx, data.device)\n        if x1_hot.ndim &gt; 3:  # einsum precision issues on A100 not A6000 for 2D inputs\n            ford_prep = ford\n            for _ in range(x1_hot.ndim - 2):\n                ford_prep = ford_prep.unsqueeze(1)\n            probs = (x1_hot.float().unsqueeze(-2) * ford_prep).sum(dim=(-2))\n        else:\n            probs = torch.einsum(\"b...j, bji -&gt; b...i\", [x1_hot.float(), ford])\n        if torch.any((probs.sum(-1) - 1.0).abs() &gt; 1e-4):\n            raise ValueError(\n                f\"**INVALID BEHAVIOR** Probability Distribution does not sum to 1.0 for time {t}. \"\n                f\"**INVESTIGATE YOUR DEVICE PRECISION**: This error has been triggered before on A100 by initializing the Qt terms on gpu. \"\n                f\"NOTE: For Blackwell, tf32 must be disabled via NVIDIA_TF32_OVERRIDE=0 even when initializing the Qt terms on cpu. \"\n                f\"Original sums: {probs.sum(-1)}\",\n            )\n        xt = self._sample_categorical(torch.log(probs) + 1.0e-6)\n        return xt\n\n    def forward_process(self, data: Tensor, t: Tensor) -&gt; Tensor:\n        \"\"\"Apply the forward process to the data at time t.\n\n        Args:\n            data (Tensor): target discrete ids\n            t (Tensor): time\n\n        Returns:\n            Tensor: x(t) after applying the forward process\n        \"\"\"\n        return self.interpolate(data, t)\n\n    def _sample_categorical(self, logits, mask: Optional[Tensor] = None, temperature: Float = 1.0) -&gt; Tensor:\n        \"\"\"Sample a categorical distribution using the Gumbel-Softmax trick.\n\n        This method samples a categorical distribution from the given logits,\n        optionally applying a mask and using a specified temperature.\n\n        Args:\n            logits (Tensor): The logits of the categorical distribution.\n            mask (Optional[Tensor], optional): An optional mask to apply to the noise added to logits. Defaults to None.\n            temperature (float, optional): The temperature to use for the Gumbel-Softmax trick. Defaults to 1.0.\n\n        Returns:\n            Tensor: A sample from the categorical distribution.\n        \"\"\"\n        noise = torch.rand_like(logits)\n        noise = torch.clip(noise, 1.0e-6, 1.0)\n        gumbel_noise = -torch.log(-torch.log(noise))\n        if mask is not None:\n            sample = torch.argmax((logits / temperature) + gumbel_noise * mask, dim=-1)\n        else:\n            sample = torch.argmax((logits / temperature) + gumbel_noise, dim=-1)\n        return sample\n\n    def _q_posterior_logits(\n        self, model_out: Tensor, t: Tensor, xt: Tensor, model_out_is_logits: bool = True\n    ) -&gt; Tensor:\n        \"\"\"Calculate the q-posterior logits using the predicted x0 and the current state xt at time t.\n\n        This method implements Equation 3 from the D3PM paper (https://arxiv.org/pdf/2107.03006), which calculates the q-posterior\n        distribution over the previous state x0 given the current state xt and the model output.\n\n        Args:\n            model_out (Tensor): The output of the model at the current time step.\n            t (Tensor): The current time step.\n            xt (Tensor): The current discrete state at time t.\n            model_out_is_logits (bool, optional): A flag indicating whether the model output is already in logits form. If True, the output is assumed to be logits; otherwise, it is converted to logits. Defaults to True.\n\n        Returns:\n            Tensor: The q-posterior logits.\n        \"\"\"\n        if not model_out_is_logits:  # model_out.dtype == torch.int64 or model_out.dtype == torch.int32:\n            # Convert model output to logits if it's a categorical distribution\n            x0_logits = torch.log(torch.nn.functional.one_hot(model_out, self.num_classes).float() + 1.0e-6)\n        else:\n            # Otherwise, assume model output is already logits\n            x0_logits = model_out.clone()\n\n        # Calculate xt_guess: the predicted probability of xt given x0 and t\n        xt_guess = torch.einsum(\n            \"b...j, bji -&gt; b...i\",\n            [\n                torch.nn.functional.one_hot(xt, self.num_classes).float(),\n                safe_index(self._Qt_transposed, t - self.last_time_idx, model_out.device),\n            ],\n        )\n\n        # Calculate softmaxed x0_logits\n        softmaxed = torch.softmax(x0_logits, dim=-1)  # bs, ..., num_classes\n\n        # Calculate x0_guess: the predicted probability of x0 given xt and t-1\n        x0_guess = torch.einsum(\n            \"b...c,bcd-&gt;b...d\",\n            softmaxed,\n            safe_index(self._Qt_prev_bar, t - self.last_time_idx, model_out.device),\n        )\n\n        # Calculate q-posterior logits\n        out = torch.log(xt_guess + 1.0e-6) + torch.log(x0_guess + 1.0e-6)\n        t_broadcast = t.reshape((t.shape[0], *[1] * (xt.dim())))\n        q_posterior_logits = torch.where(t_broadcast == self.last_time_idx, x0_logits, out)\n        return q_posterior_logits\n\n    def step(\n        self,\n        model_out: Tensor,\n        t: Tensor,\n        xt: Tensor,\n        mask: Optional[Tensor] = None,\n        temperature: Float = 1.0,\n        model_out_is_logits: bool = True,\n    ):\n        \"\"\"Perform a single step in the discrete interpolant method, transitioning from the current discrete state `xt` at time `t` to the next state.\n\n        This step involves:\n\n        1. Computing the predicted q-posterior logits using the model output `model_out` and the current state `xt` at time `t`.\n        2. Sampling the next state from the predicted q-posterior distribution using the Gumbel-Softmax trick.\n\n        Args:\n            model_out (Tensor): The output of the model at the current time step, which is used to compute the predicted q-posterior logits.\n            t (Tensor): The current time step, which is used to index into the transition matrices and compute the predicted q-posterior logits.\n            xt (Tensor): The current discrete state at time `t`, which is used to compute the predicted q-posterior logits and sample the next state.\n            mask (Optional[Tensor], optional): An optional mask to apply to the next state, which can be used to mask out certain tokens or regions. Defaults to None.\n            temperature (Float, optional): The temperature to use for the Gumbel-Softmax trick, which controls the randomness of the sampling process. Defaults to 1.0.\n            model_out_is_logits (bool, optional): A flag indicating whether the model output is already in logits form. If True, the output is assumed to be logits; otherwise, it is converted to logits. Defaults to True.\n\n        Returns:\n            Tensor: The next discrete state at time `t-1`.\n        \"\"\"\n        pred_q_posterior_logits = self._q_posterior_logits(model_out, t, xt, model_out_is_logits)\n        nonzero_mask = (t != self.last_time_idx).to(xt.dtype).reshape(xt.shape[0], *([1] * (len(xt.shape))))\n        x_next = self._sample_categorical(pred_q_posterior_logits, nonzero_mask, temperature=temperature)\n        # # Apply mask if provided\n        if mask is not None:\n            x_next = x_next * mask\n        return x_next\n\n    def loss(\n        self,\n        logits: Tensor,\n        target: Tensor,\n        xt: Tensor,\n        time: Tensor,\n        mask: Optional[Tensor] = None,\n        vb_scale: Float = 0.0,\n    ):\n        \"\"\"Calculate the cross-entropy loss between the model prediction and the target output.\n\n        The loss is calculated between the batch x node x class logits and the target batch x node. If a mask is provided, the loss is\n        calculated only for the non-masked elements. Additionally, if vb_scale is greater than 0, the variational lower bound loss is\n        calculated and added to the total loss.\n\n        Args:\n            logits (Tensor): The predicted output from the model, with shape batch x node x class.\n            target (Tensor): The target output for the model prediction, with shape batch x node.\n            xt (Tensor): The current data point.\n            time (Tensor): The time at which the loss is calculated.\n            mask (Optional[Tensor], optional): The mask for the data point. Defaults to None.\n            vb_scale (Float, optional): The scale factor for the variational lower bound loss. Defaults to 0.0.\n\n        Returns:\n            Tensor: The calculated loss tensor. If aggregate is True, the loss and variational lower bound loss are aggregated and\n            returned as a single tensor. Otherwise, the loss and variational lower bound loss are returned as separate tensors.\n        \"\"\"\n        assert target.ndim + 1 == logits.ndim\n        loss = self._loss_function(logits.transpose(-1, 1), target.long())\n        if mask is not None:\n            loss = loss * mask\n            num_non_masked_elements = torch.sum(mask, dim=-1)\n            loss = torch.sum(loss, dim=(-1)) / num_non_masked_elements\n        else:\n            loss = torch.sum(loss, dim=(-1)) / logits.size(1)\n        if vb_scale &gt; 0:\n            target = F.one_hot(target, num_classes=self.num_classes).float()\n            true_q_posterior_logits = self._q_posterior_logits(target, time, xt)\n            pred_q_posterior_logits = self._q_posterior_logits(logits, time, xt)\n            vb_loss = self._variational_lower_bound(true_q_posterior_logits, pred_q_posterior_logits)\n            vb_loss = vb_scale * vb_loss\n        else:\n            vb_loss = 0\n        if vb_scale &gt; 0:\n            loss += vb_loss\n        return loss\n\n    def _variational_lower_bound(self, dist1: Tensor, dist2: Tensor) -&gt; Tensor:\n        \"\"\"Calculate the variational lower bound (VLB) between two distributions.\n\n        The VLB measures the difference between the true and approximate posterior distributions.\n        It is used to regularize the model and encourage it to produce more accurate predictions.\n\n        Args:\n            dist1 (Tensor): The true posterior distribution.\n            dist2 (Tensor): The approximate posterior distribution.\n\n        Returns:\n            Tensor: The variational lower bound loss.\n        \"\"\"\n        # Flatten dist1 and dist2 to simplify calculations\n        dist1 = dist1.flatten(start_dim=0, end_dim=-2)\n        dist2 = dist2.flatten(start_dim=0, end_dim=-2)\n\n        # Calculate the VLB\n        out = torch.softmax(dist1 + 1.0e-6, dim=-1) * (\n            torch.log_softmax(dist1 + 1.0e-6, dim=-1) - torch.log_softmax(dist2 + 1.0e-6, dim=-1)\n        )\n        # Return the mean of the VLB across all elements\n        return out.sum(dim=-1).mean()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/discrete/d3pm/#bionemo.moco.interpolants.discrete_time.discrete.d3pm.D3PM.__init__","title":"<code>__init__(time_distribution, prior_distribution, noise_schedule, device='cpu', last_time_idx=0, rng_generator=None)</code>","text":"<p>Initializes the D3PM interpolant.</p> <p>Parameters:</p> Name Type Description Default <code>time_distribution</code> <code>TimeDistribution</code> <p>The distribution of time steps, used to sample time points for the diffusion process.</p> required <code>prior_distribution</code> <code>PriorDistribution</code> <p>The prior distribution of the variable, used as the starting point for the diffusion process.</p> required <code>noise_schedule</code> <code>DiscreteNoiseSchedule</code> <p>The schedule of noise, defining the amount of noise added at each time step.</p> required <code>device</code> <code>str</code> <p>The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".</p> <code>'cpu'</code> <code>last_time_idx</code> <code>int</code> <p>The last time index to consider in the interpolation process. Defaults to 0.</p> <code>0</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/moco/interpolants/discrete_time/discrete/d3pm.py</code> <pre><code>def __init__(\n    self,\n    time_distribution: TimeDistribution,\n    prior_distribution: DiscretePriorDistribution,\n    noise_schedule: DiscreteNoiseSchedule,\n    device: str = \"cpu\",\n    last_time_idx: int = 0,\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Initializes the D3PM interpolant.\n\n    Args:\n        time_distribution (TimeDistribution): The distribution of time steps, used to sample time points for the diffusion process.\n        prior_distribution (PriorDistribution): The prior distribution of the variable, used as the starting point for the diffusion process.\n        noise_schedule (DiscreteNoiseSchedule): The schedule of noise, defining the amount of noise added at each time step.\n        device (str, optional): The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".\n        last_time_idx (int, optional): The last time index to consider in the interpolation process. Defaults to 0.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n    \"\"\"\n    # We initialize with CPU due to numerical precision issues on A100 that are not observed on A6000\n    super().__init__(time_distribution, prior_distribution, \"cpu\", rng_generator)\n    self.noise_schedule = noise_schedule\n    self._loss_function = nn.CrossEntropyLoss(reduction=\"none\")\n    self.timesteps = noise_schedule.nsteps\n    self.num_classes = prior_distribution.num_classes\n    self.terminal_distribution = prior_distribution.prior_dist.to(self.device)\n    self._initialize_schedules(self.device)\n    self.last_time_idx = last_time_idx\n    self.to_device(device)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/discrete/d3pm/#bionemo.moco.interpolants.discrete_time.discrete.d3pm.D3PM.forward_process","title":"<code>forward_process(data, t)</code>","text":"<p>Apply the forward process to the data at time t.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>target discrete ids</p> required <code>t</code> <code>Tensor</code> <p>time</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>x(t) after applying the forward process</p> Source code in <code>bionemo/moco/interpolants/discrete_time/discrete/d3pm.py</code> <pre><code>def forward_process(self, data: Tensor, t: Tensor) -&gt; Tensor:\n    \"\"\"Apply the forward process to the data at time t.\n\n    Args:\n        data (Tensor): target discrete ids\n        t (Tensor): time\n\n    Returns:\n        Tensor: x(t) after applying the forward process\n    \"\"\"\n    return self.interpolate(data, t)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/discrete/d3pm/#bionemo.moco.interpolants.discrete_time.discrete.d3pm.D3PM.interpolate","title":"<code>interpolate(data, t)</code>","text":"<p>Interpolate using discrete interpolation method.</p> <p>This method implements Equation 2 from the D3PM paper (https://arxiv.org/pdf/2107.03006), which calculates the interpolated discrete state <code>xt</code> at time <code>t</code> given the input data and noise via q(xt|x0) = Cat(xt; p = x0*Qt_bar).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>The input data to be interpolated.</p> required <code>t</code> <code>Tensor</code> <p>The time step at which to interpolate.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The interpolated discrete state <code>xt</code> at time <code>t</code>.</p> Source code in <code>bionemo/moco/interpolants/discrete_time/discrete/d3pm.py</code> <pre><code>def interpolate(self, data: Tensor, t: Tensor):\n    \"\"\"Interpolate using discrete interpolation method.\n\n    This method implements Equation 2 from the D3PM paper (https://arxiv.org/pdf/2107.03006), which\n    calculates the interpolated discrete state `xt` at time `t` given the input data and noise\n    via q(xt|x0) = Cat(xt; p = x0*Qt_bar).\n\n    Args:\n        data (Tensor): The input data to be interpolated.\n        t (Tensor): The time step at which to interpolate.\n\n    Returns:\n        Tensor: The interpolated discrete state `xt` at time `t`.\n    \"\"\"\n    if not _is_one_hot(data, self.num_classes):\n        x1_hot = F.one_hot(data, self.num_classes)\n    else:\n        x1_hot = data\n    ford = safe_index(self._Qt_bar, t - self.last_time_idx, data.device)\n    if x1_hot.ndim &gt; 3:  # einsum precision issues on A100 not A6000 for 2D inputs\n        ford_prep = ford\n        for _ in range(x1_hot.ndim - 2):\n            ford_prep = ford_prep.unsqueeze(1)\n        probs = (x1_hot.float().unsqueeze(-2) * ford_prep).sum(dim=(-2))\n    else:\n        probs = torch.einsum(\"b...j, bji -&gt; b...i\", [x1_hot.float(), ford])\n    if torch.any((probs.sum(-1) - 1.0).abs() &gt; 1e-4):\n        raise ValueError(\n            f\"**INVALID BEHAVIOR** Probability Distribution does not sum to 1.0 for time {t}. \"\n            f\"**INVESTIGATE YOUR DEVICE PRECISION**: This error has been triggered before on A100 by initializing the Qt terms on gpu. \"\n            f\"NOTE: For Blackwell, tf32 must be disabled via NVIDIA_TF32_OVERRIDE=0 even when initializing the Qt terms on cpu. \"\n            f\"Original sums: {probs.sum(-1)}\",\n        )\n    xt = self._sample_categorical(torch.log(probs) + 1.0e-6)\n    return xt\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/discrete/d3pm/#bionemo.moco.interpolants.discrete_time.discrete.d3pm.D3PM.loss","title":"<code>loss(logits, target, xt, time, mask=None, vb_scale=0.0)</code>","text":"<p>Calculate the cross-entropy loss between the model prediction and the target output.</p> <p>The loss is calculated between the batch x node x class logits and the target batch x node. If a mask is provided, the loss is calculated only for the non-masked elements. Additionally, if vb_scale is greater than 0, the variational lower bound loss is calculated and added to the total loss.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>The predicted output from the model, with shape batch x node x class.</p> required <code>target</code> <code>Tensor</code> <p>The target output for the model prediction, with shape batch x node.</p> required <code>xt</code> <code>Tensor</code> <p>The current data point.</p> required <code>time</code> <code>Tensor</code> <p>The time at which the loss is calculated.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>The mask for the data point. Defaults to None.</p> <code>None</code> <code>vb_scale</code> <code>Float</code> <p>The scale factor for the variational lower bound loss. Defaults to 0.0.</p> <code>0.0</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The calculated loss tensor. If aggregate is True, the loss and variational lower bound loss are aggregated and</p> <p>returned as a single tensor. Otherwise, the loss and variational lower bound loss are returned as separate tensors.</p> Source code in <code>bionemo/moco/interpolants/discrete_time/discrete/d3pm.py</code> <pre><code>def loss(\n    self,\n    logits: Tensor,\n    target: Tensor,\n    xt: Tensor,\n    time: Tensor,\n    mask: Optional[Tensor] = None,\n    vb_scale: Float = 0.0,\n):\n    \"\"\"Calculate the cross-entropy loss between the model prediction and the target output.\n\n    The loss is calculated between the batch x node x class logits and the target batch x node. If a mask is provided, the loss is\n    calculated only for the non-masked elements. Additionally, if vb_scale is greater than 0, the variational lower bound loss is\n    calculated and added to the total loss.\n\n    Args:\n        logits (Tensor): The predicted output from the model, with shape batch x node x class.\n        target (Tensor): The target output for the model prediction, with shape batch x node.\n        xt (Tensor): The current data point.\n        time (Tensor): The time at which the loss is calculated.\n        mask (Optional[Tensor], optional): The mask for the data point. Defaults to None.\n        vb_scale (Float, optional): The scale factor for the variational lower bound loss. Defaults to 0.0.\n\n    Returns:\n        Tensor: The calculated loss tensor. If aggregate is True, the loss and variational lower bound loss are aggregated and\n        returned as a single tensor. Otherwise, the loss and variational lower bound loss are returned as separate tensors.\n    \"\"\"\n    assert target.ndim + 1 == logits.ndim\n    loss = self._loss_function(logits.transpose(-1, 1), target.long())\n    if mask is not None:\n        loss = loss * mask\n        num_non_masked_elements = torch.sum(mask, dim=-1)\n        loss = torch.sum(loss, dim=(-1)) / num_non_masked_elements\n    else:\n        loss = torch.sum(loss, dim=(-1)) / logits.size(1)\n    if vb_scale &gt; 0:\n        target = F.one_hot(target, num_classes=self.num_classes).float()\n        true_q_posterior_logits = self._q_posterior_logits(target, time, xt)\n        pred_q_posterior_logits = self._q_posterior_logits(logits, time, xt)\n        vb_loss = self._variational_lower_bound(true_q_posterior_logits, pred_q_posterior_logits)\n        vb_loss = vb_scale * vb_loss\n    else:\n        vb_loss = 0\n    if vb_scale &gt; 0:\n        loss += vb_loss\n    return loss\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/interpolants/discrete_time/discrete/d3pm/#bionemo.moco.interpolants.discrete_time.discrete.d3pm.D3PM.step","title":"<code>step(model_out, t, xt, mask=None, temperature=1.0, model_out_is_logits=True)</code>","text":"<p>Perform a single step in the discrete interpolant method, transitioning from the current discrete state <code>xt</code> at time <code>t</code> to the next state.</p> <p>This step involves:</p> <ol> <li>Computing the predicted q-posterior logits using the model output <code>model_out</code> and the current state <code>xt</code> at time <code>t</code>.</li> <li>Sampling the next state from the predicted q-posterior distribution using the Gumbel-Softmax trick.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>model_out</code> <code>Tensor</code> <p>The output of the model at the current time step, which is used to compute the predicted q-posterior logits.</p> required <code>t</code> <code>Tensor</code> <p>The current time step, which is used to index into the transition matrices and compute the predicted q-posterior logits.</p> required <code>xt</code> <code>Tensor</code> <p>The current discrete state at time <code>t</code>, which is used to compute the predicted q-posterior logits and sample the next state.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>An optional mask to apply to the next state, which can be used to mask out certain tokens or regions. Defaults to None.</p> <code>None</code> <code>temperature</code> <code>Float</code> <p>The temperature to use for the Gumbel-Softmax trick, which controls the randomness of the sampling process. Defaults to 1.0.</p> <code>1.0</code> <code>model_out_is_logits</code> <code>bool</code> <p>A flag indicating whether the model output is already in logits form. If True, the output is assumed to be logits; otherwise, it is converted to logits. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The next discrete state at time <code>t-1</code>.</p> Source code in <code>bionemo/moco/interpolants/discrete_time/discrete/d3pm.py</code> <pre><code>def step(\n    self,\n    model_out: Tensor,\n    t: Tensor,\n    xt: Tensor,\n    mask: Optional[Tensor] = None,\n    temperature: Float = 1.0,\n    model_out_is_logits: bool = True,\n):\n    \"\"\"Perform a single step in the discrete interpolant method, transitioning from the current discrete state `xt` at time `t` to the next state.\n\n    This step involves:\n\n    1. Computing the predicted q-posterior logits using the model output `model_out` and the current state `xt` at time `t`.\n    2. Sampling the next state from the predicted q-posterior distribution using the Gumbel-Softmax trick.\n\n    Args:\n        model_out (Tensor): The output of the model at the current time step, which is used to compute the predicted q-posterior logits.\n        t (Tensor): The current time step, which is used to index into the transition matrices and compute the predicted q-posterior logits.\n        xt (Tensor): The current discrete state at time `t`, which is used to compute the predicted q-posterior logits and sample the next state.\n        mask (Optional[Tensor], optional): An optional mask to apply to the next state, which can be used to mask out certain tokens or regions. Defaults to None.\n        temperature (Float, optional): The temperature to use for the Gumbel-Softmax trick, which controls the randomness of the sampling process. Defaults to 1.0.\n        model_out_is_logits (bool, optional): A flag indicating whether the model output is already in logits form. If True, the output is assumed to be logits; otherwise, it is converted to logits. Defaults to True.\n\n    Returns:\n        Tensor: The next discrete state at time `t-1`.\n    \"\"\"\n    pred_q_posterior_logits = self._q_posterior_logits(model_out, t, xt, model_out_is_logits)\n    nonzero_mask = (t != self.last_time_idx).to(xt.dtype).reshape(xt.shape[0], *([1] * (len(xt.shape))))\n    x_next = self._sample_categorical(pred_q_posterior_logits, nonzero_mask, temperature=temperature)\n    # # Apply mask if provided\n    if mask is not None:\n        x_next = x_next * mask\n    return x_next\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/inference_time_schedules/","title":"Inference time schedules","text":""},{"location":"main/references/API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.ContinuousInferenceSchedule","title":"<code>ContinuousInferenceSchedule</code>","text":"<p>               Bases: <code>InferenceSchedule</code></p> <p>A base class for continuous time inference schedules.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>class ContinuousInferenceSchedule(InferenceSchedule):\n    \"\"\"A base class for continuous time inference schedules.\"\"\"\n\n    def __init__(\n        self,\n        nsteps: int,\n        inclusive_end: bool = False,\n        min_t: Float = 0,\n        padding: Float = 0,\n        dilation: Float = 0,\n        direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n        device: Union[str, torch.device] = \"cpu\",\n    ):\n        \"\"\"Initialize the ContinuousInferenceSchedule.\n\n        Args:\n            nsteps (int): Number of time steps.\n            inclusive_end (bool): If True, include the end value (1.0) in the schedule otherwise ends at 1.0-1/nsteps (default is False).\n            min_t (Float): minimum time value defaults to 0.\n            padding (Float): padding time value defaults to 0.\n            dilation (Float): dilation time value defaults to 0 ie the number of replicates.\n            direction (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n        \"\"\"\n        super().__init__(nsteps, min_t, padding, dilation, direction, device)\n        self.inclusive_end = inclusive_end\n\n    def discretize(\n        self,\n        nsteps: Optional[int] = None,\n        schedule: Optional[Tensor] = None,\n        device: Optional[Union[str, torch.device]] = None,\n    ) -&gt; Tensor:\n        \"\"\"Discretize the time schedule into a list of time deltas.\n\n        Args:\n            nsteps (Optioanl[int]): Number of time steps. If None, uses the value from initialization.\n            schedule (Optional[Tensor]): Time scheudle if None will generate it with generate_schedule.\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n        Returns:\n            Tensor: A tensor of time deltas.\n        \"\"\"\n        if device is None:\n            device = self.device\n        if schedule is None:\n            schedule = self.generate_schedule(nsteps, device=device)\n        if self.direction == TimeDirection.UNIFIED:\n            schedule = torch.cat((schedule, torch.ones((1,), device=schedule.device)))\n            dt = schedule[1:] - schedule[:-1]\n        else:\n            schedule = torch.cat((schedule, torch.zeros((1,), device=schedule.device)))\n            dt = -1 * (schedule[1:] - schedule[:-1])\n        return dt\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.ContinuousInferenceSchedule.__init__","title":"<code>__init__(nsteps, inclusive_end=False, min_t=0, padding=0, dilation=0, direction=TimeDirection.UNIFIED, device='cpu')</code>","text":"<p>Initialize the ContinuousInferenceSchedule.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>int</code> <p>Number of time steps.</p> required <code>inclusive_end</code> <code>bool</code> <p>If True, include the end value (1.0) in the schedule otherwise ends at 1.0-1/nsteps (default is False).</p> <code>False</code> <code>min_t</code> <code>Float</code> <p>minimum time value defaults to 0.</p> <code>0</code> <code>padding</code> <code>Float</code> <p>padding time value defaults to 0.</p> <code>0</code> <code>dilation</code> <code>Float</code> <p>dilation time value defaults to 0 ie the number of replicates.</p> <code>0</code> <code>direction</code> <code>Optional[str]</code> <p>TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).</p> <code>UNIFIED</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>'cpu'</code> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def __init__(\n    self,\n    nsteps: int,\n    inclusive_end: bool = False,\n    min_t: Float = 0,\n    padding: Float = 0,\n    dilation: Float = 0,\n    direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n    device: Union[str, torch.device] = \"cpu\",\n):\n    \"\"\"Initialize the ContinuousInferenceSchedule.\n\n    Args:\n        nsteps (int): Number of time steps.\n        inclusive_end (bool): If True, include the end value (1.0) in the schedule otherwise ends at 1.0-1/nsteps (default is False).\n        min_t (Float): minimum time value defaults to 0.\n        padding (Float): padding time value defaults to 0.\n        dilation (Float): dilation time value defaults to 0 ie the number of replicates.\n        direction (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n    \"\"\"\n    super().__init__(nsteps, min_t, padding, dilation, direction, device)\n    self.inclusive_end = inclusive_end\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.ContinuousInferenceSchedule.discretize","title":"<code>discretize(nsteps=None, schedule=None, device=None)</code>","text":"<p>Discretize the time schedule into a list of time deltas.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>Optioanl[int]</code> <p>Number of time steps. If None, uses the value from initialization.</p> <code>None</code> <code>schedule</code> <code>Optional[Tensor]</code> <p>Time scheudle if None will generate it with generate_schedule.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor of time deltas.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def discretize(\n    self,\n    nsteps: Optional[int] = None,\n    schedule: Optional[Tensor] = None,\n    device: Optional[Union[str, torch.device]] = None,\n) -&gt; Tensor:\n    \"\"\"Discretize the time schedule into a list of time deltas.\n\n    Args:\n        nsteps (Optioanl[int]): Number of time steps. If None, uses the value from initialization.\n        schedule (Optional[Tensor]): Time scheudle if None will generate it with generate_schedule.\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n    Returns:\n        Tensor: A tensor of time deltas.\n    \"\"\"\n    if device is None:\n        device = self.device\n    if schedule is None:\n        schedule = self.generate_schedule(nsteps, device=device)\n    if self.direction == TimeDirection.UNIFIED:\n        schedule = torch.cat((schedule, torch.ones((1,), device=schedule.device)))\n        dt = schedule[1:] - schedule[:-1]\n    else:\n        schedule = torch.cat((schedule, torch.zeros((1,), device=schedule.device)))\n        dt = -1 * (schedule[1:] - schedule[:-1])\n    return dt\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.DiscreteInferenceSchedule","title":"<code>DiscreteInferenceSchedule</code>","text":"<p>               Bases: <code>InferenceSchedule</code></p> <p>A base class for discrete time inference schedules.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>class DiscreteInferenceSchedule(InferenceSchedule):\n    \"\"\"A base class for discrete time inference schedules.\"\"\"\n\n    def discretize(\n        self,\n        nsteps: Optional[int] = None,\n        device: Optional[Union[str, torch.device]] = None,\n    ) -&gt; Tensor:\n        \"\"\"Discretize the time schedule into a list of time deltas.\n\n        Args:\n            nsteps (Optioanl[int]): Number of time steps. If None, uses the value from initialization.\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n        Returns:\n            Tensor: A tensor of time deltas.\n        \"\"\"\n        if self.padding &gt; 0 or self.dilation &gt; 0:\n            raise NotImplementedError(\"discreteize is not implemented for discrete schedules with padding or dilation\")\n        if device is None:\n            device = self.device\n        return torch.full(\n            (nsteps if nsteps is not None else self.nsteps,),\n            1 / (nsteps if nsteps is not None else self.nsteps),\n            device=device,\n        )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.DiscreteInferenceSchedule.discretize","title":"<code>discretize(nsteps=None, device=None)</code>","text":"<p>Discretize the time schedule into a list of time deltas.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>Optioanl[int]</code> <p>Number of time steps. If None, uses the value from initialization.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor of time deltas.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def discretize(\n    self,\n    nsteps: Optional[int] = None,\n    device: Optional[Union[str, torch.device]] = None,\n) -&gt; Tensor:\n    \"\"\"Discretize the time schedule into a list of time deltas.\n\n    Args:\n        nsteps (Optioanl[int]): Number of time steps. If None, uses the value from initialization.\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n    Returns:\n        Tensor: A tensor of time deltas.\n    \"\"\"\n    if self.padding &gt; 0 or self.dilation &gt; 0:\n        raise NotImplementedError(\"discreteize is not implemented for discrete schedules with padding or dilation\")\n    if device is None:\n        device = self.device\n    return torch.full(\n        (nsteps if nsteps is not None else self.nsteps,),\n        1 / (nsteps if nsteps is not None else self.nsteps),\n        device=device,\n    )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.DiscreteLinearInferenceSchedule","title":"<code>DiscreteLinearInferenceSchedule</code>","text":"<p>               Bases: <code>DiscreteInferenceSchedule</code></p> <p>A linear time schedule for discrete time inference.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>class DiscreteLinearInferenceSchedule(DiscreteInferenceSchedule):\n    \"\"\"A linear time schedule for discrete time inference.\"\"\"\n\n    def __init__(\n        self,\n        nsteps: int,\n        min_t: Float = 0,\n        padding: Float = 0,\n        dilation: Float = 0,\n        direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n        device: Union[str, torch.device] = \"cpu\",\n    ):\n        \"\"\"Initialize the DiscreteLinearInferenceSchedule.\n\n        Args:\n            nsteps (int): Number of time steps.\n            min_t (Float): minimum time value defaults to 0.\n            padding (Float): padding time value defaults to 0.\n            dilation (Float): dilation time value defaults to 0 ie the number of replicates.\n            direction (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n        \"\"\"\n        super().__init__(nsteps, min_t, padding, dilation, direction, device)\n\n    def generate_schedule(\n        self, nsteps: Optional[int] = None, device: Optional[Union[str, torch.device]] = None\n    ) -&gt; Tensor:\n        \"\"\"Generate the linear time schedule as a tensor.\n\n        Args:\n            nsteps (Optional[int]): Number of time steps. If None uses the value from initialization.\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n        Returns:\n            Tensor: A tensor of time steps.\n            Tensor: A tensor of time steps.\n        \"\"\"\n        if device is None:\n            device = self.device\n        if nsteps is None:\n            nsteps = self.nsteps\n        nsteps -= self.padding\n        dilation = self.dilation + 1\n        if dilation &gt; 1:\n            if nsteps % dilation != 0:\n                raise ValueError(f\"nsteps ({nsteps}) is not divisible by dilation + 1 ({dilation})\")\n            nsteps = int(nsteps / self.dilation)\n        if nsteps is None:\n            raise ValueError(\"nsteps cannot be None\")\n        schedule = torch.arange(nsteps).to(device=device)\n        if dilation &gt; 1:\n            schedule = schedule.repeat_interleave(dilation)\n        if self.direction == TimeDirection.DIFFUSION:\n            schedule = schedule.flip(0)\n        if self.padding &gt; 0:\n            schedule = torch.cat((schedule, schedule[-1] * torch.ones(self.padding, device=device)))\n        return schedule\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.DiscreteLinearInferenceSchedule.__init__","title":"<code>__init__(nsteps, min_t=0, padding=0, dilation=0, direction=TimeDirection.UNIFIED, device='cpu')</code>","text":"<p>Initialize the DiscreteLinearInferenceSchedule.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>int</code> <p>Number of time steps.</p> required <code>min_t</code> <code>Float</code> <p>minimum time value defaults to 0.</p> <code>0</code> <code>padding</code> <code>Float</code> <p>padding time value defaults to 0.</p> <code>0</code> <code>dilation</code> <code>Float</code> <p>dilation time value defaults to 0 ie the number of replicates.</p> <code>0</code> <code>direction</code> <code>Optional[str]</code> <p>TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).</p> <code>UNIFIED</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>'cpu'</code> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def __init__(\n    self,\n    nsteps: int,\n    min_t: Float = 0,\n    padding: Float = 0,\n    dilation: Float = 0,\n    direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n    device: Union[str, torch.device] = \"cpu\",\n):\n    \"\"\"Initialize the DiscreteLinearInferenceSchedule.\n\n    Args:\n        nsteps (int): Number of time steps.\n        min_t (Float): minimum time value defaults to 0.\n        padding (Float): padding time value defaults to 0.\n        dilation (Float): dilation time value defaults to 0 ie the number of replicates.\n        direction (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n    \"\"\"\n    super().__init__(nsteps, min_t, padding, dilation, direction, device)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.DiscreteLinearInferenceSchedule.generate_schedule","title":"<code>generate_schedule(nsteps=None, device=None)</code>","text":"<p>Generate the linear time schedule as a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>Optional[int]</code> <p>Number of time steps. If None uses the value from initialization.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor of time steps.</p> <code>Tensor</code> <code>Tensor</code> <p>A tensor of time steps.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def generate_schedule(\n    self, nsteps: Optional[int] = None, device: Optional[Union[str, torch.device]] = None\n) -&gt; Tensor:\n    \"\"\"Generate the linear time schedule as a tensor.\n\n    Args:\n        nsteps (Optional[int]): Number of time steps. If None uses the value from initialization.\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n    Returns:\n        Tensor: A tensor of time steps.\n        Tensor: A tensor of time steps.\n    \"\"\"\n    if device is None:\n        device = self.device\n    if nsteps is None:\n        nsteps = self.nsteps\n    nsteps -= self.padding\n    dilation = self.dilation + 1\n    if dilation &gt; 1:\n        if nsteps % dilation != 0:\n            raise ValueError(f\"nsteps ({nsteps}) is not divisible by dilation + 1 ({dilation})\")\n        nsteps = int(nsteps / self.dilation)\n    if nsteps is None:\n        raise ValueError(\"nsteps cannot be None\")\n    schedule = torch.arange(nsteps).to(device=device)\n    if dilation &gt; 1:\n        schedule = schedule.repeat_interleave(dilation)\n    if self.direction == TimeDirection.DIFFUSION:\n        schedule = schedule.flip(0)\n    if self.padding &gt; 0:\n        schedule = torch.cat((schedule, schedule[-1] * torch.ones(self.padding, device=device)))\n    return schedule\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.EntropicInferenceSchedule","title":"<code>EntropicInferenceSchedule</code>","text":"<p>               Bases: <code>ContinuousInferenceSchedule</code></p> <p>Generates an entropic time schedule.</p> <p>It remapping time based on the remaps cumulative information gain provided by a predictor function.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>class EntropicInferenceSchedule(ContinuousInferenceSchedule):\n    \"\"\"Generates an entropic time schedule.\n\n    It remapping time based on the remaps cumulative information gain provided by a predictor function.\n    \"\"\"\n\n    def __init__(\n        self,\n        predictor: Callable[[Tensor, Tensor], Tensor],\n        x_0_sampler: Callable[[int], Tensor],\n        x_1_sampler: Callable[[int], Tensor],\n        nsteps: int,\n        n_approx_entropy_points: int = 100,\n        batch_size: int = 128,\n        inclusive_end: bool = False,\n        min_t: float = 0,\n        direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n        device: Union[str, torch.device] = \"cpu\",\n        generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Inspired by the work from Dejan Stancevic, Florian Handke, &amp; Luca Ambrogioni. (2025).\n\n        Entropic Time Schedulers for Generative Diffusion Models.\n        https://arxiv.org/abs/2504.13612\n\n        This entropy rate is used to create an optimized, data-dependent time-stepping schedule for the generative process.\n\n        Approach benefits:\n        - Can improve inference performance\n        - Prevents oversampling of less-informative time steps\n        - Prevents undersampling of critical time windows\n        - Easily adapted into any model architecture leveraging flow-matching\n        - Sample-eficient way to generate data\n\n        Args:\n            predictor (Callable[[Tensor, Tensor], Tensor]): A callable (e.g., a function\n                or functools.partial) that takes a time tensor `t` and a data\n                tensor `x` and returns the predicted vector field `v`.\n\n            x_0_sampler (Callable[[int], Tensor]): A function that takes a batch size\n                and returns a tensor of samples from the initial distribution p0.\n\n            x_1_sampler (Callable[[int], Tensor]): A function that takes a batch size\n                and returns a tensor of samples from the target distribution p1.\n\n            nsteps (int): The final number of time steps for the inference schedule.\n\n            n_approx_entropy_points (int): The number of points used to approximate the\n                cumulative entropy curve. Higher is more accurate but slower.\n\n            batch_size (int): Batch size for calculating divergence at each time step.\n\n            inclusive_end (bool): If True, include 1.0, otherwise end just before.\n\n            min_t (Float): The minimum time value for the schedule.\n\n            direction (Union[TimeDirection, str]): 'UNIFIED' for forward (0-&gt;1) or\n                'DIFFUSION' for reverse (1-&gt;0).\n\n            device (Union[str, torch.device]): The device for computation.\n\n            generator (Optional[torch.Generator]): A PyTorch generator for reproducible\n                random number generation.\n        \"\"\"\n        super().__init__(nsteps, inclusive_end, min_t=min_t, direction=direction, device=device)\n        self.predictor = predictor\n        self.x_0_sampler = x_0_sampler\n        self.x_1_sampler = x_1_sampler\n        self.n_approx_entropy_points = n_approx_entropy_points\n        self.batch_size = batch_size\n        self.generator = generator\n\n    def _hutchinson_divergence(self, t: Tensor, x: Tensor) -&gt; Tensor:\n        \"\"\"Estimates the divergence of a vector field defined by a model using Hutchinson's method.\n\n        Args:\n            t (Tensor): A tensor of time values, shape [B, 1].\n            x (Tensor): A tensor of positions, shape [B, D].\n\n        Returns:\n            Tensor: The estimated divergence for each sample in the batch, shape [B].\n        \"\"\"\n        # Gradients with respect to x\n        x = x.detach().requires_grad_(True)\n\n        # random vector from the Rademacher distribution\n        if self.generator:\n            epsilon = (torch.randint(0, 2, x.shape, generator=self.generator, device=x.device) * 2 - 1).to(x.dtype)\n        else:\n            epsilon = (torch.randint_like(x, 0, 2) * 2 - 1).to(x.dtype)\n\n        v = self.predictor(t, x)\n\n        # Jacobian-vector product (JVP)\n        jvp = torch.autograd.grad(outputs=v, inputs=x, grad_outputs=epsilon, create_graph=False)[0]\n\n        # Divergence estimator\n        divergence_est = (jvp * epsilon).view(jvp.shape[0], -1).sum(dim=-1)\n\n        return divergence_est\n\n    def _calculate_entropy_rate(self, t_val: float) -&gt; float:\n        \"\"\"Calculates the mean entropy rate at a specific time t.\"\"\"\n        t_batch = torch.full((self.batch_size, 1), t_val, device=self.device)\n        x_0 = self.x_0_sampler(self.batch_size).to(self.device)\n        x_1 = self.x_1_sampler(self.batch_size).to(self.device)\n\n        x_t = (1 - t_val) * x_0 + t_val * x_1  ## TODO: This can be modified in the future\n\n        div = self._hutchinson_divergence(t_batch, x_t)\n        return div.mean().item()\n\n    def generate_schedule(\n        self, nsteps: Optional[int] = None, device: Optional[Union[str, torch.device]] = None\n    ) -&gt; Tensor:\n        \"\"\"Generates the entropic time schedule.\"\"\"\n        dev = device if device is not None else self.device\n        n_final_steps = nsteps if nsteps is not None else self.nsteps\n\n        # Calculating entropic profile over evaluation points\n        standard_time = torch.linspace(0, 1, self.n_approx_entropy_points, device=dev)\n        entropy_rates = torch.empty(self.n_approx_entropy_points, device=dev)\n\n        for i, t_val in enumerate(standard_time):\n            entropy_rates[i] = self._calculate_entropy_rate(t_val.item())\n\n        entropy_rates = entropy_rates.clamp(min=0)\n        cumulative_entropy = torch.cumsum(entropy_rates, dim=0)\n\n        if cumulative_entropy[-1] &gt; 1e-6:\n            cumulative_entropy = cumulative_entropy / cumulative_entropy[-1]\n\n        num_interp_points = n_final_steps + 1 if not self.inclusive_end else n_final_steps\n\n        uniform_time = torch.linspace(0, 1, num_interp_points, device=dev)\n\n        # projection of cumulative entropy into a linear space for creating the schedule\n        import numpy as np\n\n        entropic_schedule = np.interp(\n            uniform_time.cpu().numpy(), cumulative_entropy.cpu().numpy(), standard_time.cpu().numpy()\n        )\n        schedule = torch.from_numpy(entropic_schedule).to(dtype=torch.float32, device=dev)\n\n        if not self.inclusive_end:\n            schedule = schedule[:-1]\n\n        schedule = torch.clamp(schedule, min=self.min_t)\n        # Flipping the schedule at the end correctly handles the diffusion direction\n        if self.direction == TimeDirection.DIFFUSION:\n            schedule = 1.0 - schedule\n\n        return schedule\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.EntropicInferenceSchedule.__init__","title":"<code>__init__(predictor, x_0_sampler, x_1_sampler, nsteps, n_approx_entropy_points=100, batch_size=128, inclusive_end=False, min_t=0, direction=TimeDirection.UNIFIED, device='cpu', generator=None)</code>","text":"<p>Inspired by the work from Dejan Stancevic, Florian Handke, &amp; Luca Ambrogioni. (2025).</p> <p>Entropic Time Schedulers for Generative Diffusion Models. https://arxiv.org/abs/2504.13612</p> <p>This entropy rate is used to create an optimized, data-dependent time-stepping schedule for the generative process.</p> <p>Approach benefits: - Can improve inference performance - Prevents oversampling of less-informative time steps - Prevents undersampling of critical time windows - Easily adapted into any model architecture leveraging flow-matching - Sample-eficient way to generate data</p> <p>Parameters:</p> Name Type Description Default <code>predictor</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>A callable (e.g., a function or functools.partial) that takes a time tensor <code>t</code> and a data tensor <code>x</code> and returns the predicted vector field <code>v</code>.</p> required <code>x_0_sampler</code> <code>Callable[[int], Tensor]</code> <p>A function that takes a batch size and returns a tensor of samples from the initial distribution p0.</p> required <code>x_1_sampler</code> <code>Callable[[int], Tensor]</code> <p>A function that takes a batch size and returns a tensor of samples from the target distribution p1.</p> required <code>nsteps</code> <code>int</code> <p>The final number of time steps for the inference schedule.</p> required <code>n_approx_entropy_points</code> <code>int</code> <p>The number of points used to approximate the cumulative entropy curve. Higher is more accurate but slower.</p> <code>100</code> <code>batch_size</code> <code>int</code> <p>Batch size for calculating divergence at each time step.</p> <code>128</code> <code>inclusive_end</code> <code>bool</code> <p>If True, include 1.0, otherwise end just before.</p> <code>False</code> <code>min_t</code> <code>Float</code> <p>The minimum time value for the schedule.</p> <code>0</code> <code>direction</code> <code>Union[TimeDirection, str]</code> <p>'UNIFIED' for forward (0-&gt;1) or 'DIFFUSION' for reverse (1-&gt;0).</p> <code>UNIFIED</code> <code>device</code> <code>Union[str, device]</code> <p>The device for computation.</p> <code>'cpu'</code> <code>generator</code> <code>Optional[Generator]</code> <p>A PyTorch generator for reproducible random number generation.</p> <code>None</code> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def __init__(\n    self,\n    predictor: Callable[[Tensor, Tensor], Tensor],\n    x_0_sampler: Callable[[int], Tensor],\n    x_1_sampler: Callable[[int], Tensor],\n    nsteps: int,\n    n_approx_entropy_points: int = 100,\n    batch_size: int = 128,\n    inclusive_end: bool = False,\n    min_t: float = 0,\n    direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n    device: Union[str, torch.device] = \"cpu\",\n    generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Inspired by the work from Dejan Stancevic, Florian Handke, &amp; Luca Ambrogioni. (2025).\n\n    Entropic Time Schedulers for Generative Diffusion Models.\n    https://arxiv.org/abs/2504.13612\n\n    This entropy rate is used to create an optimized, data-dependent time-stepping schedule for the generative process.\n\n    Approach benefits:\n    - Can improve inference performance\n    - Prevents oversampling of less-informative time steps\n    - Prevents undersampling of critical time windows\n    - Easily adapted into any model architecture leveraging flow-matching\n    - Sample-eficient way to generate data\n\n    Args:\n        predictor (Callable[[Tensor, Tensor], Tensor]): A callable (e.g., a function\n            or functools.partial) that takes a time tensor `t` and a data\n            tensor `x` and returns the predicted vector field `v`.\n\n        x_0_sampler (Callable[[int], Tensor]): A function that takes a batch size\n            and returns a tensor of samples from the initial distribution p0.\n\n        x_1_sampler (Callable[[int], Tensor]): A function that takes a batch size\n            and returns a tensor of samples from the target distribution p1.\n\n        nsteps (int): The final number of time steps for the inference schedule.\n\n        n_approx_entropy_points (int): The number of points used to approximate the\n            cumulative entropy curve. Higher is more accurate but slower.\n\n        batch_size (int): Batch size for calculating divergence at each time step.\n\n        inclusive_end (bool): If True, include 1.0, otherwise end just before.\n\n        min_t (Float): The minimum time value for the schedule.\n\n        direction (Union[TimeDirection, str]): 'UNIFIED' for forward (0-&gt;1) or\n            'DIFFUSION' for reverse (1-&gt;0).\n\n        device (Union[str, torch.device]): The device for computation.\n\n        generator (Optional[torch.Generator]): A PyTorch generator for reproducible\n            random number generation.\n    \"\"\"\n    super().__init__(nsteps, inclusive_end, min_t=min_t, direction=direction, device=device)\n    self.predictor = predictor\n    self.x_0_sampler = x_0_sampler\n    self.x_1_sampler = x_1_sampler\n    self.n_approx_entropy_points = n_approx_entropy_points\n    self.batch_size = batch_size\n    self.generator = generator\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.EntropicInferenceSchedule.generate_schedule","title":"<code>generate_schedule(nsteps=None, device=None)</code>","text":"<p>Generates the entropic time schedule.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def generate_schedule(\n    self, nsteps: Optional[int] = None, device: Optional[Union[str, torch.device]] = None\n) -&gt; Tensor:\n    \"\"\"Generates the entropic time schedule.\"\"\"\n    dev = device if device is not None else self.device\n    n_final_steps = nsteps if nsteps is not None else self.nsteps\n\n    # Calculating entropic profile over evaluation points\n    standard_time = torch.linspace(0, 1, self.n_approx_entropy_points, device=dev)\n    entropy_rates = torch.empty(self.n_approx_entropy_points, device=dev)\n\n    for i, t_val in enumerate(standard_time):\n        entropy_rates[i] = self._calculate_entropy_rate(t_val.item())\n\n    entropy_rates = entropy_rates.clamp(min=0)\n    cumulative_entropy = torch.cumsum(entropy_rates, dim=0)\n\n    if cumulative_entropy[-1] &gt; 1e-6:\n        cumulative_entropy = cumulative_entropy / cumulative_entropy[-1]\n\n    num_interp_points = n_final_steps + 1 if not self.inclusive_end else n_final_steps\n\n    uniform_time = torch.linspace(0, 1, num_interp_points, device=dev)\n\n    # projection of cumulative entropy into a linear space for creating the schedule\n    import numpy as np\n\n    entropic_schedule = np.interp(\n        uniform_time.cpu().numpy(), cumulative_entropy.cpu().numpy(), standard_time.cpu().numpy()\n    )\n    schedule = torch.from_numpy(entropic_schedule).to(dtype=torch.float32, device=dev)\n\n    if not self.inclusive_end:\n        schedule = schedule[:-1]\n\n    schedule = torch.clamp(schedule, min=self.min_t)\n    # Flipping the schedule at the end correctly handles the diffusion direction\n    if self.direction == TimeDirection.DIFFUSION:\n        schedule = 1.0 - schedule\n\n    return schedule\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.InferenceSchedule","title":"<code>InferenceSchedule</code>","text":"<p>               Bases: <code>ABC</code></p> <p>A base class for inference time schedules.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>class InferenceSchedule(ABC):\n    \"\"\"A base class for inference time schedules.\"\"\"\n\n    def __init__(\n        self,\n        nsteps: int,\n        min_t: Float = 0,\n        padding: Float = 0,\n        dilation: Float = 0,\n        direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n        device: Union[str, torch.device] = \"cpu\",\n    ):\n        \"\"\"Initialize the InferenceSchedule.\n\n        Args:\n            nsteps (int): Number of time steps.\n            min_t (Float): minimum time value defaults to 0.\n            padding (Float): padding time value defaults to 0.\n            dilation (Float): dilation time value defaults to 0 ie the number of replicates.\n            direction (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n        \"\"\"\n        self.nsteps = nsteps\n        self.min_t = min_t\n        self.padding = padding\n        self.dilation = dilation\n        self.direction = string_to_enum(direction, TimeDirection)\n        self.device = device\n\n    @abstractmethod\n    def generate_schedule(\n        self, nsteps: Optional[int] = None, device: Optional[Union[str, torch.device]] = None\n    ) -&gt; Tensor:\n        \"\"\"Generate the time schedule as a tensor.\n\n        Args:\n            nsteps (Optioanl[int]): Number of time steps. If None, uses the value from initialization.\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n        \"\"\"\n        pass\n\n    def pad_time(\n        self, n_samples: int, scalar_time: Float, device: Optional[Union[str, torch.device]] = None\n    ) -&gt; Tensor:\n        \"\"\"Creates a tensor of shape (n_samples,) filled with a scalar time value.\n\n        Args:\n            n_samples (int): The desired dimension of the output tensor.\n            scalar_time (Float): The scalar time value to fill the tensor with.\n            device (Optional[Union[str, torch.device]], optional):\n                The device to place the tensor on. Defaults to None, which uses the default device.\n\n        Returns:\n            Tensor: A tensor of shape (n_samples,) filled with the scalar time value.\n        \"\"\"\n        return torch.full((n_samples,), fill_value=scalar_time).to(device)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.InferenceSchedule.__init__","title":"<code>__init__(nsteps, min_t=0, padding=0, dilation=0, direction=TimeDirection.UNIFIED, device='cpu')</code>","text":"<p>Initialize the InferenceSchedule.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>int</code> <p>Number of time steps.</p> required <code>min_t</code> <code>Float</code> <p>minimum time value defaults to 0.</p> <code>0</code> <code>padding</code> <code>Float</code> <p>padding time value defaults to 0.</p> <code>0</code> <code>dilation</code> <code>Float</code> <p>dilation time value defaults to 0 ie the number of replicates.</p> <code>0</code> <code>direction</code> <code>Optional[str]</code> <p>TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).</p> <code>UNIFIED</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>'cpu'</code> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def __init__(\n    self,\n    nsteps: int,\n    min_t: Float = 0,\n    padding: Float = 0,\n    dilation: Float = 0,\n    direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n    device: Union[str, torch.device] = \"cpu\",\n):\n    \"\"\"Initialize the InferenceSchedule.\n\n    Args:\n        nsteps (int): Number of time steps.\n        min_t (Float): minimum time value defaults to 0.\n        padding (Float): padding time value defaults to 0.\n        dilation (Float): dilation time value defaults to 0 ie the number of replicates.\n        direction (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n    \"\"\"\n    self.nsteps = nsteps\n    self.min_t = min_t\n    self.padding = padding\n    self.dilation = dilation\n    self.direction = string_to_enum(direction, TimeDirection)\n    self.device = device\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.InferenceSchedule.generate_schedule","title":"<code>generate_schedule(nsteps=None, device=None)</code>  <code>abstractmethod</code>","text":"<p>Generate the time schedule as a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>Optioanl[int]</code> <p>Number of time steps. If None, uses the value from initialization.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>None</code> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>@abstractmethod\ndef generate_schedule(\n    self, nsteps: Optional[int] = None, device: Optional[Union[str, torch.device]] = None\n) -&gt; Tensor:\n    \"\"\"Generate the time schedule as a tensor.\n\n    Args:\n        nsteps (Optioanl[int]): Number of time steps. If None, uses the value from initialization.\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n    \"\"\"\n    pass\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.InferenceSchedule.pad_time","title":"<code>pad_time(n_samples, scalar_time, device=None)</code>","text":"<p>Creates a tensor of shape (n_samples,) filled with a scalar time value.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>The desired dimension of the output tensor.</p> required <code>scalar_time</code> <code>Float</code> <p>The scalar time value to fill the tensor with.</p> required <code>device</code> <code>Optional[Union[str, device]]</code> <p>The device to place the tensor on. Defaults to None, which uses the default device.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor of shape (n_samples,) filled with the scalar time value.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def pad_time(\n    self, n_samples: int, scalar_time: Float, device: Optional[Union[str, torch.device]] = None\n) -&gt; Tensor:\n    \"\"\"Creates a tensor of shape (n_samples,) filled with a scalar time value.\n\n    Args:\n        n_samples (int): The desired dimension of the output tensor.\n        scalar_time (Float): The scalar time value to fill the tensor with.\n        device (Optional[Union[str, torch.device]], optional):\n            The device to place the tensor on. Defaults to None, which uses the default device.\n\n    Returns:\n        Tensor: A tensor of shape (n_samples,) filled with the scalar time value.\n    \"\"\"\n    return torch.full((n_samples,), fill_value=scalar_time).to(device)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.LinearInferenceSchedule","title":"<code>LinearInferenceSchedule</code>","text":"<p>               Bases: <code>ContinuousInferenceSchedule</code></p> <p>A linear time schedule for continuous time inference.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>class LinearInferenceSchedule(ContinuousInferenceSchedule):\n    \"\"\"A linear time schedule for continuous time inference.\"\"\"\n\n    def __init__(\n        self,\n        nsteps: int,\n        inclusive_end: bool = False,\n        min_t: Float = 0,\n        padding: Float = 0,\n        dilation: Float = 0,\n        direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n        device: Union[str, torch.device] = \"cpu\",\n    ):\n        \"\"\"Initialize the LinearInferenceSchedule.\n\n        Args:\n            nsteps (int): Number of time steps.\n            inclusive_end (bool): If True, include the end value (1.0) in the schedule otherwise ends at 1.0-1/nsteps (default is False).\n            min_t (Float): minimum time value defaults to 0.\n            padding (Float): padding time value defaults to 0.\n            dilation (Float): dilation time value defaults to 0 ie the number of replicates.\n            direction (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n        \"\"\"\n        super().__init__(nsteps, inclusive_end, min_t, padding, dilation, direction, device)\n\n    def generate_schedule(\n        self,\n        nsteps: Optional[int] = None,\n        device: Optional[Union[str, torch.device]] = None,\n    ) -&gt; Tensor:\n        \"\"\"Generate the linear time schedule as a tensor.\n\n        Args:\n            nsteps (Optional[int]): Number of time steps. If None uses the value from initialization.\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n        Returns:\n            Tensor: A tensor of time steps.\n        \"\"\"\n        if device is None:\n            device = self.device\n        if nsteps is None:\n            nsteps = self.nsteps\n        nsteps -= self.padding\n        dilation = self.dilation + 1\n        if dilation &gt; 1:\n            if nsteps % dilation != 0:\n                raise ValueError(f\"nsteps ({nsteps}) is not divisible by dilation + 1 ({dilation})\")\n            nsteps = int(nsteps / dilation)\n        if nsteps is None:\n            raise ValueError(\"nsteps cannot be None\")\n        if not self.inclusive_end:\n            schedule = torch.linspace(self.min_t, 1, nsteps + 1).to(device=device)\n            schedule = schedule[:-1]\n        else:\n            schedule = torch.linspace(self.min_t, 1, nsteps).to(device=device)\n        if dilation &gt; 1:\n            schedule = schedule.repeat_interleave(dilation)\n        if self.padding &gt; 0:\n            schedule = torch.cat((schedule, torch.ones(self.padding, device=device)))\n        if self.direction == TimeDirection.DIFFUSION:\n            schedule = 1 - schedule\n        return schedule\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.LinearInferenceSchedule.__init__","title":"<code>__init__(nsteps, inclusive_end=False, min_t=0, padding=0, dilation=0, direction=TimeDirection.UNIFIED, device='cpu')</code>","text":"<p>Initialize the LinearInferenceSchedule.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>int</code> <p>Number of time steps.</p> required <code>inclusive_end</code> <code>bool</code> <p>If True, include the end value (1.0) in the schedule otherwise ends at 1.0-1/nsteps (default is False).</p> <code>False</code> <code>min_t</code> <code>Float</code> <p>minimum time value defaults to 0.</p> <code>0</code> <code>padding</code> <code>Float</code> <p>padding time value defaults to 0.</p> <code>0</code> <code>dilation</code> <code>Float</code> <p>dilation time value defaults to 0 ie the number of replicates.</p> <code>0</code> <code>direction</code> <code>Optional[str]</code> <p>TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).</p> <code>UNIFIED</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>'cpu'</code> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def __init__(\n    self,\n    nsteps: int,\n    inclusive_end: bool = False,\n    min_t: Float = 0,\n    padding: Float = 0,\n    dilation: Float = 0,\n    direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n    device: Union[str, torch.device] = \"cpu\",\n):\n    \"\"\"Initialize the LinearInferenceSchedule.\n\n    Args:\n        nsteps (int): Number of time steps.\n        inclusive_end (bool): If True, include the end value (1.0) in the schedule otherwise ends at 1.0-1/nsteps (default is False).\n        min_t (Float): minimum time value defaults to 0.\n        padding (Float): padding time value defaults to 0.\n        dilation (Float): dilation time value defaults to 0 ie the number of replicates.\n        direction (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n    \"\"\"\n    super().__init__(nsteps, inclusive_end, min_t, padding, dilation, direction, device)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.LinearInferenceSchedule.generate_schedule","title":"<code>generate_schedule(nsteps=None, device=None)</code>","text":"<p>Generate the linear time schedule as a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>Optional[int]</code> <p>Number of time steps. If None uses the value from initialization.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor of time steps.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def generate_schedule(\n    self,\n    nsteps: Optional[int] = None,\n    device: Optional[Union[str, torch.device]] = None,\n) -&gt; Tensor:\n    \"\"\"Generate the linear time schedule as a tensor.\n\n    Args:\n        nsteps (Optional[int]): Number of time steps. If None uses the value from initialization.\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n    Returns:\n        Tensor: A tensor of time steps.\n    \"\"\"\n    if device is None:\n        device = self.device\n    if nsteps is None:\n        nsteps = self.nsteps\n    nsteps -= self.padding\n    dilation = self.dilation + 1\n    if dilation &gt; 1:\n        if nsteps % dilation != 0:\n            raise ValueError(f\"nsteps ({nsteps}) is not divisible by dilation + 1 ({dilation})\")\n        nsteps = int(nsteps / dilation)\n    if nsteps is None:\n        raise ValueError(\"nsteps cannot be None\")\n    if not self.inclusive_end:\n        schedule = torch.linspace(self.min_t, 1, nsteps + 1).to(device=device)\n        schedule = schedule[:-1]\n    else:\n        schedule = torch.linspace(self.min_t, 1, nsteps).to(device=device)\n    if dilation &gt; 1:\n        schedule = schedule.repeat_interleave(dilation)\n    if self.padding &gt; 0:\n        schedule = torch.cat((schedule, torch.ones(self.padding, device=device)))\n    if self.direction == TimeDirection.DIFFUSION:\n        schedule = 1 - schedule\n    return schedule\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.LogInferenceSchedule","title":"<code>LogInferenceSchedule</code>","text":"<p>               Bases: <code>ContinuousInferenceSchedule</code></p> <p>A log time schedule for inference, where time steps are generated by taking the logarithm of a uniform schedule.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>class LogInferenceSchedule(ContinuousInferenceSchedule):\n    \"\"\"A log time schedule for inference, where time steps are generated by taking the logarithm of a uniform schedule.\"\"\"\n\n    def __init__(\n        self,\n        nsteps: int,\n        inclusive_end: bool = False,\n        min_t: Float = 0,\n        padding: Float = 0,\n        dilation: Float = 0,\n        exponent: Float = -2.0,\n        direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n        device: Union[str, torch.device] = \"cpu\",\n    ):\n        \"\"\"Initialize the LogInferenceSchedule.\n\n        Returns a log space time schedule.\n\n        Which for 100 steps with default parameters is:\n            tensor([0.0000, 0.0455, 0.0889, 0.1303, 0.1699, 0.2077, 0.2439, 0.2783, 0.3113,\n                    0.3427, 0.3728, 0.4015, 0.4288, 0.4550, 0.4800, 0.5039, 0.5266, 0.5484,\n                    0.5692, 0.5890, 0.6080, 0.6261, 0.6434, 0.6599, 0.6756, 0.6907, 0.7051,\n                    0.7188, 0.7319, 0.7444, 0.7564, 0.7678, 0.7787, 0.7891, 0.7991, 0.8086,\n                    0.8176, 0.8263, 0.8346, 0.8425, 0.8500, 0.8572, 0.8641, 0.8707, 0.8769,\n                    0.8829, 0.8887, 0.8941, 0.8993, 0.9043, 0.9091, 0.9136, 0.9180, 0.9221,\n                    0.9261, 0.9299, 0.9335, 0.9369, 0.9402, 0.9434, 0.9464, 0.9492, 0.9520,\n                    0.9546, 0.9571, 0.9595, 0.9618, 0.9639, 0.9660, 0.9680, 0.9699, 0.9717,\n                    0.9734, 0.9751, 0.9767, 0.9782, 0.9796, 0.9810, 0.9823, 0.9835, 0.9847,\n                    0.9859, 0.9870, 0.9880, 0.9890, 0.9899, 0.9909, 0.9917, 0.9925, 0.9933,\n                    0.9941, 0.9948, 0.9955, 0.9962, 0.9968, 0.9974, 0.9980, 0.9985, 0.9990,\n                    0.9995])\n\n        Args:\n            nsteps (int): Number of time steps.\n            inclusive_end (bool): If True, include the end value (1.0) in the schedule otherwise ends at &lt;1.0 (default is False).\n            min_t (Float): minimum time value defaults to 0.\n            padding (Float): padding time value defaults to 0.\n            dilation (Float): dilation time value defaults to 0 ie the number of replicates.\n            exponent (Float): log space exponent parameter defaults to -2.0. The lower number the more aggressive the acceleration of 0 to 0.9 will be thus having more steps from 0.9 to 1.0.\n            direction (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n        \"\"\"\n        super().__init__(nsteps, inclusive_end, min_t, padding, dilation, direction, device)\n        if exponent is None:\n            raise ValueError(\"exponent cannot be None for the log schedule\")\n        if exponent &gt;= 0:\n            raise ValueError(f\"exponent input must be &lt;0, got {exponent}\")\n        self.exponent = exponent\n\n    def generate_schedule(\n        self,\n        nsteps: Optional[int] = None,\n        device: Optional[Union[str, torch.device]] = None,\n    ) -&gt; Tensor:\n        \"\"\"Generate the log time schedule as a tensor.\n\n        Args:\n            nsteps (Optional[int]): Number of time steps. If None uses the value from initialization.\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n        \"\"\"\n        if device is None:\n            device = self.device\n        if nsteps is None:\n            nsteps = self.nsteps\n        nsteps -= self.padding\n        dilation = self.dilation + 1\n        if dilation &gt; 1:\n            if nsteps % dilation != 0:\n                raise ValueError(f\"nsteps ({nsteps}) is not divisible by dilation + 1 ({dilation})\")\n            nsteps = int(nsteps / self.dilation)\n        if nsteps is None:\n            raise ValueError(\"nsteps cannot be None\")\n\n        if not self.inclusive_end:\n            t = 1.0 - torch.logspace(self.exponent, 0, nsteps + 1).flip(0).to(device=device)\n            t = t - torch.min(t)\n            schedule = t / torch.max(t)\n            schedule = schedule[:-1]\n        else:\n            t = 1.0 - torch.logspace(self.exponent, 0, nsteps).flip(0).to(device=device)\n            t = t - torch.min(t)\n            schedule = t / torch.max(t)\n\n        if self.min_t &gt; 0:\n            schedule = torch.clamp(schedule, min=self.min_t)\n\n        if dilation &gt; 1:\n            schedule = schedule.repeat_interleave(dilation)\n        if self.padding &gt; 0:\n            schedule = torch.cat((schedule, torch.ones(self.padding, device=device)))\n        if self.direction == TimeDirection.DIFFUSION:\n            schedule = 1 - schedule\n        return schedule\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.LogInferenceSchedule.__init__","title":"<code>__init__(nsteps, inclusive_end=False, min_t=0, padding=0, dilation=0, exponent=-2.0, direction=TimeDirection.UNIFIED, device='cpu')</code>","text":"<p>Initialize the LogInferenceSchedule.</p> <p>Returns a log space time schedule.</p> Which for 100 steps with default parameters is <p>tensor([0.0000, 0.0455, 0.0889, 0.1303, 0.1699, 0.2077, 0.2439, 0.2783, 0.3113,         0.3427, 0.3728, 0.4015, 0.4288, 0.4550, 0.4800, 0.5039, 0.5266, 0.5484,         0.5692, 0.5890, 0.6080, 0.6261, 0.6434, 0.6599, 0.6756, 0.6907, 0.7051,         0.7188, 0.7319, 0.7444, 0.7564, 0.7678, 0.7787, 0.7891, 0.7991, 0.8086,         0.8176, 0.8263, 0.8346, 0.8425, 0.8500, 0.8572, 0.8641, 0.8707, 0.8769,         0.8829, 0.8887, 0.8941, 0.8993, 0.9043, 0.9091, 0.9136, 0.9180, 0.9221,         0.9261, 0.9299, 0.9335, 0.9369, 0.9402, 0.9434, 0.9464, 0.9492, 0.9520,         0.9546, 0.9571, 0.9595, 0.9618, 0.9639, 0.9660, 0.9680, 0.9699, 0.9717,         0.9734, 0.9751, 0.9767, 0.9782, 0.9796, 0.9810, 0.9823, 0.9835, 0.9847,         0.9859, 0.9870, 0.9880, 0.9890, 0.9899, 0.9909, 0.9917, 0.9925, 0.9933,         0.9941, 0.9948, 0.9955, 0.9962, 0.9968, 0.9974, 0.9980, 0.9985, 0.9990,         0.9995])</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>int</code> <p>Number of time steps.</p> required <code>inclusive_end</code> <code>bool</code> <p>If True, include the end value (1.0) in the schedule otherwise ends at &lt;1.0 (default is False).</p> <code>False</code> <code>min_t</code> <code>Float</code> <p>minimum time value defaults to 0.</p> <code>0</code> <code>padding</code> <code>Float</code> <p>padding time value defaults to 0.</p> <code>0</code> <code>dilation</code> <code>Float</code> <p>dilation time value defaults to 0 ie the number of replicates.</p> <code>0</code> <code>exponent</code> <code>Float</code> <p>log space exponent parameter defaults to -2.0. The lower number the more aggressive the acceleration of 0 to 0.9 will be thus having more steps from 0.9 to 1.0.</p> <code>-2.0</code> <code>direction</code> <code>Optional[str]</code> <p>TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).</p> <code>UNIFIED</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>'cpu'</code> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def __init__(\n    self,\n    nsteps: int,\n    inclusive_end: bool = False,\n    min_t: Float = 0,\n    padding: Float = 0,\n    dilation: Float = 0,\n    exponent: Float = -2.0,\n    direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n    device: Union[str, torch.device] = \"cpu\",\n):\n    \"\"\"Initialize the LogInferenceSchedule.\n\n    Returns a log space time schedule.\n\n    Which for 100 steps with default parameters is:\n        tensor([0.0000, 0.0455, 0.0889, 0.1303, 0.1699, 0.2077, 0.2439, 0.2783, 0.3113,\n                0.3427, 0.3728, 0.4015, 0.4288, 0.4550, 0.4800, 0.5039, 0.5266, 0.5484,\n                0.5692, 0.5890, 0.6080, 0.6261, 0.6434, 0.6599, 0.6756, 0.6907, 0.7051,\n                0.7188, 0.7319, 0.7444, 0.7564, 0.7678, 0.7787, 0.7891, 0.7991, 0.8086,\n                0.8176, 0.8263, 0.8346, 0.8425, 0.8500, 0.8572, 0.8641, 0.8707, 0.8769,\n                0.8829, 0.8887, 0.8941, 0.8993, 0.9043, 0.9091, 0.9136, 0.9180, 0.9221,\n                0.9261, 0.9299, 0.9335, 0.9369, 0.9402, 0.9434, 0.9464, 0.9492, 0.9520,\n                0.9546, 0.9571, 0.9595, 0.9618, 0.9639, 0.9660, 0.9680, 0.9699, 0.9717,\n                0.9734, 0.9751, 0.9767, 0.9782, 0.9796, 0.9810, 0.9823, 0.9835, 0.9847,\n                0.9859, 0.9870, 0.9880, 0.9890, 0.9899, 0.9909, 0.9917, 0.9925, 0.9933,\n                0.9941, 0.9948, 0.9955, 0.9962, 0.9968, 0.9974, 0.9980, 0.9985, 0.9990,\n                0.9995])\n\n    Args:\n        nsteps (int): Number of time steps.\n        inclusive_end (bool): If True, include the end value (1.0) in the schedule otherwise ends at &lt;1.0 (default is False).\n        min_t (Float): minimum time value defaults to 0.\n        padding (Float): padding time value defaults to 0.\n        dilation (Float): dilation time value defaults to 0 ie the number of replicates.\n        exponent (Float): log space exponent parameter defaults to -2.0. The lower number the more aggressive the acceleration of 0 to 0.9 will be thus having more steps from 0.9 to 1.0.\n        direction (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n    \"\"\"\n    super().__init__(nsteps, inclusive_end, min_t, padding, dilation, direction, device)\n    if exponent is None:\n        raise ValueError(\"exponent cannot be None for the log schedule\")\n    if exponent &gt;= 0:\n        raise ValueError(f\"exponent input must be &lt;0, got {exponent}\")\n    self.exponent = exponent\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.LogInferenceSchedule.generate_schedule","title":"<code>generate_schedule(nsteps=None, device=None)</code>","text":"<p>Generate the log time schedule as a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>Optional[int]</code> <p>Number of time steps. If None uses the value from initialization.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>None</code> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def generate_schedule(\n    self,\n    nsteps: Optional[int] = None,\n    device: Optional[Union[str, torch.device]] = None,\n) -&gt; Tensor:\n    \"\"\"Generate the log time schedule as a tensor.\n\n    Args:\n        nsteps (Optional[int]): Number of time steps. If None uses the value from initialization.\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n    \"\"\"\n    if device is None:\n        device = self.device\n    if nsteps is None:\n        nsteps = self.nsteps\n    nsteps -= self.padding\n    dilation = self.dilation + 1\n    if dilation &gt; 1:\n        if nsteps % dilation != 0:\n            raise ValueError(f\"nsteps ({nsteps}) is not divisible by dilation + 1 ({dilation})\")\n        nsteps = int(nsteps / self.dilation)\n    if nsteps is None:\n        raise ValueError(\"nsteps cannot be None\")\n\n    if not self.inclusive_end:\n        t = 1.0 - torch.logspace(self.exponent, 0, nsteps + 1).flip(0).to(device=device)\n        t = t - torch.min(t)\n        schedule = t / torch.max(t)\n        schedule = schedule[:-1]\n    else:\n        t = 1.0 - torch.logspace(self.exponent, 0, nsteps).flip(0).to(device=device)\n        t = t - torch.min(t)\n        schedule = t / torch.max(t)\n\n    if self.min_t &gt; 0:\n        schedule = torch.clamp(schedule, min=self.min_t)\n\n    if dilation &gt; 1:\n        schedule = schedule.repeat_interleave(dilation)\n    if self.padding &gt; 0:\n        schedule = torch.cat((schedule, torch.ones(self.padding, device=device)))\n    if self.direction == TimeDirection.DIFFUSION:\n        schedule = 1 - schedule\n    return schedule\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.PowerInferenceSchedule","title":"<code>PowerInferenceSchedule</code>","text":"<p>               Bases: <code>ContinuousInferenceSchedule</code></p> <p>A power time schedule for inference, where time steps are generated by raising a uniform schedule to a specified power.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>class PowerInferenceSchedule(ContinuousInferenceSchedule):\n    \"\"\"A power time schedule for inference, where time steps are generated by raising a uniform schedule to a specified power.\"\"\"\n\n    def __init__(\n        self,\n        nsteps: int,\n        inclusive_end: bool = False,\n        min_t: Float = 0,\n        padding: Float = 0,\n        dilation: Float = 0,\n        exponent: Float = 1.0,\n        direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n        device: Union[str, torch.device] = \"cpu\",\n    ):\n        \"\"\"Initialize the PowerInferenceSchedule.\n\n        Args:\n            nsteps (int): Number of time steps.\n            inclusive_end (bool): If True, include the end value (1.0) in the schedule otherwise ends at &lt;1.0 (default is False).\n            min_t (Float): minimum time value defaults to 0.\n            padding (Float): padding time value defaults to 0.\n            dilation (Float): dilation time value defaults to 0 ie the number of replicates.\n            exponent (Float): Power parameter defaults to 1.0.\n            direction (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n        \"\"\"\n        super().__init__(nsteps, inclusive_end, min_t, padding, dilation, direction, device)\n        self.exponent = exponent\n\n    def generate_schedule(\n        self,\n        nsteps: Optional[int] = None,\n        device: Optional[Union[str, torch.device]] = None,\n    ) -&gt; Tensor:\n        \"\"\"Generate the power time schedule as a tensor.\n\n        Args:\n            nsteps (Optional[int]): Number of time steps. If None uses the value from initialization.\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n\n        Returns:\n            Tensor: A tensor of time steps.\n            Tensor: A tensor of time steps.\n        \"\"\"\n        if device is None:\n            device = self.device\n        if nsteps is None:\n            nsteps = self.nsteps\n        nsteps -= self.padding\n        dilation = self.dilation + 1\n        if dilation &gt; 1:\n            if nsteps % dilation != 0:\n                raise ValueError(f\"nsteps ({nsteps}) is not divisible by dilation + 1 ({dilation})\")\n            nsteps = int(nsteps / dilation)\n        if nsteps is None:\n            raise ValueError(\"nsteps cannot be None\")\n        if not self.inclusive_end:\n            schedule = torch.linspace(self.min_t, 1, nsteps + 1).to(device=device) ** self.exponent\n            schedule = schedule[:-1]\n        else:\n            schedule = torch.linspace(self.min_t, 1, nsteps).to(device=device) ** self.exponent\n        if dilation &gt; 1:\n            schedule = schedule.repeat_interleave(dilation)\n        if self.padding &gt; 0:\n            schedule = torch.cat((schedule, torch.ones(self.padding, device=device)))\n        if self.direction == TimeDirection.DIFFUSION:\n            schedule = 1 - schedule\n        return schedule\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.PowerInferenceSchedule.__init__","title":"<code>__init__(nsteps, inclusive_end=False, min_t=0, padding=0, dilation=0, exponent=1.0, direction=TimeDirection.UNIFIED, device='cpu')</code>","text":"<p>Initialize the PowerInferenceSchedule.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>int</code> <p>Number of time steps.</p> required <code>inclusive_end</code> <code>bool</code> <p>If True, include the end value (1.0) in the schedule otherwise ends at &lt;1.0 (default is False).</p> <code>False</code> <code>min_t</code> <code>Float</code> <p>minimum time value defaults to 0.</p> <code>0</code> <code>padding</code> <code>Float</code> <p>padding time value defaults to 0.</p> <code>0</code> <code>dilation</code> <code>Float</code> <p>dilation time value defaults to 0 ie the number of replicates.</p> <code>0</code> <code>exponent</code> <code>Float</code> <p>Power parameter defaults to 1.0.</p> <code>1.0</code> <code>direction</code> <code>Optional[str]</code> <p>TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).</p> <code>UNIFIED</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>'cpu'</code> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def __init__(\n    self,\n    nsteps: int,\n    inclusive_end: bool = False,\n    min_t: Float = 0,\n    padding: Float = 0,\n    dilation: Float = 0,\n    exponent: Float = 1.0,\n    direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n    device: Union[str, torch.device] = \"cpu\",\n):\n    \"\"\"Initialize the PowerInferenceSchedule.\n\n    Args:\n        nsteps (int): Number of time steps.\n        inclusive_end (bool): If True, include the end value (1.0) in the schedule otherwise ends at &lt;1.0 (default is False).\n        min_t (Float): minimum time value defaults to 0.\n        padding (Float): padding time value defaults to 0.\n        dilation (Float): dilation time value defaults to 0 ie the number of replicates.\n        exponent (Float): Power parameter defaults to 1.0.\n        direction (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n    \"\"\"\n    super().__init__(nsteps, inclusive_end, min_t, padding, dilation, direction, device)\n    self.exponent = exponent\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.PowerInferenceSchedule.generate_schedule","title":"<code>generate_schedule(nsteps=None, device=None)</code>","text":"<p>Generate the power time schedule as a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>Optional[int]</code> <p>Number of time steps. If None uses the value from initialization.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor of time steps.</p> <code>Tensor</code> <code>Tensor</code> <p>A tensor of time steps.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def generate_schedule(\n    self,\n    nsteps: Optional[int] = None,\n    device: Optional[Union[str, torch.device]] = None,\n) -&gt; Tensor:\n    \"\"\"Generate the power time schedule as a tensor.\n\n    Args:\n        nsteps (Optional[int]): Number of time steps. If None uses the value from initialization.\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n\n    Returns:\n        Tensor: A tensor of time steps.\n        Tensor: A tensor of time steps.\n    \"\"\"\n    if device is None:\n        device = self.device\n    if nsteps is None:\n        nsteps = self.nsteps\n    nsteps -= self.padding\n    dilation = self.dilation + 1\n    if dilation &gt; 1:\n        if nsteps % dilation != 0:\n            raise ValueError(f\"nsteps ({nsteps}) is not divisible by dilation + 1 ({dilation})\")\n        nsteps = int(nsteps / dilation)\n    if nsteps is None:\n        raise ValueError(\"nsteps cannot be None\")\n    if not self.inclusive_end:\n        schedule = torch.linspace(self.min_t, 1, nsteps + 1).to(device=device) ** self.exponent\n        schedule = schedule[:-1]\n    else:\n        schedule = torch.linspace(self.min_t, 1, nsteps).to(device=device) ** self.exponent\n    if dilation &gt; 1:\n        schedule = schedule.repeat_interleave(dilation)\n    if self.padding &gt; 0:\n        schedule = torch.cat((schedule, torch.ones(self.padding, device=device)))\n    if self.direction == TimeDirection.DIFFUSION:\n        schedule = 1 - schedule\n    return schedule\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/utils/","title":"Utils","text":""},{"location":"main/references/API_reference/bionemo/moco/schedules/utils/#bionemo.moco.schedules.utils.TimeDirection","title":"<code>TimeDirection</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for the direction of the noise schedule.</p> Source code in <code>bionemo/moco/schedules/utils.py</code> <pre><code>class TimeDirection(Enum):\n    \"\"\"Enum for the direction of the noise schedule.\"\"\"\n\n    UNIFIED = \"unified\"  # Noise(0) --&gt; Data(1)\n    DIFFUSION = \"diffusion\"  # Noise(1) --&gt; Data(0)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_noise_transforms/","title":"Continuous noise transforms","text":""},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_noise_transforms/#bionemo.moco.schedules.noise.continuous_noise_transforms.ContinuousExpNoiseTransform","title":"<code>ContinuousExpNoiseTransform</code>","text":"<p>               Bases: <code>ABC</code></p> <p>A base class for continuous schedules.</p> <p>alpha = exp(- sigma) where 1 - alpha controls the masking fraction.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_noise_transforms.py</code> <pre><code>class ContinuousExpNoiseTransform(ABC):\n    \"\"\"A base class for continuous schedules.\n\n    alpha = exp(- sigma) where 1 - alpha controls the masking fraction.\n    \"\"\"\n\n    def __init__(self, direction: TimeDirection):\n        \"\"\"Initialize the DiscreteNoiseSchedule.\n\n        Args:\n            direction : TimeDirection, required this defines in which direction the scheduler was built\n        \"\"\"\n        self.direction = string_to_enum(direction, TimeDirection)\n\n    def calculate_sigma(\n        self,\n        t: Tensor,\n        device: Union[str, torch.device] = \"cpu\",\n        synchronize: Optional[TimeDirection] = None,\n    ) -&gt; Tensor:\n        \"\"\"Calculate the sigma for the given time steps.\n\n        Args:\n            t (Tensor): The input tensor representing the time steps, with values ranging from 0 to 1.\n            device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n            synchronize (optional[TimeDirection]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction,\n                this parameter allows to flip the direction to match the specified one. Defaults to None.\n\n        Returns:\n            Tensor: A tensor representing the sigma values for the given time steps.\n\n        Raises:\n            ValueError: If the input time steps exceed the maximum allowed value of 1.\n        \"\"\"\n        if t.max() &gt; 1:\n            raise ValueError(f\"Invalid value: max continuous time is 1, but got {t.max().item()}\")\n\n        if synchronize and self.direction != string_to_enum(synchronize, TimeDirection):\n            t = 1 - t\n        return self._calculate_sigma(t, device)\n\n    @abstractmethod\n    def _calculate_sigma(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n        \"\"\"Calculate the -log of the clean data value for the given time steps.\n\n        Args:\n            t (Tensor): The input tensor representing the time steps.\n            device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n\n        Returns:\n            Tensor: A tensor representing the sigma values for the given time steps.\n        \"\"\"\n        pass\n\n    def sigma_to_alpha(self, sigma: Tensor) -&gt; Tensor:\n        \"\"\"Converts sigma to alpha values by alpha = exp(- sigma).\n\n        Args:\n            sigma (Tensor): The input sigma tensor.\n\n        Returns:\n            Tensor: A tensor containing the alpha values.\n        \"\"\"\n        return torch.exp(-1 * sigma)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_noise_transforms/#bionemo.moco.schedules.noise.continuous_noise_transforms.ContinuousExpNoiseTransform.__init__","title":"<code>__init__(direction)</code>","text":"<p>Initialize the DiscreteNoiseSchedule.</p> <p>Parameters:</p> Name Type Description Default <code>direction </code> <p>TimeDirection, required this defines in which direction the scheduler was built</p> required Source code in <code>bionemo/moco/schedules/noise/continuous_noise_transforms.py</code> <pre><code>def __init__(self, direction: TimeDirection):\n    \"\"\"Initialize the DiscreteNoiseSchedule.\n\n    Args:\n        direction : TimeDirection, required this defines in which direction the scheduler was built\n    \"\"\"\n    self.direction = string_to_enum(direction, TimeDirection)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_noise_transforms/#bionemo.moco.schedules.noise.continuous_noise_transforms.ContinuousExpNoiseTransform.calculate_sigma","title":"<code>calculate_sigma(t, device='cpu', synchronize=None)</code>","text":"<p>Calculate the sigma for the given time steps.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>The input tensor representing the time steps, with values ranging from 0 to 1.</p> required <code>device</code> <code>Optional[str]</code> <p>The device to place the schedule on. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>synchronize</code> <code>optional[TimeDirection]</code> <p>TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor representing the sigma values for the given time steps.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input time steps exceed the maximum allowed value of 1.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_noise_transforms.py</code> <pre><code>def calculate_sigma(\n    self,\n    t: Tensor,\n    device: Union[str, torch.device] = \"cpu\",\n    synchronize: Optional[TimeDirection] = None,\n) -&gt; Tensor:\n    \"\"\"Calculate the sigma for the given time steps.\n\n    Args:\n        t (Tensor): The input tensor representing the time steps, with values ranging from 0 to 1.\n        device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n        synchronize (optional[TimeDirection]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction,\n            this parameter allows to flip the direction to match the specified one. Defaults to None.\n\n    Returns:\n        Tensor: A tensor representing the sigma values for the given time steps.\n\n    Raises:\n        ValueError: If the input time steps exceed the maximum allowed value of 1.\n    \"\"\"\n    if t.max() &gt; 1:\n        raise ValueError(f\"Invalid value: max continuous time is 1, but got {t.max().item()}\")\n\n    if synchronize and self.direction != string_to_enum(synchronize, TimeDirection):\n        t = 1 - t\n    return self._calculate_sigma(t, device)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_noise_transforms/#bionemo.moco.schedules.noise.continuous_noise_transforms.ContinuousExpNoiseTransform.sigma_to_alpha","title":"<code>sigma_to_alpha(sigma)</code>","text":"<p>Converts sigma to alpha values by alpha = exp(- sigma).</p> <p>Parameters:</p> Name Type Description Default <code>sigma</code> <code>Tensor</code> <p>The input sigma tensor.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor containing the alpha values.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_noise_transforms.py</code> <pre><code>def sigma_to_alpha(self, sigma: Tensor) -&gt; Tensor:\n    \"\"\"Converts sigma to alpha values by alpha = exp(- sigma).\n\n    Args:\n        sigma (Tensor): The input sigma tensor.\n\n    Returns:\n        Tensor: A tensor containing the alpha values.\n    \"\"\"\n    return torch.exp(-1 * sigma)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_noise_transforms/#bionemo.moco.schedules.noise.continuous_noise_transforms.CosineExpNoiseTransform","title":"<code>CosineExpNoiseTransform</code>","text":"<p>               Bases: <code>ContinuousExpNoiseTransform</code></p> <p>A cosine Exponential noise schedule.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_noise_transforms.py</code> <pre><code>class CosineExpNoiseTransform(ContinuousExpNoiseTransform):\n    \"\"\"A cosine Exponential noise schedule.\"\"\"\n\n    def __init__(self, eps: Float = 1.0e-3):\n        \"\"\"Initialize the CosineNoiseSchedule.\n\n        Args:\n            eps (Float): small number to prevent numerical issues.\n        \"\"\"\n        self.direction = TimeDirection.DIFFUSION\n        self.eps = eps\n\n    def _calculate_sigma(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n        \"\"\"Calculate negative log of data interpolant fraction.\n\n        Args:\n            t (Tensor): The input tensor representing the time steps.\n            device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n\n        Returns:\n            Tensor: A tensor representing the sigma values for the given time steps.\n        \"\"\"\n        cos = torch.cos(t * torch.pi / 2).to(device)\n        return -torch.log(self.eps + (1 - self.eps) * cos)\n\n    def d_dt_sigma(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n        \"\"\"Compute the derivative of sigma with respect to time.\n\n        Args:\n            t (Tensor): The input tensor representing the time steps.\n            device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n\n        Returns:\n            Tensor: A tensor representing the derivative of sigma with respect to time.\n\n        Notes:\n            The derivative of sigma as a function of time is given by:\n\n            d/dt sigma(t) = d/dt (-log(cos(t * pi / 2) + eps))\n\n            Using the chain rule, we get:\n\n            d/dt sigma(t) = (-1 / (cos(t * pi / 2) + eps)) * (-sin(t * pi / 2) * pi / 2)\n\n            This is the derivative that is computed and returned by this method.\n        \"\"\"\n        cos = (1 - self.eps) * torch.cos(t * torch.pi / 2)\n        sin = (1 - self.eps) * torch.sin(t * torch.pi / 2)\n        scale = torch.pi / 2\n        derivative = scale * sin / (cos + self.eps)\n        return derivative.to(device)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_noise_transforms/#bionemo.moco.schedules.noise.continuous_noise_transforms.CosineExpNoiseTransform.__init__","title":"<code>__init__(eps=0.001)</code>","text":"<p>Initialize the CosineNoiseSchedule.</p> <p>Parameters:</p> Name Type Description Default <code>eps</code> <code>Float</code> <p>small number to prevent numerical issues.</p> <code>0.001</code> Source code in <code>bionemo/moco/schedules/noise/continuous_noise_transforms.py</code> <pre><code>def __init__(self, eps: Float = 1.0e-3):\n    \"\"\"Initialize the CosineNoiseSchedule.\n\n    Args:\n        eps (Float): small number to prevent numerical issues.\n    \"\"\"\n    self.direction = TimeDirection.DIFFUSION\n    self.eps = eps\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_noise_transforms/#bionemo.moco.schedules.noise.continuous_noise_transforms.CosineExpNoiseTransform.d_dt_sigma","title":"<code>d_dt_sigma(t, device='cpu')</code>","text":"<p>Compute the derivative of sigma with respect to time.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>The input tensor representing the time steps.</p> required <code>device</code> <code>Optional[str]</code> <p>The device to place the schedule on. Defaults to \"cpu\".</p> <code>'cpu'</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor representing the derivative of sigma with respect to time.</p> Notes <p>The derivative of sigma as a function of time is given by:</p> <p>d/dt sigma(t) = d/dt (-log(cos(t * pi / 2) + eps))</p> <p>Using the chain rule, we get:</p> <p>d/dt sigma(t) = (-1 / (cos(t * pi / 2) + eps)) * (-sin(t * pi / 2) * pi / 2)</p> <p>This is the derivative that is computed and returned by this method.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_noise_transforms.py</code> <pre><code>def d_dt_sigma(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n    \"\"\"Compute the derivative of sigma with respect to time.\n\n    Args:\n        t (Tensor): The input tensor representing the time steps.\n        device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n\n    Returns:\n        Tensor: A tensor representing the derivative of sigma with respect to time.\n\n    Notes:\n        The derivative of sigma as a function of time is given by:\n\n        d/dt sigma(t) = d/dt (-log(cos(t * pi / 2) + eps))\n\n        Using the chain rule, we get:\n\n        d/dt sigma(t) = (-1 / (cos(t * pi / 2) + eps)) * (-sin(t * pi / 2) * pi / 2)\n\n        This is the derivative that is computed and returned by this method.\n    \"\"\"\n    cos = (1 - self.eps) * torch.cos(t * torch.pi / 2)\n    sin = (1 - self.eps) * torch.sin(t * torch.pi / 2)\n    scale = torch.pi / 2\n    derivative = scale * sin / (cos + self.eps)\n    return derivative.to(device)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_noise_transforms/#bionemo.moco.schedules.noise.continuous_noise_transforms.LogLinearExpNoiseTransform","title":"<code>LogLinearExpNoiseTransform</code>","text":"<p>               Bases: <code>ContinuousExpNoiseTransform</code></p> <p>A log linear exponential schedule.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_noise_transforms.py</code> <pre><code>class LogLinearExpNoiseTransform(ContinuousExpNoiseTransform):\n    \"\"\"A log linear exponential schedule.\"\"\"\n\n    def __init__(self, eps: Float = 1.0e-3):\n        \"\"\"Initialize the CosineNoiseSchedule.\n\n        Args:\n            eps (Float): small value to prevent numerical issues.\n        \"\"\"\n        self.direction = TimeDirection.DIFFUSION\n        self.eps = eps\n\n    def _calculate_sigma(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n        \"\"\"Calculate negative log of data interpolant fraction.\n\n        Args:\n            t (Tensor): The input tensor representing the time steps.\n            device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n\n        Returns:\n            Tensor: A tensor representing the sigma values for the given time steps.\n        \"\"\"\n        return -torch.log1p(-(1 - self.eps) * t).to(device)\n\n    def d_dt_sigma(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n        \"\"\"Compute the derivative of sigma with respect to time.\n\n        Args:\n            t (Tensor): The input tensor representing the time steps.\n            device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n\n        Returns:\n            Tensor: A tensor representing the derivative of sigma with respect to time.\n        \"\"\"\n        derivative = (1 - self.eps) / (1 - (1 - self.eps) * t)\n        return derivative.to(device)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_noise_transforms/#bionemo.moco.schedules.noise.continuous_noise_transforms.LogLinearExpNoiseTransform.__init__","title":"<code>__init__(eps=0.001)</code>","text":"<p>Initialize the CosineNoiseSchedule.</p> <p>Parameters:</p> Name Type Description Default <code>eps</code> <code>Float</code> <p>small value to prevent numerical issues.</p> <code>0.001</code> Source code in <code>bionemo/moco/schedules/noise/continuous_noise_transforms.py</code> <pre><code>def __init__(self, eps: Float = 1.0e-3):\n    \"\"\"Initialize the CosineNoiseSchedule.\n\n    Args:\n        eps (Float): small value to prevent numerical issues.\n    \"\"\"\n    self.direction = TimeDirection.DIFFUSION\n    self.eps = eps\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_noise_transforms/#bionemo.moco.schedules.noise.continuous_noise_transforms.LogLinearExpNoiseTransform.d_dt_sigma","title":"<code>d_dt_sigma(t, device='cpu')</code>","text":"<p>Compute the derivative of sigma with respect to time.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>The input tensor representing the time steps.</p> required <code>device</code> <code>Optional[str]</code> <p>The device to place the schedule on. Defaults to \"cpu\".</p> <code>'cpu'</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor representing the derivative of sigma with respect to time.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_noise_transforms.py</code> <pre><code>def d_dt_sigma(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n    \"\"\"Compute the derivative of sigma with respect to time.\n\n    Args:\n        t (Tensor): The input tensor representing the time steps.\n        device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n\n    Returns:\n        Tensor: A tensor representing the derivative of sigma with respect to time.\n    \"\"\"\n    derivative = (1 - self.eps) / (1 - (1 - self.eps) * t)\n    return derivative.to(device)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/","title":"Continuous snr transforms","text":""},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.ContinuousSNRTransform","title":"<code>ContinuousSNRTransform</code>","text":"<p>               Bases: <code>ABC</code></p> <p>A base class for continuous SNR schedules.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>class ContinuousSNRTransform(ABC):\n    \"\"\"A base class for continuous SNR schedules.\"\"\"\n\n    def __init__(self, direction: TimeDirection):\n        \"\"\"Initialize the DiscreteNoiseSchedule.\n\n        Args:\n            direction (TimeDirection): required this defines in which direction the scheduler was built\n        \"\"\"\n        self.direction = string_to_enum(direction, TimeDirection)\n\n    def calculate_log_snr(\n        self,\n        t: Tensor,\n        device: Union[str, torch.device] = \"cpu\",\n        synchronize: Optional[TimeDirection] = None,\n    ) -&gt; Tensor:\n        \"\"\"Public wrapper to generate the time schedule as a tensor.\n\n        Args:\n            t (Tensor): The input tensor representing the time steps, with values ranging from 0 to 1.\n            device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n            synchronize (optional[TimeDirection]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction,\n                this parameter allows to flip the direction to match the specified one. Defaults to None.\n\n        Returns:\n            Tensor: A tensor representing the log signal-to-noise (SNR) ratio for the given time steps.\n        \"\"\"\n        if t.max() &gt; 1:\n            raise ValueError(f\"Invalid value: max continuous time is 1, but got {t.max().item()}\")\n\n        if synchronize and self.direction != string_to_enum(synchronize, TimeDirection):\n            t = 1 - t\n        return self._calculate_log_snr(t, device)\n\n    @abstractmethod\n    def _calculate_log_snr(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n        \"\"\"Generate the log signal-to-noise (SNR) ratio.\n\n        Args:\n            t (Tensor): The input tensor representing the time steps.\n            device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n\n        Returns:\n            Tensor: A tensor representing the log SNR values for the given time steps.\n        \"\"\"\n        pass\n\n    def log_snr_to_alphas_sigmas(self, log_snr: Tensor) -&gt; Tuple[Tensor, Tensor]:\n        \"\"\"Converts log signal-to-noise ratio (SNR) to alpha and sigma values.\n\n        Args:\n            log_snr (Tensor): The input log SNR tensor.\n\n        Returns:\n            tuple[Tensor, Tensor]: A tuple containing the squared root of alpha and sigma values.\n        \"\"\"\n        squared_alpha = log_snr.sigmoid()\n        squared_sigma = (-log_snr).sigmoid()\n        return squared_alpha.sqrt(), squared_sigma.sqrt()\n\n    def derivative(self, t: Tensor, func: Callable) -&gt; Tensor:\n        \"\"\"Compute derivative of a function, it supports bached single variable inputs.\n\n        Args:\n            t (Tensor): time variable at which derivatives are taken\n            func (Callable): function for derivative calculation\n\n        Returns:\n            Tensor: derivative that is detached from the computational graph\n        \"\"\"\n        with torch.enable_grad():\n            t.requires_grad_(True)\n            derivative = torch.autograd.grad(func(t).sum(), t, create_graph=False)[0].detach()\n            t.requires_grad_(False)\n        return derivative\n\n    def calculate_general_sde_terms(self, t):\n        \"\"\"Compute the general SDE terms for a given time step t.\n\n        Args:\n            t (Tensor): The input tensor representing the time step.\n\n        Returns:\n            tuple[Tensor, Tensor]: A tuple containing the drift term f_t and the diffusion term g_t_2.\n\n        Notes:\n            This method computes the drift and diffusion terms of the general SDE, which can be used to simulate the stochastic process.\n            The drift term represents the deterministic part of the process, while the diffusion term represents the stochastic part.\n        \"\"\"\n        t = t.clone()\n        t.requires_grad_(True)\n\n        # Compute log SNR\n        log_snr = self.calculate_log_snr(t, device=t.device)\n\n        # Alpha^2 and Sigma^2\n        alpha_squared = torch.sigmoid(log_snr)\n        sigma_squared = torch.sigmoid(-log_snr)\n\n        # Log Alpha\n        log_alpha = 0.5 * torch.log(alpha_squared)\n\n        # Compute derivatives\n        log_alpha_deriv = torch.autograd.grad(log_alpha.sum(), t, create_graph=False)[0].detach()\n        sigma_squared_deriv = torch.autograd.grad(sigma_squared.sum(), t, create_graph=False)[0].detach()\n\n        # Compute drift and diffusion terms\n        f_t = log_alpha_deriv  # Drift term\n        g_t_2 = sigma_squared_deriv - 2 * log_alpha_deriv * sigma_squared  # Diffusion term\n\n        return f_t, g_t_2\n\n    def calculate_beta(self, t):\n        r\"\"\"Compute the drift coefficient for the OU process of the form $dx = -\\frac{1}{2} \\beta(t) x dt + sqrt(beta(t)) dw_t$.\n\n        beta = d/dt log(alpha**2) = 2 * 1/alpha * d/dt(alpha)\n\n        Args:\n            t (Union[float, Tensor]): t in [0, 1]\n\n        Returns:\n            Tensor: beta(t)\n        \"\"\"\n        t = t.clone()\n        t.requires_grad_(True)\n        log_snr = self.calculate_log_snr(t, device=t.device)\n        alpha = self.calculate_alpha_log_snr(log_snr).detach()\n        alpha_deriv_t = self.derivative(t, self.calculate_alpha_t).detach()\n        beta = 2.0 * alpha_deriv_t / alpha\n        # Chroma has a negative here but when removing the negative we get f = d/dt log (alpha**2) and the step_ode function works as expected\n        return beta\n\n    def calculate_alpha_log_snr(self, log_snr: Tensor) -&gt; Tensor:\n        \"\"\"Compute alpha values based on the log SNR.\n\n        Args:\n            log_snr (Tensor): The input tensor representing the log signal-to-noise ratio.\n\n        Returns:\n            Tensor: A tensor representing the alpha values for the given log SNR.\n\n        Notes:\n            This method computes alpha values as the square root of the sigmoid of the log SNR.\n        \"\"\"\n        return torch.sigmoid(log_snr).sqrt()\n\n    def calculate_alpha_t(self, t: Tensor) -&gt; Tensor:\n        \"\"\"Compute alpha values based on the log SNR schedule.\n\n        Parameters:\n            t (Tensor): The input tensor representing the time steps.\n\n        Returns:\n            Tensor: A tensor representing the alpha values for the given time steps.\n\n        Notes:\n            This method computes alpha values as the square root of the sigmoid of the log SNR.\n        \"\"\"\n        log_snr = self.calculate_log_snr(t, device=t.device)\n        alpha = torch.sigmoid(log_snr).sqrt()\n        return alpha\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.ContinuousSNRTransform.__init__","title":"<code>__init__(direction)</code>","text":"<p>Initialize the DiscreteNoiseSchedule.</p> <p>Parameters:</p> Name Type Description Default <code>direction</code> <code>TimeDirection</code> <p>required this defines in which direction the scheduler was built</p> required Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def __init__(self, direction: TimeDirection):\n    \"\"\"Initialize the DiscreteNoiseSchedule.\n\n    Args:\n        direction (TimeDirection): required this defines in which direction the scheduler was built\n    \"\"\"\n    self.direction = string_to_enum(direction, TimeDirection)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.ContinuousSNRTransform.calculate_alpha_log_snr","title":"<code>calculate_alpha_log_snr(log_snr)</code>","text":"<p>Compute alpha values based on the log SNR.</p> <p>Parameters:</p> Name Type Description Default <code>log_snr</code> <code>Tensor</code> <p>The input tensor representing the log signal-to-noise ratio.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor representing the alpha values for the given log SNR.</p> Notes <p>This method computes alpha values as the square root of the sigmoid of the log SNR.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def calculate_alpha_log_snr(self, log_snr: Tensor) -&gt; Tensor:\n    \"\"\"Compute alpha values based on the log SNR.\n\n    Args:\n        log_snr (Tensor): The input tensor representing the log signal-to-noise ratio.\n\n    Returns:\n        Tensor: A tensor representing the alpha values for the given log SNR.\n\n    Notes:\n        This method computes alpha values as the square root of the sigmoid of the log SNR.\n    \"\"\"\n    return torch.sigmoid(log_snr).sqrt()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.ContinuousSNRTransform.calculate_alpha_t","title":"<code>calculate_alpha_t(t)</code>","text":"<p>Compute alpha values based on the log SNR schedule.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>The input tensor representing the time steps.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor representing the alpha values for the given time steps.</p> Notes <p>This method computes alpha values as the square root of the sigmoid of the log SNR.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def calculate_alpha_t(self, t: Tensor) -&gt; Tensor:\n    \"\"\"Compute alpha values based on the log SNR schedule.\n\n    Parameters:\n        t (Tensor): The input tensor representing the time steps.\n\n    Returns:\n        Tensor: A tensor representing the alpha values for the given time steps.\n\n    Notes:\n        This method computes alpha values as the square root of the sigmoid of the log SNR.\n    \"\"\"\n    log_snr = self.calculate_log_snr(t, device=t.device)\n    alpha = torch.sigmoid(log_snr).sqrt()\n    return alpha\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.ContinuousSNRTransform.calculate_beta","title":"<code>calculate_beta(t)</code>","text":"<p>Compute the drift coefficient for the OU process of the form $dx = -\\frac{1}{2} \\beta(t) x dt + sqrt(beta(t)) dw_t$.</p> <p>beta = d/dt log(alpha**2) = 2 * 1/alpha * d/dt(alpha)</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Union[float, Tensor]</code> <p>t in [0, 1]</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <p>beta(t)</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def calculate_beta(self, t):\n    r\"\"\"Compute the drift coefficient for the OU process of the form $dx = -\\frac{1}{2} \\beta(t) x dt + sqrt(beta(t)) dw_t$.\n\n    beta = d/dt log(alpha**2) = 2 * 1/alpha * d/dt(alpha)\n\n    Args:\n        t (Union[float, Tensor]): t in [0, 1]\n\n    Returns:\n        Tensor: beta(t)\n    \"\"\"\n    t = t.clone()\n    t.requires_grad_(True)\n    log_snr = self.calculate_log_snr(t, device=t.device)\n    alpha = self.calculate_alpha_log_snr(log_snr).detach()\n    alpha_deriv_t = self.derivative(t, self.calculate_alpha_t).detach()\n    beta = 2.0 * alpha_deriv_t / alpha\n    # Chroma has a negative here but when removing the negative we get f = d/dt log (alpha**2) and the step_ode function works as expected\n    return beta\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.ContinuousSNRTransform.calculate_general_sde_terms","title":"<code>calculate_general_sde_terms(t)</code>","text":"<p>Compute the general SDE terms for a given time step t.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>The input tensor representing the time step.</p> required <p>Returns:</p> Type Description <p>tuple[Tensor, Tensor]: A tuple containing the drift term f_t and the diffusion term g_t_2.</p> Notes <p>This method computes the drift and diffusion terms of the general SDE, which can be used to simulate the stochastic process. The drift term represents the deterministic part of the process, while the diffusion term represents the stochastic part.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def calculate_general_sde_terms(self, t):\n    \"\"\"Compute the general SDE terms for a given time step t.\n\n    Args:\n        t (Tensor): The input tensor representing the time step.\n\n    Returns:\n        tuple[Tensor, Tensor]: A tuple containing the drift term f_t and the diffusion term g_t_2.\n\n    Notes:\n        This method computes the drift and diffusion terms of the general SDE, which can be used to simulate the stochastic process.\n        The drift term represents the deterministic part of the process, while the diffusion term represents the stochastic part.\n    \"\"\"\n    t = t.clone()\n    t.requires_grad_(True)\n\n    # Compute log SNR\n    log_snr = self.calculate_log_snr(t, device=t.device)\n\n    # Alpha^2 and Sigma^2\n    alpha_squared = torch.sigmoid(log_snr)\n    sigma_squared = torch.sigmoid(-log_snr)\n\n    # Log Alpha\n    log_alpha = 0.5 * torch.log(alpha_squared)\n\n    # Compute derivatives\n    log_alpha_deriv = torch.autograd.grad(log_alpha.sum(), t, create_graph=False)[0].detach()\n    sigma_squared_deriv = torch.autograd.grad(sigma_squared.sum(), t, create_graph=False)[0].detach()\n\n    # Compute drift and diffusion terms\n    f_t = log_alpha_deriv  # Drift term\n    g_t_2 = sigma_squared_deriv - 2 * log_alpha_deriv * sigma_squared  # Diffusion term\n\n    return f_t, g_t_2\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.ContinuousSNRTransform.calculate_log_snr","title":"<code>calculate_log_snr(t, device='cpu', synchronize=None)</code>","text":"<p>Public wrapper to generate the time schedule as a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>The input tensor representing the time steps, with values ranging from 0 to 1.</p> required <code>device</code> <code>Optional[str]</code> <p>The device to place the schedule on. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>synchronize</code> <code>optional[TimeDirection]</code> <p>TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor representing the log signal-to-noise (SNR) ratio for the given time steps.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def calculate_log_snr(\n    self,\n    t: Tensor,\n    device: Union[str, torch.device] = \"cpu\",\n    synchronize: Optional[TimeDirection] = None,\n) -&gt; Tensor:\n    \"\"\"Public wrapper to generate the time schedule as a tensor.\n\n    Args:\n        t (Tensor): The input tensor representing the time steps, with values ranging from 0 to 1.\n        device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n        synchronize (optional[TimeDirection]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction,\n            this parameter allows to flip the direction to match the specified one. Defaults to None.\n\n    Returns:\n        Tensor: A tensor representing the log signal-to-noise (SNR) ratio for the given time steps.\n    \"\"\"\n    if t.max() &gt; 1:\n        raise ValueError(f\"Invalid value: max continuous time is 1, but got {t.max().item()}\")\n\n    if synchronize and self.direction != string_to_enum(synchronize, TimeDirection):\n        t = 1 - t\n    return self._calculate_log_snr(t, device)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.ContinuousSNRTransform.derivative","title":"<code>derivative(t, func)</code>","text":"<p>Compute derivative of a function, it supports bached single variable inputs.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>time variable at which derivatives are taken</p> required <code>func</code> <code>Callable</code> <p>function for derivative calculation</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>derivative that is detached from the computational graph</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def derivative(self, t: Tensor, func: Callable) -&gt; Tensor:\n    \"\"\"Compute derivative of a function, it supports bached single variable inputs.\n\n    Args:\n        t (Tensor): time variable at which derivatives are taken\n        func (Callable): function for derivative calculation\n\n    Returns:\n        Tensor: derivative that is detached from the computational graph\n    \"\"\"\n    with torch.enable_grad():\n        t.requires_grad_(True)\n        derivative = torch.autograd.grad(func(t).sum(), t, create_graph=False)[0].detach()\n        t.requires_grad_(False)\n    return derivative\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.ContinuousSNRTransform.log_snr_to_alphas_sigmas","title":"<code>log_snr_to_alphas_sigmas(log_snr)</code>","text":"<p>Converts log signal-to-noise ratio (SNR) to alpha and sigma values.</p> <p>Parameters:</p> Name Type Description Default <code>log_snr</code> <code>Tensor</code> <p>The input log SNR tensor.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>tuple[Tensor, Tensor]: A tuple containing the squared root of alpha and sigma values.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def log_snr_to_alphas_sigmas(self, log_snr: Tensor) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"Converts log signal-to-noise ratio (SNR) to alpha and sigma values.\n\n    Args:\n        log_snr (Tensor): The input log SNR tensor.\n\n    Returns:\n        tuple[Tensor, Tensor]: A tuple containing the squared root of alpha and sigma values.\n    \"\"\"\n    squared_alpha = log_snr.sigmoid()\n    squared_sigma = (-log_snr).sigmoid()\n    return squared_alpha.sqrt(), squared_sigma.sqrt()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.CosineSNRTransform","title":"<code>CosineSNRTransform</code>","text":"<p>               Bases: <code>ContinuousSNRTransform</code></p> <p>A cosine SNR schedule.</p> <p>Parameters:</p> Name Type Description Default <code>nu</code> <code>Optional[Float]</code> <p>Hyperparameter for the cosine schedule exponent (default is 1.0).</p> <code>1.0</code> <code>s</code> <code>Optional[Float]</code> <p>Hyperparameter for the cosine schedule shift (default is 0.008).</p> <code>0.008</code> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>class CosineSNRTransform(ContinuousSNRTransform):\n    \"\"\"A cosine SNR schedule.\n\n    Args:\n        nu (Optional[Float]): Hyperparameter for the cosine schedule exponent (default is 1.0).\n        s (Optional[Float]): Hyperparameter for the cosine schedule shift (default is 0.008).\n    \"\"\"\n\n    def __init__(self, nu: Float = 1.0, s: Float = 0.008):\n        \"\"\"Initialize the CosineNoiseSchedule.\"\"\"\n        self.direction = TimeDirection.DIFFUSION\n        self.nu = nu\n        self.s = s\n\n    def _calculate_log_snr(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n        \"\"\"Calculate the log signal-to-noise ratio (SNR) for the cosine noise schedule i.e. -gamma.\n\n        The SNR is the equivalent to alpha_bar**2 / (1 - alpha_bar**2) from DDPM.\n        This method computes the log SNR as described in the paper \"Improved Denoising Diffusion Probabilistic Models\" (https://arxiv.org/pdf/2107.00630).\n        Note 1 / (1 + exp(- log_snr)) returns this cosine**2 for alpha_bar**2\n        See  https://openreview.net/attachment?id=2LdBqxc1Yv&amp;name=supplementary_material and https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/continuous_time_gaussian_diffusion.py\n\n        Args:\n            t (Tensor): The input tensor representing the time steps.\n            device (str): Device to place the schedule on (default is \"cpu\").\n\n        Returns:\n            Tensor: A tensor representing the log SNR for the given time steps.\n        \"\"\"\n        return -log((torch.cos((t**self.nu + self.s) / (1 + self.s) * math.pi * 0.5) ** -2) - 1, eps=1e-5).to(device)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.CosineSNRTransform.__init__","title":"<code>__init__(nu=1.0, s=0.008)</code>","text":"<p>Initialize the CosineNoiseSchedule.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def __init__(self, nu: Float = 1.0, s: Float = 0.008):\n    \"\"\"Initialize the CosineNoiseSchedule.\"\"\"\n    self.direction = TimeDirection.DIFFUSION\n    self.nu = nu\n    self.s = s\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.LinearLogInterpolatedSNRTransform","title":"<code>LinearLogInterpolatedSNRTransform</code>","text":"<p>               Bases: <code>ContinuousSNRTransform</code></p> <p>A Linear Log space interpolated SNR schedule.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>class LinearLogInterpolatedSNRTransform(ContinuousSNRTransform):\n    \"\"\"A Linear Log space interpolated SNR schedule.\"\"\"\n\n    def __init__(self, min_value: Float = -7.0, max_value=13.5):\n        \"\"\"Initialize the Linear log space interpolated SNR Schedule from Chroma.\n\n        Args:\n            min_value (Float): The min log SNR value.\n            max_value (Float): the max log SNR value.\n        \"\"\"\n        self.direction = TimeDirection.DIFFUSION\n        self.min_value = min_value\n        self.max_value = max_value\n\n    def _calculate_log_snr(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n        \"\"\"Calculate the log signal-to-noise ratio (SNR) for the cosine noise schedule i.e. -gamma.\n\n        See https://github.com/generatebio/chroma/blob/929407c605013613941803c6113adefdccaad679/chroma/layers/structure/diffusion.py#L316C23-L316C50\n\n        Args:\n            t (Tensor): The input tensor representing the time steps.\n            device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n\n        Returns:\n            Tensor: A tensor representing the log SNR for the given time steps.\n        \"\"\"\n        log_snr = (1 - t) * self.max_value + t * self.min_value\n        return log_snr.to(device)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.LinearLogInterpolatedSNRTransform.__init__","title":"<code>__init__(min_value=-7.0, max_value=13.5)</code>","text":"<p>Initialize the Linear log space interpolated SNR Schedule from Chroma.</p> <p>Parameters:</p> Name Type Description Default <code>min_value</code> <code>Float</code> <p>The min log SNR value.</p> <code>-7.0</code> <code>max_value</code> <code>Float</code> <p>the max log SNR value.</p> <code>13.5</code> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def __init__(self, min_value: Float = -7.0, max_value=13.5):\n    \"\"\"Initialize the Linear log space interpolated SNR Schedule from Chroma.\n\n    Args:\n        min_value (Float): The min log SNR value.\n        max_value (Float): the max log SNR value.\n    \"\"\"\n    self.direction = TimeDirection.DIFFUSION\n    self.min_value = min_value\n    self.max_value = max_value\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.LinearSNRTransform","title":"<code>LinearSNRTransform</code>","text":"<p>               Bases: <code>ContinuousSNRTransform</code></p> <p>A Linear SNR schedule.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>class LinearSNRTransform(ContinuousSNRTransform):\n    \"\"\"A Linear SNR schedule.\"\"\"\n\n    def __init__(self, min_value: Float = 1.0e-4):\n        \"\"\"Initialize the Linear SNR Transform.\n\n        Args:\n            min_value (Float): min vaue of SNR defaults to 1.e-4.\n        \"\"\"\n        self.direction = TimeDirection.DIFFUSION\n        self.min_value = min_value\n\n    def _calculate_log_snr(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n        \"\"\"Calculate the log signal-to-noise ratio (SNR) for the cosine noise schedule i.e. -gamma.\n\n        The SNR is the equivalent to alpha_bar**2 / (1 - alpha_bar**2) from DDPM.\n        See  https://openreview.net/attachment?id=2LdBqxc1Yv&amp;name=supplementary_material and https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/continuous_time_gaussian_diffusion.py\n\n        Args:\n            t (Tensor): The input tensor representing the time steps.\n            device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n\n        Returns:\n            Tensor: A tensor representing the log SNR for the given time steps.\n        \"\"\"\n        # This is equivalanet to the interpolated one from -10 to 9.2\n        return -log(torch.expm1(self.min_value + 10 * (t**2))).to(device)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.LinearSNRTransform.__init__","title":"<code>__init__(min_value=0.0001)</code>","text":"<p>Initialize the Linear SNR Transform.</p> <p>Parameters:</p> Name Type Description Default <code>min_value</code> <code>Float</code> <p>min vaue of SNR defaults to 1.e-4.</p> <code>0.0001</code> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def __init__(self, min_value: Float = 1.0e-4):\n    \"\"\"Initialize the Linear SNR Transform.\n\n    Args:\n        min_value (Float): min vaue of SNR defaults to 1.e-4.\n    \"\"\"\n    self.direction = TimeDirection.DIFFUSION\n    self.min_value = min_value\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.log","title":"<code>log(t, eps=1e-20)</code>","text":"<p>Compute the natural logarithm of a tensor, clamping values to avoid numerical instability.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>The input tensor.</p> required <code>eps</code> <code>float</code> <p>The minimum value to clamp the input tensor (default is 1e-20).</p> <code>1e-20</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The natural logarithm of the input tensor.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def log(t, eps=1e-20):\n    \"\"\"Compute the natural logarithm of a tensor, clamping values to avoid numerical instability.\n\n    Args:\n        t (Tensor): The input tensor.\n        eps (float, optional): The minimum value to clamp the input tensor (default is 1e-20).\n\n    Returns:\n        Tensor: The natural logarithm of the input tensor.\n    \"\"\"\n    return torch.log(t.clamp(min=eps))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/discrete_noise_schedules/","title":"Discrete noise schedules","text":""},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/discrete_noise_schedules/#bionemo.moco.schedules.noise.discrete_noise_schedules.DiscreteCosineNoiseSchedule","title":"<code>DiscreteCosineNoiseSchedule</code>","text":"<p>               Bases: <code>DiscreteNoiseSchedule</code></p> <p>A cosine discrete noise schedule.</p> Source code in <code>bionemo/moco/schedules/noise/discrete_noise_schedules.py</code> <pre><code>class DiscreteCosineNoiseSchedule(DiscreteNoiseSchedule):\n    \"\"\"A cosine discrete noise schedule.\"\"\"\n\n    def __init__(self, nsteps: int, nu: Float = 1.0, s: Float = 0.008):\n        \"\"\"Initialize the CosineNoiseSchedule.\n\n        Args:\n            nsteps (int): Number of discrete steps.\n            nu (Optional[Float]): Hyperparameter for the cosine schedule exponent (default is 1.0).\n            s (Optional[Float]): Hyperparameter for the cosine schedule shift (default is 0.008).\n        \"\"\"\n        super().__init__(nsteps=nsteps, direction=TimeDirection.DIFFUSION)\n        self.nu = nu\n        self.s = s\n\n    def _generate_schedule(self, nsteps: Optional[int] = None, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n        \"\"\"Generate the cosine noise schedule.\n\n        Args:\n            nsteps (Optional[int]): Number of time steps. If None, uses the value from initialization.\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n        \"\"\"\n        if nsteps is None:\n            nsteps = self.nsteps\n        steps = (\n            nsteps + 1\n        )  #! matches OpenAI code https://github.com/openai/improved-diffusion/blob/main/improved_diffusion/gaussian_diffusion.py#L62\n        x = torch.linspace(0, nsteps, steps, device=device)\n        alphas_cumprod = torch.cos(((x / nsteps) ** self.nu + self.s) / (1 + self.s) * torch.pi * 0.5) ** 2\n        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n        betas = torch.clip(betas, 0.001, 0.999)\n        return 1 - betas\n\n    def _clip_noise_schedule(self, alphas2: Tensor, clip_value: Float = 0.001) -&gt; Tensor:\n        \"\"\"For a noise schedule given by alpha^2, this clips alpha_t / alpha_t-1. This may help improve stability during sampling.\n\n        Args:\n            alphas2 (Tensor): The noise schedule given by alpha^2.\n            clip_value (Optional[Float]): The minimum value for alpha_t / alpha_t-1 (default is 0.001).\n\n        Returns:\n            Tensor: The clipped noise schedule.\n        \"\"\"\n        alphas2 = torch.cat([torch.ones(1, device=alphas2.device), alphas2], dim=0)\n\n        alphas_step = alphas2[1:] / alphas2[:-1]\n\n        alphas_step = torch.clamp(alphas_step, min=clip_value, max=1.0)\n        alphas2 = torch.cumprod(alphas_step, dim=0)\n\n        return alphas2\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/discrete_noise_schedules/#bionemo.moco.schedules.noise.discrete_noise_schedules.DiscreteCosineNoiseSchedule.__init__","title":"<code>__init__(nsteps, nu=1.0, s=0.008)</code>","text":"<p>Initialize the CosineNoiseSchedule.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>int</code> <p>Number of discrete steps.</p> required <code>nu</code> <code>Optional[Float]</code> <p>Hyperparameter for the cosine schedule exponent (default is 1.0).</p> <code>1.0</code> <code>s</code> <code>Optional[Float]</code> <p>Hyperparameter for the cosine schedule shift (default is 0.008).</p> <code>0.008</code> Source code in <code>bionemo/moco/schedules/noise/discrete_noise_schedules.py</code> <pre><code>def __init__(self, nsteps: int, nu: Float = 1.0, s: Float = 0.008):\n    \"\"\"Initialize the CosineNoiseSchedule.\n\n    Args:\n        nsteps (int): Number of discrete steps.\n        nu (Optional[Float]): Hyperparameter for the cosine schedule exponent (default is 1.0).\n        s (Optional[Float]): Hyperparameter for the cosine schedule shift (default is 0.008).\n    \"\"\"\n    super().__init__(nsteps=nsteps, direction=TimeDirection.DIFFUSION)\n    self.nu = nu\n    self.s = s\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/discrete_noise_schedules/#bionemo.moco.schedules.noise.discrete_noise_schedules.DiscreteLinearNoiseSchedule","title":"<code>DiscreteLinearNoiseSchedule</code>","text":"<p>               Bases: <code>DiscreteNoiseSchedule</code></p> <p>A linear discrete noise schedule.</p> Source code in <code>bionemo/moco/schedules/noise/discrete_noise_schedules.py</code> <pre><code>class DiscreteLinearNoiseSchedule(DiscreteNoiseSchedule):\n    \"\"\"A linear discrete noise schedule.\"\"\"\n\n    def __init__(self, nsteps: int, beta_start: Float = 1e-4, beta_end: Float = 0.02):\n        \"\"\"Initialize the CosineNoiseSchedule.\n\n        Args:\n            nsteps (Optional[int]): Number of time steps. If None, uses the value from initialization.\n            beta_start (Optional[int]): starting beta value. Defaults to 1e-4.\n            beta_end (Optional[int]): end beta value. Defaults to 0.02.\n        \"\"\"\n        super().__init__(nsteps=nsteps, direction=TimeDirection.DIFFUSION)\n        self.beta_start = beta_start\n        self.beta_end = beta_end\n\n    def _generate_schedule(self, nsteps: Optional[int] = None, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n        \"\"\"Generate the cosine noise schedule.\n\n        Args:\n            nsteps (Optional[int]): Number of time steps. If None, uses the value from initialization.\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n        \"\"\"\n        if nsteps is None:\n            nsteps = self.nsteps\n        betas = torch.linspace(self.beta_start, self.beta_end, nsteps, dtype=torch.float32, device=device)\n        return 1 - betas\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/discrete_noise_schedules/#bionemo.moco.schedules.noise.discrete_noise_schedules.DiscreteLinearNoiseSchedule.__init__","title":"<code>__init__(nsteps, beta_start=0.0001, beta_end=0.02)</code>","text":"<p>Initialize the CosineNoiseSchedule.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>Optional[int]</code> <p>Number of time steps. If None, uses the value from initialization.</p> required <code>beta_start</code> <code>Optional[int]</code> <p>starting beta value. Defaults to 1e-4.</p> <code>0.0001</code> <code>beta_end</code> <code>Optional[int]</code> <p>end beta value. Defaults to 0.02.</p> <code>0.02</code> Source code in <code>bionemo/moco/schedules/noise/discrete_noise_schedules.py</code> <pre><code>def __init__(self, nsteps: int, beta_start: Float = 1e-4, beta_end: Float = 0.02):\n    \"\"\"Initialize the CosineNoiseSchedule.\n\n    Args:\n        nsteps (Optional[int]): Number of time steps. If None, uses the value from initialization.\n        beta_start (Optional[int]): starting beta value. Defaults to 1e-4.\n        beta_end (Optional[int]): end beta value. Defaults to 0.02.\n    \"\"\"\n    super().__init__(nsteps=nsteps, direction=TimeDirection.DIFFUSION)\n    self.beta_start = beta_start\n    self.beta_end = beta_end\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/discrete_noise_schedules/#bionemo.moco.schedules.noise.discrete_noise_schedules.DiscreteNoiseSchedule","title":"<code>DiscreteNoiseSchedule</code>","text":"<p>               Bases: <code>ABC</code></p> <p>A base class for discrete noise schedules.</p> Source code in <code>bionemo/moco/schedules/noise/discrete_noise_schedules.py</code> <pre><code>class DiscreteNoiseSchedule(ABC):\n    \"\"\"A base class for discrete noise schedules.\"\"\"\n\n    def __init__(self, nsteps: int, direction: TimeDirection):\n        \"\"\"Initialize the DiscreteNoiseSchedule.\n\n        Args:\n           nsteps (int): number of discrete steps.\n           direction (TimeDirection): required this defines in which direction the scheduler was built\n        \"\"\"\n        self.nsteps = nsteps\n        self.direction = string_to_enum(direction, TimeDirection)\n\n    def generate_schedule(\n        self,\n        nsteps: Optional[int] = None,\n        device: Union[str, torch.device] = \"cpu\",\n        synchronize: Optional[TimeDirection] = None,\n    ) -&gt; Tensor:\n        \"\"\"Generate the noise schedule as a tensor.\n\n        Args:\n            nsteps (Optional[int]): Number of time steps. If None, uses the value from initialization.\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n            synchronize (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction,\n                this parameter allows to flip the direction to match the specified one (default is None).\n        \"\"\"\n        schedule = self._generate_schedule(nsteps, device)\n        if synchronize and self.direction != string_to_enum(synchronize, TimeDirection):\n            return torch.flip(schedule, dims=[0])\n        else:\n            return schedule\n\n    @abstractmethod\n    def _generate_schedule(self, nsteps: Optional[int] = None, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n        \"\"\"Generate the noise schedule tensor.\n\n        Args:\n            nsteps (Optional[int]): Number of time steps. If None, uses the value from initialization.\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n        \"\"\"\n        pass\n\n    def calculate_derivative(\n        self,\n        nsteps: Optional[int] = None,\n        device: Union[str, torch.device] = \"cpu\",\n        synchronize: Optional[TimeDirection] = None,\n    ) -&gt; Tensor:\n        \"\"\"Calculate the time derivative of the schedule.\n\n        Args:\n            nsteps (Optional[int]): Number of time steps. If None, uses the value from initialization.\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n            synchronize (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction,\n                this parameter allows to flip the direction to match the specified one (default is None).\n\n        Returns:\n            Tensor: A tensor representing the time derivative of the schedule.\n\n        Raises:\n            NotImplementedError: If the derivative calculation is not implemented for this schedule.\n        \"\"\"\n        raise NotImplementedError(\"Derivative calculation is not implemented for this schedule.\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/discrete_noise_schedules/#bionemo.moco.schedules.noise.discrete_noise_schedules.DiscreteNoiseSchedule.__init__","title":"<code>__init__(nsteps, direction)</code>","text":"<p>Initialize the DiscreteNoiseSchedule.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>int</code> <p>number of discrete steps.</p> required <code>direction</code> <code>TimeDirection</code> <p>required this defines in which direction the scheduler was built</p> required Source code in <code>bionemo/moco/schedules/noise/discrete_noise_schedules.py</code> <pre><code>def __init__(self, nsteps: int, direction: TimeDirection):\n    \"\"\"Initialize the DiscreteNoiseSchedule.\n\n    Args:\n       nsteps (int): number of discrete steps.\n       direction (TimeDirection): required this defines in which direction the scheduler was built\n    \"\"\"\n    self.nsteps = nsteps\n    self.direction = string_to_enum(direction, TimeDirection)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/discrete_noise_schedules/#bionemo.moco.schedules.noise.discrete_noise_schedules.DiscreteNoiseSchedule.calculate_derivative","title":"<code>calculate_derivative(nsteps=None, device='cpu', synchronize=None)</code>","text":"<p>Calculate the time derivative of the schedule.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>Optional[int]</code> <p>Number of time steps. If None, uses the value from initialization.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>'cpu'</code> <code>synchronize</code> <code>Optional[str]</code> <p>TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor representing the time derivative of the schedule.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the derivative calculation is not implemented for this schedule.</p> Source code in <code>bionemo/moco/schedules/noise/discrete_noise_schedules.py</code> <pre><code>def calculate_derivative(\n    self,\n    nsteps: Optional[int] = None,\n    device: Union[str, torch.device] = \"cpu\",\n    synchronize: Optional[TimeDirection] = None,\n) -&gt; Tensor:\n    \"\"\"Calculate the time derivative of the schedule.\n\n    Args:\n        nsteps (Optional[int]): Number of time steps. If None, uses the value from initialization.\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n        synchronize (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction,\n            this parameter allows to flip the direction to match the specified one (default is None).\n\n    Returns:\n        Tensor: A tensor representing the time derivative of the schedule.\n\n    Raises:\n        NotImplementedError: If the derivative calculation is not implemented for this schedule.\n    \"\"\"\n    raise NotImplementedError(\"Derivative calculation is not implemented for this schedule.\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/schedules/noise/discrete_noise_schedules/#bionemo.moco.schedules.noise.discrete_noise_schedules.DiscreteNoiseSchedule.generate_schedule","title":"<code>generate_schedule(nsteps=None, device='cpu', synchronize=None)</code>","text":"<p>Generate the noise schedule as a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>Optional[int]</code> <p>Number of time steps. If None, uses the value from initialization.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>'cpu'</code> <code>synchronize</code> <code>Optional[str]</code> <p>TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).</p> <code>None</code> Source code in <code>bionemo/moco/schedules/noise/discrete_noise_schedules.py</code> <pre><code>def generate_schedule(\n    self,\n    nsteps: Optional[int] = None,\n    device: Union[str, torch.device] = \"cpu\",\n    synchronize: Optional[TimeDirection] = None,\n) -&gt; Tensor:\n    \"\"\"Generate the noise schedule as a tensor.\n\n    Args:\n        nsteps (Optional[int]): Number of time steps. If None, uses the value from initialization.\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n        synchronize (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction,\n            this parameter allows to flip the direction to match the specified one (default is None).\n    \"\"\"\n    schedule = self._generate_schedule(nsteps, device)\n    if synchronize and self.direction != string_to_enum(synchronize, TimeDirection):\n        return torch.flip(schedule, dims=[0])\n    else:\n        return schedule\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/testing/parallel_test_utils/","title":"Parallel test utils","text":""},{"location":"main/references/API_reference/bionemo/moco/testing/parallel_test_utils/#bionemo.moco.testing.parallel_test_utils.clean_up_distributed","title":"<code>clean_up_distributed()</code>","text":"<p>Cleans up the distributed environment.</p> <p>Destroys the process group and empties the CUDA cache.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>bionemo/moco/testing/parallel_test_utils.py</code> <pre><code>def clean_up_distributed() -&gt; None:\n    \"\"\"Cleans up the distributed environment.\n\n    Destroys the process group and empties the CUDA cache.\n\n    Args:\n        None\n\n    Returns:\n        None\n    \"\"\"\n    if dist.is_initialized():\n        dist.destroy_process_group()\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/testing/parallel_test_utils/#bionemo.moco.testing.parallel_test_utils.find_free_network_port","title":"<code>find_free_network_port(address='localhost')</code>","text":"<p>Finds a free port for the specified address. Defaults to localhost.</p> Source code in <code>bionemo/moco/testing/parallel_test_utils.py</code> <pre><code>def find_free_network_port(address: str = \"localhost\") -&gt; int:\n    \"\"\"Finds a free port for the specified address. Defaults to localhost.\"\"\"\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.bind((address, 0))\n    addr_port = s.getsockname()\n    s.close()\n    if addr_port is None:\n        # Could not find any free port.\n        return None, None\n    return addr_port\n</code></pre>"},{"location":"main/references/API_reference/bionemo/moco/testing/parallel_test_utils/#bionemo.moco.testing.parallel_test_utils.parallel_context","title":"<code>parallel_context(rank=0, world_size=1)</code>","text":"<p>Context manager for torch distributed testing.</p> <p>Sets up and cleans up the distributed environment, including the device mesh.</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>int</code> <p>The rank of the process. Defaults to 0.</p> <code>0</code> <code>world_size</code> <code>int</code> <p>The world size of the distributed environment. Defaults to 1.</p> <code>1</code> <p>Yields:</p> Type Description <p>None</p> Source code in <code>bionemo/moco/testing/parallel_test_utils.py</code> <pre><code>@contextmanager\ndef parallel_context(\n    rank: int = 0,\n    world_size: int = 1,\n):\n    \"\"\"Context manager for torch distributed testing.\n\n    Sets up and cleans up the distributed environment, including the device mesh.\n\n    Args:\n        rank (int): The rank of the process. Defaults to 0.\n        world_size (int): The world size of the distributed environment. Defaults to 1.\n\n    Yields:\n        None\n    \"\"\"\n    with MonkeyPatch.context() as context:\n        clean_up_distributed()\n\n        # distributed and parallel state set up\n        if not os.environ.get(\"MASTER_ADDR\", None):\n            context.setenv(\"MASTER_ADDR\", DEFAULT_MASTER_ADDR)\n        if not os.environ.get(\"MASTER_PORT\", None):\n            network_address, free_network_port = find_free_network_port(address=DEFAULT_MASTER_ADDR)\n            context.setenv(\"MASTER_PORT\", free_network_port if free_network_port is not None else DEFAULT_MASTER_PORT)\n        context.setenv(\"RANK\", str(rank))\n\n        dist.init_process_group(backend=\"nccl\", world_size=world_size)\n\n        yield\n\n        clean_up_distributed()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/noodles/nvfaidx/","title":"Nvfaidx","text":""},{"location":"main/references/API_reference/bionemo/noodles/nvfaidx/#bionemo.noodles.nvfaidx.NvFaidx","title":"<code>NvFaidx</code>","text":"<p>NvFaidx is a rest + pyo3 replacement for PyFaidx that provides a dictionary-like interface to reference genomes.</p> <p>This class is a collection of SequenceAccessors, organized by sequence-id in a dictionary like manner. SequenceAcecessors  are similar dict-like interfaces over actual sequence entries in the underlying index. Furthermore, utilities are provided  for parsing faidx files, building faidx files, and storing faidx files to disk.</p> <p>IMPORTANT by default all fasta files build an in-memory faidx object. This is due easy mistakes that may occur if a faidx file is constructed while using multi-processing (such as a default constructor that creates these files on the fly). However, methods exist to create these methods manually where a user has more control over multiprocessing.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; index = NvFaidx(fasta_file, faidx_path=None, ignore_existing_fai=True)\n&gt;&gt;&gt; index['chr1'] # Returns a SequenceAccessor for chr1\n&gt;&gt;&gt; index['chr1'][0:10] # Returns the first 10 bases of chr1.\n&gt;&gt;&gt; faidx_filename = NvFaidx.create_faidx(fasta_file) # Creates a faidx to disk.\n&gt;&gt;&gt; index = NvFaidx(fasta_File, faidx_filename, ignore_existing_fai = True) # Uses a faidx from disk.\n</code></pre> <p>Motivation and more details:</p> <p>NvFaidx is built using Noodles as a backend for Fai objects, and memory maps for backing the underlying fasta. Using a backend of Memmaps provide the following benefits:     - The kernel implements this mechanism by using page faults     - Each read in a mmap'd file results in a page fault: there's nothing in memory to read!     - The kernel handles this page fault by going to the disk, reading the file in the specified offset + index,         then returning to the user process with what it just read, preventing penalties from context switching.</p> <p>Context: PyFaidx or any buffered read based index is not process safe, and therefore does not play nice with pytorch dataloaders. Due to the order of operations, the underlying file handle is shared between processes, when <code>seek()</code> is called to perform random lookups, this can cause unexpected behavior in the forked processes. Ref: https://github.com/mdshw5/pyfaidx/issues/211</p> For a good solution we need three things <p>1) Safe index creation, in multi-process or multi-node scenarios, this should be restricted to a single node     where all workers block until it is complete (not implemented above) 2) Index object instantion must be fast. 3) Read-only use of the index object must be both thread safe and process safe with python.</p> <p>See Also: bionemo.noodles.nvfaidx.SequenceAccessor</p> Source code in <code>bionemo/noodles/nvfaidx.py</code> <pre><code>class NvFaidx:\n    \"\"\"NvFaidx is a rest + pyo3 replacement for PyFaidx that provides a dictionary-like interface to reference genomes.\n\n    This class is a collection of SequenceAccessors, organized by sequence-id in a dictionary like manner. SequenceAcecessors\n     are similar dict-like interfaces over actual sequence entries in the underlying index. Furthermore, utilities are provided\n     for parsing faidx files, building faidx files, and storing faidx files to disk.\n\n    **IMPORTANT** by default all fasta files build an in-memory faidx object. This is due easy mistakes that may occur\n    if a faidx file is constructed while using multi-processing (such as a default constructor that creates these files on the fly).\n    However, methods exist to create these methods manually where a user has more control over multiprocessing.\n\n    Examples:\n        &gt;&gt;&gt; index = NvFaidx(fasta_file, faidx_path=None, ignore_existing_fai=True)\n        &gt;&gt;&gt; index['chr1'] # Returns a SequenceAccessor for chr1\n        &gt;&gt;&gt; index['chr1'][0:10] # Returns the first 10 bases of chr1.\n        &gt;&gt;&gt; faidx_filename = NvFaidx.create_faidx(fasta_file) # Creates a faidx to disk.\n        &gt;&gt;&gt; index = NvFaidx(fasta_File, faidx_filename, ignore_existing_fai = True) # Uses a faidx from disk.\n\n\n    Motivation and more details:\n\n    NvFaidx is built using Noodles as a backend for Fai objects, and memory maps for backing the underlying fasta.\n    Using a backend of Memmaps provide the following benefits:\n        - The kernel implements this mechanism by using page faults\n        - Each read in a mmap'd file results in a page fault: there's nothing in memory to read!\n        - The kernel handles this page fault by going to the disk, reading the file in the specified offset + index,\n            then returning to the user process with what it just read, preventing penalties from context switching.\n\n    *Context*: PyFaidx or _any_ buffered read based index is not process safe, and therefore does not play nice with pytorch dataloaders.\n    Due to the order of operations, the underlying file handle is shared between processes, when `seek()` is called to perform random lookups,\n    this can cause unexpected behavior in the forked processes.\n    Ref: https://github.com/mdshw5/pyfaidx/issues/211\n\n    For a good solution we need three things:\n        1) Safe index creation, in multi-process or multi-node scenarios, this should be restricted to a single node\n            where all workers block until it is complete (not implemented above)\n        2) Index object instantion must be fast.\n        3) Read-only use of the index object must be both thread safe and process safe with python.\n\n    See Also: bionemo.noodles.nvfaidx.SequenceAccessor\n    \"\"\"\n\n    def __init__(\n        self,\n        fasta_path: str | Path,\n        faidx_path: Optional[str | Path] = None,\n        ignore_existing_fai: bool = True,\n        allow_duplicate_seqids: bool = False,\n    ):\n        \"\"\"Construct a dict-like object representing a memmapped, indexed FASTA file.\n\n        This is an indexed fasta reader. Consequences of this are that the FASTA file must be well formed, meaning\n        sequence-ids and line-lengths must conform to FASTA standards. Additionally, the order of returned seqid, sequence\n        pairs when iterating over the index is not guaranteed to be the same order as the underlying fasta file.\n\n        Args:\n            fasta_path (str): Path to the FASTA file.\n            faidx_path (str): Path to the FAI index file. If None, one will be created.\n            ignore_existing_fai (bool): If True, ignore any existing FAI file and create an in-memory index. Note that\n                this will also ignore `faidx_path`.\n            allow_duplicate_seqids (bool): If true, will produce index for invalid fastas which contain duplicate seqids.\n                In this scenario, indexing is performed by integer rather than strings.\n\n                Example with invalid seqids.\n                    &gt;chr1 dupes|not|allowd\n                    ATGATGATGATG\n                    &gt;chr1 whoops|there|is|dupe\n                    ATGATGATGATG\n                NvFaidx:\n                    {\n                        0 : SequenceAccessor(chr1 dupes|not|allowed),\n                        1 : SequenceAccessor(chr1 whoops|there|is|dupe)\n                    }\n\n        \"\"\"\n        if isinstance(fasta_path, Path):\n            fasta_path = str(fasta_path)\n        elif not isinstance(fasta_path, str):\n            raise TypeError(f\"fasta_path must be a `str` or `pathlib.Path`, got: {type(fasta_path)}\")\n\n        if isinstance(faidx_path, Path):\n            faidx_path = str(faidx_path)\n        elif not isinstance(faidx_path, str) and faidx_path is not None:\n            raise TypeError(f\"faidx_path must be a `str`, `pathlib.Path`, or None. got: {type(faidx_path)}\")\n\n        match (fasta_path, faidx_path, ignore_existing_fai):\n            case (_, _, True):\n                self.reader = PyIndexedMmapFastaReader(fasta_path, ignore_existing_fai=ignore_existing_fai)\n            case (_, faidx_path, _) if faidx_path is not None:\n                self.reader = PyIndexedMmapFastaReader.from_fasta_and_faidx(fasta_path, faidx_path)\n            # In this case, faidx path is None and ignore_existing is False, and it covers all other cases.\n            case (_, None, False):\n                # But the logic here doesnt make sense, ignore_existing is false, but it should only use if it if it exists.\n                self.reader = PyIndexedMmapFastaReader(fasta_path, False)\n            case _:\n                raise ValueError(\"unreachable condition.\")\n\n        self.records: Dict[str | int, PyFaidxRecord] = {record.name: record for record in self.reader.records()}\n        if len(self.records) != len(self.reader.records()):\n            if not allow_duplicate_seqids:\n                raise ValueError(\n                    \"Non-unique sequence-id detected in FASTA, this is invalid. Correct headers and try again or pass allow_duplicate_seqid'\"\n                )\n            else:\n                self.records: Dict[str | int, PyFaidxRecord] = dict(enumerate(self.reader.records()))\n\n    def __getitem__(self, seqid: str) -&gt; SequenceAccessor:  # noqa: D105\n        if seqid not in self.records:\n            raise KeyError(f\"Sequence '{seqid}' not found in index.\")\n\n        # Return a SequenceAccessor for slicing access\n        record_length = self.records[seqid].length\n        return SequenceAccessor(self.reader, seqid, record_length)\n\n    def __contains__(self, seqid: str) -&gt; bool:  # noqa: D105\n        return seqid in self.records\n\n    def __len__(self) -&gt; int:  # noqa: D105\n        return len(self.records)\n\n    def keys(self) -&gt; set[str]:  # noqa: D102\n        return set(self.records.keys())\n\n    # These provide dict like iteration functionality\n    def __iter__(self):  # noqa: D105\n        return iter(self.keys())\n\n    def items(self):  # noqa: D102\n        for key in self.keys():\n            yield key, self[key][:]\n\n    def values(self):  # noqa: D102\n        for key in self.keys():\n            yield self[key][:]\n\n    @staticmethod\n    def create_faidx(fasta_filename: str | Path, force: bool = False) -&gt; str:\n        \"\"\"Create a FAI index for a FASTA file, the result is saved in the same location as `fasta_filename`, with a .fai extension.\n\n        Args:\n            fasta_filename (str): Path to the FASTA file to be indexed.\n            force (bool): Delete existing faidx file and create a new index file.\n        \"\"\"\n        if isinstance(fasta_filename, Path):\n            fasta_filename = str(fasta_filename)\n        return PyIndexedMmapFastaReader.create_faidx(fasta_filename, force)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/noodles/nvfaidx/#bionemo.noodles.nvfaidx.NvFaidx.__init__","title":"<code>__init__(fasta_path, faidx_path=None, ignore_existing_fai=True, allow_duplicate_seqids=False)</code>","text":"<p>Construct a dict-like object representing a memmapped, indexed FASTA file.</p> <p>This is an indexed fasta reader. Consequences of this are that the FASTA file must be well formed, meaning sequence-ids and line-lengths must conform to FASTA standards. Additionally, the order of returned seqid, sequence pairs when iterating over the index is not guaranteed to be the same order as the underlying fasta file.</p> <p>Parameters:</p> Name Type Description Default <code>fasta_path</code> <code>str</code> <p>Path to the FASTA file.</p> required <code>faidx_path</code> <code>str</code> <p>Path to the FAI index file. If None, one will be created.</p> <code>None</code> <code>ignore_existing_fai</code> <code>bool</code> <p>If True, ignore any existing FAI file and create an in-memory index. Note that this will also ignore <code>faidx_path</code>.</p> <code>True</code> <code>allow_duplicate_seqids</code> <code>bool</code> <p>If true, will produce index for invalid fastas which contain duplicate seqids. In this scenario, indexing is performed by integer rather than strings.</p> <p>Example with invalid seqids.     &gt;chr1 dupes|not|allowd     ATGATGATGATG     &gt;chr1 whoops|there|is|dupe     ATGATGATGATG NvFaidx:     {         0 : SequenceAccessor(chr1 dupes|not|allowed),         1 : SequenceAccessor(chr1 whoops|there|is|dupe)     }</p> <code>False</code> Source code in <code>bionemo/noodles/nvfaidx.py</code> <pre><code>def __init__(\n    self,\n    fasta_path: str | Path,\n    faidx_path: Optional[str | Path] = None,\n    ignore_existing_fai: bool = True,\n    allow_duplicate_seqids: bool = False,\n):\n    \"\"\"Construct a dict-like object representing a memmapped, indexed FASTA file.\n\n    This is an indexed fasta reader. Consequences of this are that the FASTA file must be well formed, meaning\n    sequence-ids and line-lengths must conform to FASTA standards. Additionally, the order of returned seqid, sequence\n    pairs when iterating over the index is not guaranteed to be the same order as the underlying fasta file.\n\n    Args:\n        fasta_path (str): Path to the FASTA file.\n        faidx_path (str): Path to the FAI index file. If None, one will be created.\n        ignore_existing_fai (bool): If True, ignore any existing FAI file and create an in-memory index. Note that\n            this will also ignore `faidx_path`.\n        allow_duplicate_seqids (bool): If true, will produce index for invalid fastas which contain duplicate seqids.\n            In this scenario, indexing is performed by integer rather than strings.\n\n            Example with invalid seqids.\n                &gt;chr1 dupes|not|allowd\n                ATGATGATGATG\n                &gt;chr1 whoops|there|is|dupe\n                ATGATGATGATG\n            NvFaidx:\n                {\n                    0 : SequenceAccessor(chr1 dupes|not|allowed),\n                    1 : SequenceAccessor(chr1 whoops|there|is|dupe)\n                }\n\n    \"\"\"\n    if isinstance(fasta_path, Path):\n        fasta_path = str(fasta_path)\n    elif not isinstance(fasta_path, str):\n        raise TypeError(f\"fasta_path must be a `str` or `pathlib.Path`, got: {type(fasta_path)}\")\n\n    if isinstance(faidx_path, Path):\n        faidx_path = str(faidx_path)\n    elif not isinstance(faidx_path, str) and faidx_path is not None:\n        raise TypeError(f\"faidx_path must be a `str`, `pathlib.Path`, or None. got: {type(faidx_path)}\")\n\n    match (fasta_path, faidx_path, ignore_existing_fai):\n        case (_, _, True):\n            self.reader = PyIndexedMmapFastaReader(fasta_path, ignore_existing_fai=ignore_existing_fai)\n        case (_, faidx_path, _) if faidx_path is not None:\n            self.reader = PyIndexedMmapFastaReader.from_fasta_and_faidx(fasta_path, faidx_path)\n        # In this case, faidx path is None and ignore_existing is False, and it covers all other cases.\n        case (_, None, False):\n            # But the logic here doesnt make sense, ignore_existing is false, but it should only use if it if it exists.\n            self.reader = PyIndexedMmapFastaReader(fasta_path, False)\n        case _:\n            raise ValueError(\"unreachable condition.\")\n\n    self.records: Dict[str | int, PyFaidxRecord] = {record.name: record for record in self.reader.records()}\n    if len(self.records) != len(self.reader.records()):\n        if not allow_duplicate_seqids:\n            raise ValueError(\n                \"Non-unique sequence-id detected in FASTA, this is invalid. Correct headers and try again or pass allow_duplicate_seqid'\"\n            )\n        else:\n            self.records: Dict[str | int, PyFaidxRecord] = dict(enumerate(self.reader.records()))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/noodles/nvfaidx/#bionemo.noodles.nvfaidx.NvFaidx.create_faidx","title":"<code>create_faidx(fasta_filename, force=False)</code>  <code>staticmethod</code>","text":"<p>Create a FAI index for a FASTA file, the result is saved in the same location as <code>fasta_filename</code>, with a .fai extension.</p> <p>Parameters:</p> Name Type Description Default <code>fasta_filename</code> <code>str</code> <p>Path to the FASTA file to be indexed.</p> required <code>force</code> <code>bool</code> <p>Delete existing faidx file and create a new index file.</p> <code>False</code> Source code in <code>bionemo/noodles/nvfaidx.py</code> <pre><code>@staticmethod\ndef create_faidx(fasta_filename: str | Path, force: bool = False) -&gt; str:\n    \"\"\"Create a FAI index for a FASTA file, the result is saved in the same location as `fasta_filename`, with a .fai extension.\n\n    Args:\n        fasta_filename (str): Path to the FASTA file to be indexed.\n        force (bool): Delete existing faidx file and create a new index file.\n    \"\"\"\n    if isinstance(fasta_filename, Path):\n        fasta_filename = str(fasta_filename)\n    return PyIndexedMmapFastaReader.create_faidx(fasta_filename, force)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/noodles/nvfaidx/#bionemo.noodles.nvfaidx.SequenceAccessor","title":"<code>SequenceAccessor</code>","text":"<p>SequenceAccessor provides a dictionary-like interface to a single sequence in an indexed FASTA file.</p> <p>This allows for random access to the sequence, either by index, or by slice.</p> Source code in <code>bionemo/noodles/nvfaidx.py</code> <pre><code>class SequenceAccessor:\n    \"\"\"SequenceAccessor provides a dictionary-like interface to a single sequence in an indexed FASTA file.\n\n    This allows for random access to the sequence, either by index, or by slice.\n    \"\"\"\n\n    def __init__(self, reader: PyIndexedMmapFastaReader, seqid: str, length: int) -&gt; None:\n        \"\"\"Construct a SequenceAccessor object. Ultimately this is used as a convenience object with NvFaidx.\n\n        When querying the following are true:\n            - Negative indexing is supported, but it does not wrap. so query[-10000] for a sequence of length 1 will fail.\n            - out of bounds indexing is truncated: query[1:999999999] will return a string from position 1 to the terminus.\n            - reversed slices return the empty string: query[999:1] is the empty string.\n            - empty slice returns the full string: query[:] is the full string of the sequence.\n            - beginning of slice is beyond the range of the contig, the empty string is returned.\n\n        Additionally there are convenience methods that you may find useful in the class definition.\n\n        Args:\n            reader (PyIndexedMmapFastaReader): The indexed reader object that provides access to the underlying FASTA file.\n            seqid (str): The sequence identifier.\n            length (int): The length of the sequence.\n        \"\"\"\n        self.reader = reader\n        self.seqid = seqid\n        self.length = length\n\n    def __getitem__(self, key: int | slice) -&gt; str:  # noqa: D105\n        if isinstance(key, slice):\n            # Provide defaults for missing arguments in the slice.\n            start = key.start if key.start is not None else 0\n            stop = key.stop if key.stop is not None else self.length\n\n            # Handle negative cases, remember, you can be arbitrarily negative in a slice.\n            if start &lt; 0:\n                start += self.length\n            if stop &lt; 0:\n                stop += self.length\n\n            # Clamp normalized indices to valid range\n            start = max(0, min(self.length, start))\n            stop = max(0, min(self.length, stop))\n\n            # Bounds checking after normalization\n            if start &gt; stop:\n                return \"\"  # Return empty string for an empty slice\n\n            # Construct region string\n            region_str = f\"{self.seqid}:{start + 1}-{stop}\"  # +1 for 1-based indexing\n            return self.reader.read_sequence_mmap(region_str)\n\n        elif isinstance(key, int):\n            # Normalize single integer for negative indexing\n            if key &lt; 0:\n                key += self.length\n\n            # Bounds checking\n            if key &lt; 0 or key &gt;= self.length:\n                raise IndexError(f\"Position {key} is out of bounds for '{self.seqid}' with length {self.length}.\")\n\n            # Query single nucleotide by creating a 1-length region\n            region_str = f\"{self.seqid}:{key + 1}-{key + 1}\"  # +1 for 1-based indexing\n            return self.reader.read_sequence_mmap(region_str)\n\n        else:\n            raise TypeError(\"Index must be an integer or a slice.\")\n\n    def __len__(self) -&gt; int:  # noqa: D105\n        return self.length\n\n    def sequence_id(self) -&gt; str:\n        \"\"\"Returns the sequenceid of this SequenceAccessor.\"\"\"\n        return self.seqid\n\n    def sequence(self) -&gt; str:\n        \"\"\"Returns the sequence associated with this SequenceAccessor as a string.\"\"\"\n        return self[:]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/noodles/nvfaidx/#bionemo.noodles.nvfaidx.SequenceAccessor.__init__","title":"<code>__init__(reader, seqid, length)</code>","text":"<p>Construct a SequenceAccessor object. Ultimately this is used as a convenience object with NvFaidx.</p> When querying the following are true <ul> <li>Negative indexing is supported, but it does not wrap. so query[-10000] for a sequence of length 1 will fail.</li> <li>out of bounds indexing is truncated: query[1:999999999] will return a string from position 1 to the terminus.</li> <li>reversed slices return the empty string: query[999:1] is the empty string.</li> <li>empty slice returns the full string: query[:] is the full string of the sequence.</li> <li>beginning of slice is beyond the range of the contig, the empty string is returned.</li> </ul> <p>Additionally there are convenience methods that you may find useful in the class definition.</p> <p>Parameters:</p> Name Type Description Default <code>reader</code> <code>PyIndexedMmapFastaReader</code> <p>The indexed reader object that provides access to the underlying FASTA file.</p> required <code>seqid</code> <code>str</code> <p>The sequence identifier.</p> required <code>length</code> <code>int</code> <p>The length of the sequence.</p> required Source code in <code>bionemo/noodles/nvfaidx.py</code> <pre><code>def __init__(self, reader: PyIndexedMmapFastaReader, seqid: str, length: int) -&gt; None:\n    \"\"\"Construct a SequenceAccessor object. Ultimately this is used as a convenience object with NvFaidx.\n\n    When querying the following are true:\n        - Negative indexing is supported, but it does not wrap. so query[-10000] for a sequence of length 1 will fail.\n        - out of bounds indexing is truncated: query[1:999999999] will return a string from position 1 to the terminus.\n        - reversed slices return the empty string: query[999:1] is the empty string.\n        - empty slice returns the full string: query[:] is the full string of the sequence.\n        - beginning of slice is beyond the range of the contig, the empty string is returned.\n\n    Additionally there are convenience methods that you may find useful in the class definition.\n\n    Args:\n        reader (PyIndexedMmapFastaReader): The indexed reader object that provides access to the underlying FASTA file.\n        seqid (str): The sequence identifier.\n        length (int): The length of the sequence.\n    \"\"\"\n    self.reader = reader\n    self.seqid = seqid\n    self.length = length\n</code></pre>"},{"location":"main/references/API_reference/bionemo/noodles/nvfaidx/#bionemo.noodles.nvfaidx.SequenceAccessor.sequence","title":"<code>sequence()</code>","text":"<p>Returns the sequence associated with this SequenceAccessor as a string.</p> Source code in <code>bionemo/noodles/nvfaidx.py</code> <pre><code>def sequence(self) -&gt; str:\n    \"\"\"Returns the sequence associated with this SequenceAccessor as a string.\"\"\"\n    return self[:]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/noodles/nvfaidx/#bionemo.noodles.nvfaidx.SequenceAccessor.sequence_id","title":"<code>sequence_id()</code>","text":"<p>Returns the sequenceid of this SequenceAccessor.</p> Source code in <code>bionemo/noodles/nvfaidx.py</code> <pre><code>def sequence_id(self) -&gt; str:\n    \"\"\"Returns the sequenceid of this SequenceAccessor.\"\"\"\n    return self.seqid\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/api/single_cell_row_dataset/","title":"Single cell row dataset","text":""},{"location":"main/references/API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDataset","title":"<code>SingleCellRowDataset</code>","text":"<p>               Bases: <code>SingleCellRowDatasetCore</code>, <code>Dataset</code></p> <p>One row in an ann dataframe (hdf5 file with a spare array format).</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>class SingleCellRowDataset(SingleCellRowDatasetCore, Dataset):\n    \"\"\"One row in an ann dataframe (hdf5 file with a spare array format).\"\"\"\n\n    @abstractmethod\n    def load(self, data_path: str) -&gt; None:\n        \"\"\"Loads the data from datapath.\n\n        Calls to __len__ and __getitem__ Must be valid after a call to\n        this method.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def save(self, data_path: str) -&gt; None:\n        \"\"\"Saves the class to an archive at datapath.\"\"\"\n        raise NotImplementedError()\n\n    pass\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDataset.load","title":"<code>load(data_path)</code>  <code>abstractmethod</code>","text":"<p>Loads the data from datapath.</p> <p>Calls to len and getitem Must be valid after a call to this method.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>@abstractmethod\ndef load(self, data_path: str) -&gt; None:\n    \"\"\"Loads the data from datapath.\n\n    Calls to __len__ and __getitem__ Must be valid after a call to\n    this method.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDataset.save","title":"<code>save(data_path)</code>  <code>abstractmethod</code>","text":"<p>Saves the class to an archive at datapath.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>@abstractmethod\ndef save(self, data_path: str) -&gt; None:\n    \"\"\"Saves the class to an archive at datapath.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore","title":"<code>SingleCellRowDatasetCore</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Implements the actual ann data-like interface.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>class SingleCellRowDatasetCore(ABC):\n    \"\"\"Implements the actual ann data-like interface.\"\"\"\n\n    @abstractmethod\n    def load_h5ad(self, h5ad_path: str) -&gt; None:\n        \"\"\"Loads an H5AD file and converts it into the backing representation.\n\n        Calls to __len__ and __getitem__ Must be valid after a call to\n        this method.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def number_nonzero_values(self) -&gt; int:\n        \"\"\"Return the number of non-zero values in the data.\"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def number_of_values(self) -&gt; int:\n        \"\"\"Return the total number of values in the data.\"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def number_of_rows(self) -&gt; int:\n        \"\"\"Return the number of rows in the data.\"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def shape(self) -&gt; Tuple[int, List[int]]:\n        \"\"\"Returns the shape of the object, which may be ragged.\n\n        A ragged dataset is where the number and dimension of features\n        can be different at every row.\n        \"\"\"\n        raise NotImplementedError()\n\n    def sparsity(self) -&gt; float:\n        \"\"\"Return the sparsity of the underlying data.\n\n        Sparsity is defined as the fraction of zero values in the data.\n        It is within the range [0, 1.0]. If there are no values, the\n        sparsity is defined as 0.0.\n        \"\"\"\n        total_values = self.number_of_values()\n        if total_values == 0:\n            return 0.0\n\n        nonzero_values = self.number_nonzero_values()\n        zero_values = total_values - nonzero_values\n        sparsity_value = zero_values / total_values\n        return sparsity_value\n\n    @abstractmethod\n    def version(self) -&gt; str:\n        \"\"\"Returns a version number.\n\n        (following &lt;major&gt;.&lt;minor&gt;.&lt;point&gt; convention).\n        \"\"\"\n        pass\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore.load_h5ad","title":"<code>load_h5ad(h5ad_path)</code>  <code>abstractmethod</code>","text":"<p>Loads an H5AD file and converts it into the backing representation.</p> <p>Calls to len and getitem Must be valid after a call to this method.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>@abstractmethod\ndef load_h5ad(self, h5ad_path: str) -&gt; None:\n    \"\"\"Loads an H5AD file and converts it into the backing representation.\n\n    Calls to __len__ and __getitem__ Must be valid after a call to\n    this method.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore.number_nonzero_values","title":"<code>number_nonzero_values()</code>  <code>abstractmethod</code>","text":"<p>Return the number of non-zero values in the data.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>@abstractmethod\ndef number_nonzero_values(self) -&gt; int:\n    \"\"\"Return the number of non-zero values in the data.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore.number_of_rows","title":"<code>number_of_rows()</code>  <code>abstractmethod</code>","text":"<p>Return the number of rows in the data.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>@abstractmethod\ndef number_of_rows(self) -&gt; int:\n    \"\"\"Return the number of rows in the data.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore.number_of_values","title":"<code>number_of_values()</code>  <code>abstractmethod</code>","text":"<p>Return the total number of values in the data.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>@abstractmethod\ndef number_of_values(self) -&gt; int:\n    \"\"\"Return the total number of values in the data.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore.shape","title":"<code>shape()</code>  <code>abstractmethod</code>","text":"<p>Returns the shape of the object, which may be ragged.</p> <p>A ragged dataset is where the number and dimension of features can be different at every row.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>@abstractmethod\ndef shape(self) -&gt; Tuple[int, List[int]]:\n    \"\"\"Returns the shape of the object, which may be ragged.\n\n    A ragged dataset is where the number and dimension of features\n    can be different at every row.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore.sparsity","title":"<code>sparsity()</code>","text":"<p>Return the sparsity of the underlying data.</p> <p>Sparsity is defined as the fraction of zero values in the data. It is within the range [0, 1.0]. If there are no values, the sparsity is defined as 0.0.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>def sparsity(self) -&gt; float:\n    \"\"\"Return the sparsity of the underlying data.\n\n    Sparsity is defined as the fraction of zero values in the data.\n    It is within the range [0, 1.0]. If there are no values, the\n    sparsity is defined as 0.0.\n    \"\"\"\n    total_values = self.number_of_values()\n    if total_values == 0:\n        return 0.0\n\n    nonzero_values = self.number_nonzero_values()\n    zero_values = total_values - nonzero_values\n    sparsity_value = zero_values / total_values\n    return sparsity_value\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore.version","title":"<code>version()</code>  <code>abstractmethod</code>","text":"<p>Returns a version number.</p> <p>(following .. convention). Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>@abstractmethod\ndef version(self) -&gt; str:\n    \"\"\"Returns a version number.\n\n    (following &lt;major&gt;.&lt;minor&gt;.&lt;point&gt; convention).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/data/load/","title":"Load","text":""},{"location":"main/references/API_reference/bionemo/scdl/data/load/#bionemo.scdl.data.load.NGCDownloader","title":"<code>NGCDownloader</code>  <code>dataclass</code>","text":"<p>A class to download files from NGC in a Pooch-compatible way.</p> <p>NGC downloads are typically structured as directories, while pooch expects a single file. This class downloads a single file from an NGC directory and moves it to the desired location.</p> Source code in <code>bionemo/scdl/data/load.py</code> <pre><code>@dataclass\nclass NGCDownloader:\n    \"\"\"A class to download files from NGC in a Pooch-compatible way.\n\n    NGC downloads are typically structured as directories, while pooch expects a single file. This class\n    downloads a single file from an NGC directory and moves it to the desired location.\n    \"\"\"\n\n    filename: str\n\n    def __call__(self, url: str, output_file: str | Path, _: pooch.Pooch) -&gt; None:\n        \"\"\"Download a file from NGC.\"\"\"\n        try:\n            import nest_asyncio\n        except ImportError:\n            raise ImportError(\n                \"nest_asyncio is required for NGC downloads. Please install nest_asyncio or use PBSS source instead.\"\n            )\n\n        client = default_ngc_client()\n        nest_asyncio.apply()\n\n        # SCDL only uses NGC resources, never models\n        download_fn = client.registry.resource.download_version\n\n        output_file = Path(output_file)\n        output_file.parent.mkdir(parents=True, exist_ok=True)\n\n        # NGC seems to always download to a specific directory that we can't specify ourselves.\n        ngc_dirname = Path(url).name.replace(\":\", \"_v\")\n\n        with tempfile.TemporaryDirectory(dir=output_file.parent) as temp_dir:\n            download_fn(url, temp_dir, file_patterns=[self.filename])\n            shutil.move(Path(temp_dir) / ngc_dirname / self.filename, output_file)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/data/load/#bionemo.scdl.data.load.NGCDownloader.__call__","title":"<code>__call__(url, output_file, _)</code>","text":"<p>Download a file from NGC.</p> Source code in <code>bionemo/scdl/data/load.py</code> <pre><code>def __call__(self, url: str, output_file: str | Path, _: pooch.Pooch) -&gt; None:\n    \"\"\"Download a file from NGC.\"\"\"\n    try:\n        import nest_asyncio\n    except ImportError:\n        raise ImportError(\n            \"nest_asyncio is required for NGC downloads. Please install nest_asyncio or use PBSS source instead.\"\n        )\n\n    client = default_ngc_client()\n    nest_asyncio.apply()\n\n    # SCDL only uses NGC resources, never models\n    download_fn = client.registry.resource.download_version\n\n    output_file = Path(output_file)\n    output_file.parent.mkdir(parents=True, exist_ok=True)\n\n    # NGC seems to always download to a specific directory that we can't specify ourselves.\n    ngc_dirname = Path(url).name.replace(\":\", \"_v\")\n\n    with tempfile.TemporaryDirectory(dir=output_file.parent) as temp_dir:\n        download_fn(url, temp_dir, file_patterns=[self.filename])\n        shutil.move(Path(temp_dir) / ngc_dirname / self.filename, output_file)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/data/load/#bionemo.scdl.data.load.Resource","title":"<code>Resource</code>  <code>dataclass</code>","text":"<p>Class that represents a remote resource for downloading and caching test data.</p> Source code in <code>bionemo/scdl/data/load.py</code> <pre><code>@dataclass\nclass Resource:\n    \"\"\"Class that represents a remote resource for downloading and caching test data.\"\"\"\n\n    tag: str\n    \"\"\"A unique identifier for the resource.\"\"\"\n\n    ngc: str | None = None\n    \"\"\"The NGC URL for the resource.\"\"\"\n\n    pbss: str | None = None\n    \"\"\"The PBSS URL of the resource.\"\"\"\n\n    sha256: str | None = None\n    \"\"\"The SHA256 checksum of the resource.\"\"\"\n\n    owner: str = \"\"\n    \"\"\"The owner or primary point of contact for the resource.\"\"\"\n\n    description: str | None = None\n    \"\"\"A description of the file(s).\"\"\"\n\n    unpack: Literal[False, None] = None\n    \"\"\"Whether the resource should be unpacked after download.\"\"\"\n\n    decompress: Literal[False, None] = None\n    \"\"\"Whether the resource should be decompressed after download.\"\"\"\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/data/load/#bionemo.scdl.data.load.Resource.decompress","title":"<code>decompress = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the resource should be decompressed after download.</p>"},{"location":"main/references/API_reference/bionemo/scdl/data/load/#bionemo.scdl.data.load.Resource.description","title":"<code>description = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A description of the file(s).</p>"},{"location":"main/references/API_reference/bionemo/scdl/data/load/#bionemo.scdl.data.load.Resource.ngc","title":"<code>ngc = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The NGC URL for the resource.</p>"},{"location":"main/references/API_reference/bionemo/scdl/data/load/#bionemo.scdl.data.load.Resource.owner","title":"<code>owner = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The owner or primary point of contact for the resource.</p>"},{"location":"main/references/API_reference/bionemo/scdl/data/load/#bionemo.scdl.data.load.Resource.pbss","title":"<code>pbss = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The PBSS URL of the resource.</p>"},{"location":"main/references/API_reference/bionemo/scdl/data/load/#bionemo.scdl.data.load.Resource.sha256","title":"<code>sha256 = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The SHA256 checksum of the resource.</p>"},{"location":"main/references/API_reference/bionemo/scdl/data/load/#bionemo.scdl.data.load.Resource.tag","title":"<code>tag</code>  <code>instance-attribute</code>","text":"<p>A unique identifier for the resource.</p>"},{"location":"main/references/API_reference/bionemo/scdl/data/load/#bionemo.scdl.data.load.Resource.unpack","title":"<code>unpack = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the resource should be unpacked after download.</p>"},{"location":"main/references/API_reference/bionemo/scdl/data/load/#bionemo.scdl.data.load.default_ngc_client","title":"<code>default_ngc_client(use_guest_if_api_key_invalid=True)</code>","text":"<p>Create a default NGC client.</p> <p>This should load the NGC API key from ~/.ngc/config, or from environment variables passed to the docker container.</p> Source code in <code>bionemo/scdl/data/load.py</code> <pre><code>def default_ngc_client(use_guest_if_api_key_invalid: bool = True) -&gt; \"ngcsdk.Client\":\n    \"\"\"Create a default NGC client.\n\n    This should load the NGC API key from ~/.ngc/config, or from environment variables passed to the docker container.\n    \"\"\"\n    import ngcsdk\n\n    client = ngcsdk.Client()\n\n    try:\n        client.configure()\n\n    except ValueError as e:\n        if use_guest_if_api_key_invalid:\n            logger.error(f\"Error configuring NGC client: {e}, signing in as guest.\")\n            client = ngcsdk.Client(\"no-apikey\")\n            client.configure(\n                api_key=\"no-apikey\",  # pragma: allowlist secret\n                org_name=\"no-org\",\n                team_name=\"no-team\",\n                ace_name=\"no-ace\",\n            )\n\n        else:\n            raise\n\n    return client\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/data/load/#bionemo.scdl.data.load.default_pbss_client","title":"<code>default_pbss_client()</code>","text":"<p>Create a default S3 client for PBSS.</p> Source code in <code>bionemo/scdl/data/load.py</code> <pre><code>def default_pbss_client():\n    \"\"\"Create a default S3 client for PBSS.\"\"\"\n    try:\n        import boto3\n        from botocore.config import Config\n    except ImportError:\n        raise ImportError(\"boto3 and botocore are required to download from PBSS.\")\n\n    retry_config = Config(retries={\"max_attempts\": 10, \"mode\": \"standard\"})\n    return boto3.client(\"s3\", endpoint_url=\"https://pbss.s8k.io\", config=retry_config)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/data/load/#bionemo.scdl.data.load.get_all_resources","title":"<code>get_all_resources(resource_path=None)</code>  <code>cached</code>","text":"<p>Return a dictionary of all resources.</p> Source code in <code>bionemo/scdl/data/load.py</code> <pre><code>@functools.cache\ndef get_all_resources(resource_path: Path | None = None) -&gt; dict[str, Resource]:\n    \"\"\"Return a dictionary of all resources.\"\"\"\n    if not resource_path:\n        # Use importlib.resources to access bundled package resources\n        try:\n            resource_files = resources.files(\"bionemo.scdl.data.resources\")\n            resources_files = [f for f in resource_files.iterdir() if f.is_file() and f.suffix in {\".yaml\", \".yml\"}]\n        except (ImportError, FileNotFoundError):\n            # Fallback to local directory for development/testing\n            resource_path = Path(__file__).parent / \"resources\"\n            resources_files = itertools.chain(resource_path.glob(\"*.yaml\"), resource_path.glob(\"*.yml\"))\n    else:\n        resources_files = itertools.chain(resource_path.glob(\"*.yaml\"), resource_path.glob(\"*.yml\"))\n\n    all_resources = [resource for file in resources_files for resource in _parse_resource_file(file)]\n\n    try:\n        import pydantic\n\n        resource_list = pydantic.TypeAdapter(list[Resource]).validate_python(all_resources)\n    except ImportError:\n        # If pydantic is not available, create Resource objects directly\n        resource_list = [Resource(**resource) for resource in all_resources]\n    resource_dict = {resource.tag: resource for resource in resource_list}\n\n    if len(resource_dict) != len(resource_list):\n        # Show the # of and which ones are duplicated so that a user can begin debugging and resolve the issue.\n        tag_counts = Counter([resource.tag for resource in resource_list])\n        raise ValueError(f\"Duplicate resource tags found!: {[tag for tag, count in tag_counts.items() if count &gt; 1]}\")\n\n    return resource_dict\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/data/load/#bionemo.scdl.data.load.load","title":"<code>load(model_or_data_tag, source=DEFAULT_SOURCE, resources=None, cache_dir=None)</code>","text":"<p>Download a resource from PBSS or NGC.</p> <p>Parameters:</p> Name Type Description Default <code>model_or_data_tag</code> <code>str</code> <p>A pointer to the desired resource. Must be a key in the resources dictionary.</p> required <code>source</code> <code>SourceOptions</code> <p>Either \"pbss\" (NVIDIA-internal) or \"ngc\" (NGC). Defaults to DEFAULT_SOURCE (from environment variable BIONEMO_DATA_SOURCE; defaults to \"ngc\").</p> <code>DEFAULT_SOURCE</code> <code>resources</code> <code>dict[str, Resource] | None</code> <p>A custom dictionary of resources. If None, the default resources will be used. (Mostly for testing.)</p> <code>None</code> <code>cache_dir</code> <code>Path | None</code> <p>The directory to store downloaded files. Defaults to BIONEMO_CACHE_DIR. (Mostly for testing.)</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the desired tag was not found, or if an NGC url was requested but not provided.</p> <p>Returns:</p> Type Description <code>Path</code> <p>A Path object pointing either at the downloaded file, or at a decompressed folder containing the</p> <code>Path</code> <p>file(s).</p> <p>Examples:</p> <p>For a resource specified in 'filename.yaml' with tag 'tag', the following will download the file:</p> <pre><code>&gt;&gt;&gt; load(\"filename/tag\")\nPosixPath(/tmp/bionemo/downloaded-file-name)\n</code></pre> Source code in <code>bionemo/scdl/data/load.py</code> <pre><code>def load(\n    model_or_data_tag: str,\n    source: SourceOptions = DEFAULT_SOURCE,\n    resources: dict[str, Resource] | None = None,\n    cache_dir: Path | None = None,\n) -&gt; Path:\n    \"\"\"Download a resource from PBSS or NGC.\n\n    Args:\n        model_or_data_tag: A pointer to the desired resource. Must be a key in the resources dictionary.\n        source: Either \"pbss\" (NVIDIA-internal) or \"ngc\" (NGC). Defaults to DEFAULT_SOURCE\n            (from environment variable BIONEMO_DATA_SOURCE; defaults to \"ngc\").\n        resources: A custom dictionary of resources. If None, the default resources will be used. (Mostly for testing.)\n        cache_dir: The directory to store downloaded files. Defaults to BIONEMO_CACHE_DIR. (Mostly for testing.)\n\n    Raises:\n        ValueError: If the desired tag was not found, or if an NGC url was requested but not provided.\n\n    Returns:\n        A Path object pointing either at the downloaded file, or at a decompressed folder containing the\n        file(s).\n\n    Examples:\n        For a resource specified in 'filename.yaml' with tag 'tag', the following will download the file:\n        &gt;&gt;&gt; load(\"filename/tag\")\n        PosixPath(/tmp/bionemo/downloaded-file-name)\n    \"\"\"\n    if resources is None:\n        # Get resources from the local scdl data directory\n        resources = get_all_resources()\n\n    if cache_dir is None:\n        cache_dir = BIONEMO_CACHE_DIR\n\n    if model_or_data_tag not in resources:\n        raise ValueError(f\"Resource '{model_or_data_tag}' not found.\")\n\n    if source == \"ngc\" and resources[model_or_data_tag].ngc is None:\n        raise ValueError(f\"Resource '{model_or_data_tag}' does not have an NGC URL.\")\n\n    resource = resources[model_or_data_tag]\n    filename = str(resource.pbss).split(\"/\")[-1]\n\n    # Determine the right Pooch processor based on filename suffixes\n    processor = _get_processor(filename, resource.unpack, resource.decompress)\n\n    if source == \"pbss\":\n        download_fn = _s3_download\n        url = resource.pbss\n\n    elif source == \"ngc\":\n        download_fn = NGCDownloader(filename=filename)\n        url = resource.ngc\n\n    else:\n        raise ValueError(f\"Source '{source}' not supported.\")\n\n    download = pooch.retrieve(\n        url=str(url),\n        fname=f\"{resource.sha256}-{filename}\",\n        known_hash=resource.sha256,\n        path=cache_dir,\n        downloader=download_fn,\n        processor=processor,\n    )\n\n    # Pooch by default returns a list of unpacked files if they unpack a zipped or tarred directory. Instead of that, we\n    # just want the unpacked, parent folder.\n    if isinstance(download, list):\n        return Path(processor.extract_dir)  # type: ignore\n\n    else:\n        return Path(download)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/index/row_feature_index/","title":"Row feature index","text":"<p>Feature index abstractions for SCDL.</p> <p>This module defines an abstract base <code>RowFeatureIndex</code> and two concrete implementations:</p> <ul> <li><code>ObservedFeatureIndex</code>: row-oriented features, where a lookup returns one   scalar per selected feature for a given row.</li> <li><code>VariableFeatureIndex</code>: column-oriented features, where a lookup returns full   feature arrays for the block containing a given row.</li> </ul> <p>Data are stored in blocks of feature dictionaries. <code>_cumulative_sum_index</code> tracks row boundaries between blocks. The feature dictionarires are stroed in <code>_feature_arr</code>.</p>"},{"location":"main/references/API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.ObservedFeatureIndex","title":"<code>ObservedFeatureIndex</code>","text":"<p>               Bases: <code>RowFeatureIndex</code></p> <p>Feature index for observed (row) features.</p> <p>Each block is a dictionary mapping feature name to the full column array. A lookup at a row returns one scalar per selected feature and the block label. Successive blocks with identical schemas (same keys) are merged by concatenating column arrays.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>class ObservedFeatureIndex(RowFeatureIndex):\n    \"\"\"Feature index for observed (row) features.\n\n    Each block is a dictionary mapping feature name to the full column array. A\n    lookup at a row returns one scalar per selected feature and the block label.\n    Successive blocks with identical schemas (same keys) are merged by\n    concatenating column arrays.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Create an observed (row) feature index.\"\"\"\n        super().__init__()\n\n    def _check_if_can_merge_features_with_last_block(self, features: dict[str, np.ndarray], total_csum: int) -&gt; bool:\n        \"\"\"Merge into last block when schemas match.\n\n        If the last block has the same set of keys, concatenate each column to\n        extend the block and update the cumulative sum index.\n\n        Returns:\n            True if merged into the last block; False otherwise.\n        \"\"\"\n        if len(self._feature_arr) &gt; 0:\n            last_features = self._feature_arr[-1]\n            if last_features.keys() == features.keys():\n                merged = {k: np.concatenate([last_features[k], np.asarray(features[k])]) for k in features}\n                self._feature_arr[-1] = merged\n                self._cumulative_sum_index[-1] = total_csum\n                return True\n        return False\n\n    def lookup(self, row: int, select_features: Optional[list[str]] = None) -&gt; Tuple[list[np.ndarray], Optional[str]]:\n        \"\"\"Return scalar feature values and block label for a given row.\"\"\"\n        d_id = self._get_dataset_id(row)\n        features_dict: dict[str, np.ndarray] = self._feature_arr[d_id]\n        start = self._cumulative_sum_index[d_id] if d_id &gt; 0 else 0\n        row_idx_in_block = row - start\n        if select_features is not None:\n            arrays = self._filter_features(features_dict, select_features)\n        else:\n            arrays = [features_dict[f] for f in features_dict]\n        vals = [np.asarray(arr)[row_idx_in_block] for arr in arrays]\n        return vals, self._labels[d_id]\n\n    def _extend_num_entries_per_row(self, features: dict[str, np.ndarray]) -&gt; None:\n        \"\"\"Extend the number of entries per row for the observed feature index.\"\"\"\n        num_entries = len(features)\n        self._num_entries_per_row.append(num_entries)\n\n    @staticmethod\n    def load(datapath: str) -&gt; \"ObservedFeatureIndex\":\n        \"\"\"Load a observed (row) feature index from a directory.\n\n        This will  load the parquet files in sorted order. In the SCDL use case, this expects a directory with\n        parquet files named dataframe_&lt;index&gt;.parquet.\n        \"\"\"\n        return RowFeatureIndex._load_common(datapath, ObservedFeatureIndex())\n\n    def __getitem__(self, idx):\n        \"\"\"Access one row or a slice of rows for observed features (.obs).\n\n        - If `idx` is an int, returns `(values, label)` for that row:\n            - `values` is a list of scalar values (one per selected feature)\n            - `label` is the block label\n        - If `idx` is a slice, returns `(blocks, labels)`:\n            - `blocks` is a list of feature dictionaries, one per intersected\n              block, with arrays sliced to the selected rows for that block.\n            - `labels` is a list of block labels in the same order.\n        \"\"\"\n        if isinstance(idx, int):\n            n = self.number_of_rows()\n            if idx &lt; 0:\n                idx += n\n            return self.lookup(idx)\n\n        if isinstance(idx, slice):\n            n = self.number_of_rows()\n            start, stop, step = idx.indices(n)\n            if step == 0:\n                raise ValueError(\"slice step cannot be zero\")\n            rows = list(range(start, stop, step))\n            if not rows:\n                return [], []\n            ends = self._cumulative_sum_index[1:]\n            by_block: dict[int, list[int]] = {}\n            for r in rows:\n                d_id = int(np.searchsorted(ends, r, side=\"right\"))\n                by_block.setdefault(d_id, []).append(r)\n            out = []\n            labels = []\n            for d_id, rs in by_block.items():\n                rs.sort()\n                prev_end = self._cumulative_sum_index[d_id] if d_id &gt; 0 else 0\n                relative_indices = [r - prev_end for r in rs]\n                sub = {k: np.asarray(v)[relative_indices] for k, v in self._feature_arr[d_id].items()}\n                out.append(sub)\n                labels.append(self._labels[d_id])\n            return out, labels\n\n        raise TypeError(\"Index must be int or slice\")\n\n    def concat(\n        self,\n        other_row_index: ObservedFeatureIndex,\n    ) -&gt; ObservedFeatureIndex:\n        \"\"\"Concatenates the other ObservedFeatureIndex to this one.\n\n        Returns the new, updated index. Warning: modifies this index in-place.\n\n        Args:\n            other_row_index: another ObservedFeatureIndex\n            error if an empty row index is passed in.\n\n        Returns:\n            self, the RowIndexFeature after the concatenations.\n\n        Raises:\n            TypeError if other_row_index is not a ObservedFeatureIndex\n        \"\"\"\n        # Require the exact same concrete subclass to ensure semantic compatibility\n        if not isinstance(other_row_index, ObservedFeatureIndex):\n            raise TypeError(\"Error: trying to concatenate something that's not a ObservedFeatureIndex.\")\n        for i, feats in enumerate(list(other_row_index._feature_arr)):\n            label = other_row_index._labels[i]\n            self.append_features(feats, label)\n\n        return self\n\n    def append_features(self, features: dict[str, np.ndarray], label: Optional[str] = None) -&gt; None:\n        \"\"\"Append features, delegating validation and merge behavior to subclasses.\"\"\"\n        feature_size = self._validate_features_get_entries_count(features)\n        \"\"\" There are no features to append, so we return early.\"\"\"\n        if feature_size == 0:\n            return\n        total_csum = max(self._cumulative_sum_index[-1], 0) + feature_size\n        self._append_block(features, label, total_csum)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.ObservedFeatureIndex.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Access one row or a slice of rows for observed features (.obs).</p> <ul> <li>If <code>idx</code> is an int, returns <code>(values, label)</code> for that row:<ul> <li><code>values</code> is a list of scalar values (one per selected feature)</li> <li><code>label</code> is the block label</li> </ul> </li> <li>If <code>idx</code> is a slice, returns <code>(blocks, labels)</code>:<ul> <li><code>blocks</code> is a list of feature dictionaries, one per intersected   block, with arrays sliced to the selected rows for that block.</li> <li><code>labels</code> is a list of block labels in the same order.</li> </ul> </li> </ul> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"Access one row or a slice of rows for observed features (.obs).\n\n    - If `idx` is an int, returns `(values, label)` for that row:\n        - `values` is a list of scalar values (one per selected feature)\n        - `label` is the block label\n    - If `idx` is a slice, returns `(blocks, labels)`:\n        - `blocks` is a list of feature dictionaries, one per intersected\n          block, with arrays sliced to the selected rows for that block.\n        - `labels` is a list of block labels in the same order.\n    \"\"\"\n    if isinstance(idx, int):\n        n = self.number_of_rows()\n        if idx &lt; 0:\n            idx += n\n        return self.lookup(idx)\n\n    if isinstance(idx, slice):\n        n = self.number_of_rows()\n        start, stop, step = idx.indices(n)\n        if step == 0:\n            raise ValueError(\"slice step cannot be zero\")\n        rows = list(range(start, stop, step))\n        if not rows:\n            return [], []\n        ends = self._cumulative_sum_index[1:]\n        by_block: dict[int, list[int]] = {}\n        for r in rows:\n            d_id = int(np.searchsorted(ends, r, side=\"right\"))\n            by_block.setdefault(d_id, []).append(r)\n        out = []\n        labels = []\n        for d_id, rs in by_block.items():\n            rs.sort()\n            prev_end = self._cumulative_sum_index[d_id] if d_id &gt; 0 else 0\n            relative_indices = [r - prev_end for r in rs]\n            sub = {k: np.asarray(v)[relative_indices] for k, v in self._feature_arr[d_id].items()}\n            out.append(sub)\n            labels.append(self._labels[d_id])\n        return out, labels\n\n    raise TypeError(\"Index must be int or slice\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.ObservedFeatureIndex.__init__","title":"<code>__init__()</code>","text":"<p>Create an observed (row) feature index.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Create an observed (row) feature index.\"\"\"\n    super().__init__()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.ObservedFeatureIndex.append_features","title":"<code>append_features(features, label=None)</code>","text":"<p>Append features, delegating validation and merge behavior to subclasses.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def append_features(self, features: dict[str, np.ndarray], label: Optional[str] = None) -&gt; None:\n    \"\"\"Append features, delegating validation and merge behavior to subclasses.\"\"\"\n    feature_size = self._validate_features_get_entries_count(features)\n    \"\"\" There are no features to append, so we return early.\"\"\"\n    if feature_size == 0:\n        return\n    total_csum = max(self._cumulative_sum_index[-1], 0) + feature_size\n    self._append_block(features, label, total_csum)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.ObservedFeatureIndex.concat","title":"<code>concat(other_row_index)</code>","text":"<p>Concatenates the other ObservedFeatureIndex to this one.</p> <p>Returns the new, updated index. Warning: modifies this index in-place.</p> <p>Parameters:</p> Name Type Description Default <code>other_row_index</code> <code>ObservedFeatureIndex</code> <p>another ObservedFeatureIndex</p> required <p>Returns:</p> Type Description <code>ObservedFeatureIndex</code> <p>self, the RowIndexFeature after the concatenations.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def concat(\n    self,\n    other_row_index: ObservedFeatureIndex,\n) -&gt; ObservedFeatureIndex:\n    \"\"\"Concatenates the other ObservedFeatureIndex to this one.\n\n    Returns the new, updated index. Warning: modifies this index in-place.\n\n    Args:\n        other_row_index: another ObservedFeatureIndex\n        error if an empty row index is passed in.\n\n    Returns:\n        self, the RowIndexFeature after the concatenations.\n\n    Raises:\n        TypeError if other_row_index is not a ObservedFeatureIndex\n    \"\"\"\n    # Require the exact same concrete subclass to ensure semantic compatibility\n    if not isinstance(other_row_index, ObservedFeatureIndex):\n        raise TypeError(\"Error: trying to concatenate something that's not a ObservedFeatureIndex.\")\n    for i, feats in enumerate(list(other_row_index._feature_arr)):\n        label = other_row_index._labels[i]\n        self.append_features(feats, label)\n\n    return self\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.ObservedFeatureIndex.load","title":"<code>load(datapath)</code>  <code>staticmethod</code>","text":"<p>Load a observed (row) feature index from a directory.</p> <p>This will  load the parquet files in sorted order. In the SCDL use case, this expects a directory with parquet files named dataframe_.parquet. Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>@staticmethod\ndef load(datapath: str) -&gt; \"ObservedFeatureIndex\":\n    \"\"\"Load a observed (row) feature index from a directory.\n\n    This will  load the parquet files in sorted order. In the SCDL use case, this expects a directory with\n    parquet files named dataframe_&lt;index&gt;.parquet.\n    \"\"\"\n    return RowFeatureIndex._load_common(datapath, ObservedFeatureIndex())\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.ObservedFeatureIndex.lookup","title":"<code>lookup(row, select_features=None)</code>","text":"<p>Return scalar feature values and block label for a given row.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def lookup(self, row: int, select_features: Optional[list[str]] = None) -&gt; Tuple[list[np.ndarray], Optional[str]]:\n    \"\"\"Return scalar feature values and block label for a given row.\"\"\"\n    d_id = self._get_dataset_id(row)\n    features_dict: dict[str, np.ndarray] = self._feature_arr[d_id]\n    start = self._cumulative_sum_index[d_id] if d_id &gt; 0 else 0\n    row_idx_in_block = row - start\n    if select_features is not None:\n        arrays = self._filter_features(features_dict, select_features)\n    else:\n        arrays = [features_dict[f] for f in features_dict]\n    vals = [np.asarray(arr)[row_idx_in_block] for arr in arrays]\n    return vals, self._labels[d_id]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex","title":"<code>RowFeatureIndex</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for ragged feature indices.</p> <p>Represents datasets where the number and/or shape of features can differ per row. Data are organized in blocks (feature dictionaries), with a cumulative sum index delineating block boundaries.</p> <p>Attributes:</p> Name Type Description <code>_cumulative_sum_index</code> <code>array</code> <p>Cumulative row counts that delineate block boundaries. For example, with <code>[-1, 200, 350]</code>, rows <code>0..199</code> are in block 0, which is in _feature_arr[0], and rows <code>200..349</code> are in block 1, which is in _feature_arr[1].</p> <code>_feature_arr</code> <code>list[dict[str, ndarray]]</code> <p>List of feature dictionaries, one per block.</p> <code>_num_entries_per_row</code> <code>list[int]</code> <p>Per-block counts used by <code>number_vars_at_row</code> and <code>column_dims</code>.</p> <code>_labels</code> <code>list[Optional[str]]</code> <p>Optional label per block (e.g., dataset ID or name).</p> <code>_version</code> <p>Version string for the dataset.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>class RowFeatureIndex(ABC):\n    \"\"\"Abstract base for ragged feature indices.\n\n    Represents datasets where the number and/or shape of features can differ per\n    row. Data are organized in blocks (feature dictionaries), with a cumulative\n    sum index delineating block boundaries.\n\n    Attributes:\n        _cumulative_sum_index: Cumulative row counts that delineate block\n            boundaries. For example, with `[-1, 200, 350]`, rows `0..199` are in\n            block 0, which is in _feature_arr[0], and rows `200..349` are in block 1,\n            which is in _feature_arr[1].\n        _feature_arr: List of feature dictionaries, one per block.\n        _num_entries_per_row: Per-block counts used by `number_vars_at_row` and\n            `column_dims`.\n        _labels: Optional label per block (e.g., dataset ID or name).\n        _version: Version string for the dataset.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Instantiates the index.\"\"\"\n        self._cumulative_sum_index: np.array = np.array([-1])\n        self._feature_arr: list[dict[str, np.ndarray]] = []\n        self._num_entries_per_row: list[int] = []\n        self._version = importlib.metadata.version(\"bionemo.scdl\")\n        self._labels: list[Optional[str]] = []\n\n    def _get_dataset_id(self, row) -&gt; int:\n        \"\"\"Gets the dataset id for a specified row index.\n\n        Args:\n            row (int): The index of the row.\n\n        Returns:\n            An int representing the dataset id the row belongs to.\n        \"\"\"\n        if row &lt; 0:\n            raise IndexError(f\"Row index {row} is not valid. It must be non-negative.\")\n        if len(self._cumulative_sum_index) &lt; 2:\n            raise IndexError(\"There are no features to lookup.\")\n        if row &gt;= self._cumulative_sum_index[-1]:\n            raise IndexError(\n                f\"Row index {row} is larger than number of rows in FeatureIndex ({self._cumulative_sum_index[-1]}).\"\n            )\n\n        # creates a mask for values where cumulative sum &gt; row\n        mask = ~(self._cumulative_sum_index &gt; row)\n        # Sum these to get the index of the first range &gt; row\n        # Subtract one to get the range containing row.\n        d_id = sum(mask) - 1\n        return d_id\n\n    @staticmethod\n    def _load_common(datapath: str, instance: \"RowFeatureIndex\") -&gt; \"RowFeatureIndex\":\n        \"\"\"Load state common to all concrete indices from a directory.\n\n        Reads block data from Parquet files under `datapath`, plus the saved\n        cumulative sum index, labels, and version.\n\n        Args:\n            datapath: Directory containing saved index files.\n            instance: An empty instance of the concrete subclass to populate.\n\n        Returns:\n            The populated `instance`.\n        \"\"\"\n        parquet_data_paths = sorted(Path(datapath).rglob(\"*.parquet\"))\n        data_tables = [pq.read_table(csv_path) for csv_path in parquet_data_paths]\n        instance._feature_arr = [\n            {column: table[column].to_numpy() for column in table.column_names} for table in data_tables\n        ]\n        instance._num_entries_per_row = []\n        for features in instance._feature_arr:\n            instance._extend_num_entries_per_row(features)\n        instance._cumulative_sum_index = np.load(Path(datapath) / \"cumulative_sum_index.npy\")\n        instance._labels = np.load(Path(datapath) / \"labels.npy\", allow_pickle=True)\n        instance._version = np.load(Path(datapath) / \"version.npy\").item()\n        return instance\n\n    def _filter_features(\n        self, features_dict: dict[str, np.ndarray], select_features: Optional[list[str]]\n    ) -&gt; list[np.ndarray]:\n        \"\"\"Select and order features by name from a feature dictionary.\n\n        If `select_features` is None, all features are returned in dictionary\n        iteration order. Otherwise, features are returned in the provided order,\n        and a ValueError is raised if any name is missing.\n        \"\"\"\n        if select_features is not None:\n            features: list[np.ndarray] = []\n            for feature in select_features:\n                if feature not in features_dict:\n                    raise ValueError(f\"Provided feature column {feature} in select_features not present in dataset.\")\n                features.append(features_dict[feature])\n            return features\n        return [features_dict[f] for f in features_dict]\n\n    def version(self) -&gt; str:\n        \"\"\"Returns a version number.\n\n        (following &lt;major&gt;.&lt;minor&gt;.&lt;point&gt; convention).\n        \"\"\"\n        return self._version\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of blocks (feature dictionaries).\"\"\"\n        return len(self._feature_arr)\n\n    @abstractmethod\n    def _extend_num_entries_per_row(self, features: dict[str, np.ndarray]) -&gt; None:\n        \"\"\"Extend the number of entries per row for a concrete index implementation.\"\"\"\n        ...\n\n    @abstractmethod\n    def _check_if_can_merge_features_with_last_block(\n        self, features: dict[str, np.ndarray], total_csum: Optional[int] = None\n    ) -&gt; bool:\n        \"\"\"Optionally merge a new block into the last block.\n\n        Subclasses may coalesce compatible blocks and update `_cumulative_sum_index`.\n        Return True if merged; False otherwise.\n        \"\"\"\n        ...\n\n    def _append_block(self, features: dict[str, np.ndarray], label: Optional[str], total_csum: int) -&gt; None:\n        \"\"\"Append a new block after merge check, updating internal state.\n\n        This centralizes the common append path used by concrete subclasses.\n        \"\"\"\n        # Optionally merge into previous block\n        if self._check_if_can_merge_features_with_last_block(features, total_csum):\n            return\n\n        # Otherwise start a new block\n        self._cumulative_sum_index = np.append(self._cumulative_sum_index, total_csum)\n        self._feature_arr.append(features)\n        self._labels.append(label)\n        self._extend_num_entries_per_row(features)\n\n    def number_vars_at_row(self, row: int) -&gt; int:\n        \"\"\"Return the number of variables for the block that contains `row`.\"\"\"\n        dataset_idx = self._get_dataset_id(row)\n        return self._num_entries_per_row[dataset_idx]\n\n    def column_dims(self) -&gt; list[int]:\n        \"\"\"Return per-block feature counts.\n\n        For `ObservedFeatureIndex`, this is the number of feature columns per\n        block. For `VariableFeatureIndex`, this is the per-row array length per\n        block.\n        \"\"\"\n        return self._num_entries_per_row\n\n    def _validate_features_get_entries_count(self, features: dict[str, np.ndarray]) -&gt; int:\n        \"\"\"Validate feature input and return the expected number of entries for each value in the feature dictionary.\n\n        Ensures `features` is a dictionary with arrays of equal length. Returns\n        the common length (0 if empty).\n\n        Returns:\n            The length of the features.\n\n        Raises:\n            TypeError: If features is not a dictionary\n            ValueError: If the features are not all the same length\n        \"\"\"\n        if not isinstance(features, dict):\n            raise TypeError(f\"{self.__class__.__name__}.append_features expects a dict of arrays\")\n\n        if len(features) &gt; 0:\n            first_length = len(next(iter(features.values())))\n            if any(len(v) != first_length for v in features.values()):\n                raise ValueError(\"All feature arrays must have the same length\")\n            return first_length\n        else:\n            return 0\n\n    def number_of_values(self) -&gt; list[int]:\n        \"\"\"Return total value counts per block.\n\n        For each block, `(rows in block) * (per-block feature count)`. Returns\n        `[0]` when there are no blocks.\n        \"\"\"\n        if len(self._feature_arr) == 0:\n            return [0]\n        rows = [\n            self._cumulative_sum_index[i] - max(self._cumulative_sum_index[i - 1], 0)\n            for i in range(1, len(self._cumulative_sum_index))\n        ]\n        vals = []\n        vals = [n_rows * self._num_entries_per_row[i] for i, n_rows in enumerate(rows)]\n        return vals\n\n    @abstractmethod\n    def concat(self, other_row_index: \"RowFeatureIndex\") -&gt; \"RowFeatureIndex\":\n        \"\"\"Concatenate another FeatureIndex to this one.\n\n        Args:\n            other_row_index: another FeatureIndex\n\n        Returns:\n            self (updated).\n\n        Raises:\n            TypeError or ValueError\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def append_features(self, *args, **kwargs) -&gt; None:\n        \"\"\"Append features, delegating validation and merge behavior to subclasses.\n\n        May or may not have n_obs as an argument, depending on subclass.\n        \"\"\"\n        ...\n\n    def number_of_rows(self) -&gt; int:\n        \"\"\"The number of rows in the index.\n\n        Returns:\n            An integer corresponding to the number or rows in the index\n        \"\"\"\n        return int(max(self._cumulative_sum_index[-1], 0))\n\n    def save(self, datapath: str) -&gt; None:\n        \"\"\"Saves the FeatureIndex to a given path.\n\n        Args:\n            datapath: path to save the index\n        \"\"\"\n        Path(datapath).mkdir(parents=True, exist_ok=True)\n        num_digits = len(str(len(self._feature_arr)))\n        for index, feature_dict in enumerate(self._feature_arr):\n            table = pa.table({col: _to_arrow_array(vals) for col, vals in feature_dict.items()})\n            dataframe_str_index = f\"{index:0{num_digits}d}\"\n            pq.write_table(table, f\"{datapath}/dataframe_{dataframe_str_index}.parquet\")\n        np.save(Path(datapath) / \"cumulative_sum_index.npy\", self._cumulative_sum_index)\n        np.save(Path(datapath) / \"labels.npy\", self._labels)\n        np.save(Path(datapath) / \"version.npy\", np.array(self._version))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.__init__","title":"<code>__init__()</code>","text":"<p>Instantiates the index.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Instantiates the index.\"\"\"\n    self._cumulative_sum_index: np.array = np.array([-1])\n    self._feature_arr: list[dict[str, np.ndarray]] = []\n    self._num_entries_per_row: list[int] = []\n    self._version = importlib.metadata.version(\"bionemo.scdl\")\n    self._labels: list[Optional[str]] = []\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of blocks (feature dictionaries).</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of blocks (feature dictionaries).\"\"\"\n    return len(self._feature_arr)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.append_features","title":"<code>append_features(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Append features, delegating validation and merge behavior to subclasses.</p> <p>May or may not have n_obs as an argument, depending on subclass.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>@abstractmethod\ndef append_features(self, *args, **kwargs) -&gt; None:\n    \"\"\"Append features, delegating validation and merge behavior to subclasses.\n\n    May or may not have n_obs as an argument, depending on subclass.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.column_dims","title":"<code>column_dims()</code>","text":"<p>Return per-block feature counts.</p> <p>For <code>ObservedFeatureIndex</code>, this is the number of feature columns per block. For <code>VariableFeatureIndex</code>, this is the per-row array length per block.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def column_dims(self) -&gt; list[int]:\n    \"\"\"Return per-block feature counts.\n\n    For `ObservedFeatureIndex`, this is the number of feature columns per\n    block. For `VariableFeatureIndex`, this is the per-row array length per\n    block.\n    \"\"\"\n    return self._num_entries_per_row\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.concat","title":"<code>concat(other_row_index)</code>  <code>abstractmethod</code>","text":"<p>Concatenate another FeatureIndex to this one.</p> <p>Parameters:</p> Name Type Description Default <code>other_row_index</code> <code>'RowFeatureIndex'</code> <p>another FeatureIndex</p> required <p>Returns:</p> Type Description <code>'RowFeatureIndex'</code> <p>self (updated).</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>@abstractmethod\ndef concat(self, other_row_index: \"RowFeatureIndex\") -&gt; \"RowFeatureIndex\":\n    \"\"\"Concatenate another FeatureIndex to this one.\n\n    Args:\n        other_row_index: another FeatureIndex\n\n    Returns:\n        self (updated).\n\n    Raises:\n        TypeError or ValueError\n    \"\"\"\n    ...\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.number_of_rows","title":"<code>number_of_rows()</code>","text":"<p>The number of rows in the index.</p> <p>Returns:</p> Type Description <code>int</code> <p>An integer corresponding to the number or rows in the index</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def number_of_rows(self) -&gt; int:\n    \"\"\"The number of rows in the index.\n\n    Returns:\n        An integer corresponding to the number or rows in the index\n    \"\"\"\n    return int(max(self._cumulative_sum_index[-1], 0))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.number_of_values","title":"<code>number_of_values()</code>","text":"<p>Return total value counts per block.</p> <p>For each block, <code>(rows in block) * (per-block feature count)</code>. Returns <code>[0]</code> when there are no blocks.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def number_of_values(self) -&gt; list[int]:\n    \"\"\"Return total value counts per block.\n\n    For each block, `(rows in block) * (per-block feature count)`. Returns\n    `[0]` when there are no blocks.\n    \"\"\"\n    if len(self._feature_arr) == 0:\n        return [0]\n    rows = [\n        self._cumulative_sum_index[i] - max(self._cumulative_sum_index[i - 1], 0)\n        for i in range(1, len(self._cumulative_sum_index))\n    ]\n    vals = []\n    vals = [n_rows * self._num_entries_per_row[i] for i, n_rows in enumerate(rows)]\n    return vals\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.number_vars_at_row","title":"<code>number_vars_at_row(row)</code>","text":"<p>Return the number of variables for the block that contains <code>row</code>.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def number_vars_at_row(self, row: int) -&gt; int:\n    \"\"\"Return the number of variables for the block that contains `row`.\"\"\"\n    dataset_idx = self._get_dataset_id(row)\n    return self._num_entries_per_row[dataset_idx]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.save","title":"<code>save(datapath)</code>","text":"<p>Saves the FeatureIndex to a given path.</p> <p>Parameters:</p> Name Type Description Default <code>datapath</code> <code>str</code> <p>path to save the index</p> required Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def save(self, datapath: str) -&gt; None:\n    \"\"\"Saves the FeatureIndex to a given path.\n\n    Args:\n        datapath: path to save the index\n    \"\"\"\n    Path(datapath).mkdir(parents=True, exist_ok=True)\n    num_digits = len(str(len(self._feature_arr)))\n    for index, feature_dict in enumerate(self._feature_arr):\n        table = pa.table({col: _to_arrow_array(vals) for col, vals in feature_dict.items()})\n        dataframe_str_index = f\"{index:0{num_digits}d}\"\n        pq.write_table(table, f\"{datapath}/dataframe_{dataframe_str_index}.parquet\")\n    np.save(Path(datapath) / \"cumulative_sum_index.npy\", self._cumulative_sum_index)\n    np.save(Path(datapath) / \"labels.npy\", self._labels)\n    np.save(Path(datapath) / \"version.npy\", np.array(self._version))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.version","title":"<code>version()</code>","text":"<p>Returns a version number.</p> <p>(following .. convention). Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def version(self) -&gt; str:\n    \"\"\"Returns a version number.\n\n    (following &lt;major&gt;.&lt;minor&gt;.&lt;point&gt; convention).\n    \"\"\"\n    return self._version\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.VariableFeatureIndex","title":"<code>VariableFeatureIndex</code>","text":"<p>               Bases: <code>RowFeatureIndex</code></p> <p>Feature index for variables (columns).</p> <p>Lookup returns full arrays for the block that contains a given row. When successive blocks have identical feature dictionaries, they are merged and only <code>_cumulative_sum_index</code> advances.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>class VariableFeatureIndex(RowFeatureIndex):\n    \"\"\"Feature index for variables (columns).\n\n    Lookup returns full arrays for the block that contains a given row. When\n    successive blocks have identical feature dictionaries, they are merged and\n    only `_cumulative_sum_index` advances.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Create a variable (column) feature index.\"\"\"\n        super().__init__()\n\n    def _check_if_can_merge_features_with_last_block(self, features: dict[str, np.ndarray], total_csum: int) -&gt; bool:\n        \"\"\"Check if the features are the same as the last features in the index and if so, merge the current block with the last block.\"\"\"\n        if len(self._feature_arr) &gt; 0 and are_dicts_equal(self._feature_arr[-1], features):\n            self._cumulative_sum_index[-1] = total_csum\n            return True\n        return False\n\n    def _extend_num_entries_per_row(self, features: dict[str, np.ndarray]) -&gt; None:\n        \"\"\"Record per-row array length for this block.\"\"\"\n        if self._validate_features_get_entries_count(features) == 0:\n            num_entries = 0\n        else:\n            num_entries = len(features[next(iter(features.keys()))])\n        self._num_entries_per_row.append(num_entries)\n\n    @staticmethod\n    def load(datapath: str) -&gt; \"VariableFeatureIndex\":\n        \"\"\"Load a variable (column) feature index from a directory.\n\n        This will  load the parquet files in sorted order. In the SCDL use case, this expects a directory with\n        parquet files named dataframe_&lt;index&gt;.parquet.\n        \"\"\"\n        return RowFeatureIndex._load_common(datapath, VariableFeatureIndex())\n\n    def lookup(self, row: int, select_features: Optional[list[str]] = None) -&gt; Tuple[list[np.ndarray], Optional[str]]:\n        \"\"\"Return feature arrays and the block label for the block containing `row`.\"\"\"\n        d_id = self._get_dataset_id(row)\n        features_dict = self._feature_arr[d_id]\n        features = self._filter_features(features_dict, select_features)\n        return features, self._labels[d_id]\n\n    def append_features(self, n_obs: int, features: dict[str, np.ndarray], label: Optional[str] = None) -&gt; None:\n        \"\"\"Append a new block, or merge into the last block when possible.\"\"\"\n        self._validate_features_get_entries_count(features)\n        total_csum = max(self._cumulative_sum_index[-1], 0) + n_obs\n        self._append_block(features, label, total_csum)\n\n    def concat(self, other_row_index: VariableFeatureIndex) -&gt; VariableFeatureIndex:\n        \"\"\"Concatenates the other VariableFeatureIndex to this one.\n\n        Returns the new, updated index. Warning: modifies this index in-place.\n\n        Args:\n            other_row_index: another VariableFeatureIndex\n\n        Returns:\n            self, the VariableFeatureIndex after the concatenations.\n\n        Raises:\n            TypeError if other_row_index is not a VariableFeatureIndex\n            ValueError if an empty FeatureIndex is passed and the function is\n            set to fail in this case.\n        \"\"\"\n        # Require the exact same concrete subclass to ensure semantic compatibility\n        if not isinstance(other_row_index, VariableFeatureIndex):\n            raise TypeError(\"Error: trying to concatenate something that's not a VariableFeatureIndex.\")\n        for i, feats in enumerate(list(other_row_index._feature_arr)):\n            c_span = other_row_index._cumulative_sum_index[i + 1] - max(0, other_row_index._cumulative_sum_index[i])\n            label = other_row_index._labels[i]\n            self.append_features(c_span, feats, label)\n\n        return self\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.VariableFeatureIndex.__init__","title":"<code>__init__()</code>","text":"<p>Create a variable (column) feature index.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Create a variable (column) feature index.\"\"\"\n    super().__init__()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.VariableFeatureIndex.append_features","title":"<code>append_features(n_obs, features, label=None)</code>","text":"<p>Append a new block, or merge into the last block when possible.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def append_features(self, n_obs: int, features: dict[str, np.ndarray], label: Optional[str] = None) -&gt; None:\n    \"\"\"Append a new block, or merge into the last block when possible.\"\"\"\n    self._validate_features_get_entries_count(features)\n    total_csum = max(self._cumulative_sum_index[-1], 0) + n_obs\n    self._append_block(features, label, total_csum)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.VariableFeatureIndex.concat","title":"<code>concat(other_row_index)</code>","text":"<p>Concatenates the other VariableFeatureIndex to this one.</p> <p>Returns the new, updated index. Warning: modifies this index in-place.</p> <p>Parameters:</p> Name Type Description Default <code>other_row_index</code> <code>VariableFeatureIndex</code> <p>another VariableFeatureIndex</p> required <p>Returns:</p> Type Description <code>VariableFeatureIndex</code> <p>self, the VariableFeatureIndex after the concatenations.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def concat(self, other_row_index: VariableFeatureIndex) -&gt; VariableFeatureIndex:\n    \"\"\"Concatenates the other VariableFeatureIndex to this one.\n\n    Returns the new, updated index. Warning: modifies this index in-place.\n\n    Args:\n        other_row_index: another VariableFeatureIndex\n\n    Returns:\n        self, the VariableFeatureIndex after the concatenations.\n\n    Raises:\n        TypeError if other_row_index is not a VariableFeatureIndex\n        ValueError if an empty FeatureIndex is passed and the function is\n        set to fail in this case.\n    \"\"\"\n    # Require the exact same concrete subclass to ensure semantic compatibility\n    if not isinstance(other_row_index, VariableFeatureIndex):\n        raise TypeError(\"Error: trying to concatenate something that's not a VariableFeatureIndex.\")\n    for i, feats in enumerate(list(other_row_index._feature_arr)):\n        c_span = other_row_index._cumulative_sum_index[i + 1] - max(0, other_row_index._cumulative_sum_index[i])\n        label = other_row_index._labels[i]\n        self.append_features(c_span, feats, label)\n\n    return self\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.VariableFeatureIndex.load","title":"<code>load(datapath)</code>  <code>staticmethod</code>","text":"<p>Load a variable (column) feature index from a directory.</p> <p>This will  load the parquet files in sorted order. In the SCDL use case, this expects a directory with parquet files named dataframe_.parquet. Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>@staticmethod\ndef load(datapath: str) -&gt; \"VariableFeatureIndex\":\n    \"\"\"Load a variable (column) feature index from a directory.\n\n    This will  load the parquet files in sorted order. In the SCDL use case, this expects a directory with\n    parquet files named dataframe_&lt;index&gt;.parquet.\n    \"\"\"\n    return RowFeatureIndex._load_common(datapath, VariableFeatureIndex())\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.VariableFeatureIndex.lookup","title":"<code>lookup(row, select_features=None)</code>","text":"<p>Return feature arrays and the block label for the block containing <code>row</code>.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def lookup(self, row: int, select_features: Optional[list[str]] = None) -&gt; Tuple[list[np.ndarray], Optional[str]]:\n    \"\"\"Return feature arrays and the block label for the block containing `row`.\"\"\"\n    d_id = self._get_dataset_id(row)\n    features_dict = self._feature_arr[d_id]\n    features = self._filter_features(features_dict, select_features)\n    return features, self._labels[d_id]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.are_dicts_equal","title":"<code>are_dicts_equal(dict1, dict2)</code>","text":"<p>Compare two dictionaries with string keys and numpy.ndarray values.</p> <p>Parameters:</p> Name Type Description Default <code>dict1</code> <code>dict[str, ndarray]</code> <p>The first dictionary to compare.</p> required <code>dict2</code> <code>dict[str, ndarray]</code> <p>The second dictionary to compare.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the dictionaries have the same keys and all corresponding   numpy arrays are equal; False otherwise.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def are_dicts_equal(dict1: dict[str, np.ndarray], dict2: dict[str, np.ndarray]) -&gt; bool:\n    \"\"\"Compare two dictionaries with string keys and numpy.ndarray values.\n\n    Args:\n        dict1 (dict[str, np.ndarray]): The first dictionary to compare.\n        dict2 (dict[str, np.ndarray]): The second dictionary to compare.\n\n    Returns:\n        bool: True if the dictionaries have the same keys and all corresponding\n              numpy arrays are equal; False otherwise.\n    \"\"\"\n    return dict1.keys() == dict2.keys() and all(np.array_equal(dict1[k], dict2[k]) for k in dict1)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_collection/","title":"Single cell collection","text":""},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.FileNames","title":"<code>FileNames</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Names of files that are generated in SingleCellCollection.</p> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>class FileNames(str, Enum):\n    \"\"\"Names of files that are generated in SingleCellCollection.\"\"\"\n\n    VERSION = \"version.json\"\n    METADATA = \"metadata.json\"\n    FEATURES = \"features\"\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection","title":"<code>SingleCellCollection</code>","text":"<p>               Bases: <code>SingleCellRowDatasetCore</code></p> <p>A collection of one or more SingleCellMemMapDatasets.</p> <p>SingleCellCollection support most of the functionality of the SingleCellDataSet API. An SingleCellCollection can be converted to a single SingleCellMemMapDataset. A SingleCellCollection enables the use of heterogeneous datasets, such as those composed of many AnnData files.</p> <p>Attributes:</p> Name Type Description <code>_version</code> <code>str</code> <p>The version of the dataset</p> <code>data_path</code> <code>str</code> <p>The directory where the colleection of datasets is stored.</p> <code>_feature_index</code> <code>str</code> <p>The corresponding VariableFeatureIndex where features are</p> <code>fname_to_mmap</code> <code>Dict[str, SingleCellMemMapDataset]</code> <p>dictionary to hold each SingleCellMemMapDataset object.</p> <code>False</code> <code>Dict[str, SingleCellMemMapDataset]</code> <p>not ragged; all SingleCellMemMapDataset have same column dimemsion</p> <code>True</code> <code>Dict[str, SingleCellMemMapDataset]</code> <p>ragged; scmmap column dimemsions vary</p> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>class SingleCellCollection(SingleCellRowDatasetCore):\n    \"\"\"A collection of one or more SingleCellMemMapDatasets.\n\n    SingleCellCollection support most of the functionality of the\n    SingleCellDataSet API. An SingleCellCollection can be converted\n    to a single SingleCellMemMapDataset. A SingleCellCollection\n    enables the use of heterogeneous datasets, such as those composed of many\n    AnnData files.\n\n    Attributes:\n        _version: The version of the dataset\n        data_path: The directory where the colleection of datasets is stored.\n        _feature_index: The corresponding VariableFeatureIndex where features are\n        stored.\n        fname_to_mmap:  dictionary to hold each SingleCellMemMapDataset object.\n        This maps from the path to the dataset.\n        ragged dataset is an dataset of arrays where the arrays have different\n        lengths\n        False: not ragged; all SingleCellMemMapDataset have same column dimemsion\n        True: ragged; scmmap column dimemsions vary\n    \"\"\"\n\n    def __init__(self, data_path: str) -&gt; None:\n        \"\"\"Instantiate the class.\n\n        Args:\n            data_path: Where the class will be stored.\n        \"\"\"\n        self.data_path: str = data_path\n        self._version: str = importlib.metadata.version(\"bionemo.scdl\")\n        self.metadata: Dict[str, int] = {}\n        self._var_feature_index: VariableFeatureIndex = VariableFeatureIndex()\n        self.fname_to_mmap: Dict[str, SingleCellMemMapDataset] = {}\n        Path(self.data_path).mkdir(parents=True, exist_ok=True)\n\n        # Write the version\n        if not os.path.exists(f\"{self.data_path}/{FileNames.VERSION.value}\"):\n            with open(f\"{self.data_path}/{FileNames.VERSION.value}\", \"w\") as vfi:\n                json.dump(self.version(), vfi)\n\n    def version(self) -&gt; str:\n        \"\"\"Returns a version number.\n\n        (following &lt;major&gt;.&lt;minor&gt;.&lt;point&gt; convention).\n        \"\"\"\n        return self._version\n\n    def load_h5ad(\n        self,\n        h5ad_path: str,\n        paginated_load_cutoff: int = 10_000,\n        load_block_row_size: int = 1_000_000,\n        use_X_not_raw: bool = False,\n    ) -&gt; None:\n        \"\"\"Loads data from an existing AnnData archive.\n\n        This creates and saves a new backing data structure.\n        Then, the location and the data and the dataset are stored.\n\n        Args:\n            h5ad_path: the path to AnnData archive\n            paginated_load_cutoff: The cutoff in MB for paginated loading of the AnnData files.\n            load_block_row_size: The number of rows to load into memory at a time for paginated loading of the AnnData files.\n            use_X_not_raw: If True, prefer `adata.X`; otherwise use `adata.raw.X`.\n        \"\"\"\n        mmap_path = Path(self.data_path) / Path(h5ad_path).stem\n        self.fname_to_mmap[mmap_path] = _create_single_cell_memmap_dataset_from_h5ad(\n            h5ad_path=h5ad_path,\n            base_directory_path=self.data_path,\n            paginated_load_cutoff=paginated_load_cutoff,\n            load_block_row_size=load_block_row_size,\n            use_X_not_raw=use_X_not_raw,\n        )\n        self._var_feature_index.concat(self.fname_to_mmap[mmap_path]._var_feature_index)\n\n    def load_h5ad_multi(\n        self,\n        directory_path: str,\n        max_workers: int = 5,\n        use_processes: bool = False,\n        data_dtype: str | None = None,\n        paginated_load_cutoff: int = 10_000,\n        load_block_row_size: int = 1_000_000,\n        use_X_not_raw: bool = False,\n    ) -&gt; None:\n        \"\"\"Loads one or more AnnData files and adds them to the collection.\n\n        Args:\n            directory_path: The path to the directory with the AnnData files\n            max_workers: the maximal number of workers to use\n            use_processes: If True, use ProcessPoolExecutor; otherwise, use\n                ThreadPoolExecutor\n            paginated_load_cutoff: The cutoff in MB for paginated loading of the AnnData files.\n            load_block_row_size: The number of rows to load into memory at a time for paginated loading of the AnnData files.\n            data_dtype: Optional dtype string propagated to dataset creation\n            use_X_not_raw: If True, prefer `adata.X`; otherwise use `adata.raw.X`.\n\n        Raises:\n            FileNotFoundError: If no h5ad files are found in the directory.\n            RuntimeError: If an error occurs in the loading of any of the h5ad files.\n\n        Args:\n            data_dtype: Optional dtype string propagated to dataset creation\n        \"\"\"\n        directory_path = Path(directory_path)\n        ann_data_paths = sorted(directory_path.rglob(\"*.h5ad\"))\n        if len(ann_data_paths) == 0:\n            raise FileNotFoundError(f\"There a no h5ad files in {directory_path}.\")\n        mmap_paths = [Path(self.data_path) / Path(ann_datapath).stem for ann_datapath in ann_data_paths]\n        queue = AsyncWorkQueue(max_workers=max_workers, use_processes=use_processes)\n        for ann in ann_data_paths:\n            queue.submit_task(\n                _create_single_cell_memmap_dataset_from_h5ad,\n                ann,\n                base_directory_path=self.data_path,\n                data_dtype=data_dtype,\n                paginated_load_cutoff=paginated_load_cutoff,\n                load_block_row_size=load_block_row_size,\n                use_X_not_raw=use_X_not_raw,\n            )\n        queue.wait()\n        mmaps = queue.get_task_results()\n\n        for result_path, result in zip(ann_data_paths, mmaps):\n            if isinstance(result, Exception):\n                raise RuntimeError(f\"Error in processing file {result_path}: {result}\") from result\n\n        for mmap_path, mmap in zip(mmap_paths, mmaps):\n            if isinstance(mmap, Exception):\n                raise RuntimeError(f\"Error in processing file {mmap_path}: {mmap}\") from mmap\n\n            self.fname_to_mmap[mmap_path] = mmap\n            self._var_feature_index.concat(self.fname_to_mmap[mmap_path]._var_feature_index)\n\n    def number_nonzero_values(self) -&gt; int:\n        \"\"\"Sum of the number of non zero entries in each dataset.\"\"\"\n        return sum([self.fname_to_mmap[mmap_path].number_nonzero_values() for mmap_path in self.fname_to_mmap])\n\n    def number_of_values(self) -&gt; int:\n        \"\"\"Sum of the number of values in each dataset.\"\"\"\n        return sum([self.fname_to_mmap[mmap_path].number_of_values() for mmap_path in self.fname_to_mmap])\n\n    def number_of_rows(self) -&gt; int:\n        \"\"\"The number of rows in the dataset.\n\n        Returns:\n            The number of rows in the dataset\n        Raises:\n            ValueError if the length of the number of rows in the feature\n            index does not correspond to the number of stored rows.\n        \"\"\"\n        row_sum_from_datasets = sum(\n            [self.fname_to_mmap[mmap_path].number_of_rows() for mmap_path in self.fname_to_mmap]\n        )\n        if len(self._var_feature_index) &gt; 0 and self._var_feature_index.number_of_rows() != row_sum_from_datasets:\n            raise ValueError(\n                f\"\"\"The nuber of rows in the feature index {self._var_feature_index.number_of_rows()}\n                             does not correspond to the number of rows in the datasets {row_sum_from_datasets}\"\"\"\n            )\n\n        return row_sum_from_datasets\n\n    def number_of_variables(self) -&gt; List[int]:\n        \"\"\"If ragged, returns a list of variable lengths.\n\n        If not ragged, returns a list with one entry. A ragged\n        collection is one where the datasets have different lengths.\n        \"\"\"\n        if len(self._var_feature_index) == 0:\n            return [0]\n        else:\n            num_vars = self._var_feature_index.column_dims()\n            return num_vars\n\n    def shape(self) -&gt; Tuple[int, List[int]]:\n        \"\"\"Get the shape of the dataset.\n\n        This is the number of entries by the the length of the feature index\n        corresponding to that variable.\n\n        Returns:\n            The total number of elements across dataset\n            A list containing the number of variables for each entry in the\n                VariableFeatureIndex.\n        \"\"\"\n        return self.number_of_rows(), self.number_of_variables()\n\n    def flatten(\n        self,\n        output_path: str,\n        destroy_on_copy: bool = False,\n        extend_copy_size: int = 10 * 1_024 * 1_024,\n    ) -&gt; None:\n        \"\"\"Flattens the collection into a single SingleCellMemMapDataset.\n\n        This is done by concatenating all of the SingleCellMemMapDatasets together. This can be used\n        to create a single dataset from h5ad files that are in a directory:\n                coll = SingleCellCollection(temp_dir)\n                coll.load_h5ad_multi(data_path)\n                coll.flatten(output_path, destroy_on_copy=True)\n        Then, there would be one SingleCellMemMapDataset dataset in output_path. read in with\n        SingleCellMemmpDataset(output_path).\n\n        Args:\n            output_path: location to store new dataset\n            destroy_on_copy: Whether to remove the current data_path\n            extend_copy_size: how much to copy in memory at once\n\n        \"\"\"\n        output = next(iter(self.fname_to_mmap.values()))\n\n        single_cell_list = list(self.fname_to_mmap.values())[1:]\n        if len(single_cell_list) &gt; 0:\n            output.concat(\n                single_cell_list,\n                extend_copy_size=extend_copy_size,\n                output_path=output_path,\n                destroy_on_copy=destroy_on_copy,\n            )\n        else:\n            shutil.move(output.data_path, output_path)\n            output.data_path = output_path\n        # Hit save!\n        output.save()\n\n        if destroy_on_copy:\n            shutil.rmtree(self.data_path)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.__init__","title":"<code>__init__(data_path)</code>","text":"<p>Instantiate the class.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Where the class will be stored.</p> required Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def __init__(self, data_path: str) -&gt; None:\n    \"\"\"Instantiate the class.\n\n    Args:\n        data_path: Where the class will be stored.\n    \"\"\"\n    self.data_path: str = data_path\n    self._version: str = importlib.metadata.version(\"bionemo.scdl\")\n    self.metadata: Dict[str, int] = {}\n    self._var_feature_index: VariableFeatureIndex = VariableFeatureIndex()\n    self.fname_to_mmap: Dict[str, SingleCellMemMapDataset] = {}\n    Path(self.data_path).mkdir(parents=True, exist_ok=True)\n\n    # Write the version\n    if not os.path.exists(f\"{self.data_path}/{FileNames.VERSION.value}\"):\n        with open(f\"{self.data_path}/{FileNames.VERSION.value}\", \"w\") as vfi:\n            json.dump(self.version(), vfi)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.flatten","title":"<code>flatten(output_path, destroy_on_copy=False, extend_copy_size=10 * 1024 * 1024)</code>","text":"<p>Flattens the collection into a single SingleCellMemMapDataset.</p> <p>This is done by concatenating all of the SingleCellMemMapDatasets together. This can be used to create a single dataset from h5ad files that are in a directory:         coll = SingleCellCollection(temp_dir)         coll.load_h5ad_multi(data_path)         coll.flatten(output_path, destroy_on_copy=True) Then, there would be one SingleCellMemMapDataset dataset in output_path. read in with SingleCellMemmpDataset(output_path).</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>location to store new dataset</p> required <code>destroy_on_copy</code> <code>bool</code> <p>Whether to remove the current data_path</p> <code>False</code> <code>extend_copy_size</code> <code>int</code> <p>how much to copy in memory at once</p> <code>10 * 1024 * 1024</code> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def flatten(\n    self,\n    output_path: str,\n    destroy_on_copy: bool = False,\n    extend_copy_size: int = 10 * 1_024 * 1_024,\n) -&gt; None:\n    \"\"\"Flattens the collection into a single SingleCellMemMapDataset.\n\n    This is done by concatenating all of the SingleCellMemMapDatasets together. This can be used\n    to create a single dataset from h5ad files that are in a directory:\n            coll = SingleCellCollection(temp_dir)\n            coll.load_h5ad_multi(data_path)\n            coll.flatten(output_path, destroy_on_copy=True)\n    Then, there would be one SingleCellMemMapDataset dataset in output_path. read in with\n    SingleCellMemmpDataset(output_path).\n\n    Args:\n        output_path: location to store new dataset\n        destroy_on_copy: Whether to remove the current data_path\n        extend_copy_size: how much to copy in memory at once\n\n    \"\"\"\n    output = next(iter(self.fname_to_mmap.values()))\n\n    single_cell_list = list(self.fname_to_mmap.values())[1:]\n    if len(single_cell_list) &gt; 0:\n        output.concat(\n            single_cell_list,\n            extend_copy_size=extend_copy_size,\n            output_path=output_path,\n            destroy_on_copy=destroy_on_copy,\n        )\n    else:\n        shutil.move(output.data_path, output_path)\n        output.data_path = output_path\n    # Hit save!\n    output.save()\n\n    if destroy_on_copy:\n        shutil.rmtree(self.data_path)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.load_h5ad","title":"<code>load_h5ad(h5ad_path, paginated_load_cutoff=10000, load_block_row_size=1000000, use_X_not_raw=False)</code>","text":"<p>Loads data from an existing AnnData archive.</p> <p>This creates and saves a new backing data structure. Then, the location and the data and the dataset are stored.</p> <p>Parameters:</p> Name Type Description Default <code>h5ad_path</code> <code>str</code> <p>the path to AnnData archive</p> required <code>paginated_load_cutoff</code> <code>int</code> <p>The cutoff in MB for paginated loading of the AnnData files.</p> <code>10000</code> <code>load_block_row_size</code> <code>int</code> <p>The number of rows to load into memory at a time for paginated loading of the AnnData files.</p> <code>1000000</code> <code>use_X_not_raw</code> <code>bool</code> <p>If True, prefer <code>adata.X</code>; otherwise use <code>adata.raw.X</code>.</p> <code>False</code> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def load_h5ad(\n    self,\n    h5ad_path: str,\n    paginated_load_cutoff: int = 10_000,\n    load_block_row_size: int = 1_000_000,\n    use_X_not_raw: bool = False,\n) -&gt; None:\n    \"\"\"Loads data from an existing AnnData archive.\n\n    This creates and saves a new backing data structure.\n    Then, the location and the data and the dataset are stored.\n\n    Args:\n        h5ad_path: the path to AnnData archive\n        paginated_load_cutoff: The cutoff in MB for paginated loading of the AnnData files.\n        load_block_row_size: The number of rows to load into memory at a time for paginated loading of the AnnData files.\n        use_X_not_raw: If True, prefer `adata.X`; otherwise use `adata.raw.X`.\n    \"\"\"\n    mmap_path = Path(self.data_path) / Path(h5ad_path).stem\n    self.fname_to_mmap[mmap_path] = _create_single_cell_memmap_dataset_from_h5ad(\n        h5ad_path=h5ad_path,\n        base_directory_path=self.data_path,\n        paginated_load_cutoff=paginated_load_cutoff,\n        load_block_row_size=load_block_row_size,\n        use_X_not_raw=use_X_not_raw,\n    )\n    self._var_feature_index.concat(self.fname_to_mmap[mmap_path]._var_feature_index)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.load_h5ad_multi","title":"<code>load_h5ad_multi(directory_path, max_workers=5, use_processes=False, data_dtype=None, paginated_load_cutoff=10000, load_block_row_size=1000000, use_X_not_raw=False)</code>","text":"<p>Loads one or more AnnData files and adds them to the collection.</p> <p>Parameters:</p> Name Type Description Default <code>directory_path</code> <code>str</code> <p>The path to the directory with the AnnData files</p> required <code>max_workers</code> <code>int</code> <p>the maximal number of workers to use</p> <code>5</code> <code>use_processes</code> <code>bool</code> <p>If True, use ProcessPoolExecutor; otherwise, use ThreadPoolExecutor</p> <code>False</code> <code>paginated_load_cutoff</code> <code>int</code> <p>The cutoff in MB for paginated loading of the AnnData files.</p> <code>10000</code> <code>load_block_row_size</code> <code>int</code> <p>The number of rows to load into memory at a time for paginated loading of the AnnData files.</p> <code>1000000</code> <code>data_dtype</code> <code>str | None</code> <p>Optional dtype string propagated to dataset creation</p> <code>None</code> <code>use_X_not_raw</code> <code>bool</code> <p>If True, prefer <code>adata.X</code>; otherwise use <code>adata.raw.X</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If no h5ad files are found in the directory.</p> <code>RuntimeError</code> <p>If an error occurs in the loading of any of the h5ad files.</p> <p>Parameters:</p> Name Type Description Default <code>data_dtype</code> <code>str | None</code> <p>Optional dtype string propagated to dataset creation</p> <code>None</code> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def load_h5ad_multi(\n    self,\n    directory_path: str,\n    max_workers: int = 5,\n    use_processes: bool = False,\n    data_dtype: str | None = None,\n    paginated_load_cutoff: int = 10_000,\n    load_block_row_size: int = 1_000_000,\n    use_X_not_raw: bool = False,\n) -&gt; None:\n    \"\"\"Loads one or more AnnData files and adds them to the collection.\n\n    Args:\n        directory_path: The path to the directory with the AnnData files\n        max_workers: the maximal number of workers to use\n        use_processes: If True, use ProcessPoolExecutor; otherwise, use\n            ThreadPoolExecutor\n        paginated_load_cutoff: The cutoff in MB for paginated loading of the AnnData files.\n        load_block_row_size: The number of rows to load into memory at a time for paginated loading of the AnnData files.\n        data_dtype: Optional dtype string propagated to dataset creation\n        use_X_not_raw: If True, prefer `adata.X`; otherwise use `adata.raw.X`.\n\n    Raises:\n        FileNotFoundError: If no h5ad files are found in the directory.\n        RuntimeError: If an error occurs in the loading of any of the h5ad files.\n\n    Args:\n        data_dtype: Optional dtype string propagated to dataset creation\n    \"\"\"\n    directory_path = Path(directory_path)\n    ann_data_paths = sorted(directory_path.rglob(\"*.h5ad\"))\n    if len(ann_data_paths) == 0:\n        raise FileNotFoundError(f\"There a no h5ad files in {directory_path}.\")\n    mmap_paths = [Path(self.data_path) / Path(ann_datapath).stem for ann_datapath in ann_data_paths]\n    queue = AsyncWorkQueue(max_workers=max_workers, use_processes=use_processes)\n    for ann in ann_data_paths:\n        queue.submit_task(\n            _create_single_cell_memmap_dataset_from_h5ad,\n            ann,\n            base_directory_path=self.data_path,\n            data_dtype=data_dtype,\n            paginated_load_cutoff=paginated_load_cutoff,\n            load_block_row_size=load_block_row_size,\n            use_X_not_raw=use_X_not_raw,\n        )\n    queue.wait()\n    mmaps = queue.get_task_results()\n\n    for result_path, result in zip(ann_data_paths, mmaps):\n        if isinstance(result, Exception):\n            raise RuntimeError(f\"Error in processing file {result_path}: {result}\") from result\n\n    for mmap_path, mmap in zip(mmap_paths, mmaps):\n        if isinstance(mmap, Exception):\n            raise RuntimeError(f\"Error in processing file {mmap_path}: {mmap}\") from mmap\n\n        self.fname_to_mmap[mmap_path] = mmap\n        self._var_feature_index.concat(self.fname_to_mmap[mmap_path]._var_feature_index)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.number_nonzero_values","title":"<code>number_nonzero_values()</code>","text":"<p>Sum of the number of non zero entries in each dataset.</p> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def number_nonzero_values(self) -&gt; int:\n    \"\"\"Sum of the number of non zero entries in each dataset.\"\"\"\n    return sum([self.fname_to_mmap[mmap_path].number_nonzero_values() for mmap_path in self.fname_to_mmap])\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.number_of_rows","title":"<code>number_of_rows()</code>","text":"<p>The number of rows in the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of rows in the dataset</p> <p>Raises:     ValueError if the length of the number of rows in the feature     index does not correspond to the number of stored rows.</p> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def number_of_rows(self) -&gt; int:\n    \"\"\"The number of rows in the dataset.\n\n    Returns:\n        The number of rows in the dataset\n    Raises:\n        ValueError if the length of the number of rows in the feature\n        index does not correspond to the number of stored rows.\n    \"\"\"\n    row_sum_from_datasets = sum(\n        [self.fname_to_mmap[mmap_path].number_of_rows() for mmap_path in self.fname_to_mmap]\n    )\n    if len(self._var_feature_index) &gt; 0 and self._var_feature_index.number_of_rows() != row_sum_from_datasets:\n        raise ValueError(\n            f\"\"\"The nuber of rows in the feature index {self._var_feature_index.number_of_rows()}\n                         does not correspond to the number of rows in the datasets {row_sum_from_datasets}\"\"\"\n        )\n\n    return row_sum_from_datasets\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.number_of_values","title":"<code>number_of_values()</code>","text":"<p>Sum of the number of values in each dataset.</p> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def number_of_values(self) -&gt; int:\n    \"\"\"Sum of the number of values in each dataset.\"\"\"\n    return sum([self.fname_to_mmap[mmap_path].number_of_values() for mmap_path in self.fname_to_mmap])\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.number_of_variables","title":"<code>number_of_variables()</code>","text":"<p>If ragged, returns a list of variable lengths.</p> <p>If not ragged, returns a list with one entry. A ragged collection is one where the datasets have different lengths.</p> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def number_of_variables(self) -&gt; List[int]:\n    \"\"\"If ragged, returns a list of variable lengths.\n\n    If not ragged, returns a list with one entry. A ragged\n    collection is one where the datasets have different lengths.\n    \"\"\"\n    if len(self._var_feature_index) == 0:\n        return [0]\n    else:\n        num_vars = self._var_feature_index.column_dims()\n        return num_vars\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.shape","title":"<code>shape()</code>","text":"<p>Get the shape of the dataset.</p> <p>This is the number of entries by the the length of the feature index corresponding to that variable.</p> <p>Returns:</p> Type Description <code>int</code> <p>The total number of elements across dataset</p> <code>List[int]</code> <p>A list containing the number of variables for each entry in the VariableFeatureIndex.</p> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def shape(self) -&gt; Tuple[int, List[int]]:\n    \"\"\"Get the shape of the dataset.\n\n    This is the number of entries by the the length of the feature index\n    corresponding to that variable.\n\n    Returns:\n        The total number of elements across dataset\n        A list containing the number of variables for each entry in the\n            VariableFeatureIndex.\n    \"\"\"\n    return self.number_of_rows(), self.number_of_variables()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.version","title":"<code>version()</code>","text":"<p>Returns a version number.</p> <p>(following .. convention). Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def version(self) -&gt; str:\n    \"\"\"Returns a version number.\n\n    (following &lt;major&gt;.&lt;minor&gt;.&lt;point&gt; convention).\n    \"\"\"\n    return self._version\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/","title":"Single cell memmap dataset","text":""},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset","title":"<code>SingleCellMemMapDataset</code>","text":"<p>               Bases: <code>SingleCellRowDataset</code></p> <p>Represents one or more AnnData matrices.</p> <p>Data is stored in large, memory-mapped arrays that enables fast access of datasets larger than the available amount of RAM on a system. SCMMAP implements a consistent API defined in SingleCellRowDataset.</p> <p>Attributes:</p> Name Type Description <code>data_path</code> <code>str</code> <p>Location of np.memmap files to be loaded from or that will be</p> <code>mode</code> <code>Mode</code> <p>Whether the dataset will be read in (r+) from np.memmap files or</p> <code>data</code> <code>Optional[ndarray]</code> <p>A numpy array of the data</p> <code>row_index</code> <code>Optional[ndarray]</code> <p>A numpy array of row pointers</p> <code>col_index</code> <code>Optional[ndarray]</code> <p>A numpy array of column values</p> <code>metadata</code> <code>Dict[str, int]</code> <p>Various metadata about the dataset.</p> <code>_feature_index</code> <code>Dict[str, int]</code> <p>The corresponding VariableFeatureIndex where features are</p> <code>dtypes</code> <code>Dict[FileNames, str]</code> <p>A dictionary containing the datatypes of the data, row_index,</p> <code>_version</code> <code>str</code> <p>The version of the dataset</p> <code>load_neighbors</code> <code>bool</code> <p>Whether to load and utilize neighbor information from the 'neighbor_key' in AnnData's .obsp. Defaults to False.</p> <code>neighbor_key</code> <code>str</code> <p>The key in AnnData's .obsp containing the sparse adjacency matrix for neighbors. Defaults to 'next_cell_ids'.</p> <code>neighbor_sampling_strategy</code> <code>str</code> <p>Strategy for sampling neighbors ('random'). Defaults to 'random'.</p> <code>fallback_to_identity</code> <code>bool</code> <p>If a cell has no neighbors, whether to use the cell itself as its neighbor. Defaults to True.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>class SingleCellMemMapDataset(SingleCellRowDataset):\n    \"\"\"Represents one or more AnnData matrices.\n\n    Data is stored in large, memory-mapped arrays that enables fast access of\n    datasets larger than the available amount of RAM on a system. SCMMAP\n    implements a consistent API defined in SingleCellRowDataset.\n\n    Attributes:\n        data_path: Location of np.memmap files to be loaded from or that will be\n        created.\n        mode: Whether the dataset will be read in (r+) from np.memmap files or\n        written to np.memmap files (w+).\n        data: A numpy array of the data\n        row_index: A numpy array of row pointers\n        col_index: A numpy array of column values\n        metadata: Various metadata about the dataset.\n        _feature_index: The corresponding VariableFeatureIndex where features are\n        stored\n        dtypes: A dictionary containing the datatypes of the data, row_index,\n        and col_index arrays.\n        _version: The version of the dataset\n        load_neighbors (bool, optional): Whether to load and utilize neighbor information\n            from the 'neighbor_key' in AnnData's .obsp. Defaults to False.\n        neighbor_key (str, optional): The key in AnnData's .obsp containing the\n            sparse adjacency matrix for neighbors. Defaults to 'next_cell_ids'.\n        neighbor_sampling_strategy (str, optional): Strategy for sampling neighbors ('random').\n            Defaults to 'random'.\n        fallback_to_identity (bool, optional): If a cell has no neighbors, whether\n            to use the cell itself as its neighbor. Defaults to True.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_path: str,\n        h5ad_path: Optional[str] = None,\n        num_elements: Optional[int] = None,\n        num_rows: Optional[int] = None,\n        mode: Mode = Mode.READ_APPEND,\n        paginated_load_cutoff: int = 10_000,\n        load_block_row_size: int = 1_000_000,\n        var_feature_index_name=\"var_feature_id\",\n        obs_feature_index_name=\"obs_feature_id\",\n        # --- Neighbor Args ---\n        load_neighbors: bool = False,\n        neighbor_key: str = \"next_cell_ids\",\n        neighbor_sampling_strategy: str = NeighborSamplingStrategy.RANDOM,\n        fallback_to_identity: bool = True,\n        data_dtype: Optional[str] = None,  # Must be one of INT_ORDER or FLOAT_ORDER in scdl_constants\n        data_dtype_tolerance: float = 1e-08,\n        use_X_not_raw: bool = False,  # If True, use .X instead of .raw.X for the data\n    ) -&gt; None:\n        \"\"\"Instantiate the class.\n\n        Args:\n            data_path: The location where the data np.memmap files are read from\n            or stored.\n            h5ad_path: Optional, the location of the h5_ad path.\n            num_elements: The total number of elements in the array.\n            num_rows: The number of rows in the data frame.\n            mode: Whether to read or write from the data_path.\n            paginated_load_cutoff: MB size on disk at which to load the h5ad structure with paginated load.\n            load_block_row_size: Number of rows to load into memory with paginated load\n            var_feature_index_name: The name of the features if the features are only stored in features_df.index.values\n            obs_feature_index_name: The name of the obs features if the features are only stored in features_df.index.values\n            # --- New Neighbor Args ---\n            load_neighbors (bool, optional): Boolean to control to control whether to load and utilize neighbor information\n            neighbor_key (str, optional): The key in AnnData's .obsp containing neighbor information.\n            neighbor_sampling_strategy (str, optional): Sampling strategy for neighbors.\n            fallback_to_identity (bool, optional): If a cell has no neighbors, whether to use the cell itself as its neighbor.\n            data_dtype (str | None, optional): Desired dtype for `data.npy` when creating\n                new datasets; if None, defaults to 'float32'. Must be one of\n                'uint8','uint16','uint32','uint64','float16','float32','float64'.\n            data_dtype_tolerance (float, optional): Tolerance for data type conversion. Defaults to 1e-08.\n            use_X_not_raw (bool, optional): If True, use .X instead of .raw.X for the data.\n        \"\"\"\n        self._version: str = importlib.metadata.version(\"bionemo.scdl\")\n        self.data_path: str = data_path\n        self.header: SCDLHeader = None\n        self.mode: Mode = mode\n        self.paginated_load_cutoff = paginated_load_cutoff\n        self.load_block_row_size = load_block_row_size\n        self.var_feature_index_name = var_feature_index_name\n        self.obs_feature_index_name = obs_feature_index_name\n        self.use_X_not_raw = use_X_not_raw\n        # Backing arrays\n        self.data: Optional[np.ndarray] = None\n        self.row_index: Optional[np.ndarray] = None\n        # Metadata and attributes\n        self.metadata: Dict[str, int] = {}\n\n        # Stores the Feature Index, which tracks\n        # the original AnnData features (e.g., gene names)\n        # and allows us to store ragged arrays in our SCMMAP structure.\n        self._var_feature_index: VariableFeatureIndex = VariableFeatureIndex()\n        self._obs_feature_index: ObservedFeatureIndex = ObservedFeatureIndex()\n        allowed_dtypes = list(INT_ORDER + FLOAT_ORDER)\n        if data_dtype is not None and data_dtype not in allowed_dtypes:\n            raise ValueError(f\"Invalid data_dtype '{data_dtype}'. Must be one of: {', '.join(allowed_dtypes)}\")\n        # Variables for int packing / reduced precision\n        self.dtypes: Dict[FileNames, str] = {\n            f\"{FileNames.DATA.value}\": \"float32\" if data_dtype is None else data_dtype,\n            f\"{FileNames.COLPTR.value}\": \"uint32\",\n            f\"{FileNames.ROWPTR.value}\": \"uint64\",\n            f\"{FileNames.NEIGHBOR_INDICES.value}\": \"uint32\",\n            f\"{FileNames.NEIGHBOR_INDICES_PTR.value}\": \"uint64\",\n            f\"{FileNames.NEIGHBOR_VALUES.value}\": \"float32\",\n        }\n        self.data_dtype_tolerance = data_dtype_tolerance\n        # Neighbor configuration\n        self.load_neighbors = load_neighbors\n        self._has_neighbors = False\n        if load_neighbors:\n            self._init_neighbor_args(neighbor_key, neighbor_sampling_strategy, fallback_to_identity)\n\n        if mode == Mode.CREATE_APPEND and os.path.exists(data_path):\n            raise FileExistsError(f\"Output directory already exists: {data_path}\")\n\n        if h5ad_path is not None and (data_path is not None and os.path.exists(data_path)):\n            raise FileExistsError(\n                \"Invalid input; both an existing SCMMAP and an h5ad file were passed. \"\n                \"Please pass either an existing SCMMAP or an h5ad file.\"\n            )\n\n        # If there is only a data path, and it exists already, load SCMMAP data.\n        elif data_path is not None and os.path.exists(data_path):\n            self.__init__obj()\n            self.load(data_path)\n\n        # If there is only an h5ad path, load the HDF5 data\n        elif h5ad_path is not None:\n            self.__init__obj()\n            self.load_h5ad(h5ad_path)\n        else:\n            match num_rows, num_elements:\n                case (int(), int()):\n                    self.__init__obj()\n                    self._init_arrs(num_elements=num_elements, num_rows=num_rows)\n                case _:\n                    raise ValueError(\"An np.memmap path, an h5ad path, or the number of elements and rows is required\")\n\n    def _path_in_archive(self, filename: str | Path) -&gt; str:\n        \"\"\"Returns the full path to a file within the archive, joining self.data_path and the filename.\n\n        Args:\n            filename: The filename or Path object to resolve within the archive.\n\n        Returns:\n            The full path as a string.\n        \"\"\"\n        if isinstance(filename, Path):\n            filename = str(filename)\n        return os.path.join(self.data_path, filename)\n\n    @property\n    def header_path(self) -&gt; str:\n        \"\"\"Returns the full path to the header file in the archive.\n\n        Example:\n            &gt;&gt;&gt; ds = SingleCellMemMapDataset(data_path=\"my_data\")\n            &gt;&gt;&gt; ds.header_path\n            'my_data/scdl_header.json'\n        \"\"\"\n        return self._path_in_archive(FileNames.HEADER.value)\n\n    def _init_neighbor_args(self, neighbor_key, neighbor_sampling_strategy, fallback_to_identity):\n        # Neighbor tracking\n        self._has_neighbors = False  # Track if neighbor data was successfully loaded/found\n\n        self.neighbor_key = neighbor_key\n        try:\n            # Convert string to enum if a string was passed\n            if isinstance(neighbor_sampling_strategy, str):\n                neighbor_sampling_strategy = NeighborSamplingStrategy(neighbor_sampling_strategy)\n            # Validate that it's a valid enum value\n            if not isinstance(neighbor_sampling_strategy, NeighborSamplingStrategy):\n                raise ValueError(f\"Unsupported neighbor_sampling_strategy: {neighbor_sampling_strategy}\")\n        except ValueError:\n            raise ValueError(f\"Unsupported neighbor_sampling_strategy: {neighbor_sampling_strategy}\")\n\n        self.neighbor_sampling_strategy = neighbor_sampling_strategy\n        self.fallback_to_identity = fallback_to_identity\n\n    def __init__obj(self):\n        \"\"\"Initializes the data path and writes the version.\"\"\"\n        os.makedirs(self.data_path, exist_ok=True)\n\n        # Write the version\n        if not os.path.exists(f\"{self.data_path}/{FileNames.VERSION.value}\"):\n            with open(f\"{self.data_path}/{FileNames.VERSION.value}\", \"w\") as vfi:\n                json.dump(self.version(), vfi)\n\n    def _init_arrs(self, num_elements: int, num_rows: int) -&gt; None:\n        self.mode = Mode.CREATE_APPEND\n        data_arr, col_arr, row_arr = _create_compressed_sparse_row_memmaps(\n            num_elements=num_elements,\n            num_rows=num_rows,\n            memmap_dir_path=Path(self.data_path),\n            mode=self.mode,\n            dtypes=self.dtypes,\n        )\n        self.data = data_arr\n        self.col_index = col_arr\n        self.row_index = row_arr\n\n    def version(self) -&gt; str:\n        \"\"\"Returns a version number.\n\n        (following &lt;major&gt;.&lt;minor&gt;.&lt;point&gt; convention).\n        \"\"\"\n        return self._version\n\n    def _extract_neighbor_data(self, adata) -&gt; bool:\n        \"\"\"Extracts neighbor data from AnnData.obsp object and saves to memmap files.\n\n        Args:\n            adata: AnnData object containing neighbor information\n        Returns:\n            bool: True if neighbor data was successfully loaded/found, False otherwise.\n        \"\"\"\n        # Check if neighbor key exists in AnnData.obsp\n        if self.neighbor_key not in adata.obsp:\n            warnings.warn(f\"Neighbor key '{self.neighbor_key}' not found in AnnData.obsp. Neighbor loading skipped.\")\n            return False\n\n        logger.info(f\"Extracting neighbor data from {self.neighbor_key} in AnnData.obsp\")\n\n        # Get the neighbor matrix from obsp\n        neighbor_matrix = adata.obsp[self.neighbor_key]\n\n        # Check if the neighbor matrix is a sparse matrix\n        if not scipy.sparse.issparse(neighbor_matrix):\n            raise ValueError(f\"Neighbor matrix for key '{self.neighbor_key}' is not a sparse matrix.\")\n\n        # Initialize memory-mapped arrays for neighbor data with proper sizes\n        indptr_len = len(neighbor_matrix.indptr)\n        nnz = len(neighbor_matrix.indices)  # number of non-zero elements\n        # No need to calculate data_len separately since it equals nnz\n\n        # Create memory-mapped arrays for neighbor data\n        self._neighbor_indptr = np.memmap(\n            f\"{self.data_path}/{FileNames.NEIGHBOR_INDICES_PTR.value}\",\n            dtype=self.dtypes[f\"{FileNames.NEIGHBOR_INDICES_PTR.value}\"],\n            mode=Mode.CREATE_APPEND.value,\n            shape=(indptr_len,),\n        )\n\n        self._neighbor_indices = np.memmap(\n            f\"{self.data_path}/{FileNames.NEIGHBOR_INDICES.value}\",\n            dtype=self.dtypes[f\"{FileNames.NEIGHBOR_INDICES.value}\"],\n            mode=Mode.CREATE_APPEND.value,\n            shape=(nnz,),\n        )\n\n        self._neighbor_data = np.memmap(\n            f\"{self.data_path}/{FileNames.NEIGHBOR_VALUES.value}\",\n            dtype=self.dtypes[f\"{FileNames.NEIGHBOR_VALUES.value}\"],\n            mode=Mode.CREATE_APPEND.value,\n            shape=(nnz,),\n        )\n\n        # Copy data into memory-mapped arrays (with dtype conversion)\n        self._neighbor_indptr[:] = neighbor_matrix.indptr.astype(\n            self.dtypes[f\"{FileNames.NEIGHBOR_INDICES_PTR.value}\"]\n        )\n        self._neighbor_indices[:] = neighbor_matrix.indices.astype(self.dtypes[f\"{FileNames.NEIGHBOR_INDICES.value}\"])\n        self._neighbor_data[:] = neighbor_matrix.data.astype(self.dtypes[f\"{FileNames.NEIGHBOR_VALUES.value}\"])\n\n        logger.info(\"Neighbor data extracted to memory-mapped arrays\")\n        return True\n\n    def cast_data_to_dtype(self, dtype: str) -&gt; None:\n        \"\"\"Casts the data dtype of the dataset to the given dtype. This will convert the data memory map in-place on the disk.\n\n        Args:\n            dtype: The dtype to cast the data to. Must be one of INT_ORDER + FLOAT_ORDER.\n        \"\"\"\n        allowed_dtypes = list(INT_ORDER + FLOAT_ORDER)\n\n        if dtype is None or dtype not in allowed_dtypes:\n            raise ValueError(f\"Invalid data_dtype '{dtype}'. Must be one of: {', '.join(allowed_dtypes)}\")\n\n        # writes the new dtype to the disk\n        self._convert_dataset_to_new_dtypes(new_dtypes={FileNames.DATA.value: dtype}, allow_downscaling=True)\n        # Save the updated header (with a new data dtype to the disk)\n        self._write_header()\n\n    def _extract_neighbor_data_paginated(self, adata) -&gt; bool:\n        \"\"\"Extracts neighbor data using paginated approach for large datasets.\n\n        Uses the same pattern as paginated_load_h5ad with binary file I/O and chunking\n        to efficiently handle large neighbor matrices without loading everything at once.\n\n        Args:\n            adata: AnnData object containing neighbor information\n        Returns:\n            bool: True if neighbor data was successfully loaded/found, False otherwise.\n        \"\"\"\n        # Check if neighbor key exists in AnnData.obsp\n        if self.neighbor_key not in adata.obsp:\n            warnings.warn(f\"Neighbor key '{self.neighbor_key}' not found in AnnData.obsp. Neighbor loading skipped.\")\n            return False\n\n        logger.info(f\"Extracting neighbor data from {self.neighbor_key} in AnnData.obsp using chunked approach\")\n\n        # Get the neighbor matrix from obsp\n        neighbor_matrix = adata.obsp[self.neighbor_key]\n\n        # Check if the neighbor matrix is a sparse matrix\n        if not scipy.sparse.issparse(neighbor_matrix):\n            raise ValueError(f\"Neighbor matrix for key '{self.neighbor_key}' is not a sparse matrix.\")\n\n        # First write indptr which gives us the structure - this is usually small enough to handle in one go\n        memmap_dir_path = Path(self.data_path)\n        with open(f\"{memmap_dir_path}/{FileNames.NEIGHBOR_INDICES_PTR.value}\", \"wb\") as indptr_file:\n            # Convert to hardcoded dtype before writing\n            indptr_converted = neighbor_matrix.indptr.astype(self.dtypes[f\"{FileNames.NEIGHBOR_INDICES_PTR.value}\"])\n            indptr_file.write(indptr_converted.tobytes())\n\n        # Get dimensions from indptr\n        num_rows = len(neighbor_matrix.indptr) - 1\n        # Process indices and data in chunks based on rows\n        with (\n            open(f\"{memmap_dir_path}/{FileNames.NEIGHBOR_INDICES.value}\", \"wb\") as indices_file,\n            open(f\"{memmap_dir_path}/{FileNames.NEIGHBOR_VALUES.value}\", \"wb\") as data_file,\n        ):\n            for row_start in range(0, num_rows, self.load_block_row_size):\n                row_end = min(row_start + self.load_block_row_size, num_rows)\n\n                # Get slice of the matrix for this chunk of rows\n                chunk = neighbor_matrix[row_start:row_end]\n\n                # Convert to hardcoded dtypes before writing\n                indices_converted = chunk.indices.astype(self.dtypes[f\"{FileNames.NEIGHBOR_INDICES.value}\"])\n                data_converted = chunk.data.astype(self.dtypes[f\"{FileNames.NEIGHBOR_VALUES.value}\"])\n\n                # Write chunk data to files\n                indices_file.write(indices_converted.tobytes())\n                data_file.write(data_converted.tobytes())\n\n                logger.info(f\"Processed neighbor data rows {row_start} to {row_end - 1}\")\n\n        # Then re-open as memory-mapped arrays with the final shapes\n        self._neighbor_indptr = np.memmap(\n            f\"{self.data_path}/{FileNames.NEIGHBOR_INDICES_PTR.value}\",\n            dtype=self.dtypes[f\"{FileNames.NEIGHBOR_INDICES_PTR.value}\"],\n            mode=Mode.READ_APPEND.value,\n            shape=(len(neighbor_matrix.indptr),),\n        )\n\n        self._neighbor_indices = np.memmap(\n            f\"{self.data_path}/{FileNames.NEIGHBOR_INDICES.value}\",\n            dtype=self.dtypes[f\"{FileNames.NEIGHBOR_INDICES.value}\"],\n            mode=Mode.READ_APPEND.value,\n            shape=(len(neighbor_matrix.indices),),\n        )\n\n        self._neighbor_data = np.memmap(\n            f\"{self.data_path}/{FileNames.NEIGHBOR_VALUES.value}\",\n            dtype=self.dtypes[f\"{FileNames.NEIGHBOR_VALUES.value}\"],\n            mode=Mode.READ_APPEND.value,\n            shape=(len(neighbor_matrix.data),),\n        )\n\n        logger.info(\"Neighbor data extracted to memory-mapped arrays using chunked approach\")\n        return True\n\n    def get_row(\n        self,\n        index: int,\n        return_var_features: bool = False,\n        var_feature_names: Optional[List[str]] = None,\n        return_obs_features: bool = False,\n        obs_feature_names: Optional[List[str]] = None,\n    ) -&gt; Tuple[Tuple[np.ndarray, np.ndarray], List[np.ndarray], List[np.ndarray]]:\n        \"\"\"Returns a given row in the dataset along with optional features.\n\n        Args:\n            index: The row to be returned. This is in the range of [0, num_rows)\n            return_var_features: boolean that indicates whether to return features\n            var_feature_names: Optional, variable feature names to extract\n            return_obs_features: boolean indicating whether to return observed (row) features\n            obs_feature_names: Optional, observed feature variables to extract\n        Return:\n            [Tuple[np.ndarray, np.ndarray]: data values and column pointes\n            List[np.ndarray]: optional, corresponding variable (column) features.\n            List[np.ndarray]: optional, corresponding observed (row) features.\n        \"\"\"\n        start = self.row_index[index]\n        end = self.row_index[index + 1]\n        values = self.data[start:end]\n        columns = self.col_index[start:end]\n        ret = (values, columns)\n        var_features = (\n            self._var_feature_index.lookup(index, select_features=var_feature_names)[0]\n            if return_var_features\n            else None\n        )\n        obs_features = (\n            self._obs_feature_index.lookup(index, select_features=obs_feature_names)[0]\n            if return_obs_features\n            else None\n        )\n        return ret, var_features, obs_features\n\n    def get_row_with_neighbor(\n        self,\n        index: int,\n        return_var_features: bool = False,\n        var_feature_names: Optional[List[str]] = None,\n        return_obs_features: bool = False,\n        obs_feature_names: Optional[List[str]] = None,\n    ) -&gt; Dict[str, Union[Tuple[np.ndarray, np.ndarray], int, Optional[List[np.ndarray]]]]:\n        \"\"\"Returns a given row in the dataset along with optional features and neighbor data.\n\n        Args:\n            index: The row to be returned. This is in the range of [0, num_rows)\n            return_var_features: Boolean that indicates whether to return variable features\n            var_feature_names: Optional, variable feature names to extract\n            return_obs_features: Boolean that indicates whether to return observed features\n            obs_feature_names: Optional, observed feature variables to extract\n\n        Returns:\n            Dict with keys:\n            - 'current_cell': Tuple[np.ndarray, np.ndarray] - (values, columns) for current cell\n            - 'next_cell': Tuple[np.ndarray, np.ndarray] - (values, columns) for neighbor cell\n            - 'current_cell_index': int - Index of current cell\n            - 'next_cell_index': int - Index of neighbor cell\n            - 'var_features': List[np.ndarray] - Variable features if return_features is True, else None\n            - 'obs_features': List[np.ndarray] - Observed features if return_obs_features is True, else None\n\n        Raises:\n            ValueError: If neighbor functionality is disabled or no neighbor data is available\n        \"\"\"\n        # Validate neighbor availability since this function requires neighbors\n        if not (self.load_neighbors and self._has_neighbors):\n            raise ValueError(\n                \"Cannot include neighbor data: neighbor functionality is disabled or no neighbor data available\"\n            )\n\n        # Get current cell data using the existing get_row function\n        current_cell_data, var_features, obs_features = self.get_row(\n            index, return_var_features, var_feature_names, return_obs_features, obs_feature_names\n        )\n\n        # Sample neighbor and get its data\n        neighbor_index = self.sample_neighbor_index(index)\n\n        # Case where neighbor is the same as current cell\n        if neighbor_index == index:\n            next_cell_data = current_cell_data\n        else:\n            # Get neighbor cell data using the get_row function\n            next_cell_data, _, _ = self.get_row(neighbor_index, False, None)\n\n        # Return all data in a dictionary format\n        return {\n            \"current_cell\": current_cell_data,\n            \"next_cell\": next_cell_data,\n            \"current_cell_index\": index,\n            \"next_cell_index\": neighbor_index,\n            \"var_features\": var_features,\n            \"obs_features\": obs_features,\n        }\n\n    def get_row_padded(\n        self,\n        index: int,\n        return_var_features: bool = False,\n        var_feature_names: Optional[List[str]] = None,\n        return_obs_features: bool = False,\n        obs_feature_names: Optional[List[str]] = None,\n    ) -&gt; Tuple[np.ndarray, List[np.ndarray], List[np.ndarray]]:\n        \"\"\"Returns a padded version of a row in the dataset.\n\n        A padded version is one where the a sparse array representation is\n        converted to a conventional represenentation. Optionally, features are\n        returned.\n\n        Args:\n            index: The row to be returned\n            return_var_features: boolean that indicates whether to return variable features\n            var_feature_names: Optional, variable feature names to extract\n            return_obs_features: Boolean that indicates whether to return observed features\n            obs_feature_names: Optional, observed feature variables to extract\n        Return:\n            np.ndarray: conventional row representation\n            List[np.ndarray]: optional, corresponding variable (column) features.\n            List[np.ndarray]: optional, corresponding observed (row) features.\n        \"\"\"\n        (row_values, row_column_pointer), var_features, obs_features = self.get_row(\n            index, return_var_features, var_feature_names, return_obs_features, obs_feature_names\n        )\n        return (\n            _pad_sparse_array(row_values, row_column_pointer, self._var_feature_index.number_vars_at_row(index)),\n            var_features,\n            obs_features,\n        )\n\n    def get_row_padded_with_neighbor(\n        self,\n        index: int,\n        return_var_features: bool = False,\n        var_feature_names: Optional[List[str]] = None,\n        return_obs_features: bool = False,\n        obs_feature_names: Optional[List[str]] = None,\n    ) -&gt; Dict[str, Union[np.ndarray, int, List[np.ndarray]]]:\n        \"\"\"Returns a padded version of a row with optional neighbor data.\n\n        A padded version converts sparse representation to a dense array where\n        missing values are filled with zeros.\n\n        Args:\n            index: The row to be returned\n            return_var_features: Boolean that indicates whether to return variable features\n            var_feature_names: Optional, variable feature names to extract\n            return_obs_features: Boolean that indicates whether to return observed features\n            obs_feature_names: Optional, observed feature variables to extract\n\n        Returns:\n            Dict with keys:\n            - 'current_cell': np.ndarray - Padded array for current cell\n            - 'next_cell': np.ndarray - Padded array for neighbor cell\n            - 'current_cell_index': int - Index of current cell\n            - 'next_cell_index': int - Index of neighbor cell\n            - 'features': List[np.ndarray] - Variable features if return_features is True, else None\n\n        Raises:\n            ValueError: If neighbor functionality is disabled or no neighbor data is available\n        \"\"\"\n        # Validate neighbor availability since this function requires neighbors\n        if not (self.load_neighbors and self._has_neighbors):\n            raise ValueError(\n                \"Cannot include neighbor data: neighbor functionality is disabled or no neighbor data available\"\n            )\n\n        # Get both current cell and neighbor data\n        result = self.get_row_with_neighbor(\n            index, return_var_features, var_feature_names, return_obs_features, obs_feature_names\n        )\n\n        # Get current cell padded array using get_row_padded\n        curr_padded, _, _ = self.get_row_padded(index, False, None, False, None)\n\n        # For neighbor, get the padded array\n        next_idx = result[\"next_cell_index\"]\n        if next_idx == index:\n            # If neighbor is the same as current cell, reuse the current padded array\n            next_padded = curr_padded\n        else:\n            # Otherwise get the neighbor's padded array\n            next_padded, _, _ = self.get_row_padded(next_idx, False, None)\n\n        # Return in dictionary format\n        return {\n            \"current_cell\": curr_padded,\n            \"next_cell\": next_padded,\n            \"current_cell_index\": result[\"current_cell_index\"],\n            \"next_cell_index\": result[\"next_cell_index\"],\n            \"var_features\": result[\"var_features\"],\n            \"obs_features\": result[\"obs_features\"],\n        }\n\n    def get_row_column(self, index: int, column: int, impute_missing_zeros: bool = True) -&gt; Optional[float]:\n        \"\"\"Returns the value at a given index and the corresponding column.\n\n        Args:\n            index: The index to be returned\n            column: The column to be returned\n            impute_missing_zeros: boolean that indicates whether to set missing\n            data to 0\n        Return:\n            A float that is the value in the array or None.\n        \"\"\"\n        (row_values, row_column_pointer), _, _ = self.get_row(index)\n        if column is not None:\n            for col_index, col in enumerate(row_column_pointer):\n                if col == column:\n                    # return the value at this position\n                    return row_values[col_index]\n                elif col &gt; column:\n                    try:\n                        raise ValueError(f\"Column pointer {col} is larger than the column {column}.\")\n                    except ValueError:\n                        break\n            return 0.0 if impute_missing_zeros else None\n\n    def var_features(self) -&gt; Optional[VariableFeatureIndex]:\n        \"\"\"Return the corresponding VariableFeatureIndex.\"\"\"\n        return self._var_feature_index\n\n    def obs_features(self) -&gt; Optional[ObservedFeatureIndex]:\n        \"\"\"Return the corresponding ObservedFeatureIndex.\"\"\"\n        return self._obs_feature_index\n\n    def _load_mmap_file_if_exists(self, file_path, dtype):\n        if os.path.exists(file_path):\n            return np.memmap(file_path, dtype=dtype, mode=self.mode.value)\n        else:\n            raise FileNotFoundError(f\"The mmap file at {file_path} is missing\")\n\n    def load(self, stored_path: str) -&gt; None:\n        \"\"\"Loads the data at store_path that is an np.memmap format.\n\n        Args:\n            stored_path: directory with np.memmap files\n        Raises:\n            FileNotFoundError if the corresponding directory or files are not\n            found, or if the metadata file is not present.\n        \"\"\"\n        if not os.path.exists(stored_path):\n            raise FileNotFoundError(\n                f\"\"\"Error: the specified data path to the mmap files {stored_path} does not exist.\n                                    Specify an updated filepath or provide an h5ad path to the dataset. The data can\n                                    be loaded with SingleCellMemMapDataset.load_h5ad. Alternatively, the class can be instantiated\n                                    with  SingleCellMemMapDataset(&lt;path to data that will be created&gt;, h5ad_path=&lt;path to h5ad file&gt;\"\"\"\n            )\n        self.data_path = stored_path\n        self.mode = Mode.READ_APPEND\n        # Load header if present; keep None if missing or unreadable\n        if os.path.exists(self.header_path):\n            try:\n                self.header = SCDLHeader.load(str(self.header_path))\n            except Exception as e:\n                warnings.warn(f\"Failed to load SCDL header at {self.header_path}: {e}\")\n                self.header = None\n        else:\n            warnings.warn(f\"SCDL header missing at {self.header_path}; continuing without header.\")\n            self.header = None\n        # If header is loaded, extract dtypes from header and set self.dtypes accordingly\n        if self.header is not None and hasattr(self.header, \"arrays\"):\n            # Map from FileNames.value to dtype string\n            for array_info in self.header.arrays:\n                if FileNames[array_info.name].value not in self.dtypes:\n                    raise ValueError(f\"Array name {FileNames[array_info.name].value} not found in dtypes\")\n                self.dtypes[FileNames[array_info.name].value] = array_info.dtype.numpy_dtype_string\n\n        # Metadata is required, so we must check if it exists and fail if not.\n        if not os.path.exists(f\"{self.data_path}/{FileNames.METADATA.value}\"):\n            raise FileNotFoundError(\n                f\"Error: the metadata file {self.data_path}/{FileNames.METADATA.value} does not exist.\"\n            )\n\n        with open(f\"{self.data_path}/{FileNames.METADATA.value}\", Mode.READ_APPEND.value) as mfi:\n            self.metadata = json.load(mfi)\n\n        if os.path.exists(f\"{self.data_path}/{FileNames.VAR_FEATURES.value}\"):\n            self._var_feature_index = VariableFeatureIndex.load(f\"{self.data_path}/{FileNames.VAR_FEATURES.value}\")\n        elif os.path.exists(\n            f\"{self.data_path}/{FileNames.FEATURES.value}\"\n        ):  # Backward compatibility with old features file\n            self._var_feature_index = VariableFeatureIndex.load(f\"{self.data_path}/{FileNames.FEATURES.value}\")\n        if os.path.exists(f\"{self.data_path}/{FileNames.OBS_FEATURES.value}\"):\n            self._obs_feature_index = ObservedFeatureIndex.load(f\"{self.data_path}/{FileNames.OBS_FEATURES.value}\")\n        # mmap the existing arrays\n        self.data = self._load_mmap_file_if_exists(\n            f\"{self.data_path}/{FileNames.DATA.value}\", self.dtypes[f\"{FileNames.DATA.value}\"]\n        )\n        self.row_index = self._load_mmap_file_if_exists(\n            f\"{self.data_path}/{FileNames.ROWPTR.value}\", dtype=self.dtypes[f\"{FileNames.ROWPTR.value}\"]\n        )\n        self.col_index = self._load_mmap_file_if_exists(\n            f\"{self.data_path}/{FileNames.COLPTR.value}\", dtype=self.dtypes[f\"{FileNames.COLPTR.value}\"]\n        )\n\n        # Load neighbor data\n        if self.load_neighbors:\n            self._load_neighbor_memmaps()\n\n    def _write_metadata(self) -&gt; None:\n        with open(f\"{self.data_path}/{FileNames.METADATA.value}\", f\"{Mode.CREATE.value}\") as mfi:\n            json.dump(self.metadata, mfi)\n\n    def _check_data_downcast(self, count_data: scipy.sparse.spmatrix, warning_prefix: str = \"Warning\") -&gt; None:\n        count_data_downcast = count_data.data.astype(self.dtypes[f\"{FileNames.DATA.value}\"])\n        if not np.allclose(count_data_downcast, count_data.data, rtol=0, atol=self.data_dtype_tolerance):\n            warnings.warn(\n                f\"{warning_prefix}: Downcasted data values for '{FileNames.DATA.value}' are not close to original values. \"\n                f\"Max abs diff: {np.max(np.abs(count_data_downcast - count_data.data))}\"\n            )\n        return count_data_downcast\n\n    def regular_load_h5ad(\n        self,\n        anndata_path: str,\n    ) -&gt; Tuple[pd.DataFrame, int]:\n        \"\"\"Method for loading an h5ad file into memorySu and converting it to the SCDL format.\n\n        Args:\n            anndata_path: location of data to load\n        Raises:\n            NotImplementedError if the data is not in scipy.sparse.spmatrix format\n            ValueError it there is not count data\n        Returns:\n            pd.DataFrame: var variables for features\n            int: number of rows in the dataframe.\n\n        \"\"\"\n        adata = ad.read_h5ad(anndata_path)  # slow\n        count_data = self._get_matrix_X(adata)\n        if not isinstance(count_data, scipy.sparse.spmatrix):\n            raise NotImplementedError(\"Error: dense matrix loading not yet implemented.\")\n\n        self._check_data_downcast(count_data, \"First 1000 rows of the dataset\")\n\n        # Check and load neighbor data\n        # NOTE: More clear to have a check here and not call _extract_neighbor_data() if there no neighbors\n        if self.load_neighbors:\n            self._has_neighbors = self._extract_neighbor_data(adata)\n\n        num_rows, num_cols = count_data.shape\n\n        num_elements_stored = count_data.nnz\n        # Currently, anndata is assumed to be sparse\n        self.dtypes[f\"{FileNames.ROWPTR.value}\"] = smallest_uint_dtype(num_elements_stored)\n        self.dtypes[f\"{FileNames.COLPTR.value}\"] = smallest_uint_dtype(num_cols - 1)\n        # Create the arrays.\n        self._init_arrs(num_elements_stored, num_rows)\n        # Store data\n        count_data_downcast = self._check_data_downcast(count_data, \"Full Dataset\")\n        self.data[0:num_elements_stored] = count_data_downcast\n        # Store the col idx array\n        self.col_index[0:num_elements_stored] = count_data.indices.astype(self.dtypes[f\"{FileNames.COLPTR.value}\"])\n\n        # Store the row idx array\n        self.row_index[0 : num_rows + 1] = count_data.indptr.astype(self.dtypes[f\"{FileNames.ROWPTR.value}\"])\n        vars = adata.var\n        obs = adata.obs\n        file_handle = getattr(adata, \"file\", None)\n        if file_handle is not None:\n            try:\n                file_handle.close()\n            except Exception:\n                pass\n        return vars, obs, num_rows\n\n    def _get_matrix_X(self, adata_obj: Optional[ad.AnnData] = None):\n        if self.use_X_not_raw:\n            if adata_obj.X is None:\n                raise ValueError(\n                    \"This file does not have count data; set use_X_not_raw=False to use raw counts instead.\"\n                )\n            return adata_obj.X\n        else:\n            if getattr(getattr(adata_obj, \"raw\", None), \"X\", None) is None:\n                raise ValueError(\n                    \"This file does not have raw count data; set use_X_not_raw=True to use normalized counts instead.\"\n                )\n            return adata_obj.raw.X\n\n    def paginated_load_h5ad(\n        self,\n        anndata_path: str,\n    ) -&gt; Tuple[pd.DataFrame, int]:\n        \"\"\"Method for block loading a larger h5ad file and converting it to the SCDL format.\n\n        This should be used in the case when the entire anndata file cannot be loaded into memory.\n        The anndata is loaded into memory load_block_row_size number of rows at a time. Each chunk\n        is converted into numpy memory maps which are then concatenated together.\n\n        Raises:\n            NotImplementedError if the data is not loaded in the CSRDataset format.\n\n        Returns:\n            pd.DataFrame: var variables for features\n            int: number of rows in the dataframe.\n        \"\"\"\n        adata = ad.read_h5ad(anndata_path, backed=True)\n        if self.load_neighbors:\n            self._has_neighbors = self._extract_neighbor_data_paginated(adata)\n        X_full = self._get_matrix_X(adata)\n        if not isinstance(X_full, ad.experimental.CSRDataset):\n            raise NotImplementedError(\"Error: dense matrix loading not yet implemented.\")\n\n        # Use slice-then-raw when sampling rows\n        count_data = self._get_matrix_X(adata[:1_000])\n\n        # Use full matrix for pointers and shapes\n        n_elements = X_full._indptr[-1]\n        row_index = X_full._indptr.astype(self.dtypes[f\"{FileNames.ROWPTR.value}\"])\n\n        self._check_data_downcast(count_data, \"First 1000 rows of the dataset\")\n        num_rows, num_cols = X_full.shape\n        self.dtypes[f\"{FileNames.COLPTR.value}\"] = smallest_uint_dtype(num_cols - 1)\n        self.dtypes[f\"{FileNames.ROWPTR.value}\"] = smallest_uint_dtype(n_elements)\n        # Read the row indices into a memory map.\n        mode = Mode.CREATE_APPEND\n        self.row_index = _create_row_memmaps(num_rows, Path(self.data_path), mode, self.dtypes)\n        self.row_index[:] = row_index\n\n        # The data from each column and data chunk of the original anndata file is read in. This is saved into the final\n        # location of the memmap file. In this step, it is saved in the binary file format.\n        memmap_dir_path = Path(self.data_path)\n        with (\n            open(f\"{memmap_dir_path}/{FileNames.COLPTR.value}\", \"wb\") as col_file,\n            open(f\"{memmap_dir_path}/{FileNames.DATA.value}\", \"wb\") as data_file,\n        ):\n            for row_start in range(0, num_rows, self.load_block_row_size):\n                adata_block = adata[row_start : row_start + self.load_block_row_size]\n                adata_block_X = self._get_matrix_X(adata_block)\n                # Write each array's data to the file in binary format\n                col_block = adata_block_X.indices.astype(self.dtypes[f\"{FileNames.COLPTR.value}\"])\n                col_file.write(col_block.tobytes())\n                count_data_downcast = self._check_data_downcast(\n                    adata_block_X, f\"Rows {row_start} to {row_start + self.load_block_row_size - 1} of the dataset\"\n                )\n\n                data_file.write(count_data_downcast.tobytes())\n\n        # The column and data files are re-opened as memory-mapped arrays with the final shape\n        mode = Mode.READ_APPEND\n        self.col_index = np.memmap(\n            f\"{memmap_dir_path}/{FileNames.COLPTR.value}\",\n            self.dtypes[f\"{FileNames.COLPTR.value}\"],\n            mode=mode,\n            shape=(n_elements,),\n        )\n        self.data = np.memmap(\n            f\"{memmap_dir_path}/{FileNames.DATA.value}\",\n            dtype=self.dtypes[f\"{FileNames.DATA.value}\"],\n            mode=mode,\n            shape=(n_elements,),\n        )\n        vars = adata.var\n        obs = adata.obs\n        file_handle = getattr(adata, \"file\", None)\n        if file_handle is not None:\n            try:\n                file_handle.close()\n            except Exception:\n                pass\n\n        return vars, obs, num_rows\n\n    def _load_neighbor_memmaps(self):\n        try:\n            # mmap the existing arrays\n            self._neighbor_indices = self._load_mmap_file_if_exists(\n                f\"{self.data_path}/{FileNames.NEIGHBOR_INDICES.value}\",\n                self.dtypes[f\"{FileNames.NEIGHBOR_INDICES.value}\"],\n            )\n            self._neighbor_indptr = self._load_mmap_file_if_exists(\n                f\"{self.data_path}/{FileNames.NEIGHBOR_INDICES_PTR.value}\",\n                self.dtypes[f\"{FileNames.NEIGHBOR_INDICES_PTR.value}\"],\n            )\n            self._neighbor_data = self._load_mmap_file_if_exists(\n                f\"{self.data_path}/{FileNames.NEIGHBOR_VALUES.value}\",\n                self.dtypes[f\"{FileNames.NEIGHBOR_VALUES.value}\"],\n            )\n\n            self._has_neighbors = True\n\n        except FileNotFoundError:\n            # Neighbor files don't exist - this is OK if load_neighbors=False\n            # or if dataset was created without neighbors\n            self._has_neighbors = False\n            if self.load_neighbors:\n                warnings.warn(\"Neighbor loading was requested but neighbor files are missing\")\n\n    def load_h5ad(\n        self,\n        anndata_path: str,\n    ) -&gt; None:\n        \"\"\"Loads an existing AnnData archive from disk.\n\n        This creates a new backing data structure which is saved.\n        Note: the storage utilized will roughly double. Currently, the data must\n        be in a scipy.sparse.spmatrix format.\n\n        Args:\n            anndata_path: location of data to load\n        Raises:\n            FileNotFoundError if the data path does not exist.\n            NotImplementedError if the data is not in scipy.sparse.spmatrix\n            format\n            ValueError it there is not count data\n        \"\"\"\n        if not os.path.exists(anndata_path):\n            raise FileNotFoundError(f\"Error: could not find h5ad path {anndata_path}\")\n        file_size_MB = os.path.getsize(anndata_path) / (1_024**2)\n\n        if file_size_MB &lt; self.paginated_load_cutoff:\n            var_features_df, obs_features_df, num_rows = self.regular_load_h5ad(anndata_path)\n        else:\n            var_features_df, obs_features_df, num_rows = self.paginated_load_h5ad(anndata_path)\n\n        var_features = _extract_features(var_features_df, self.var_feature_index_name)\n        obs_features = _extract_features(obs_features_df, self.obs_feature_index_name)\n        self._var_feature_index.append_features(n_obs=num_rows, features=var_features, label=anndata_path)\n        self._obs_feature_index.append_features(features=obs_features, label=anndata_path)\n        self.save()\n\n    def _write_header(self):\n        ## Write the SCDL header.\n        arrays: List[ArrayInfo] = []\n        # Use FileNames enums directly to ensure correct dtype lookup\n        for fname, matrix in [\n            (FileNames.DATA, self.data),\n            (FileNames.ROWPTR, self.row_index),\n            (FileNames.COLPTR, self.col_index),\n        ]:\n            # Convert numpy dtype to ArrayDType enum, defaulting reasonably on failures\n            dtype_value = self.dtypes.get(fname.value, self.dtypes[FileNames.DATA.value])\n            try:\n                array_dtype = ArrayDType.from_numpy_dtype(dtype_value)\n            except ValueError:\n                array_dtype = ArrayDType.FLOAT32_ARRAY\n\n            info = ArrayInfo(\n                fname.name,\n                len(matrix),\n                array_dtype,\n                None,\n            )\n            arrays.append(info)\n\n        # Populate FeatureIndexInfo entries for the feature index directory\n        indices: List[FeatureIndexInfo] = []\n        for feature_index, feature_index_path in [\n            (self._var_feature_index, FileNames.VAR_FEATURES.value),\n            (self._obs_feature_index, FileNames.OBS_FEATURES.value),\n        ]:\n            # If feature index is None, it is not recorded in the header\n            if feature_index is None:\n                continue\n\n            try:\n                num_frames = len(feature_index)\n                num_rows = feature_index.number_of_rows()\n            except Exception as e:\n                warnings.warn(f\"Unable to determine length or number_of_rows of feature index: {e}\")\n                continue\n\n            feature_array_dtype = ArrayDType.STRING_ARRAY\n            features_rel_path = f\"{feature_index_path}\"\n            index_files: List[str] = [\n                f\"{features_rel_path}/cumulative_sum_index.npy\",\n                f\"{features_rel_path}/labels.npy\",\n                f\"{features_rel_path}/version.npy\",\n            ]\n            if num_frames &gt; 0:\n                num_digits = len(str(num_frames))\n                for i in range(num_frames):\n                    index_files.append(f\"{features_rel_path}/dataframe_{i:0{num_digits}d}.parquet\")\n\n            fi_info = FeatureIndexInfo(\n                name=feature_index_path,\n                length=num_rows,\n                dtype=feature_array_dtype,\n                index_files=index_files,\n                shape=None,\n            )\n            indices.append(fi_info)\n\n        header = (\n            self.header\n            if self.header is not None\n            else SCDLHeader(\n                CurrentSCDLVersion(),\n                Backend.MEMMAP_V0,\n                arrays,\n                indices,\n            )\n        )\n        header.save(self.header_path)\n\n    def save(self, output_path: Optional[str] = None) -&gt; None:\n        \"\"\"Saves the class to a given output path.\n\n        Args:\n            output_path: The location to save - not yet implemented and should\n            be self.data_path\n\n        Raises:\n           NotImplementedError if output_path is not None.\n        \"\"\"\n        self._write_header()\n        if \"num_rows\" not in self.metadata:\n            self.metadata[\"num_rows\"] = self.number_of_rows()\n\n        self._write_metadata()\n        # Write the var and obs feature index. This may not exist.\n        self._var_feature_index.save(f\"{self.data_path}/{FileNames.VAR_FEATURES.value}\")\n        self._obs_feature_index.save(f\"{self.data_path}/{FileNames.OBS_FEATURES.value}\")\n        # Ensure the object is in a valid state. These are saved at creation!\n        for postfix in [\n            f\"{FileNames.VERSION.value}\",\n            f\"{FileNames.DATA.value}\",\n            f\"{FileNames.COLPTR.value}\",\n            f\"{FileNames.ROWPTR.value}\",\n            f\"{FileNames.VAR_FEATURES.value}\",\n            f\"{FileNames.OBS_FEATURES.value}\",\n        ]:\n            if not os.path.exists(f\"{self.data_path}/{postfix}\"):\n                raise FileNotFoundError(f\"This file should exist from object creation: {self.data_path}/{postfix}\")\n\n        self.data.flush()  # NOTE: saves the data to disk, do the approach for neighbor data\n        self.row_index.flush()\n        self.col_index.flush()\n\n        # Flush neighbor data to disk if it exists\n        if self._has_neighbors and self._neighbor_indptr is not None:\n            self._neighbor_indptr.flush()\n            self._neighbor_indices.flush()\n            self._neighbor_data.flush()\n\n        if output_path is not None:\n            raise NotImplementedError(\"Saving to separate path is not yet implemented.\")\n\n        return True\n\n    def get_neighbor_indices_for_cell(self, cell_index: int) -&gt; np.ndarray:\n        \"\"\"Returns the array of neighbor indices for a given cell.\n\n        Args:\n            cell_index: Index of the cell to get neighbors for\n\n        Returns:\n            np.ndarray: Array of neighbor indices, empty if no neighbors or neighbor data unavailable\n\n        Raises:\n            IndexError: If cell_index is out of bounds\n            ValueError: If neighbor functionality was explicitly enabled but data is unavailable\n        \"\"\"\n        if not (0 &lt;= cell_index &lt; self.number_of_rows()):\n            raise IndexError(f\"Cell index {cell_index} out of bounds for dataset with {self.number_of_rows()} cells\")\n\n        # Check if neighbor functionality was requested but is unavailable\n        if self.load_neighbors and not self._has_neighbors:\n            raise ValueError(\"Neighbor functionality was enabled but no neighbor data is available\")\n\n        if not self.load_neighbors or not self._has_neighbors or self._neighbor_indptr is None:\n            return np.array([], dtype=int)  # Return empty array if neighbor data not available\n\n        # Get neighbor indices using CSR format indptr and indices\n        start = self._neighbor_indptr[cell_index]\n        end = self._neighbor_indptr[cell_index + 1]\n        return self._neighbor_indices[start:end]\n\n    def get_neighbor_weights_for_cell(self, cell_index: int) -&gt; np.ndarray:\n        \"\"\"Returns the array of neighbor weights (e.g., pseudotime differences) for a given cell.\n\n        Args:\n            cell_index: Index of the cell to get neighbor weights for\n\n        Returns:\n            np.ndarray: Array of weights corresponding to neighbors, empty if no neighbors\n\n        Raises:\n            IndexError: If cell_index is out of bounds\n        \"\"\"\n        # Check if neighbor functionality was requested but is unavailable\n        if self.load_neighbors and not self._has_neighbors:\n            raise ValueError(\"Neighbor functionality was enabled but no neighbor data is available\")\n\n        if (\n            not self.load_neighbors\n            or not self._has_neighbors\n            or self._neighbor_indptr is None\n            or self._neighbor_data is None\n        ):\n            return np.array([], dtype=float)\n\n        if not (0 &lt;= cell_index &lt; self.number_of_rows()):\n            raise IndexError(f\"Cell index {cell_index} out of bounds for dataset with {self.number_of_rows()} cells\")\n\n        # Get neighbor weights using CSR format indptr and data\n        start = self._neighbor_indptr[cell_index]\n        end = self._neighbor_indptr[cell_index + 1]\n        return self._neighbor_data[start:end]\n\n    def sample_neighbor_index(self, cell_index: int) -&gt; int:\n        \"\"\"Samples a neighbor index for the given cell based on the configured sampling strategy.\n\n        Args:\n            cell_index: Index of the cell to sample a neighbor for\n\n        Returns:\n            int: Index of the sampled neighbor\n                 If no neighbors exist and fallback_to_identity is True, returns cell_index\n\n        Raises:\n            ValueError: If an unsupported sampling strategy is specified\n            IndexError: If cell_index is out of bounds\n        \"\"\"\n        # Basic validation\n        if not (0 &lt;= cell_index &lt; self.number_of_rows()):\n            raise IndexError(f\"Cell index {cell_index} out of bounds for dataset with {self.number_of_rows()} cells\")\n\n        # Check if neighbor functionality was requested but is unavailable\n        if self.load_neighbors and not self._has_neighbors:\n            raise ValueError(\"Neighbor functionality was enabled but no neighbor data is available\")\n\n        # Skip sampling if neighbor functionality is disabled\n        if not self.load_neighbors:\n            return cell_index  # Always return self as neighbor when neighbors disabled\n\n        # Get the neighbor indices for this cell\n        neighbor_indices = self.get_neighbor_indices_for_cell(cell_index)\n\n        # If no neighbors found, handle according to fallback policy\n        if len(neighbor_indices) == 0:\n            if self.fallback_to_identity:\n                return cell_index  # Return the cell itself\n            else:\n                # NOTE: implement fallback policy here if needed\n                warnings.warn(\n                    f\"Cell {cell_index} has no neighbors and fallback_to_identity=False. \"\n                    f\"Returning cell index itself anyway.\"\n                )\n                return cell_index  # Currently always return self if no neighbors\n\n        # Sample neighbor based on strategy\n        if self.neighbor_sampling_strategy == NeighborSamplingStrategy.RANDOM:\n            # Simple random sampling with equal probability\n            chosen_index = np.random.choice(neighbor_indices)\n            return chosen_index\n        elif self.neighbor_sampling_strategy == NeighborSamplingStrategy.FIRST:\n            # First neighbor sampling\n            return neighbor_indices[0]\n        # NOTE: Future - Add weighted sampling strategy\n        else:\n            raise ValueError(f\"Unsupported neighbor sampling strategy: {self.neighbor_sampling_strategy}\")\n\n    def get_neighbor_stats(self) -&gt; dict:\n        \"\"\"Returns statistics about the neighbors in the dataset.\n\n        Returns:\n            dict: Dictionary with neighbor statistics:\n                - has_neighbors: Whether dataset has neighbor data\n                - total_connections: Total number of neighbor relationships\n                - min_neighbors_per_cell: Minimum number of neighbors any cell has\n                - max_neighbors_per_cell: Maximum number of neighbors any cell has\n                - avg_neighbors_per_cell: Average number of neighbors per cell\n                - cells_with_no_neighbors: Count of cells that have no neighbors\n        \"\"\"\n        if not self._has_neighbors or self._neighbor_indptr is None or self._neighbor_indices is None:\n            return {\"has_neighbors\": False}\n\n        # Calculate stats based on CSR indptr (difference between consecutive elements)\n        neighbor_counts = np.diff(self._neighbor_indptr)\n\n        return {\n            \"has_neighbors\": True,\n            \"total_connections\": len(self._neighbor_indices),\n            \"min_neighbors_per_cell\": int(np.min(neighbor_counts)),\n            \"max_neighbors_per_cell\": int(np.max(neighbor_counts)),\n            \"avg_neighbors_per_cell\": float(np.mean(neighbor_counts)),\n            \"cells_with_no_neighbors\": int(np.sum(neighbor_counts == 0)),\n        }\n\n    def number_of_values(self) -&gt; int:\n        \"\"\"Get the total number of values in the array.\n\n        For each index, the length of the corresponding np.ndarray of features is counted.\n\n        Returns:\n            The sum of lengths of the features in every row\n        \"\"\"\n        return sum(self._var_feature_index.number_of_values())\n\n    def number_of_rows(self) -&gt; int:\n        \"\"\"The number of rows in the dataset.\n\n        Returns:\n            The number of rows in the dataset\n        Raises:\n            ValueError if the length of the number of rows in the feature\n            index does not correspond to the number of stored rows.\n        \"\"\"\n        if len(self._var_feature_index) &gt; 0 and self._var_feature_index.number_of_rows() != self.row_index.size - 1:\n            raise ValueError(\n                f\"\"\"The number of rows in the feature index {self._var_feature_index.number_of_rows()}\n                             does not correspond to the number of rows in the row_index {self.row_index.size - 1}\"\"\"\n            )\n        return self._var_feature_index.number_of_rows()\n\n    def number_nonzero_values(self) -&gt; int:\n        \"\"\"Number of non zero entries in the dataset.\"\"\"\n        return self.data.size\n\n    def __len__(self):\n        \"\"\"Return the number of rows.\"\"\"\n        return self.number_of_rows()\n\n    def __getitem__(self, idx: int) -&gt; torch.Tensor:\n        \"\"\"Get the row values located and index idx.\"\"\"\n        return torch.from_numpy(np.stack(self.get_row(idx)[0]))\n\n    def number_of_variables(self) -&gt; List[int]:\n        \"\"\"Get the number of features in every entry in the dataset.\n\n        Returns:\n            A list containing the lengths of the features in every row\n        \"\"\"\n        feats = self._var_feature_index\n        if len(feats) == 0:\n            return [0]\n        num_vars = feats.column_dims()\n        return num_vars\n\n    def shape(self) -&gt; Tuple[int, List[int]]:\n        \"\"\"Get the shape of the dataset.\n\n        This is the number of entries by the the length of the feature index\n        corresponding to that variable.\n\n        Returns:\n            The number of elements in the dataset\n            A list containing the number of variables for each row.\n        \"\"\"\n        return self.number_of_rows(), self.number_of_variables()\n\n    def _verify_concat_compatibility_and_types(\n        self, other_dataset: Union[list[\"SingleCellMemMapDataset\"], \"SingleCellMemMapDataset\"]\n    ) -&gt; None:\n        cumulative_elements = self.number_nonzero_values()\n        column_dtypes = [self.dtypes[FileNames.COLPTR.value]]\n        data_dtypes = [self.dtypes[FileNames.DATA.value]]\n\n        for dataset in other_dataset:\n            if self.version() != dataset.version():\n                raise ValueError(\n                    f\"\"\"Incompatable versions: input version: {dataset.version()},\n            this version:  {self.version}\"\"\"\n                )\n            column_dtypes.append(str(dataset.dtypes[FileNames.COLPTR.value]))\n            data_dtypes.append(str(dataset.dtypes[FileNames.DATA.value]))\n            cumulative_elements += dataset.number_nonzero_values()\n\n        if not (\n            all(np.dtype(dt).name in FLOAT_ORDER for dt in data_dtypes)\n            or all(np.dtype(dt).name in INT_ORDER for dt in data_dtypes)\n        ):\n            float_file_names = []\n            int_file_names = []\n            for dt, name in zip(\n                data_dtypes, [self.data_path.name] + [dataset.data_path.name for dataset in other_dataset]\n            ):\n                dtype_name = np.dtype(dt).name\n                if dtype_name in FLOAT_ORDER:\n                    float_file_names.append(name)\n                elif dtype_name in INT_ORDER:\n                    int_file_names.append(name)\n\n            raise ValueError(f\"\"\"Cannot merge datasets with a mix of int and float dtypes for data: {data_dtypes};\n            Float Data datasets: {\", \".join(float_file_names)};\n            Int Data datasets: {\", \".join(int_file_names)}\n            Cast all of the datasets to either int dtypes or to float dtypes before concatenation.\n            For example for a dataset with data_dtype \"uint8\", you can cast it to \"float32\" with:\n            ds = load(data_set_path)\n            ds.cast_data_to_dtype(\"float32\")\n            This will allow downscaling of the data dtype so it is advisable to examine the data if downcasting.\n            \"\"\")\n\n        new_dtypes = {\n            FileNames.COLPTR.value: determine_dtype(column_dtypes),\n            FileNames.DATA.value: determine_dtype(data_dtypes),\n            FileNames.ROWPTR.value: smallest_uint_dtype(cumulative_elements),\n        }\n        return new_dtypes\n\n    def _convert_dataset_to_new_dtypes(\n        self, new_dtypes: Dict[str, str], extend_copy_size: int = 10 * 1_024 * 1_024, allow_downscaling: bool = False\n    ) -&gt; None:\n        # If any dtype is changing, convert the file in-place to the new dtype using extend_files.\n        # This ensures that after this block, self.dtypes and the on-disk files are updated to the new dtype.\n        for key in [FileNames.COLPTR.value, FileNames.DATA.value, FileNames.ROWPTR.value]:\n            if key not in new_dtypes:\n                continue\n            current_dtype = self.dtypes[key]\n            target_dtype = new_dtypes[key]\n            if current_dtype != target_dtype:\n                # Convert the file in-place to the new dtype using a temporary file\n                src_file = f\"{self.data_path}/{key}\"\n                tmp_file = f\"{self.data_path}/{key}.tmp\"\n                # Move the original file to tmp_file\n                os.rename(src_file, tmp_file)\n                # Create an empty destination file for extend_files to append into\n                open(src_file, \"wb\").close()\n                # Use extend_files to convert from tmp_file to src_file (now as the new dtype)\n                extend_files(\n                    src_file,\n                    tmp_file,\n                    elements_per_chunk=extend_copy_size,\n                    delete_file2_on_complete=True,\n                    source_dtype=current_dtype,\n                    dest_dtype=target_dtype,\n                    allow_downscaling=allow_downscaling,\n                )\n                # Update dtype in self.dtypes\n                self.dtypes[key] = target_dtype\n\n    def concat(\n        self,\n        other_dataset: Union[list[\"SingleCellMemMapDataset\"], \"SingleCellMemMapDataset\"],\n        extend_copy_size: int = 10 * 1_024 * 1_024,\n        output_path: str | None = None,\n        destroy_on_copy: bool = False,\n    ) -&gt; None:\n        \"\"\"Concatenates one or a list of SingleCellMemMapDatasest to the existing one.\n\n        The data is stored in the same place as for the original data set or at output_path\n        if it is set. Then, at output_path or at self.data_path, there would be a saved\n        SingleCellMemmpDataset, which can be read in with SingleCellMemmpDataset(output_path).\n\n        Args:\n            other_dataset: A SingleCellMemMapDataset or a list of\n            SingleCellMemMapDatasets\n            extend_copy_size: how much to copy in memory at once\n            output_path: location to store new dataset\n            destroy_on_copy: Whether to remove the current data_path\n\n        Raises:\n           ValueError if the other dataset(s) are not of the same version or\n           something of another type is passed in.\n        \"\"\"\n        match other_dataset:\n            case self.__class__():\n                other_dataset = [other_dataset]\n            case list():\n                pass\n            case _:\n                raise ValueError(\n                    f\"Expecting either a {SingleCellMemMapDataset} or a list thereof. Actually got: {type(other_dataset)}\"\n                )\n\n        # Verify the other dataset or datasets are of the same type.\n        new_dtypes = self._verify_concat_compatibility_and_types(other_dataset)\n\n        # Set our mode:\n        self.mode: Mode = Mode.READ_APPEND\n        if output_path is not None:\n            if destroy_on_copy:\n                shutil.move(self.data_path, output_path)\n            else:\n                shutil.copytree(self.data_path, output_path)\n            self.data_path = output_path\n\n        self._convert_dataset_to_new_dtypes(new_dtypes)\n        # Copy the data from self and other into the new arrays.\n        element_counter = self.number_nonzero_values()\n        row_counter = self.number_of_rows()\n        for mmap in other_dataset:\n            extend_files(\n                f\"{self.data_path}/{FileNames.ROWPTR.value}\",\n                f\"{mmap.data_path}/{FileNames.ROWPTR.value}\",\n                elements_per_chunk=extend_copy_size,\n                delete_file2_on_complete=destroy_on_copy,\n                offset=np.dtype(mmap.dtypes[f\"{FileNames.ROWPTR.value}\"]).itemsize,\n                source_dtype=mmap.dtypes[f\"{FileNames.ROWPTR.value}\"],\n                dest_dtype=new_dtypes[f\"{FileNames.ROWPTR.value}\"],\n                add_value=element_counter,\n            )\n\n            extend_files(\n                f\"{self.data_path}/{FileNames.DATA.value}\",\n                f\"{mmap.data_path}/{FileNames.DATA.value}\",\n                elements_per_chunk=extend_copy_size,\n                delete_file2_on_complete=destroy_on_copy,\n                source_dtype=mmap.dtypes[f\"{FileNames.DATA.value}\"],\n                dest_dtype=new_dtypes[f\"{FileNames.DATA.value}\"],\n            )\n            extend_files(\n                f\"{self.data_path}/{FileNames.COLPTR.value}\",\n                f\"{mmap.data_path}/{FileNames.COLPTR.value}\",\n                elements_per_chunk=extend_copy_size,\n                delete_file2_on_complete=destroy_on_copy,\n                source_dtype=mmap.dtypes[f\"{FileNames.COLPTR.value}\"],\n                dest_dtype=new_dtypes[f\"{FileNames.COLPTR.value}\"],\n            )\n            self._var_feature_index.concat(mmap._var_feature_index)\n            self._obs_feature_index.concat(mmap._obs_feature_index)\n            # Update counters\n            element_counter += mmap.number_nonzero_values()\n            row_counter += mmap.number_of_rows()\n\n        # Reopen the data, colptr, and rowptr arrays\n        self.data = np.memmap(\n            f\"{self.data_path}/{FileNames.DATA.value}\",\n            dtype=self.dtypes[f\"{FileNames.DATA.value}\"],\n            shape=(element_counter,),\n            mode=Mode.READ_APPEND.value,\n        )\n        self.row_index = np.memmap(\n            f\"{self.data_path}/{FileNames.ROWPTR.value}\",\n            dtype=self.dtypes[f\"{FileNames.ROWPTR.value}\"],\n            shape=(row_counter + 1,),\n            mode=Mode.READ_APPEND.value,\n        )\n        self.col_index = np.memmap(\n            f\"{self.data_path}/{FileNames.COLPTR.value}\",\n            dtype=self.dtypes[f\"{FileNames.COLPTR.value}\"],\n            shape=(element_counter,),\n            mode=Mode.READ_APPEND.value,\n        )\n        self.save()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.header_path","title":"<code>header_path</code>  <code>property</code>","text":"<p>Returns the full path to the header file in the archive.</p> Example <p>ds = SingleCellMemMapDataset(data_path=\"my_data\") ds.header_path 'my_data/scdl_header.json'</p>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Get the row values located and index idx.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; torch.Tensor:\n    \"\"\"Get the row values located and index idx.\"\"\"\n    return torch.from_numpy(np.stack(self.get_row(idx)[0]))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.__init__","title":"<code>__init__(data_path, h5ad_path=None, num_elements=None, num_rows=None, mode=Mode.READ_APPEND, paginated_load_cutoff=10000, load_block_row_size=1000000, var_feature_index_name='var_feature_id', obs_feature_index_name='obs_feature_id', load_neighbors=False, neighbor_key='next_cell_ids', neighbor_sampling_strategy=NeighborSamplingStrategy.RANDOM, fallback_to_identity=True, data_dtype=None, data_dtype_tolerance=1e-08, use_X_not_raw=False)</code>","text":"<p>Instantiate the class.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>The location where the data np.memmap files are read from</p> required <code>h5ad_path</code> <code>Optional[str]</code> <p>Optional, the location of the h5_ad path.</p> <code>None</code> <code>num_elements</code> <code>Optional[int]</code> <p>The total number of elements in the array.</p> <code>None</code> <code>num_rows</code> <code>Optional[int]</code> <p>The number of rows in the data frame.</p> <code>None</code> <code>mode</code> <code>Mode</code> <p>Whether to read or write from the data_path.</p> <code>READ_APPEND</code> <code>paginated_load_cutoff</code> <code>int</code> <p>MB size on disk at which to load the h5ad structure with paginated load.</p> <code>10000</code> <code>load_block_row_size</code> <code>int</code> <p>Number of rows to load into memory with paginated load</p> <code>1000000</code> <code>var_feature_index_name</code> <p>The name of the features if the features are only stored in features_df.index.values</p> <code>'var_feature_id'</code> <code>obs_feature_index_name</code> <p>The name of the obs features if the features are only stored in features_df.index.values</p> <code>'obs_feature_id'</code> <code>load_neighbors</code> <code>bool</code> <p>Boolean to control to control whether to load and utilize neighbor information</p> <code>False</code> <code>neighbor_key</code> <code>str</code> <p>The key in AnnData's .obsp containing neighbor information.</p> <code>'next_cell_ids'</code> <code>neighbor_sampling_strategy</code> <code>str</code> <p>Sampling strategy for neighbors.</p> <code>RANDOM</code> <code>fallback_to_identity</code> <code>bool</code> <p>If a cell has no neighbors, whether to use the cell itself as its neighbor.</p> <code>True</code> <code>data_dtype</code> <code>str | None</code> <p>Desired dtype for <code>data.npy</code> when creating new datasets; if None, defaults to 'float32'. Must be one of 'uint8','uint16','uint32','uint64','float16','float32','float64'.</p> <code>None</code> <code>data_dtype_tolerance</code> <code>float</code> <p>Tolerance for data type conversion. Defaults to 1e-08.</p> <code>1e-08</code> <code>use_X_not_raw</code> <code>bool</code> <p>If True, use .X instead of .raw.X for the data.</p> <code>False</code> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    h5ad_path: Optional[str] = None,\n    num_elements: Optional[int] = None,\n    num_rows: Optional[int] = None,\n    mode: Mode = Mode.READ_APPEND,\n    paginated_load_cutoff: int = 10_000,\n    load_block_row_size: int = 1_000_000,\n    var_feature_index_name=\"var_feature_id\",\n    obs_feature_index_name=\"obs_feature_id\",\n    # --- Neighbor Args ---\n    load_neighbors: bool = False,\n    neighbor_key: str = \"next_cell_ids\",\n    neighbor_sampling_strategy: str = NeighborSamplingStrategy.RANDOM,\n    fallback_to_identity: bool = True,\n    data_dtype: Optional[str] = None,  # Must be one of INT_ORDER or FLOAT_ORDER in scdl_constants\n    data_dtype_tolerance: float = 1e-08,\n    use_X_not_raw: bool = False,  # If True, use .X instead of .raw.X for the data\n) -&gt; None:\n    \"\"\"Instantiate the class.\n\n    Args:\n        data_path: The location where the data np.memmap files are read from\n        or stored.\n        h5ad_path: Optional, the location of the h5_ad path.\n        num_elements: The total number of elements in the array.\n        num_rows: The number of rows in the data frame.\n        mode: Whether to read or write from the data_path.\n        paginated_load_cutoff: MB size on disk at which to load the h5ad structure with paginated load.\n        load_block_row_size: Number of rows to load into memory with paginated load\n        var_feature_index_name: The name of the features if the features are only stored in features_df.index.values\n        obs_feature_index_name: The name of the obs features if the features are only stored in features_df.index.values\n        # --- New Neighbor Args ---\n        load_neighbors (bool, optional): Boolean to control to control whether to load and utilize neighbor information\n        neighbor_key (str, optional): The key in AnnData's .obsp containing neighbor information.\n        neighbor_sampling_strategy (str, optional): Sampling strategy for neighbors.\n        fallback_to_identity (bool, optional): If a cell has no neighbors, whether to use the cell itself as its neighbor.\n        data_dtype (str | None, optional): Desired dtype for `data.npy` when creating\n            new datasets; if None, defaults to 'float32'. Must be one of\n            'uint8','uint16','uint32','uint64','float16','float32','float64'.\n        data_dtype_tolerance (float, optional): Tolerance for data type conversion. Defaults to 1e-08.\n        use_X_not_raw (bool, optional): If True, use .X instead of .raw.X for the data.\n    \"\"\"\n    self._version: str = importlib.metadata.version(\"bionemo.scdl\")\n    self.data_path: str = data_path\n    self.header: SCDLHeader = None\n    self.mode: Mode = mode\n    self.paginated_load_cutoff = paginated_load_cutoff\n    self.load_block_row_size = load_block_row_size\n    self.var_feature_index_name = var_feature_index_name\n    self.obs_feature_index_name = obs_feature_index_name\n    self.use_X_not_raw = use_X_not_raw\n    # Backing arrays\n    self.data: Optional[np.ndarray] = None\n    self.row_index: Optional[np.ndarray] = None\n    # Metadata and attributes\n    self.metadata: Dict[str, int] = {}\n\n    # Stores the Feature Index, which tracks\n    # the original AnnData features (e.g., gene names)\n    # and allows us to store ragged arrays in our SCMMAP structure.\n    self._var_feature_index: VariableFeatureIndex = VariableFeatureIndex()\n    self._obs_feature_index: ObservedFeatureIndex = ObservedFeatureIndex()\n    allowed_dtypes = list(INT_ORDER + FLOAT_ORDER)\n    if data_dtype is not None and data_dtype not in allowed_dtypes:\n        raise ValueError(f\"Invalid data_dtype '{data_dtype}'. Must be one of: {', '.join(allowed_dtypes)}\")\n    # Variables for int packing / reduced precision\n    self.dtypes: Dict[FileNames, str] = {\n        f\"{FileNames.DATA.value}\": \"float32\" if data_dtype is None else data_dtype,\n        f\"{FileNames.COLPTR.value}\": \"uint32\",\n        f\"{FileNames.ROWPTR.value}\": \"uint64\",\n        f\"{FileNames.NEIGHBOR_INDICES.value}\": \"uint32\",\n        f\"{FileNames.NEIGHBOR_INDICES_PTR.value}\": \"uint64\",\n        f\"{FileNames.NEIGHBOR_VALUES.value}\": \"float32\",\n    }\n    self.data_dtype_tolerance = data_dtype_tolerance\n    # Neighbor configuration\n    self.load_neighbors = load_neighbors\n    self._has_neighbors = False\n    if load_neighbors:\n        self._init_neighbor_args(neighbor_key, neighbor_sampling_strategy, fallback_to_identity)\n\n    if mode == Mode.CREATE_APPEND and os.path.exists(data_path):\n        raise FileExistsError(f\"Output directory already exists: {data_path}\")\n\n    if h5ad_path is not None and (data_path is not None and os.path.exists(data_path)):\n        raise FileExistsError(\n            \"Invalid input; both an existing SCMMAP and an h5ad file were passed. \"\n            \"Please pass either an existing SCMMAP or an h5ad file.\"\n        )\n\n    # If there is only a data path, and it exists already, load SCMMAP data.\n    elif data_path is not None and os.path.exists(data_path):\n        self.__init__obj()\n        self.load(data_path)\n\n    # If there is only an h5ad path, load the HDF5 data\n    elif h5ad_path is not None:\n        self.__init__obj()\n        self.load_h5ad(h5ad_path)\n    else:\n        match num_rows, num_elements:\n            case (int(), int()):\n                self.__init__obj()\n                self._init_arrs(num_elements=num_elements, num_rows=num_rows)\n            case _:\n                raise ValueError(\"An np.memmap path, an h5ad path, or the number of elements and rows is required\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.__init__obj","title":"<code>__init__obj()</code>","text":"<p>Initializes the data path and writes the version.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def __init__obj(self):\n    \"\"\"Initializes the data path and writes the version.\"\"\"\n    os.makedirs(self.data_path, exist_ok=True)\n\n    # Write the version\n    if not os.path.exists(f\"{self.data_path}/{FileNames.VERSION.value}\"):\n        with open(f\"{self.data_path}/{FileNames.VERSION.value}\", \"w\") as vfi:\n            json.dump(self.version(), vfi)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of rows.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"Return the number of rows.\"\"\"\n    return self.number_of_rows()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.cast_data_to_dtype","title":"<code>cast_data_to_dtype(dtype)</code>","text":"<p>Casts the data dtype of the dataset to the given dtype. This will convert the data memory map in-place on the disk.</p> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <code>str</code> <p>The dtype to cast the data to. Must be one of INT_ORDER + FLOAT_ORDER.</p> required Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def cast_data_to_dtype(self, dtype: str) -&gt; None:\n    \"\"\"Casts the data dtype of the dataset to the given dtype. This will convert the data memory map in-place on the disk.\n\n    Args:\n        dtype: The dtype to cast the data to. Must be one of INT_ORDER + FLOAT_ORDER.\n    \"\"\"\n    allowed_dtypes = list(INT_ORDER + FLOAT_ORDER)\n\n    if dtype is None or dtype not in allowed_dtypes:\n        raise ValueError(f\"Invalid data_dtype '{dtype}'. Must be one of: {', '.join(allowed_dtypes)}\")\n\n    # writes the new dtype to the disk\n    self._convert_dataset_to_new_dtypes(new_dtypes={FileNames.DATA.value: dtype}, allow_downscaling=True)\n    # Save the updated header (with a new data dtype to the disk)\n    self._write_header()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.concat","title":"<code>concat(other_dataset, extend_copy_size=10 * 1024 * 1024, output_path=None, destroy_on_copy=False)</code>","text":"<p>Concatenates one or a list of SingleCellMemMapDatasest to the existing one.</p> <p>The data is stored in the same place as for the original data set or at output_path if it is set. Then, at output_path or at self.data_path, there would be a saved SingleCellMemmpDataset, which can be read in with SingleCellMemmpDataset(output_path).</p> <p>Parameters:</p> Name Type Description Default <code>other_dataset</code> <code>Union[list[SingleCellMemMapDataset], SingleCellMemMapDataset]</code> <p>A SingleCellMemMapDataset or a list of</p> required <code>extend_copy_size</code> <code>int</code> <p>how much to copy in memory at once</p> <code>10 * 1024 * 1024</code> <code>output_path</code> <code>str | None</code> <p>location to store new dataset</p> <code>None</code> <code>destroy_on_copy</code> <code>bool</code> <p>Whether to remove the current data_path</p> <code>False</code> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def concat(\n    self,\n    other_dataset: Union[list[\"SingleCellMemMapDataset\"], \"SingleCellMemMapDataset\"],\n    extend_copy_size: int = 10 * 1_024 * 1_024,\n    output_path: str | None = None,\n    destroy_on_copy: bool = False,\n) -&gt; None:\n    \"\"\"Concatenates one or a list of SingleCellMemMapDatasest to the existing one.\n\n    The data is stored in the same place as for the original data set or at output_path\n    if it is set. Then, at output_path or at self.data_path, there would be a saved\n    SingleCellMemmpDataset, which can be read in with SingleCellMemmpDataset(output_path).\n\n    Args:\n        other_dataset: A SingleCellMemMapDataset or a list of\n        SingleCellMemMapDatasets\n        extend_copy_size: how much to copy in memory at once\n        output_path: location to store new dataset\n        destroy_on_copy: Whether to remove the current data_path\n\n    Raises:\n       ValueError if the other dataset(s) are not of the same version or\n       something of another type is passed in.\n    \"\"\"\n    match other_dataset:\n        case self.__class__():\n            other_dataset = [other_dataset]\n        case list():\n            pass\n        case _:\n            raise ValueError(\n                f\"Expecting either a {SingleCellMemMapDataset} or a list thereof. Actually got: {type(other_dataset)}\"\n            )\n\n    # Verify the other dataset or datasets are of the same type.\n    new_dtypes = self._verify_concat_compatibility_and_types(other_dataset)\n\n    # Set our mode:\n    self.mode: Mode = Mode.READ_APPEND\n    if output_path is not None:\n        if destroy_on_copy:\n            shutil.move(self.data_path, output_path)\n        else:\n            shutil.copytree(self.data_path, output_path)\n        self.data_path = output_path\n\n    self._convert_dataset_to_new_dtypes(new_dtypes)\n    # Copy the data from self and other into the new arrays.\n    element_counter = self.number_nonzero_values()\n    row_counter = self.number_of_rows()\n    for mmap in other_dataset:\n        extend_files(\n            f\"{self.data_path}/{FileNames.ROWPTR.value}\",\n            f\"{mmap.data_path}/{FileNames.ROWPTR.value}\",\n            elements_per_chunk=extend_copy_size,\n            delete_file2_on_complete=destroy_on_copy,\n            offset=np.dtype(mmap.dtypes[f\"{FileNames.ROWPTR.value}\"]).itemsize,\n            source_dtype=mmap.dtypes[f\"{FileNames.ROWPTR.value}\"],\n            dest_dtype=new_dtypes[f\"{FileNames.ROWPTR.value}\"],\n            add_value=element_counter,\n        )\n\n        extend_files(\n            f\"{self.data_path}/{FileNames.DATA.value}\",\n            f\"{mmap.data_path}/{FileNames.DATA.value}\",\n            elements_per_chunk=extend_copy_size,\n            delete_file2_on_complete=destroy_on_copy,\n            source_dtype=mmap.dtypes[f\"{FileNames.DATA.value}\"],\n            dest_dtype=new_dtypes[f\"{FileNames.DATA.value}\"],\n        )\n        extend_files(\n            f\"{self.data_path}/{FileNames.COLPTR.value}\",\n            f\"{mmap.data_path}/{FileNames.COLPTR.value}\",\n            elements_per_chunk=extend_copy_size,\n            delete_file2_on_complete=destroy_on_copy,\n            source_dtype=mmap.dtypes[f\"{FileNames.COLPTR.value}\"],\n            dest_dtype=new_dtypes[f\"{FileNames.COLPTR.value}\"],\n        )\n        self._var_feature_index.concat(mmap._var_feature_index)\n        self._obs_feature_index.concat(mmap._obs_feature_index)\n        # Update counters\n        element_counter += mmap.number_nonzero_values()\n        row_counter += mmap.number_of_rows()\n\n    # Reopen the data, colptr, and rowptr arrays\n    self.data = np.memmap(\n        f\"{self.data_path}/{FileNames.DATA.value}\",\n        dtype=self.dtypes[f\"{FileNames.DATA.value}\"],\n        shape=(element_counter,),\n        mode=Mode.READ_APPEND.value,\n    )\n    self.row_index = np.memmap(\n        f\"{self.data_path}/{FileNames.ROWPTR.value}\",\n        dtype=self.dtypes[f\"{FileNames.ROWPTR.value}\"],\n        shape=(row_counter + 1,),\n        mode=Mode.READ_APPEND.value,\n    )\n    self.col_index = np.memmap(\n        f\"{self.data_path}/{FileNames.COLPTR.value}\",\n        dtype=self.dtypes[f\"{FileNames.COLPTR.value}\"],\n        shape=(element_counter,),\n        mode=Mode.READ_APPEND.value,\n    )\n    self.save()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.get_neighbor_indices_for_cell","title":"<code>get_neighbor_indices_for_cell(cell_index)</code>","text":"<p>Returns the array of neighbor indices for a given cell.</p> <p>Parameters:</p> Name Type Description Default <code>cell_index</code> <code>int</code> <p>Index of the cell to get neighbors for</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of neighbor indices, empty if no neighbors or neighbor data unavailable</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If cell_index is out of bounds</p> <code>ValueError</code> <p>If neighbor functionality was explicitly enabled but data is unavailable</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def get_neighbor_indices_for_cell(self, cell_index: int) -&gt; np.ndarray:\n    \"\"\"Returns the array of neighbor indices for a given cell.\n\n    Args:\n        cell_index: Index of the cell to get neighbors for\n\n    Returns:\n        np.ndarray: Array of neighbor indices, empty if no neighbors or neighbor data unavailable\n\n    Raises:\n        IndexError: If cell_index is out of bounds\n        ValueError: If neighbor functionality was explicitly enabled but data is unavailable\n    \"\"\"\n    if not (0 &lt;= cell_index &lt; self.number_of_rows()):\n        raise IndexError(f\"Cell index {cell_index} out of bounds for dataset with {self.number_of_rows()} cells\")\n\n    # Check if neighbor functionality was requested but is unavailable\n    if self.load_neighbors and not self._has_neighbors:\n        raise ValueError(\"Neighbor functionality was enabled but no neighbor data is available\")\n\n    if not self.load_neighbors or not self._has_neighbors or self._neighbor_indptr is None:\n        return np.array([], dtype=int)  # Return empty array if neighbor data not available\n\n    # Get neighbor indices using CSR format indptr and indices\n    start = self._neighbor_indptr[cell_index]\n    end = self._neighbor_indptr[cell_index + 1]\n    return self._neighbor_indices[start:end]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.get_neighbor_stats","title":"<code>get_neighbor_stats()</code>","text":"<p>Returns statistics about the neighbors in the dataset.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary with neighbor statistics: - has_neighbors: Whether dataset has neighbor data - total_connections: Total number of neighbor relationships - min_neighbors_per_cell: Minimum number of neighbors any cell has - max_neighbors_per_cell: Maximum number of neighbors any cell has - avg_neighbors_per_cell: Average number of neighbors per cell - cells_with_no_neighbors: Count of cells that have no neighbors</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def get_neighbor_stats(self) -&gt; dict:\n    \"\"\"Returns statistics about the neighbors in the dataset.\n\n    Returns:\n        dict: Dictionary with neighbor statistics:\n            - has_neighbors: Whether dataset has neighbor data\n            - total_connections: Total number of neighbor relationships\n            - min_neighbors_per_cell: Minimum number of neighbors any cell has\n            - max_neighbors_per_cell: Maximum number of neighbors any cell has\n            - avg_neighbors_per_cell: Average number of neighbors per cell\n            - cells_with_no_neighbors: Count of cells that have no neighbors\n    \"\"\"\n    if not self._has_neighbors or self._neighbor_indptr is None or self._neighbor_indices is None:\n        return {\"has_neighbors\": False}\n\n    # Calculate stats based on CSR indptr (difference between consecutive elements)\n    neighbor_counts = np.diff(self._neighbor_indptr)\n\n    return {\n        \"has_neighbors\": True,\n        \"total_connections\": len(self._neighbor_indices),\n        \"min_neighbors_per_cell\": int(np.min(neighbor_counts)),\n        \"max_neighbors_per_cell\": int(np.max(neighbor_counts)),\n        \"avg_neighbors_per_cell\": float(np.mean(neighbor_counts)),\n        \"cells_with_no_neighbors\": int(np.sum(neighbor_counts == 0)),\n    }\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.get_neighbor_weights_for_cell","title":"<code>get_neighbor_weights_for_cell(cell_index)</code>","text":"<p>Returns the array of neighbor weights (e.g., pseudotime differences) for a given cell.</p> <p>Parameters:</p> Name Type Description Default <code>cell_index</code> <code>int</code> <p>Index of the cell to get neighbor weights for</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of weights corresponding to neighbors, empty if no neighbors</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If cell_index is out of bounds</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def get_neighbor_weights_for_cell(self, cell_index: int) -&gt; np.ndarray:\n    \"\"\"Returns the array of neighbor weights (e.g., pseudotime differences) for a given cell.\n\n    Args:\n        cell_index: Index of the cell to get neighbor weights for\n\n    Returns:\n        np.ndarray: Array of weights corresponding to neighbors, empty if no neighbors\n\n    Raises:\n        IndexError: If cell_index is out of bounds\n    \"\"\"\n    # Check if neighbor functionality was requested but is unavailable\n    if self.load_neighbors and not self._has_neighbors:\n        raise ValueError(\"Neighbor functionality was enabled but no neighbor data is available\")\n\n    if (\n        not self.load_neighbors\n        or not self._has_neighbors\n        or self._neighbor_indptr is None\n        or self._neighbor_data is None\n    ):\n        return np.array([], dtype=float)\n\n    if not (0 &lt;= cell_index &lt; self.number_of_rows()):\n        raise IndexError(f\"Cell index {cell_index} out of bounds for dataset with {self.number_of_rows()} cells\")\n\n    # Get neighbor weights using CSR format indptr and data\n    start = self._neighbor_indptr[cell_index]\n    end = self._neighbor_indptr[cell_index + 1]\n    return self._neighbor_data[start:end]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.get_row","title":"<code>get_row(index, return_var_features=False, var_feature_names=None, return_obs_features=False, obs_feature_names=None)</code>","text":"<p>Returns a given row in the dataset along with optional features.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The row to be returned. This is in the range of [0, num_rows)</p> required <code>return_var_features</code> <code>bool</code> <p>boolean that indicates whether to return features</p> <code>False</code> <code>var_feature_names</code> <code>Optional[List[str]]</code> <p>Optional, variable feature names to extract</p> <code>None</code> <code>return_obs_features</code> <code>bool</code> <p>boolean indicating whether to return observed (row) features</p> <code>False</code> <code>obs_feature_names</code> <code>Optional[List[str]]</code> <p>Optional, observed feature variables to extract</p> <code>None</code> <p>Return:     [Tuple[np.ndarray, np.ndarray]: data values and column pointes     List[np.ndarray]: optional, corresponding variable (column) features.     List[np.ndarray]: optional, corresponding observed (row) features.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def get_row(\n    self,\n    index: int,\n    return_var_features: bool = False,\n    var_feature_names: Optional[List[str]] = None,\n    return_obs_features: bool = False,\n    obs_feature_names: Optional[List[str]] = None,\n) -&gt; Tuple[Tuple[np.ndarray, np.ndarray], List[np.ndarray], List[np.ndarray]]:\n    \"\"\"Returns a given row in the dataset along with optional features.\n\n    Args:\n        index: The row to be returned. This is in the range of [0, num_rows)\n        return_var_features: boolean that indicates whether to return features\n        var_feature_names: Optional, variable feature names to extract\n        return_obs_features: boolean indicating whether to return observed (row) features\n        obs_feature_names: Optional, observed feature variables to extract\n    Return:\n        [Tuple[np.ndarray, np.ndarray]: data values and column pointes\n        List[np.ndarray]: optional, corresponding variable (column) features.\n        List[np.ndarray]: optional, corresponding observed (row) features.\n    \"\"\"\n    start = self.row_index[index]\n    end = self.row_index[index + 1]\n    values = self.data[start:end]\n    columns = self.col_index[start:end]\n    ret = (values, columns)\n    var_features = (\n        self._var_feature_index.lookup(index, select_features=var_feature_names)[0]\n        if return_var_features\n        else None\n    )\n    obs_features = (\n        self._obs_feature_index.lookup(index, select_features=obs_feature_names)[0]\n        if return_obs_features\n        else None\n    )\n    return ret, var_features, obs_features\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.get_row_column","title":"<code>get_row_column(index, column, impute_missing_zeros=True)</code>","text":"<p>Returns the value at a given index and the corresponding column.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index to be returned</p> required <code>column</code> <code>int</code> <p>The column to be returned</p> required <code>impute_missing_zeros</code> <code>bool</code> <p>boolean that indicates whether to set missing</p> <code>True</code> <p>Return:     A float that is the value in the array or None.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def get_row_column(self, index: int, column: int, impute_missing_zeros: bool = True) -&gt; Optional[float]:\n    \"\"\"Returns the value at a given index and the corresponding column.\n\n    Args:\n        index: The index to be returned\n        column: The column to be returned\n        impute_missing_zeros: boolean that indicates whether to set missing\n        data to 0\n    Return:\n        A float that is the value in the array or None.\n    \"\"\"\n    (row_values, row_column_pointer), _, _ = self.get_row(index)\n    if column is not None:\n        for col_index, col in enumerate(row_column_pointer):\n            if col == column:\n                # return the value at this position\n                return row_values[col_index]\n            elif col &gt; column:\n                try:\n                    raise ValueError(f\"Column pointer {col} is larger than the column {column}.\")\n                except ValueError:\n                    break\n        return 0.0 if impute_missing_zeros else None\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.get_row_padded","title":"<code>get_row_padded(index, return_var_features=False, var_feature_names=None, return_obs_features=False, obs_feature_names=None)</code>","text":"<p>Returns a padded version of a row in the dataset.</p> <p>A padded version is one where the a sparse array representation is converted to a conventional represenentation. Optionally, features are returned.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The row to be returned</p> required <code>return_var_features</code> <code>bool</code> <p>boolean that indicates whether to return variable features</p> <code>False</code> <code>var_feature_names</code> <code>Optional[List[str]]</code> <p>Optional, variable feature names to extract</p> <code>None</code> <code>return_obs_features</code> <code>bool</code> <p>Boolean that indicates whether to return observed features</p> <code>False</code> <code>obs_feature_names</code> <code>Optional[List[str]]</code> <p>Optional, observed feature variables to extract</p> <code>None</code> <p>Return:     np.ndarray: conventional row representation     List[np.ndarray]: optional, corresponding variable (column) features.     List[np.ndarray]: optional, corresponding observed (row) features.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def get_row_padded(\n    self,\n    index: int,\n    return_var_features: bool = False,\n    var_feature_names: Optional[List[str]] = None,\n    return_obs_features: bool = False,\n    obs_feature_names: Optional[List[str]] = None,\n) -&gt; Tuple[np.ndarray, List[np.ndarray], List[np.ndarray]]:\n    \"\"\"Returns a padded version of a row in the dataset.\n\n    A padded version is one where the a sparse array representation is\n    converted to a conventional represenentation. Optionally, features are\n    returned.\n\n    Args:\n        index: The row to be returned\n        return_var_features: boolean that indicates whether to return variable features\n        var_feature_names: Optional, variable feature names to extract\n        return_obs_features: Boolean that indicates whether to return observed features\n        obs_feature_names: Optional, observed feature variables to extract\n    Return:\n        np.ndarray: conventional row representation\n        List[np.ndarray]: optional, corresponding variable (column) features.\n        List[np.ndarray]: optional, corresponding observed (row) features.\n    \"\"\"\n    (row_values, row_column_pointer), var_features, obs_features = self.get_row(\n        index, return_var_features, var_feature_names, return_obs_features, obs_feature_names\n    )\n    return (\n        _pad_sparse_array(row_values, row_column_pointer, self._var_feature_index.number_vars_at_row(index)),\n        var_features,\n        obs_features,\n    )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.get_row_padded_with_neighbor","title":"<code>get_row_padded_with_neighbor(index, return_var_features=False, var_feature_names=None, return_obs_features=False, obs_feature_names=None)</code>","text":"<p>Returns a padded version of a row with optional neighbor data.</p> <p>A padded version converts sparse representation to a dense array where missing values are filled with zeros.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The row to be returned</p> required <code>return_var_features</code> <code>bool</code> <p>Boolean that indicates whether to return variable features</p> <code>False</code> <code>var_feature_names</code> <code>Optional[List[str]]</code> <p>Optional, variable feature names to extract</p> <code>None</code> <code>return_obs_features</code> <code>bool</code> <p>Boolean that indicates whether to return observed features</p> <code>False</code> <code>obs_feature_names</code> <code>Optional[List[str]]</code> <p>Optional, observed feature variables to extract</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Union[ndarray, int, List[ndarray]]]</code> <p>Dict with keys:</p> <code>Dict[str, Union[ndarray, int, List[ndarray]]]</code> <ul> <li>'current_cell': np.ndarray - Padded array for current cell</li> </ul> <code>Dict[str, Union[ndarray, int, List[ndarray]]]</code> <ul> <li>'next_cell': np.ndarray - Padded array for neighbor cell</li> </ul> <code>Dict[str, Union[ndarray, int, List[ndarray]]]</code> <ul> <li>'current_cell_index': int - Index of current cell</li> </ul> <code>Dict[str, Union[ndarray, int, List[ndarray]]]</code> <ul> <li>'next_cell_index': int - Index of neighbor cell</li> </ul> <code>Dict[str, Union[ndarray, int, List[ndarray]]]</code> <ul> <li>'features': List[np.ndarray] - Variable features if return_features is True, else None</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neighbor functionality is disabled or no neighbor data is available</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def get_row_padded_with_neighbor(\n    self,\n    index: int,\n    return_var_features: bool = False,\n    var_feature_names: Optional[List[str]] = None,\n    return_obs_features: bool = False,\n    obs_feature_names: Optional[List[str]] = None,\n) -&gt; Dict[str, Union[np.ndarray, int, List[np.ndarray]]]:\n    \"\"\"Returns a padded version of a row with optional neighbor data.\n\n    A padded version converts sparse representation to a dense array where\n    missing values are filled with zeros.\n\n    Args:\n        index: The row to be returned\n        return_var_features: Boolean that indicates whether to return variable features\n        var_feature_names: Optional, variable feature names to extract\n        return_obs_features: Boolean that indicates whether to return observed features\n        obs_feature_names: Optional, observed feature variables to extract\n\n    Returns:\n        Dict with keys:\n        - 'current_cell': np.ndarray - Padded array for current cell\n        - 'next_cell': np.ndarray - Padded array for neighbor cell\n        - 'current_cell_index': int - Index of current cell\n        - 'next_cell_index': int - Index of neighbor cell\n        - 'features': List[np.ndarray] - Variable features if return_features is True, else None\n\n    Raises:\n        ValueError: If neighbor functionality is disabled or no neighbor data is available\n    \"\"\"\n    # Validate neighbor availability since this function requires neighbors\n    if not (self.load_neighbors and self._has_neighbors):\n        raise ValueError(\n            \"Cannot include neighbor data: neighbor functionality is disabled or no neighbor data available\"\n        )\n\n    # Get both current cell and neighbor data\n    result = self.get_row_with_neighbor(\n        index, return_var_features, var_feature_names, return_obs_features, obs_feature_names\n    )\n\n    # Get current cell padded array using get_row_padded\n    curr_padded, _, _ = self.get_row_padded(index, False, None, False, None)\n\n    # For neighbor, get the padded array\n    next_idx = result[\"next_cell_index\"]\n    if next_idx == index:\n        # If neighbor is the same as current cell, reuse the current padded array\n        next_padded = curr_padded\n    else:\n        # Otherwise get the neighbor's padded array\n        next_padded, _, _ = self.get_row_padded(next_idx, False, None)\n\n    # Return in dictionary format\n    return {\n        \"current_cell\": curr_padded,\n        \"next_cell\": next_padded,\n        \"current_cell_index\": result[\"current_cell_index\"],\n        \"next_cell_index\": result[\"next_cell_index\"],\n        \"var_features\": result[\"var_features\"],\n        \"obs_features\": result[\"obs_features\"],\n    }\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.get_row_with_neighbor","title":"<code>get_row_with_neighbor(index, return_var_features=False, var_feature_names=None, return_obs_features=False, obs_feature_names=None)</code>","text":"<p>Returns a given row in the dataset along with optional features and neighbor data.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The row to be returned. This is in the range of [0, num_rows)</p> required <code>return_var_features</code> <code>bool</code> <p>Boolean that indicates whether to return variable features</p> <code>False</code> <code>var_feature_names</code> <code>Optional[List[str]]</code> <p>Optional, variable feature names to extract</p> <code>None</code> <code>return_obs_features</code> <code>bool</code> <p>Boolean that indicates whether to return observed features</p> <code>False</code> <code>obs_feature_names</code> <code>Optional[List[str]]</code> <p>Optional, observed feature variables to extract</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Union[Tuple[ndarray, ndarray], int, Optional[List[ndarray]]]]</code> <p>Dict with keys:</p> <code>Dict[str, Union[Tuple[ndarray, ndarray], int, Optional[List[ndarray]]]]</code> <ul> <li>'current_cell': Tuple[np.ndarray, np.ndarray] - (values, columns) for current cell</li> </ul> <code>Dict[str, Union[Tuple[ndarray, ndarray], int, Optional[List[ndarray]]]]</code> <ul> <li>'next_cell': Tuple[np.ndarray, np.ndarray] - (values, columns) for neighbor cell</li> </ul> <code>Dict[str, Union[Tuple[ndarray, ndarray], int, Optional[List[ndarray]]]]</code> <ul> <li>'current_cell_index': int - Index of current cell</li> </ul> <code>Dict[str, Union[Tuple[ndarray, ndarray], int, Optional[List[ndarray]]]]</code> <ul> <li>'next_cell_index': int - Index of neighbor cell</li> </ul> <code>Dict[str, Union[Tuple[ndarray, ndarray], int, Optional[List[ndarray]]]]</code> <ul> <li>'var_features': List[np.ndarray] - Variable features if return_features is True, else None</li> </ul> <code>Dict[str, Union[Tuple[ndarray, ndarray], int, Optional[List[ndarray]]]]</code> <ul> <li>'obs_features': List[np.ndarray] - Observed features if return_obs_features is True, else None</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neighbor functionality is disabled or no neighbor data is available</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def get_row_with_neighbor(\n    self,\n    index: int,\n    return_var_features: bool = False,\n    var_feature_names: Optional[List[str]] = None,\n    return_obs_features: bool = False,\n    obs_feature_names: Optional[List[str]] = None,\n) -&gt; Dict[str, Union[Tuple[np.ndarray, np.ndarray], int, Optional[List[np.ndarray]]]]:\n    \"\"\"Returns a given row in the dataset along with optional features and neighbor data.\n\n    Args:\n        index: The row to be returned. This is in the range of [0, num_rows)\n        return_var_features: Boolean that indicates whether to return variable features\n        var_feature_names: Optional, variable feature names to extract\n        return_obs_features: Boolean that indicates whether to return observed features\n        obs_feature_names: Optional, observed feature variables to extract\n\n    Returns:\n        Dict with keys:\n        - 'current_cell': Tuple[np.ndarray, np.ndarray] - (values, columns) for current cell\n        - 'next_cell': Tuple[np.ndarray, np.ndarray] - (values, columns) for neighbor cell\n        - 'current_cell_index': int - Index of current cell\n        - 'next_cell_index': int - Index of neighbor cell\n        - 'var_features': List[np.ndarray] - Variable features if return_features is True, else None\n        - 'obs_features': List[np.ndarray] - Observed features if return_obs_features is True, else None\n\n    Raises:\n        ValueError: If neighbor functionality is disabled or no neighbor data is available\n    \"\"\"\n    # Validate neighbor availability since this function requires neighbors\n    if not (self.load_neighbors and self._has_neighbors):\n        raise ValueError(\n            \"Cannot include neighbor data: neighbor functionality is disabled or no neighbor data available\"\n        )\n\n    # Get current cell data using the existing get_row function\n    current_cell_data, var_features, obs_features = self.get_row(\n        index, return_var_features, var_feature_names, return_obs_features, obs_feature_names\n    )\n\n    # Sample neighbor and get its data\n    neighbor_index = self.sample_neighbor_index(index)\n\n    # Case where neighbor is the same as current cell\n    if neighbor_index == index:\n        next_cell_data = current_cell_data\n    else:\n        # Get neighbor cell data using the get_row function\n        next_cell_data, _, _ = self.get_row(neighbor_index, False, None)\n\n    # Return all data in a dictionary format\n    return {\n        \"current_cell\": current_cell_data,\n        \"next_cell\": next_cell_data,\n        \"current_cell_index\": index,\n        \"next_cell_index\": neighbor_index,\n        \"var_features\": var_features,\n        \"obs_features\": obs_features,\n    }\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.load","title":"<code>load(stored_path)</code>","text":"<p>Loads the data at store_path that is an np.memmap format.</p> <p>Parameters:</p> Name Type Description Default <code>stored_path</code> <code>str</code> <p>directory with np.memmap files</p> required <p>Raises:     FileNotFoundError if the corresponding directory or files are not     found, or if the metadata file is not present.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def load(self, stored_path: str) -&gt; None:\n    \"\"\"Loads the data at store_path that is an np.memmap format.\n\n    Args:\n        stored_path: directory with np.memmap files\n    Raises:\n        FileNotFoundError if the corresponding directory or files are not\n        found, or if the metadata file is not present.\n    \"\"\"\n    if not os.path.exists(stored_path):\n        raise FileNotFoundError(\n            f\"\"\"Error: the specified data path to the mmap files {stored_path} does not exist.\n                                Specify an updated filepath or provide an h5ad path to the dataset. The data can\n                                be loaded with SingleCellMemMapDataset.load_h5ad. Alternatively, the class can be instantiated\n                                with  SingleCellMemMapDataset(&lt;path to data that will be created&gt;, h5ad_path=&lt;path to h5ad file&gt;\"\"\"\n        )\n    self.data_path = stored_path\n    self.mode = Mode.READ_APPEND\n    # Load header if present; keep None if missing or unreadable\n    if os.path.exists(self.header_path):\n        try:\n            self.header = SCDLHeader.load(str(self.header_path))\n        except Exception as e:\n            warnings.warn(f\"Failed to load SCDL header at {self.header_path}: {e}\")\n            self.header = None\n    else:\n        warnings.warn(f\"SCDL header missing at {self.header_path}; continuing without header.\")\n        self.header = None\n    # If header is loaded, extract dtypes from header and set self.dtypes accordingly\n    if self.header is not None and hasattr(self.header, \"arrays\"):\n        # Map from FileNames.value to dtype string\n        for array_info in self.header.arrays:\n            if FileNames[array_info.name].value not in self.dtypes:\n                raise ValueError(f\"Array name {FileNames[array_info.name].value} not found in dtypes\")\n            self.dtypes[FileNames[array_info.name].value] = array_info.dtype.numpy_dtype_string\n\n    # Metadata is required, so we must check if it exists and fail if not.\n    if not os.path.exists(f\"{self.data_path}/{FileNames.METADATA.value}\"):\n        raise FileNotFoundError(\n            f\"Error: the metadata file {self.data_path}/{FileNames.METADATA.value} does not exist.\"\n        )\n\n    with open(f\"{self.data_path}/{FileNames.METADATA.value}\", Mode.READ_APPEND.value) as mfi:\n        self.metadata = json.load(mfi)\n\n    if os.path.exists(f\"{self.data_path}/{FileNames.VAR_FEATURES.value}\"):\n        self._var_feature_index = VariableFeatureIndex.load(f\"{self.data_path}/{FileNames.VAR_FEATURES.value}\")\n    elif os.path.exists(\n        f\"{self.data_path}/{FileNames.FEATURES.value}\"\n    ):  # Backward compatibility with old features file\n        self._var_feature_index = VariableFeatureIndex.load(f\"{self.data_path}/{FileNames.FEATURES.value}\")\n    if os.path.exists(f\"{self.data_path}/{FileNames.OBS_FEATURES.value}\"):\n        self._obs_feature_index = ObservedFeatureIndex.load(f\"{self.data_path}/{FileNames.OBS_FEATURES.value}\")\n    # mmap the existing arrays\n    self.data = self._load_mmap_file_if_exists(\n        f\"{self.data_path}/{FileNames.DATA.value}\", self.dtypes[f\"{FileNames.DATA.value}\"]\n    )\n    self.row_index = self._load_mmap_file_if_exists(\n        f\"{self.data_path}/{FileNames.ROWPTR.value}\", dtype=self.dtypes[f\"{FileNames.ROWPTR.value}\"]\n    )\n    self.col_index = self._load_mmap_file_if_exists(\n        f\"{self.data_path}/{FileNames.COLPTR.value}\", dtype=self.dtypes[f\"{FileNames.COLPTR.value}\"]\n    )\n\n    # Load neighbor data\n    if self.load_neighbors:\n        self._load_neighbor_memmaps()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.load_h5ad","title":"<code>load_h5ad(anndata_path)</code>","text":"<p>Loads an existing AnnData archive from disk.</p> <p>This creates a new backing data structure which is saved. Note: the storage utilized will roughly double. Currently, the data must be in a scipy.sparse.spmatrix format.</p> <p>Parameters:</p> Name Type Description Default <code>anndata_path</code> <code>str</code> <p>location of data to load</p> required <p>Raises:     FileNotFoundError if the data path does not exist.     NotImplementedError if the data is not in scipy.sparse.spmatrix     format     ValueError it there is not count data</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def load_h5ad(\n    self,\n    anndata_path: str,\n) -&gt; None:\n    \"\"\"Loads an existing AnnData archive from disk.\n\n    This creates a new backing data structure which is saved.\n    Note: the storage utilized will roughly double. Currently, the data must\n    be in a scipy.sparse.spmatrix format.\n\n    Args:\n        anndata_path: location of data to load\n    Raises:\n        FileNotFoundError if the data path does not exist.\n        NotImplementedError if the data is not in scipy.sparse.spmatrix\n        format\n        ValueError it there is not count data\n    \"\"\"\n    if not os.path.exists(anndata_path):\n        raise FileNotFoundError(f\"Error: could not find h5ad path {anndata_path}\")\n    file_size_MB = os.path.getsize(anndata_path) / (1_024**2)\n\n    if file_size_MB &lt; self.paginated_load_cutoff:\n        var_features_df, obs_features_df, num_rows = self.regular_load_h5ad(anndata_path)\n    else:\n        var_features_df, obs_features_df, num_rows = self.paginated_load_h5ad(anndata_path)\n\n    var_features = _extract_features(var_features_df, self.var_feature_index_name)\n    obs_features = _extract_features(obs_features_df, self.obs_feature_index_name)\n    self._var_feature_index.append_features(n_obs=num_rows, features=var_features, label=anndata_path)\n    self._obs_feature_index.append_features(features=obs_features, label=anndata_path)\n    self.save()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.number_nonzero_values","title":"<code>number_nonzero_values()</code>","text":"<p>Number of non zero entries in the dataset.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def number_nonzero_values(self) -&gt; int:\n    \"\"\"Number of non zero entries in the dataset.\"\"\"\n    return self.data.size\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.number_of_rows","title":"<code>number_of_rows()</code>","text":"<p>The number of rows in the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of rows in the dataset</p> <p>Raises:     ValueError if the length of the number of rows in the feature     index does not correspond to the number of stored rows.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def number_of_rows(self) -&gt; int:\n    \"\"\"The number of rows in the dataset.\n\n    Returns:\n        The number of rows in the dataset\n    Raises:\n        ValueError if the length of the number of rows in the feature\n        index does not correspond to the number of stored rows.\n    \"\"\"\n    if len(self._var_feature_index) &gt; 0 and self._var_feature_index.number_of_rows() != self.row_index.size - 1:\n        raise ValueError(\n            f\"\"\"The number of rows in the feature index {self._var_feature_index.number_of_rows()}\n                         does not correspond to the number of rows in the row_index {self.row_index.size - 1}\"\"\"\n        )\n    return self._var_feature_index.number_of_rows()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.number_of_values","title":"<code>number_of_values()</code>","text":"<p>Get the total number of values in the array.</p> <p>For each index, the length of the corresponding np.ndarray of features is counted.</p> <p>Returns:</p> Type Description <code>int</code> <p>The sum of lengths of the features in every row</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def number_of_values(self) -&gt; int:\n    \"\"\"Get the total number of values in the array.\n\n    For each index, the length of the corresponding np.ndarray of features is counted.\n\n    Returns:\n        The sum of lengths of the features in every row\n    \"\"\"\n    return sum(self._var_feature_index.number_of_values())\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.number_of_variables","title":"<code>number_of_variables()</code>","text":"<p>Get the number of features in every entry in the dataset.</p> <p>Returns:</p> Type Description <code>List[int]</code> <p>A list containing the lengths of the features in every row</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def number_of_variables(self) -&gt; List[int]:\n    \"\"\"Get the number of features in every entry in the dataset.\n\n    Returns:\n        A list containing the lengths of the features in every row\n    \"\"\"\n    feats = self._var_feature_index\n    if len(feats) == 0:\n        return [0]\n    num_vars = feats.column_dims()\n    return num_vars\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.obs_features","title":"<code>obs_features()</code>","text":"<p>Return the corresponding ObservedFeatureIndex.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def obs_features(self) -&gt; Optional[ObservedFeatureIndex]:\n    \"\"\"Return the corresponding ObservedFeatureIndex.\"\"\"\n    return self._obs_feature_index\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.paginated_load_h5ad","title":"<code>paginated_load_h5ad(anndata_path)</code>","text":"<p>Method for block loading a larger h5ad file and converting it to the SCDL format.</p> <p>This should be used in the case when the entire anndata file cannot be loaded into memory. The anndata is loaded into memory load_block_row_size number of rows at a time. Each chunk is converted into numpy memory maps which are then concatenated together.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <p>pd.DataFrame: var variables for features</p> <code>int</code> <code>int</code> <p>number of rows in the dataframe.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def paginated_load_h5ad(\n    self,\n    anndata_path: str,\n) -&gt; Tuple[pd.DataFrame, int]:\n    \"\"\"Method for block loading a larger h5ad file and converting it to the SCDL format.\n\n    This should be used in the case when the entire anndata file cannot be loaded into memory.\n    The anndata is loaded into memory load_block_row_size number of rows at a time. Each chunk\n    is converted into numpy memory maps which are then concatenated together.\n\n    Raises:\n        NotImplementedError if the data is not loaded in the CSRDataset format.\n\n    Returns:\n        pd.DataFrame: var variables for features\n        int: number of rows in the dataframe.\n    \"\"\"\n    adata = ad.read_h5ad(anndata_path, backed=True)\n    if self.load_neighbors:\n        self._has_neighbors = self._extract_neighbor_data_paginated(adata)\n    X_full = self._get_matrix_X(adata)\n    if not isinstance(X_full, ad.experimental.CSRDataset):\n        raise NotImplementedError(\"Error: dense matrix loading not yet implemented.\")\n\n    # Use slice-then-raw when sampling rows\n    count_data = self._get_matrix_X(adata[:1_000])\n\n    # Use full matrix for pointers and shapes\n    n_elements = X_full._indptr[-1]\n    row_index = X_full._indptr.astype(self.dtypes[f\"{FileNames.ROWPTR.value}\"])\n\n    self._check_data_downcast(count_data, \"First 1000 rows of the dataset\")\n    num_rows, num_cols = X_full.shape\n    self.dtypes[f\"{FileNames.COLPTR.value}\"] = smallest_uint_dtype(num_cols - 1)\n    self.dtypes[f\"{FileNames.ROWPTR.value}\"] = smallest_uint_dtype(n_elements)\n    # Read the row indices into a memory map.\n    mode = Mode.CREATE_APPEND\n    self.row_index = _create_row_memmaps(num_rows, Path(self.data_path), mode, self.dtypes)\n    self.row_index[:] = row_index\n\n    # The data from each column and data chunk of the original anndata file is read in. This is saved into the final\n    # location of the memmap file. In this step, it is saved in the binary file format.\n    memmap_dir_path = Path(self.data_path)\n    with (\n        open(f\"{memmap_dir_path}/{FileNames.COLPTR.value}\", \"wb\") as col_file,\n        open(f\"{memmap_dir_path}/{FileNames.DATA.value}\", \"wb\") as data_file,\n    ):\n        for row_start in range(0, num_rows, self.load_block_row_size):\n            adata_block = adata[row_start : row_start + self.load_block_row_size]\n            adata_block_X = self._get_matrix_X(adata_block)\n            # Write each array's data to the file in binary format\n            col_block = adata_block_X.indices.astype(self.dtypes[f\"{FileNames.COLPTR.value}\"])\n            col_file.write(col_block.tobytes())\n            count_data_downcast = self._check_data_downcast(\n                adata_block_X, f\"Rows {row_start} to {row_start + self.load_block_row_size - 1} of the dataset\"\n            )\n\n            data_file.write(count_data_downcast.tobytes())\n\n    # The column and data files are re-opened as memory-mapped arrays with the final shape\n    mode = Mode.READ_APPEND\n    self.col_index = np.memmap(\n        f\"{memmap_dir_path}/{FileNames.COLPTR.value}\",\n        self.dtypes[f\"{FileNames.COLPTR.value}\"],\n        mode=mode,\n        shape=(n_elements,),\n    )\n    self.data = np.memmap(\n        f\"{memmap_dir_path}/{FileNames.DATA.value}\",\n        dtype=self.dtypes[f\"{FileNames.DATA.value}\"],\n        mode=mode,\n        shape=(n_elements,),\n    )\n    vars = adata.var\n    obs = adata.obs\n    file_handle = getattr(adata, \"file\", None)\n    if file_handle is not None:\n        try:\n            file_handle.close()\n        except Exception:\n            pass\n\n    return vars, obs, num_rows\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.regular_load_h5ad","title":"<code>regular_load_h5ad(anndata_path)</code>","text":"<p>Method for loading an h5ad file into memorySu and converting it to the SCDL format.</p> <p>Parameters:</p> Name Type Description Default <code>anndata_path</code> <code>str</code> <p>location of data to load</p> required <p>Raises:     NotImplementedError if the data is not in scipy.sparse.spmatrix format     ValueError it there is not count data Returns:     pd.DataFrame: var variables for features     int: number of rows in the dataframe.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def regular_load_h5ad(\n    self,\n    anndata_path: str,\n) -&gt; Tuple[pd.DataFrame, int]:\n    \"\"\"Method for loading an h5ad file into memorySu and converting it to the SCDL format.\n\n    Args:\n        anndata_path: location of data to load\n    Raises:\n        NotImplementedError if the data is not in scipy.sparse.spmatrix format\n        ValueError it there is not count data\n    Returns:\n        pd.DataFrame: var variables for features\n        int: number of rows in the dataframe.\n\n    \"\"\"\n    adata = ad.read_h5ad(anndata_path)  # slow\n    count_data = self._get_matrix_X(adata)\n    if not isinstance(count_data, scipy.sparse.spmatrix):\n        raise NotImplementedError(\"Error: dense matrix loading not yet implemented.\")\n\n    self._check_data_downcast(count_data, \"First 1000 rows of the dataset\")\n\n    # Check and load neighbor data\n    # NOTE: More clear to have a check here and not call _extract_neighbor_data() if there no neighbors\n    if self.load_neighbors:\n        self._has_neighbors = self._extract_neighbor_data(adata)\n\n    num_rows, num_cols = count_data.shape\n\n    num_elements_stored = count_data.nnz\n    # Currently, anndata is assumed to be sparse\n    self.dtypes[f\"{FileNames.ROWPTR.value}\"] = smallest_uint_dtype(num_elements_stored)\n    self.dtypes[f\"{FileNames.COLPTR.value}\"] = smallest_uint_dtype(num_cols - 1)\n    # Create the arrays.\n    self._init_arrs(num_elements_stored, num_rows)\n    # Store data\n    count_data_downcast = self._check_data_downcast(count_data, \"Full Dataset\")\n    self.data[0:num_elements_stored] = count_data_downcast\n    # Store the col idx array\n    self.col_index[0:num_elements_stored] = count_data.indices.astype(self.dtypes[f\"{FileNames.COLPTR.value}\"])\n\n    # Store the row idx array\n    self.row_index[0 : num_rows + 1] = count_data.indptr.astype(self.dtypes[f\"{FileNames.ROWPTR.value}\"])\n    vars = adata.var\n    obs = adata.obs\n    file_handle = getattr(adata, \"file\", None)\n    if file_handle is not None:\n        try:\n            file_handle.close()\n        except Exception:\n            pass\n    return vars, obs, num_rows\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.sample_neighbor_index","title":"<code>sample_neighbor_index(cell_index)</code>","text":"<p>Samples a neighbor index for the given cell based on the configured sampling strategy.</p> <p>Parameters:</p> Name Type Description Default <code>cell_index</code> <code>int</code> <p>Index of the cell to sample a neighbor for</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Index of the sampled neighbor  If no neighbors exist and fallback_to_identity is True, returns cell_index</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported sampling strategy is specified</p> <code>IndexError</code> <p>If cell_index is out of bounds</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def sample_neighbor_index(self, cell_index: int) -&gt; int:\n    \"\"\"Samples a neighbor index for the given cell based on the configured sampling strategy.\n\n    Args:\n        cell_index: Index of the cell to sample a neighbor for\n\n    Returns:\n        int: Index of the sampled neighbor\n             If no neighbors exist and fallback_to_identity is True, returns cell_index\n\n    Raises:\n        ValueError: If an unsupported sampling strategy is specified\n        IndexError: If cell_index is out of bounds\n    \"\"\"\n    # Basic validation\n    if not (0 &lt;= cell_index &lt; self.number_of_rows()):\n        raise IndexError(f\"Cell index {cell_index} out of bounds for dataset with {self.number_of_rows()} cells\")\n\n    # Check if neighbor functionality was requested but is unavailable\n    if self.load_neighbors and not self._has_neighbors:\n        raise ValueError(\"Neighbor functionality was enabled but no neighbor data is available\")\n\n    # Skip sampling if neighbor functionality is disabled\n    if not self.load_neighbors:\n        return cell_index  # Always return self as neighbor when neighbors disabled\n\n    # Get the neighbor indices for this cell\n    neighbor_indices = self.get_neighbor_indices_for_cell(cell_index)\n\n    # If no neighbors found, handle according to fallback policy\n    if len(neighbor_indices) == 0:\n        if self.fallback_to_identity:\n            return cell_index  # Return the cell itself\n        else:\n            # NOTE: implement fallback policy here if needed\n            warnings.warn(\n                f\"Cell {cell_index} has no neighbors and fallback_to_identity=False. \"\n                f\"Returning cell index itself anyway.\"\n            )\n            return cell_index  # Currently always return self if no neighbors\n\n    # Sample neighbor based on strategy\n    if self.neighbor_sampling_strategy == NeighborSamplingStrategy.RANDOM:\n        # Simple random sampling with equal probability\n        chosen_index = np.random.choice(neighbor_indices)\n        return chosen_index\n    elif self.neighbor_sampling_strategy == NeighborSamplingStrategy.FIRST:\n        # First neighbor sampling\n        return neighbor_indices[0]\n    # NOTE: Future - Add weighted sampling strategy\n    else:\n        raise ValueError(f\"Unsupported neighbor sampling strategy: {self.neighbor_sampling_strategy}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.save","title":"<code>save(output_path=None)</code>","text":"<p>Saves the class to a given output path.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>Optional[str]</code> <p>The location to save - not yet implemented and should</p> <code>None</code> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def save(self, output_path: Optional[str] = None) -&gt; None:\n    \"\"\"Saves the class to a given output path.\n\n    Args:\n        output_path: The location to save - not yet implemented and should\n        be self.data_path\n\n    Raises:\n       NotImplementedError if output_path is not None.\n    \"\"\"\n    self._write_header()\n    if \"num_rows\" not in self.metadata:\n        self.metadata[\"num_rows\"] = self.number_of_rows()\n\n    self._write_metadata()\n    # Write the var and obs feature index. This may not exist.\n    self._var_feature_index.save(f\"{self.data_path}/{FileNames.VAR_FEATURES.value}\")\n    self._obs_feature_index.save(f\"{self.data_path}/{FileNames.OBS_FEATURES.value}\")\n    # Ensure the object is in a valid state. These are saved at creation!\n    for postfix in [\n        f\"{FileNames.VERSION.value}\",\n        f\"{FileNames.DATA.value}\",\n        f\"{FileNames.COLPTR.value}\",\n        f\"{FileNames.ROWPTR.value}\",\n        f\"{FileNames.VAR_FEATURES.value}\",\n        f\"{FileNames.OBS_FEATURES.value}\",\n    ]:\n        if not os.path.exists(f\"{self.data_path}/{postfix}\"):\n            raise FileNotFoundError(f\"This file should exist from object creation: {self.data_path}/{postfix}\")\n\n    self.data.flush()  # NOTE: saves the data to disk, do the approach for neighbor data\n    self.row_index.flush()\n    self.col_index.flush()\n\n    # Flush neighbor data to disk if it exists\n    if self._has_neighbors and self._neighbor_indptr is not None:\n        self._neighbor_indptr.flush()\n        self._neighbor_indices.flush()\n        self._neighbor_data.flush()\n\n    if output_path is not None:\n        raise NotImplementedError(\"Saving to separate path is not yet implemented.\")\n\n    return True\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.shape","title":"<code>shape()</code>","text":"<p>Get the shape of the dataset.</p> <p>This is the number of entries by the the length of the feature index corresponding to that variable.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of elements in the dataset</p> <code>List[int]</code> <p>A list containing the number of variables for each row.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def shape(self) -&gt; Tuple[int, List[int]]:\n    \"\"\"Get the shape of the dataset.\n\n    This is the number of entries by the the length of the feature index\n    corresponding to that variable.\n\n    Returns:\n        The number of elements in the dataset\n        A list containing the number of variables for each row.\n    \"\"\"\n    return self.number_of_rows(), self.number_of_variables()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.var_features","title":"<code>var_features()</code>","text":"<p>Return the corresponding VariableFeatureIndex.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def var_features(self) -&gt; Optional[VariableFeatureIndex]:\n    \"\"\"Return the corresponding VariableFeatureIndex.\"\"\"\n    return self._var_feature_index\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.version","title":"<code>version()</code>","text":"<p>Returns a version number.</p> <p>(following .. convention). Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def version(self) -&gt; str:\n    \"\"\"Returns a version number.\n\n    (following &lt;major&gt;.&lt;minor&gt;.&lt;point&gt; convention).\n    \"\"\"\n    return self._version\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/","title":"Header","text":"<p>SCDL Archive Header Implementation.</p> <p>This module provides comprehensive header serialization/deserialization for SCDL archives, implementing the formal specification defined in scdl-schema.md.</p>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.ArrayInfo","title":"<code>ArrayInfo</code>","text":"<p>Information about an array in the SCDL archive.</p> <p>Represents metadata for a single array as defined in the SCDL schema specification.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>class ArrayInfo:\n    \"\"\"Information about an array in the SCDL archive.\n\n    Represents metadata for a single array as defined in the SCDL schema specification.\n    \"\"\"\n\n    def __init__(self, name: str, length: int, dtype: ArrayDType, shape: Optional[Tuple[int, ...]] = None):\n        \"\"\"Initialize array information.\n\n        Args:\n            name: Filename of the array\n            length: Number of elements in the array\n            dtype: Data type of the array elements\n            shape: Optional shape tuple for multidimensional arrays\n        \"\"\"\n        self.name = name\n        self.length = length\n        self.dtype = dtype\n        self.shape = shape\n\n    def serialize(self, codec: BinaryHeaderCodec) -&gt; bytes:\n        \"\"\"Serialize this ArrayInfo to binary format.\n\n        Args:\n            codec: Binary codec for serialization\n\n        Returns:\n            Binary representation following SCDL schema\n\n        Raises:\n            HeaderSerializationError: If validation fails\n        \"\"\"\n        # Validate before serialization (per schema requirements)\n        self._validate()\n\n        data = b\"\"\n\n        # name_len + name\n        data += codec.pack_string(self.name)\n\n        # length (uint64)\n        data += codec.pack_uint64(self.length)\n\n        # dtype (uint32 enum value)\n        data += codec.pack_uint32(int(self.dtype))\n\n        # has_shape + optional shape data\n        if self.shape is not None:\n            data += codec.pack_uint8(1)  # has_shape = true\n            data += codec.pack_uint32(len(self.shape))  # shape_dims\n            for dim in self.shape:\n                data += codec.pack_uint32(dim)  # shape array\n        else:\n            data += codec.pack_uint8(0)  # has_shape = false\n\n        return data\n\n    def _validate(self) -&gt; None:\n        \"\"\"Validate ArrayInfo according to SCDL schema requirements.\n\n        Raises:\n            HeaderSerializationError: If validation fails\n        \"\"\"\n        # Schema requirement: All string lengths must be &gt; 0\n        if not self.name or len(self.name.strip()) == 0:\n            raise HeaderSerializationError(\"Array name cannot be empty (schema requirement)\")\n\n        # Additional reasonable validations\n        if self.length &lt; 0:\n            raise HeaderSerializationError(f\"Array length cannot be negative: {self.length}\")\n\n        if self.shape is not None:\n            if len(self.shape) == 0:\n                raise HeaderSerializationError(\"Shape cannot be empty when specified\")\n            for i, dim in enumerate(self.shape):\n                if dim &lt;= 0:\n                    raise HeaderSerializationError(f\"Shape dimension {i} must be positive: {dim}\")\n\n        # Validate UTF-8 encoding\n        try:\n            self.name.encode(\"utf-8\")\n        except UnicodeEncodeError as e:\n            raise HeaderSerializationError(f\"Array name contains invalid UTF-8: {e}\")\n\n    @classmethod\n    def deserialize(cls, codec: BinaryHeaderCodec, data: bytes, offset: int = 0) -&gt; Tuple[\"ArrayInfo\", int]:\n        \"\"\"Deserialize ArrayInfo from binary data.\n\n        Args:\n            codec: Binary codec for deserialization\n            data: Binary data containing serialized ArrayInfo\n            offset: Starting offset in data\n\n        Returns:\n            Tuple of (ArrayInfo instance, bytes consumed)\n\n        Raises:\n            HeaderSerializationError: If data is invalid\n        \"\"\"\n        current_offset = offset\n\n        # Read name\n        name, name_bytes = codec.unpack_string(data[current_offset:])\n        current_offset += name_bytes\n\n        # Read length\n        length = codec.unpack_uint64(data[current_offset : current_offset + 8])\n        current_offset += 8\n\n        # Read dtype\n        dtype_value = codec.unpack_uint32(data[current_offset : current_offset + 4])\n        current_offset += 4\n\n        try:\n            dtype = ArrayDType(dtype_value)\n        except ValueError:\n            raise HeaderSerializationError(f\"Invalid ArrayDType value: {dtype_value}\")\n\n        # Read optional shape\n        has_shape = codec.unpack_uint8(data[current_offset : current_offset + 1])\n        current_offset += 1\n\n        shape = None\n        if has_shape:\n            shape_dims = codec.unpack_uint32(data[current_offset : current_offset + 4])\n            current_offset += 4\n\n            shape = []\n            for _ in range(shape_dims):\n                dim = codec.unpack_uint32(data[current_offset : current_offset + 4])\n                shape.append(dim)\n                current_offset += 4\n            shape = tuple(shape)\n\n        array_info = cls(name=name, length=length, dtype=dtype, shape=shape)\n        bytes_consumed = current_offset - offset\n\n        return array_info, bytes_consumed\n\n    def calculate_size(self) -&gt; int:\n        \"\"\"Calculate the serialized size of this ArrayInfo in bytes.\"\"\"\n        # name_len (4) + name length + length (8) + dtype (4) + has_shape (1)\n        size = 4 + len(self.name.encode(\"utf-8\")) + 8 + 4 + 1\n\n        if self.shape is not None:\n            # shape_dims (4) + shape array (4 * dimensions)\n            size += 4 + (4 * len(self.shape))\n\n        return size\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a human-readable description of the array info.\n\n        Returns:\n            str: Summary including name, length, dtype, and optional shape.\n        \"\"\"\n        shape_str = f\", shape={self.shape}\" if self.shape else \"\"\n        return f\"ArrayInfo(name='{self.name}', length={self.length}, dtype={self.dtype.name}{shape_str})\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return a developer-focused representation of the array info.\n\n        Returns:\n            str: Representation mirroring ``__str__`` for succinct debugging.\n        \"\"\"\n        return self.__str__()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.ArrayInfo.__init__","title":"<code>__init__(name, length, dtype, shape=None)</code>","text":"<p>Initialize array information.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Filename of the array</p> required <code>length</code> <code>int</code> <p>Number of elements in the array</p> required <code>dtype</code> <code>ArrayDType</code> <p>Data type of the array elements</p> required <code>shape</code> <code>Optional[Tuple[int, ...]]</code> <p>Optional shape tuple for multidimensional arrays</p> <code>None</code> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def __init__(self, name: str, length: int, dtype: ArrayDType, shape: Optional[Tuple[int, ...]] = None):\n    \"\"\"Initialize array information.\n\n    Args:\n        name: Filename of the array\n        length: Number of elements in the array\n        dtype: Data type of the array elements\n        shape: Optional shape tuple for multidimensional arrays\n    \"\"\"\n    self.name = name\n    self.length = length\n    self.dtype = dtype\n    self.shape = shape\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.ArrayInfo.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a developer-focused representation of the array info.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Representation mirroring <code>__str__</code> for succinct debugging.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return a developer-focused representation of the array info.\n\n    Returns:\n        str: Representation mirroring ``__str__`` for succinct debugging.\n    \"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.ArrayInfo.__str__","title":"<code>__str__()</code>","text":"<p>Return a human-readable description of the array info.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Summary including name, length, dtype, and optional shape.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a human-readable description of the array info.\n\n    Returns:\n        str: Summary including name, length, dtype, and optional shape.\n    \"\"\"\n    shape_str = f\", shape={self.shape}\" if self.shape else \"\"\n    return f\"ArrayInfo(name='{self.name}', length={self.length}, dtype={self.dtype.name}{shape_str})\"\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.ArrayInfo.calculate_size","title":"<code>calculate_size()</code>","text":"<p>Calculate the serialized size of this ArrayInfo in bytes.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def calculate_size(self) -&gt; int:\n    \"\"\"Calculate the serialized size of this ArrayInfo in bytes.\"\"\"\n    # name_len (4) + name length + length (8) + dtype (4) + has_shape (1)\n    size = 4 + len(self.name.encode(\"utf-8\")) + 8 + 4 + 1\n\n    if self.shape is not None:\n        # shape_dims (4) + shape array (4 * dimensions)\n        size += 4 + (4 * len(self.shape))\n\n    return size\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.ArrayInfo.deserialize","title":"<code>deserialize(codec, data, offset=0)</code>  <code>classmethod</code>","text":"<p>Deserialize ArrayInfo from binary data.</p> <p>Parameters:</p> Name Type Description Default <code>codec</code> <code>BinaryHeaderCodec</code> <p>Binary codec for deserialization</p> required <code>data</code> <code>bytes</code> <p>Binary data containing serialized ArrayInfo</p> required <code>offset</code> <code>int</code> <p>Starting offset in data</p> <code>0</code> <p>Returns:</p> Type Description <code>Tuple[ArrayInfo, int]</code> <p>Tuple of (ArrayInfo instance, bytes consumed)</p> <p>Raises:</p> Type Description <code>HeaderSerializationError</code> <p>If data is invalid</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>@classmethod\ndef deserialize(cls, codec: BinaryHeaderCodec, data: bytes, offset: int = 0) -&gt; Tuple[\"ArrayInfo\", int]:\n    \"\"\"Deserialize ArrayInfo from binary data.\n\n    Args:\n        codec: Binary codec for deserialization\n        data: Binary data containing serialized ArrayInfo\n        offset: Starting offset in data\n\n    Returns:\n        Tuple of (ArrayInfo instance, bytes consumed)\n\n    Raises:\n        HeaderSerializationError: If data is invalid\n    \"\"\"\n    current_offset = offset\n\n    # Read name\n    name, name_bytes = codec.unpack_string(data[current_offset:])\n    current_offset += name_bytes\n\n    # Read length\n    length = codec.unpack_uint64(data[current_offset : current_offset + 8])\n    current_offset += 8\n\n    # Read dtype\n    dtype_value = codec.unpack_uint32(data[current_offset : current_offset + 4])\n    current_offset += 4\n\n    try:\n        dtype = ArrayDType(dtype_value)\n    except ValueError:\n        raise HeaderSerializationError(f\"Invalid ArrayDType value: {dtype_value}\")\n\n    # Read optional shape\n    has_shape = codec.unpack_uint8(data[current_offset : current_offset + 1])\n    current_offset += 1\n\n    shape = None\n    if has_shape:\n        shape_dims = codec.unpack_uint32(data[current_offset : current_offset + 4])\n        current_offset += 4\n\n        shape = []\n        for _ in range(shape_dims):\n            dim = codec.unpack_uint32(data[current_offset : current_offset + 4])\n            shape.append(dim)\n            current_offset += 4\n        shape = tuple(shape)\n\n    array_info = cls(name=name, length=length, dtype=dtype, shape=shape)\n    bytes_consumed = current_offset - offset\n\n    return array_info, bytes_consumed\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.ArrayInfo.serialize","title":"<code>serialize(codec)</code>","text":"<p>Serialize this ArrayInfo to binary format.</p> <p>Parameters:</p> Name Type Description Default <code>codec</code> <code>BinaryHeaderCodec</code> <p>Binary codec for serialization</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>Binary representation following SCDL schema</p> <p>Raises:</p> Type Description <code>HeaderSerializationError</code> <p>If validation fails</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def serialize(self, codec: BinaryHeaderCodec) -&gt; bytes:\n    \"\"\"Serialize this ArrayInfo to binary format.\n\n    Args:\n        codec: Binary codec for serialization\n\n    Returns:\n        Binary representation following SCDL schema\n\n    Raises:\n        HeaderSerializationError: If validation fails\n    \"\"\"\n    # Validate before serialization (per schema requirements)\n    self._validate()\n\n    data = b\"\"\n\n    # name_len + name\n    data += codec.pack_string(self.name)\n\n    # length (uint64)\n    data += codec.pack_uint64(self.length)\n\n    # dtype (uint32 enum value)\n    data += codec.pack_uint32(int(self.dtype))\n\n    # has_shape + optional shape data\n    if self.shape is not None:\n        data += codec.pack_uint8(1)  # has_shape = true\n        data += codec.pack_uint32(len(self.shape))  # shape_dims\n        for dim in self.shape:\n            data += codec.pack_uint32(dim)  # shape array\n    else:\n        data += codec.pack_uint8(0)  # has_shape = false\n\n    return data\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.FeatureIndexInfo","title":"<code>FeatureIndexInfo</code>","text":"<p>Information about a feature index in the SCDL archive.</p> <p>Feature indices provide fast lookups for specific features in the data. As specified in the schema, each FeatureIndex may optionally store a header.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>class FeatureIndexInfo:\n    \"\"\"Information about a feature index in the SCDL archive.\n\n    Feature indices provide fast lookups for specific features in the data.\n    As specified in the schema, each FeatureIndex may optionally store a header.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        length: int,\n        dtype: ArrayDType,\n        index_files: Optional[List[str]] = None,\n        shape: Optional[Tuple[int, ...]] = None,\n    ):\n        \"\"\"Initialize feature index information.\n\n        Args:\n            name: Name of the feature index\n            length: Number of entries in the index\n            dtype: Data type of index entries\n            index_files: List of paths to feature index files\n            shape: Optional shape for multidimensional indices\n        \"\"\"\n        self.name = name\n        self.length = length\n        self.dtype = dtype\n        self.index_files = index_files or []\n        self.shape = shape\n\n    def serialize(self, codec: BinaryHeaderCodec) -&gt; bytes:\n        \"\"\"Serialize this FeatureIndexInfo to binary format.\n\n        Args:\n            codec: Binary codec for serialization\n\n        Returns:\n            Binary representation following SCDL schema\n\n        Raises:\n            HeaderSerializationError: If validation fails\n        \"\"\"\n        # Validate before serialization\n        self._validate()\n\n        data = b\"\"\n\n        # name_len + name\n        data += codec.pack_string(self.name)\n\n        # length (uint64)\n        data += codec.pack_uint64(self.length)\n\n        # dtype (uint32 enum value)\n        data += codec.pack_uint32(int(self.dtype))\n\n        # index_files_count + index_files\n        data += codec.pack_uint32(len(self.index_files))\n        for file_path in self.index_files:\n            data += codec.pack_string(file_path)\n\n        # has_shape + optional shape data\n        if self.shape is not None:\n            data += codec.pack_uint8(1)  # has_shape = true\n            data += codec.pack_uint32(len(self.shape))  # shape_dims\n            for dim in self.shape:\n                data += codec.pack_uint32(dim)  # shape array\n        else:\n            data += codec.pack_uint8(0)  # has_shape = false\n\n        return data\n\n    @classmethod\n    def deserialize(cls, codec: BinaryHeaderCodec, data: bytes, offset: int = 0) -&gt; Tuple[\"FeatureIndexInfo\", int]:\n        \"\"\"Deserialize FeatureIndexInfo from binary data.\n\n        Args:\n            codec: Binary codec for deserialization\n            data: Binary data containing serialized FeatureIndexInfo\n            offset: Starting offset in data\n\n        Returns:\n            Tuple of (FeatureIndexInfo instance, bytes consumed)\n\n        Raises:\n            HeaderSerializationError: If data is invalid\n        \"\"\"\n        current_offset = offset\n\n        # Read name\n        name, name_bytes = codec.unpack_string(data[current_offset:])\n        current_offset += name_bytes\n\n        # Read length\n        length = codec.unpack_uint64(data[current_offset : current_offset + 8])\n        current_offset += 8\n\n        # Read dtype\n        dtype_value = codec.unpack_uint32(data[current_offset : current_offset + 4])\n        current_offset += 4\n\n        try:\n            dtype = ArrayDType(dtype_value)\n        except ValueError:\n            raise HeaderSerializationError(f\"Invalid ArrayDType value in FeatureIndex: {dtype_value}\")\n\n        # Read index files\n        files_count = codec.unpack_uint32(data[current_offset : current_offset + 4])\n        current_offset += 4\n\n        index_files = []\n        for _ in range(files_count):\n            file_path, file_bytes = codec.unpack_string(data[current_offset:])\n            index_files.append(file_path)\n            current_offset += file_bytes\n\n        # Read optional shape\n        has_shape = codec.unpack_uint8(data[current_offset : current_offset + 1])\n        current_offset += 1\n\n        shape = None\n        if has_shape:\n            shape_dims = codec.unpack_uint32(data[current_offset : current_offset + 4])\n            current_offset += 4\n\n            shape = []\n            for _ in range(shape_dims):\n                dim = codec.unpack_uint32(data[current_offset : current_offset + 4])\n                shape.append(dim)\n                current_offset += 4\n            shape = tuple(shape)\n\n        feature_index = cls(name=name, length=length, dtype=dtype, index_files=index_files, shape=shape)\n        bytes_consumed = current_offset - offset\n\n        return feature_index, bytes_consumed\n\n    def _validate(self) -&gt; None:\n        \"\"\"Validate FeatureIndexInfo according to SCDL schema requirements.\n\n        Raises:\n            HeaderSerializationError: If validation fails\n        \"\"\"\n        # Schema requirement: All string lengths must be &gt; 0\n        if not self.name or len(self.name.strip()) == 0:\n            raise HeaderSerializationError(\"FeatureIndex name cannot be empty (schema requirement)\")\n\n        # Validate index files\n        for i, file_path in enumerate(self.index_files):\n            if not file_path or len(file_path.strip()) == 0:\n                raise HeaderSerializationError(f\"FeatureIndex file path {i} cannot be empty\")\n\n        # Additional reasonable validations\n        if self.length &lt; 0:\n            raise HeaderSerializationError(f\"FeatureIndex length cannot be negative: {self.length}\")\n\n        if self.shape is not None:\n            if len(self.shape) == 0:\n                raise HeaderSerializationError(\"FeatureIndex shape cannot be empty when specified\")\n            for i, dim in enumerate(self.shape):\n                if dim &lt;= 0:\n                    raise HeaderSerializationError(f\"FeatureIndex shape dimension {i} must be positive: {dim}\")\n\n        # Validate UTF-8 encoding\n        try:\n            self.name.encode(\"utf-8\")\n            for file_path in self.index_files:\n                file_path.encode(\"utf-8\")\n        except UnicodeEncodeError as e:\n            raise HeaderSerializationError(f\"FeatureIndex contains invalid UTF-8: {e}\")\n\n    def calculate_size(self) -&gt; int:\n        \"\"\"Calculate the serialized size of this FeatureIndexInfo in bytes.\"\"\"\n        # name_len (4) + name length + length (8) + dtype (4) + files_count (4)\n        size = 4 + len(self.name.encode(\"utf-8\")) + 8 + 4 + 4\n\n        # Add size for each file path\n        for file_path in self.index_files:\n            size += 4 + len(file_path.encode(\"utf-8\"))  # len + content\n\n        # has_shape (1)\n        size += 1\n\n        if self.shape is not None:\n            # shape_dims (4) + shape array (4 * dimensions)\n            size += 4 + (4 * len(self.shape))\n\n        return size\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a human-readable description of the feature index info.\n\n        Returns:\n            str: Summary including name, length, dtype, file count, and optional shape.\n        \"\"\"\n        shape_str = f\", shape={self.shape}\" if self.shape else \"\"\n        files_str = f\", files={len(self.index_files)}\"\n        return f\"FeatureIndexInfo(name='{self.name}', length={self.length}, dtype={self.dtype.name}{files_str}{shape_str})\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return a developer-focused representation of the feature index info.\n\n        Returns:\n            str: Representation mirroring ``__str__`` for succinct debugging.\n        \"\"\"\n        return self.__str__()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.FeatureIndexInfo.__init__","title":"<code>__init__(name, length, dtype, index_files=None, shape=None)</code>","text":"<p>Initialize feature index information.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the feature index</p> required <code>length</code> <code>int</code> <p>Number of entries in the index</p> required <code>dtype</code> <code>ArrayDType</code> <p>Data type of index entries</p> required <code>index_files</code> <code>Optional[List[str]]</code> <p>List of paths to feature index files</p> <code>None</code> <code>shape</code> <code>Optional[Tuple[int, ...]]</code> <p>Optional shape for multidimensional indices</p> <code>None</code> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    length: int,\n    dtype: ArrayDType,\n    index_files: Optional[List[str]] = None,\n    shape: Optional[Tuple[int, ...]] = None,\n):\n    \"\"\"Initialize feature index information.\n\n    Args:\n        name: Name of the feature index\n        length: Number of entries in the index\n        dtype: Data type of index entries\n        index_files: List of paths to feature index files\n        shape: Optional shape for multidimensional indices\n    \"\"\"\n    self.name = name\n    self.length = length\n    self.dtype = dtype\n    self.index_files = index_files or []\n    self.shape = shape\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.FeatureIndexInfo.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a developer-focused representation of the feature index info.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Representation mirroring <code>__str__</code> for succinct debugging.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return a developer-focused representation of the feature index info.\n\n    Returns:\n        str: Representation mirroring ``__str__`` for succinct debugging.\n    \"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.FeatureIndexInfo.__str__","title":"<code>__str__()</code>","text":"<p>Return a human-readable description of the feature index info.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Summary including name, length, dtype, file count, and optional shape.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a human-readable description of the feature index info.\n\n    Returns:\n        str: Summary including name, length, dtype, file count, and optional shape.\n    \"\"\"\n    shape_str = f\", shape={self.shape}\" if self.shape else \"\"\n    files_str = f\", files={len(self.index_files)}\"\n    return f\"FeatureIndexInfo(name='{self.name}', length={self.length}, dtype={self.dtype.name}{files_str}{shape_str})\"\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.FeatureIndexInfo.calculate_size","title":"<code>calculate_size()</code>","text":"<p>Calculate the serialized size of this FeatureIndexInfo in bytes.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def calculate_size(self) -&gt; int:\n    \"\"\"Calculate the serialized size of this FeatureIndexInfo in bytes.\"\"\"\n    # name_len (4) + name length + length (8) + dtype (4) + files_count (4)\n    size = 4 + len(self.name.encode(\"utf-8\")) + 8 + 4 + 4\n\n    # Add size for each file path\n    for file_path in self.index_files:\n        size += 4 + len(file_path.encode(\"utf-8\"))  # len + content\n\n    # has_shape (1)\n    size += 1\n\n    if self.shape is not None:\n        # shape_dims (4) + shape array (4 * dimensions)\n        size += 4 + (4 * len(self.shape))\n\n    return size\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.FeatureIndexInfo.deserialize","title":"<code>deserialize(codec, data, offset=0)</code>  <code>classmethod</code>","text":"<p>Deserialize FeatureIndexInfo from binary data.</p> <p>Parameters:</p> Name Type Description Default <code>codec</code> <code>BinaryHeaderCodec</code> <p>Binary codec for deserialization</p> required <code>data</code> <code>bytes</code> <p>Binary data containing serialized FeatureIndexInfo</p> required <code>offset</code> <code>int</code> <p>Starting offset in data</p> <code>0</code> <p>Returns:</p> Type Description <code>Tuple[FeatureIndexInfo, int]</code> <p>Tuple of (FeatureIndexInfo instance, bytes consumed)</p> <p>Raises:</p> Type Description <code>HeaderSerializationError</code> <p>If data is invalid</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>@classmethod\ndef deserialize(cls, codec: BinaryHeaderCodec, data: bytes, offset: int = 0) -&gt; Tuple[\"FeatureIndexInfo\", int]:\n    \"\"\"Deserialize FeatureIndexInfo from binary data.\n\n    Args:\n        codec: Binary codec for deserialization\n        data: Binary data containing serialized FeatureIndexInfo\n        offset: Starting offset in data\n\n    Returns:\n        Tuple of (FeatureIndexInfo instance, bytes consumed)\n\n    Raises:\n        HeaderSerializationError: If data is invalid\n    \"\"\"\n    current_offset = offset\n\n    # Read name\n    name, name_bytes = codec.unpack_string(data[current_offset:])\n    current_offset += name_bytes\n\n    # Read length\n    length = codec.unpack_uint64(data[current_offset : current_offset + 8])\n    current_offset += 8\n\n    # Read dtype\n    dtype_value = codec.unpack_uint32(data[current_offset : current_offset + 4])\n    current_offset += 4\n\n    try:\n        dtype = ArrayDType(dtype_value)\n    except ValueError:\n        raise HeaderSerializationError(f\"Invalid ArrayDType value in FeatureIndex: {dtype_value}\")\n\n    # Read index files\n    files_count = codec.unpack_uint32(data[current_offset : current_offset + 4])\n    current_offset += 4\n\n    index_files = []\n    for _ in range(files_count):\n        file_path, file_bytes = codec.unpack_string(data[current_offset:])\n        index_files.append(file_path)\n        current_offset += file_bytes\n\n    # Read optional shape\n    has_shape = codec.unpack_uint8(data[current_offset : current_offset + 1])\n    current_offset += 1\n\n    shape = None\n    if has_shape:\n        shape_dims = codec.unpack_uint32(data[current_offset : current_offset + 4])\n        current_offset += 4\n\n        shape = []\n        for _ in range(shape_dims):\n            dim = codec.unpack_uint32(data[current_offset : current_offset + 4])\n            shape.append(dim)\n            current_offset += 4\n        shape = tuple(shape)\n\n    feature_index = cls(name=name, length=length, dtype=dtype, index_files=index_files, shape=shape)\n    bytes_consumed = current_offset - offset\n\n    return feature_index, bytes_consumed\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.FeatureIndexInfo.serialize","title":"<code>serialize(codec)</code>","text":"<p>Serialize this FeatureIndexInfo to binary format.</p> <p>Parameters:</p> Name Type Description Default <code>codec</code> <code>BinaryHeaderCodec</code> <p>Binary codec for serialization</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>Binary representation following SCDL schema</p> <p>Raises:</p> Type Description <code>HeaderSerializationError</code> <p>If validation fails</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def serialize(self, codec: BinaryHeaderCodec) -&gt; bytes:\n    \"\"\"Serialize this FeatureIndexInfo to binary format.\n\n    Args:\n        codec: Binary codec for serialization\n\n    Returns:\n        Binary representation following SCDL schema\n\n    Raises:\n        HeaderSerializationError: If validation fails\n    \"\"\"\n    # Validate before serialization\n    self._validate()\n\n    data = b\"\"\n\n    # name_len + name\n    data += codec.pack_string(self.name)\n\n    # length (uint64)\n    data += codec.pack_uint64(self.length)\n\n    # dtype (uint32 enum value)\n    data += codec.pack_uint32(int(self.dtype))\n\n    # index_files_count + index_files\n    data += codec.pack_uint32(len(self.index_files))\n    for file_path in self.index_files:\n        data += codec.pack_string(file_path)\n\n    # has_shape + optional shape data\n    if self.shape is not None:\n        data += codec.pack_uint8(1)  # has_shape = true\n        data += codec.pack_uint32(len(self.shape))  # shape_dims\n        for dim in self.shape:\n            data += codec.pack_uint32(dim)  # shape array\n    else:\n        data += codec.pack_uint8(0)  # has_shape = false\n\n    return data\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.HeaderReader","title":"<code>HeaderReader</code>","text":"<p>Optimized reader for SCDL headers with caching and validation.</p> <p>Provides efficient access to header information without full deserialization when only specific fields are needed.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>class HeaderReader:\n    \"\"\"Optimized reader for SCDL headers with caching and validation.\n\n    Provides efficient access to header information without full deserialization\n    when only specific fields are needed.\n    \"\"\"\n\n    def __init__(self, file_path: str):\n        \"\"\"Initialize with header file path.\"\"\"\n        self.file_path = file_path\n        self._cached_header = None\n        self._core_header_cached = False\n        self._magic = None\n        self._version = None\n        self._backend = None\n        self._array_count = None\n\n    def validate_magic(self) -&gt; bool:\n        \"\"\"Quickly validate magic number without full deserialization.\"\"\"\n        if self._magic is None:\n            with open(self.file_path, \"rb\") as f:\n                self._magic = f.read(4)\n        return self._magic == SCDL_MAGIC_NUMBER\n\n    def get_version(self) -&gt; SCDLVersion:\n        \"\"\"Get version information quickly.\"\"\"\n        self._ensure_core_header()\n        return self._version\n\n    def get_backend(self) -&gt; Backend:\n        \"\"\"Get backend information quickly.\"\"\"\n        self._ensure_core_header()\n        return self._backend\n\n    def get_array_count(self) -&gt; int:\n        \"\"\"Get array count quickly.\"\"\"\n        self._ensure_core_header()\n        return self._array_count\n\n    def get_full_header(self) -&gt; SCDLHeader:\n        \"\"\"Get complete header (cached after first access).\"\"\"\n        if self._cached_header is None:\n            self._cached_header = SCDLHeader.load(self.file_path)\n        return self._cached_header\n\n    def _ensure_core_header(self):\n        \"\"\"Read core header fields if not cached.\"\"\"\n        if self._core_header_cached:\n            return\n\n        codec = BinaryHeaderCodec(Endianness.NETWORK)\n        with open(self.file_path, \"rb\") as f:\n            core_data = f.read(SCDLHeader.CORE_HEADER_SIZE)\n\n        if len(core_data) &lt; SCDLHeader.CORE_HEADER_SIZE:\n            raise HeaderSerializationError(\"Invalid header file\")\n\n        offset = 0\n\n        # Magic number\n        self._magic = core_data[offset : offset + 4]\n        offset += 4\n\n        # Version\n        version = SCDLVersion()\n        version.major = codec.unpack_uint8(core_data[offset : offset + 1])\n        offset += 1\n        version.minor = codec.unpack_uint8(core_data[offset : offset + 1])\n        offset += 1\n        version.point = codec.unpack_uint8(core_data[offset : offset + 1])\n        offset += 1\n        self._version = version\n\n        # Skip endianness\n        offset += 1\n\n        # Backend\n        backend_value = codec.unpack_uint32(core_data[offset : offset + 4])\n        self._backend = Backend(backend_value)\n        offset += 4\n\n        # Array count\n        self._array_count = codec.unpack_uint32(core_data[offset : offset + 4])\n\n        self._core_header_cached = True\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.HeaderReader.__init__","title":"<code>__init__(file_path)</code>","text":"<p>Initialize with header file path.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def __init__(self, file_path: str):\n    \"\"\"Initialize with header file path.\"\"\"\n    self.file_path = file_path\n    self._cached_header = None\n    self._core_header_cached = False\n    self._magic = None\n    self._version = None\n    self._backend = None\n    self._array_count = None\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.HeaderReader.get_array_count","title":"<code>get_array_count()</code>","text":"<p>Get array count quickly.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def get_array_count(self) -&gt; int:\n    \"\"\"Get array count quickly.\"\"\"\n    self._ensure_core_header()\n    return self._array_count\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.HeaderReader.get_backend","title":"<code>get_backend()</code>","text":"<p>Get backend information quickly.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def get_backend(self) -&gt; Backend:\n    \"\"\"Get backend information quickly.\"\"\"\n    self._ensure_core_header()\n    return self._backend\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.HeaderReader.get_full_header","title":"<code>get_full_header()</code>","text":"<p>Get complete header (cached after first access).</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def get_full_header(self) -&gt; SCDLHeader:\n    \"\"\"Get complete header (cached after first access).\"\"\"\n    if self._cached_header is None:\n        self._cached_header = SCDLHeader.load(self.file_path)\n    return self._cached_header\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.HeaderReader.get_version","title":"<code>get_version()</code>","text":"<p>Get version information quickly.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def get_version(self) -&gt; SCDLVersion:\n    \"\"\"Get version information quickly.\"\"\"\n    self._ensure_core_header()\n    return self._version\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.HeaderReader.validate_magic","title":"<code>validate_magic()</code>","text":"<p>Quickly validate magic number without full deserialization.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def validate_magic(self) -&gt; bool:\n    \"\"\"Quickly validate magic number without full deserialization.\"\"\"\n    if self._magic is None:\n        with open(self.file_path, \"rb\") as f:\n            self._magic = f.read(4)\n    return self._magic == SCDL_MAGIC_NUMBER\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.SCDLHeader","title":"<code>SCDLHeader</code>","text":"<p>Header for a SCDL archive following the official schema specification.</p> <p>Contains metadata about the archive including version, backend, and array information. The header is stored in binary format and is not human-readable by design.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>class SCDLHeader:\n    \"\"\"Header for a SCDL archive following the official schema specification.\n\n    Contains metadata about the archive including version, backend, and array information.\n    The header is stored in binary format and is not human-readable by design.\n    \"\"\"\n\n    # Core header size is fixed at 16 bytes\n    CORE_HEADER_SIZE = 16\n\n    def __init__(\n        self,\n        version: Optional[SCDLVersion] = None,\n        backend: Backend = Backend.MEMMAP_V0,\n        arrays: Optional[List[ArrayInfo]] = None,\n        feature_indices: Optional[List[FeatureIndexInfo]] = None,\n    ):\n        \"\"\"Initialize SCDL header.\n\n        Args:\n            version: SCDL schema version (defaults to current version)\n            backend: Storage backend type\n            arrays: List of arrays in the archive\n            feature_indices: Optional list of feature indices in the archive\n        \"\"\"\n        self.version = version or CurrentSCDLVersion()\n        self.endianness = Endianness.NETWORK  # Always network byte order per spec\n        self.backend = backend\n        self.arrays = arrays or []\n        self.feature_indices = feature_indices or []\n\n        # Create codec with network byte order\n        self._codec = BinaryHeaderCodec(self.endianness)\n\n    def add_array(self, array_info: ArrayInfo) -&gt; None:\n        \"\"\"Add an array to the header.\"\"\"\n        self.arrays.append(array_info)\n\n    def get_array(self, name: str) -&gt; Optional[ArrayInfo]:\n        \"\"\"Get array info by name.\"\"\"\n        for array in self.arrays:\n            if array.name == name:\n                return array\n        return None\n\n    def remove_array(self, name: str) -&gt; bool:\n        \"\"\"Remove array by name. Returns True if found and removed.\"\"\"\n        for i, array in enumerate(self.arrays):\n            if array.name == name:\n                del self.arrays[i]\n                return True\n        return False\n\n    def add_feature_index(self, feature_index: FeatureIndexInfo) -&gt; None:\n        \"\"\"Add a feature index to the header.\"\"\"\n        self.feature_indices.append(feature_index)\n\n    def get_feature_index(self, name: str) -&gt; Optional[FeatureIndexInfo]:\n        \"\"\"Get feature index info by name.\"\"\"\n        for feature_index in self.feature_indices:\n            if feature_index.name == name:\n                return feature_index\n        return None\n\n    def remove_feature_index(self, name: str) -&gt; bool:\n        \"\"\"Remove feature index by name. Returns True if found and removed.\"\"\"\n        for i, feature_index in enumerate(self.feature_indices):\n            if feature_index.name == name:\n                del self.feature_indices[i]\n                return True\n        return False\n\n    def serialize(self) -&gt; bytes:\n        \"\"\"Serialize the header to binary format following SCDL schema.\n\n        Returns:\n            Binary representation of the complete header\n\n        Raises:\n            HeaderSerializationError: If serialization fails\n        \"\"\"\n        try:\n            # Validate header before serialization\n            self.validate()\n\n            data = b\"\"\n\n            # Core Header (16 bytes fixed)\n            # Magic number (4 bytes)\n            data += SCDL_MAGIC_NUMBER\n\n            # Version (3 bytes: major, minor, point)\n            data += self._codec.pack_uint8(self.version.major)\n            data += self._codec.pack_uint8(self.version.minor)\n            data += self._codec.pack_uint8(self.version.point)\n\n            # Endianness (1 byte) - always NETWORK per spec\n            data += self._codec.pack_uint8(1)  # NETWORK = 1\n\n            # Backend (4 bytes)\n            data += self._codec.pack_uint32(int(self.backend))\n\n            # Array count (4 bytes) - schema requires this matches actual descriptors\n            array_count = len(self.arrays)\n            data += self._codec.pack_uint32(array_count)\n\n            # Array descriptors (variable size)\n            for array in self.arrays:\n                data += array.serialize(self._codec)\n\n            # Feature indices (optional extension after arrays)\n            # feature_index_count (4 bytes)\n            data += self._codec.pack_uint32(len(self.feature_indices))\n\n            # Feature index descriptors (variable size)\n            for feature_index in self.feature_indices:\n                data += feature_index.serialize(self._codec)\n\n            return data\n\n        except Exception as e:\n            raise HeaderSerializationError(f\"Failed to serialize SCDL header: {e}\")\n\n    @classmethod\n    def deserialize(cls, data: bytes) -&gt; \"SCDLHeader\":\n        \"\"\"Deserialize header from binary data.\n\n        Args:\n            data: Binary data containing SCDL header\n\n        Returns:\n            SCDLHeader instance\n\n        Raises:\n            HeaderSerializationError: If deserialization fails or data is invalid\n        \"\"\"\n        if len(data) &lt; cls.CORE_HEADER_SIZE:\n            raise HeaderSerializationError(\n                f\"Header data too short: {len(data)} bytes &lt; {cls.CORE_HEADER_SIZE} bytes minimum\"\n            )\n\n        # Use network byte order for reading\n        codec = BinaryHeaderCodec(Endianness.NETWORK)\n        offset = 0\n\n        try:\n            # Validate magic number\n            magic = data[offset : offset + 4]\n            if magic != SCDL_MAGIC_NUMBER:\n                raise HeaderSerializationError(f\"Invalid magic number: {magic} != {SCDL_MAGIC_NUMBER}\")\n            offset += 4\n\n            # Read version\n            version_major = codec.unpack_uint8(data[offset : offset + 1])\n            offset += 1\n            version_minor = codec.unpack_uint8(data[offset : offset + 1])\n            offset += 1\n            version_point = codec.unpack_uint8(data[offset : offset + 1])\n            offset += 1\n\n            version = SCDLVersion()\n            version.major = version_major\n            version.minor = version_minor\n            version.point = version_point\n\n            # Read and validate endianness\n            endianness_value = codec.unpack_uint8(data[offset : offset + 1])\n            offset += 1\n            if endianness_value != 1:  # Must be NETWORK\n                raise HeaderSerializationError(f\"Invalid endianness: {endianness_value} (must be 1 for NETWORK)\")\n\n            # Read backend\n            backend_value = codec.unpack_uint32(data[offset : offset + 4])\n            offset += 4\n            try:\n                backend = Backend(backend_value)\n            except ValueError:\n                raise HeaderSerializationError(f\"Invalid backend value: {backend_value}\")\n\n            # Read array count\n            array_count = codec.unpack_uint32(data[offset : offset + 4])\n            offset += 4\n\n            # Read array descriptors\n            arrays = []\n            for i in range(array_count):\n                if offset &gt;= len(data):\n                    raise HeaderSerializationError(f\"Unexpected end of data while reading array {i}\")\n\n                array_info, bytes_consumed = ArrayInfo.deserialize(codec, data, offset)\n                arrays.append(array_info)\n                offset += bytes_consumed\n\n            # Read feature indices (optional, for backwards compatibility)\n            feature_indices = []\n            if offset &lt; len(data):\n                # Check if we have enough data for feature index count\n                if offset + 4 &lt;= len(data):\n                    feature_index_count = codec.unpack_uint32(data[offset : offset + 4])\n                    offset += 4\n\n                    # Read feature index descriptors\n                    for i in range(feature_index_count):\n                        if offset &gt;= len(data):\n                            raise HeaderSerializationError(f\"Unexpected end of data while reading feature index {i}\")\n\n                        feature_index, bytes_consumed = FeatureIndexInfo.deserialize(codec, data, offset)\n                        feature_indices.append(feature_index)\n                        offset += bytes_consumed\n\n            header = cls(version=version, backend=backend, arrays=arrays, feature_indices=feature_indices)\n            return header\n\n        except HeaderSerializationError:\n            raise\n        except Exception as e:\n            raise HeaderSerializationError(f\"Failed to deserialize SCDL header: {e}\")\n\n    def save(self, file_path: str) -&gt; None:\n        \"\"\"Save the header to a binary file.\n\n        Args:\n            file_path: Path to save the header file\n\n        Raises:\n            HeaderSerializationError: If saving fails\n        \"\"\"\n        try:\n            with open(file_path, \"wb\") as f:\n                f.write(self.serialize())\n        except Exception as e:\n            raise HeaderSerializationError(f\"Failed to save header to {file_path}: {e}\")\n\n    @classmethod\n    def load(cls, file_path: str) -&gt; \"SCDLHeader\":\n        \"\"\"Load header from a binary file.\n\n        Args:\n            file_path: Path to the header file\n\n        Returns:\n            SCDLHeader instance\n\n        Raises:\n            HeaderSerializationError: If loading fails\n        \"\"\"\n        try:\n            with open(file_path, \"rb\") as f:\n                data = f.read()\n            return cls.deserialize(data)\n        except FileNotFoundError:\n            raise HeaderSerializationError(f\"Header file not found: {file_path}\")\n        except Exception as e:\n            raise HeaderSerializationError(f\"Failed to load header from {file_path}: {e}\")\n\n    def calculate_total_size(self) -&gt; int:\n        \"\"\"Calculate the total serialized size of the header in bytes.\"\"\"\n        total_size = self.CORE_HEADER_SIZE\n\n        # Array descriptors\n        for array in self.arrays:\n            total_size += array.calculate_size()\n\n        # Feature index count (4 bytes) + feature index descriptors\n        total_size += 4\n        for feature_index in self.feature_indices:\n            total_size += feature_index.calculate_size()\n\n        return total_size\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate the header for consistency and correctness.\n\n        Raises:\n            HeaderSerializationError: If validation fails\n        \"\"\"\n        # Check version compatibility\n        current_version = CurrentSCDLVersion()\n        if self.version.major &gt; current_version.major:\n            raise HeaderSerializationError(f\"Unsupported version: {self.version} &gt; {current_version}\")\n\n        # Check array names are unique\n        names = [array.name for array in self.arrays]\n        if len(names) != len(set(names)):\n            raise HeaderSerializationError(\"Duplicate array names found\")\n\n        # Check array names are valid\n        for array in self.arrays:\n            if not array.name or not array.name.strip():\n                raise HeaderSerializationError(\"Empty array name found\")\n            if len(array.name.encode(\"utf-8\")) &gt; 1024:  # Reasonable limit\n                raise HeaderSerializationError(f\"Array name too long: {array.name}\")\n\n        # Check feature index names are unique\n        feature_names = [fi.name for fi in self.feature_indices]\n        if len(feature_names) != len(set(feature_names)):\n            raise HeaderSerializationError(\"Duplicate feature index names found\")\n\n        # Check feature index names are valid\n        for feature_index in self.feature_indices:\n            if not feature_index.name or not feature_index.name.strip():\n                raise HeaderSerializationError(\"Empty feature index name found\")\n            if len(feature_index.name.encode(\"utf-8\")) &gt; 1024:  # Reasonable limit\n                raise HeaderSerializationError(f\"Feature index name too long: {feature_index.name}\")\n\n        # Check for name conflicts between arrays and feature indices\n        all_names = names + feature_names\n        if len(all_names) != len(set(all_names)):\n            raise HeaderSerializationError(\"Name conflicts between arrays and feature indices\")\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a human-readable string representation of the header.\"\"\"\n        return (\n            f\"SCDLHeader(version={self.version}, backend={self.backend.name}, \"\n            f\"arrays={len(self.arrays)}, feature_indices={len(self.feature_indices)})\"\n        )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return a developer-focused representation of the header.\n\n        Returns:\n            str: Representation mirroring ``__str__`` for succinct debugging.\n        \"\"\"\n        return self.__str__()\n\n    def to_json(self) -&gt; str:\n        \"\"\"Return a JSON string representation of the header.\n\n        Note: This is for debugging/inspection only, not for serialization.\n        \"\"\"\n\n        def default(o):\n            if hasattr(o, \"name\"):\n                return o.name\n            if hasattr(o, \"__dict__\"):\n                return o.__dict__\n            return str(o)\n\n        data = {\n            \"version\": {\"major\": self.version.major, \"minor\": self.version.minor, \"point\": self.version.point},\n            \"endianness\": self.endianness.name,\n            \"backend\": self.backend.name,\n            \"arrays\": [\n                {\"name\": array.name, \"length\": array.length, \"dtype\": array.dtype.name, \"shape\": array.shape}\n                for array in self.arrays\n            ],\n            \"feature_indices\": [\n                {\n                    \"name\": fi.name,\n                    \"length\": fi.length,\n                    \"dtype\": fi.dtype.name,\n                    \"index_files\": fi.index_files,\n                    \"shape\": fi.shape,\n                }\n                for fi in self.feature_indices\n            ],\n        }\n\n        return json.dumps(data, indent=2, default=default)\n\n    def to_yaml(self) -&gt; str:\n        \"\"\"Return a YAML string representation of the header.\n\n        Note: This is for debugging/inspection only, not for serialization.\n        \"\"\"\n        try:\n            import yaml\n        except ImportError:\n            raise RuntimeError(\"PyYAML is required for YAML serialization\")\n\n        data = {\n            \"version\": f\"{self.version.major}.{self.version.minor}.{self.version.point}\",\n            \"endianness\": self.endianness.name,\n            \"backend\": self.backend.name,\n            \"arrays\": [\n                {\n                    \"name\": array.name,\n                    \"length\": array.length,\n                    \"dtype\": array.dtype.name,\n                    \"shape\": list(array.shape) if array.shape else None,\n                }\n                for array in self.arrays\n            ],\n            \"feature_indices\": [\n                {\n                    \"name\": fi.name,\n                    \"length\": fi.length,\n                    \"dtype\": fi.dtype.name,\n                    \"index_files\": fi.index_files,\n                    \"shape\": list(fi.shape) if fi.shape else None,\n                }\n                for fi in self.feature_indices\n            ],\n        }\n\n        return yaml.dump(data, default_flow_style=False)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.SCDLHeader.__init__","title":"<code>__init__(version=None, backend=Backend.MEMMAP_V0, arrays=None, feature_indices=None)</code>","text":"<p>Initialize SCDL header.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>Optional[SCDLVersion]</code> <p>SCDL schema version (defaults to current version)</p> <code>None</code> <code>backend</code> <code>Backend</code> <p>Storage backend type</p> <code>MEMMAP_V0</code> <code>arrays</code> <code>Optional[List[ArrayInfo]]</code> <p>List of arrays in the archive</p> <code>None</code> <code>feature_indices</code> <code>Optional[List[FeatureIndexInfo]]</code> <p>Optional list of feature indices in the archive</p> <code>None</code> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def __init__(\n    self,\n    version: Optional[SCDLVersion] = None,\n    backend: Backend = Backend.MEMMAP_V0,\n    arrays: Optional[List[ArrayInfo]] = None,\n    feature_indices: Optional[List[FeatureIndexInfo]] = None,\n):\n    \"\"\"Initialize SCDL header.\n\n    Args:\n        version: SCDL schema version (defaults to current version)\n        backend: Storage backend type\n        arrays: List of arrays in the archive\n        feature_indices: Optional list of feature indices in the archive\n    \"\"\"\n    self.version = version or CurrentSCDLVersion()\n    self.endianness = Endianness.NETWORK  # Always network byte order per spec\n    self.backend = backend\n    self.arrays = arrays or []\n    self.feature_indices = feature_indices or []\n\n    # Create codec with network byte order\n    self._codec = BinaryHeaderCodec(self.endianness)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.SCDLHeader.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a developer-focused representation of the header.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Representation mirroring <code>__str__</code> for succinct debugging.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return a developer-focused representation of the header.\n\n    Returns:\n        str: Representation mirroring ``__str__`` for succinct debugging.\n    \"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.SCDLHeader.__str__","title":"<code>__str__()</code>","text":"<p>Return a human-readable string representation of the header.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a human-readable string representation of the header.\"\"\"\n    return (\n        f\"SCDLHeader(version={self.version}, backend={self.backend.name}, \"\n        f\"arrays={len(self.arrays)}, feature_indices={len(self.feature_indices)})\"\n    )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.SCDLHeader.add_array","title":"<code>add_array(array_info)</code>","text":"<p>Add an array to the header.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def add_array(self, array_info: ArrayInfo) -&gt; None:\n    \"\"\"Add an array to the header.\"\"\"\n    self.arrays.append(array_info)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.SCDLHeader.add_feature_index","title":"<code>add_feature_index(feature_index)</code>","text":"<p>Add a feature index to the header.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def add_feature_index(self, feature_index: FeatureIndexInfo) -&gt; None:\n    \"\"\"Add a feature index to the header.\"\"\"\n    self.feature_indices.append(feature_index)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.SCDLHeader.calculate_total_size","title":"<code>calculate_total_size()</code>","text":"<p>Calculate the total serialized size of the header in bytes.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def calculate_total_size(self) -&gt; int:\n    \"\"\"Calculate the total serialized size of the header in bytes.\"\"\"\n    total_size = self.CORE_HEADER_SIZE\n\n    # Array descriptors\n    for array in self.arrays:\n        total_size += array.calculate_size()\n\n    # Feature index count (4 bytes) + feature index descriptors\n    total_size += 4\n    for feature_index in self.feature_indices:\n        total_size += feature_index.calculate_size()\n\n    return total_size\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.SCDLHeader.deserialize","title":"<code>deserialize(data)</code>  <code>classmethod</code>","text":"<p>Deserialize header from binary data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Binary data containing SCDL header</p> required <p>Returns:</p> Type Description <code>SCDLHeader</code> <p>SCDLHeader instance</p> <p>Raises:</p> Type Description <code>HeaderSerializationError</code> <p>If deserialization fails or data is invalid</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>@classmethod\ndef deserialize(cls, data: bytes) -&gt; \"SCDLHeader\":\n    \"\"\"Deserialize header from binary data.\n\n    Args:\n        data: Binary data containing SCDL header\n\n    Returns:\n        SCDLHeader instance\n\n    Raises:\n        HeaderSerializationError: If deserialization fails or data is invalid\n    \"\"\"\n    if len(data) &lt; cls.CORE_HEADER_SIZE:\n        raise HeaderSerializationError(\n            f\"Header data too short: {len(data)} bytes &lt; {cls.CORE_HEADER_SIZE} bytes minimum\"\n        )\n\n    # Use network byte order for reading\n    codec = BinaryHeaderCodec(Endianness.NETWORK)\n    offset = 0\n\n    try:\n        # Validate magic number\n        magic = data[offset : offset + 4]\n        if magic != SCDL_MAGIC_NUMBER:\n            raise HeaderSerializationError(f\"Invalid magic number: {magic} != {SCDL_MAGIC_NUMBER}\")\n        offset += 4\n\n        # Read version\n        version_major = codec.unpack_uint8(data[offset : offset + 1])\n        offset += 1\n        version_minor = codec.unpack_uint8(data[offset : offset + 1])\n        offset += 1\n        version_point = codec.unpack_uint8(data[offset : offset + 1])\n        offset += 1\n\n        version = SCDLVersion()\n        version.major = version_major\n        version.minor = version_minor\n        version.point = version_point\n\n        # Read and validate endianness\n        endianness_value = codec.unpack_uint8(data[offset : offset + 1])\n        offset += 1\n        if endianness_value != 1:  # Must be NETWORK\n            raise HeaderSerializationError(f\"Invalid endianness: {endianness_value} (must be 1 for NETWORK)\")\n\n        # Read backend\n        backend_value = codec.unpack_uint32(data[offset : offset + 4])\n        offset += 4\n        try:\n            backend = Backend(backend_value)\n        except ValueError:\n            raise HeaderSerializationError(f\"Invalid backend value: {backend_value}\")\n\n        # Read array count\n        array_count = codec.unpack_uint32(data[offset : offset + 4])\n        offset += 4\n\n        # Read array descriptors\n        arrays = []\n        for i in range(array_count):\n            if offset &gt;= len(data):\n                raise HeaderSerializationError(f\"Unexpected end of data while reading array {i}\")\n\n            array_info, bytes_consumed = ArrayInfo.deserialize(codec, data, offset)\n            arrays.append(array_info)\n            offset += bytes_consumed\n\n        # Read feature indices (optional, for backwards compatibility)\n        feature_indices = []\n        if offset &lt; len(data):\n            # Check if we have enough data for feature index count\n            if offset + 4 &lt;= len(data):\n                feature_index_count = codec.unpack_uint32(data[offset : offset + 4])\n                offset += 4\n\n                # Read feature index descriptors\n                for i in range(feature_index_count):\n                    if offset &gt;= len(data):\n                        raise HeaderSerializationError(f\"Unexpected end of data while reading feature index {i}\")\n\n                    feature_index, bytes_consumed = FeatureIndexInfo.deserialize(codec, data, offset)\n                    feature_indices.append(feature_index)\n                    offset += bytes_consumed\n\n        header = cls(version=version, backend=backend, arrays=arrays, feature_indices=feature_indices)\n        return header\n\n    except HeaderSerializationError:\n        raise\n    except Exception as e:\n        raise HeaderSerializationError(f\"Failed to deserialize SCDL header: {e}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.SCDLHeader.get_array","title":"<code>get_array(name)</code>","text":"<p>Get array info by name.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def get_array(self, name: str) -&gt; Optional[ArrayInfo]:\n    \"\"\"Get array info by name.\"\"\"\n    for array in self.arrays:\n        if array.name == name:\n            return array\n    return None\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.SCDLHeader.get_feature_index","title":"<code>get_feature_index(name)</code>","text":"<p>Get feature index info by name.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def get_feature_index(self, name: str) -&gt; Optional[FeatureIndexInfo]:\n    \"\"\"Get feature index info by name.\"\"\"\n    for feature_index in self.feature_indices:\n        if feature_index.name == name:\n            return feature_index\n    return None\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.SCDLHeader.load","title":"<code>load(file_path)</code>  <code>classmethod</code>","text":"<p>Load header from a binary file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the header file</p> required <p>Returns:</p> Type Description <code>SCDLHeader</code> <p>SCDLHeader instance</p> <p>Raises:</p> Type Description <code>HeaderSerializationError</code> <p>If loading fails</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>@classmethod\ndef load(cls, file_path: str) -&gt; \"SCDLHeader\":\n    \"\"\"Load header from a binary file.\n\n    Args:\n        file_path: Path to the header file\n\n    Returns:\n        SCDLHeader instance\n\n    Raises:\n        HeaderSerializationError: If loading fails\n    \"\"\"\n    try:\n        with open(file_path, \"rb\") as f:\n            data = f.read()\n        return cls.deserialize(data)\n    except FileNotFoundError:\n        raise HeaderSerializationError(f\"Header file not found: {file_path}\")\n    except Exception as e:\n        raise HeaderSerializationError(f\"Failed to load header from {file_path}: {e}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.SCDLHeader.remove_array","title":"<code>remove_array(name)</code>","text":"<p>Remove array by name. Returns True if found and removed.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def remove_array(self, name: str) -&gt; bool:\n    \"\"\"Remove array by name. Returns True if found and removed.\"\"\"\n    for i, array in enumerate(self.arrays):\n        if array.name == name:\n            del self.arrays[i]\n            return True\n    return False\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.SCDLHeader.remove_feature_index","title":"<code>remove_feature_index(name)</code>","text":"<p>Remove feature index by name. Returns True if found and removed.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def remove_feature_index(self, name: str) -&gt; bool:\n    \"\"\"Remove feature index by name. Returns True if found and removed.\"\"\"\n    for i, feature_index in enumerate(self.feature_indices):\n        if feature_index.name == name:\n            del self.feature_indices[i]\n            return True\n    return False\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.SCDLHeader.save","title":"<code>save(file_path)</code>","text":"<p>Save the header to a binary file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to save the header file</p> required <p>Raises:</p> Type Description <code>HeaderSerializationError</code> <p>If saving fails</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def save(self, file_path: str) -&gt; None:\n    \"\"\"Save the header to a binary file.\n\n    Args:\n        file_path: Path to save the header file\n\n    Raises:\n        HeaderSerializationError: If saving fails\n    \"\"\"\n    try:\n        with open(file_path, \"wb\") as f:\n            f.write(self.serialize())\n    except Exception as e:\n        raise HeaderSerializationError(f\"Failed to save header to {file_path}: {e}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.SCDLHeader.serialize","title":"<code>serialize()</code>","text":"<p>Serialize the header to binary format following SCDL schema.</p> <p>Returns:</p> Type Description <code>bytes</code> <p>Binary representation of the complete header</p> <p>Raises:</p> Type Description <code>HeaderSerializationError</code> <p>If serialization fails</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def serialize(self) -&gt; bytes:\n    \"\"\"Serialize the header to binary format following SCDL schema.\n\n    Returns:\n        Binary representation of the complete header\n\n    Raises:\n        HeaderSerializationError: If serialization fails\n    \"\"\"\n    try:\n        # Validate header before serialization\n        self.validate()\n\n        data = b\"\"\n\n        # Core Header (16 bytes fixed)\n        # Magic number (4 bytes)\n        data += SCDL_MAGIC_NUMBER\n\n        # Version (3 bytes: major, minor, point)\n        data += self._codec.pack_uint8(self.version.major)\n        data += self._codec.pack_uint8(self.version.minor)\n        data += self._codec.pack_uint8(self.version.point)\n\n        # Endianness (1 byte) - always NETWORK per spec\n        data += self._codec.pack_uint8(1)  # NETWORK = 1\n\n        # Backend (4 bytes)\n        data += self._codec.pack_uint32(int(self.backend))\n\n        # Array count (4 bytes) - schema requires this matches actual descriptors\n        array_count = len(self.arrays)\n        data += self._codec.pack_uint32(array_count)\n\n        # Array descriptors (variable size)\n        for array in self.arrays:\n            data += array.serialize(self._codec)\n\n        # Feature indices (optional extension after arrays)\n        # feature_index_count (4 bytes)\n        data += self._codec.pack_uint32(len(self.feature_indices))\n\n        # Feature index descriptors (variable size)\n        for feature_index in self.feature_indices:\n            data += feature_index.serialize(self._codec)\n\n        return data\n\n    except Exception as e:\n        raise HeaderSerializationError(f\"Failed to serialize SCDL header: {e}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.SCDLHeader.to_json","title":"<code>to_json()</code>","text":"<p>Return a JSON string representation of the header.</p> <p>Note: This is for debugging/inspection only, not for serialization.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def to_json(self) -&gt; str:\n    \"\"\"Return a JSON string representation of the header.\n\n    Note: This is for debugging/inspection only, not for serialization.\n    \"\"\"\n\n    def default(o):\n        if hasattr(o, \"name\"):\n            return o.name\n        if hasattr(o, \"__dict__\"):\n            return o.__dict__\n        return str(o)\n\n    data = {\n        \"version\": {\"major\": self.version.major, \"minor\": self.version.minor, \"point\": self.version.point},\n        \"endianness\": self.endianness.name,\n        \"backend\": self.backend.name,\n        \"arrays\": [\n            {\"name\": array.name, \"length\": array.length, \"dtype\": array.dtype.name, \"shape\": array.shape}\n            for array in self.arrays\n        ],\n        \"feature_indices\": [\n            {\n                \"name\": fi.name,\n                \"length\": fi.length,\n                \"dtype\": fi.dtype.name,\n                \"index_files\": fi.index_files,\n                \"shape\": fi.shape,\n            }\n            for fi in self.feature_indices\n        ],\n    }\n\n    return json.dumps(data, indent=2, default=default)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.SCDLHeader.to_yaml","title":"<code>to_yaml()</code>","text":"<p>Return a YAML string representation of the header.</p> <p>Note: This is for debugging/inspection only, not for serialization.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def to_yaml(self) -&gt; str:\n    \"\"\"Return a YAML string representation of the header.\n\n    Note: This is for debugging/inspection only, not for serialization.\n    \"\"\"\n    try:\n        import yaml\n    except ImportError:\n        raise RuntimeError(\"PyYAML is required for YAML serialization\")\n\n    data = {\n        \"version\": f\"{self.version.major}.{self.version.minor}.{self.version.point}\",\n        \"endianness\": self.endianness.name,\n        \"backend\": self.backend.name,\n        \"arrays\": [\n            {\n                \"name\": array.name,\n                \"length\": array.length,\n                \"dtype\": array.dtype.name,\n                \"shape\": list(array.shape) if array.shape else None,\n            }\n            for array in self.arrays\n        ],\n        \"feature_indices\": [\n            {\n                \"name\": fi.name,\n                \"length\": fi.length,\n                \"dtype\": fi.dtype.name,\n                \"index_files\": fi.index_files,\n                \"shape\": list(fi.shape) if fi.shape else None,\n            }\n            for fi in self.feature_indices\n        ],\n    }\n\n    return yaml.dump(data, default_flow_style=False)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.SCDLHeader.validate","title":"<code>validate()</code>","text":"<p>Validate the header for consistency and correctness.</p> <p>Raises:</p> Type Description <code>HeaderSerializationError</code> <p>If validation fails</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate the header for consistency and correctness.\n\n    Raises:\n        HeaderSerializationError: If validation fails\n    \"\"\"\n    # Check version compatibility\n    current_version = CurrentSCDLVersion()\n    if self.version.major &gt; current_version.major:\n        raise HeaderSerializationError(f\"Unsupported version: {self.version} &gt; {current_version}\")\n\n    # Check array names are unique\n    names = [array.name for array in self.arrays]\n    if len(names) != len(set(names)):\n        raise HeaderSerializationError(\"Duplicate array names found\")\n\n    # Check array names are valid\n    for array in self.arrays:\n        if not array.name or not array.name.strip():\n            raise HeaderSerializationError(\"Empty array name found\")\n        if len(array.name.encode(\"utf-8\")) &gt; 1024:  # Reasonable limit\n            raise HeaderSerializationError(f\"Array name too long: {array.name}\")\n\n    # Check feature index names are unique\n    feature_names = [fi.name for fi in self.feature_indices]\n    if len(feature_names) != len(set(feature_names)):\n        raise HeaderSerializationError(\"Duplicate feature index names found\")\n\n    # Check feature index names are valid\n    for feature_index in self.feature_indices:\n        if not feature_index.name or not feature_index.name.strip():\n            raise HeaderSerializationError(\"Empty feature index name found\")\n        if len(feature_index.name.encode(\"utf-8\")) &gt; 1024:  # Reasonable limit\n            raise HeaderSerializationError(f\"Feature index name too long: {feature_index.name}\")\n\n    # Check for name conflicts between arrays and feature indices\n    all_names = names + feature_names\n    if len(all_names) != len(set(all_names)):\n        raise HeaderSerializationError(\"Name conflicts between arrays and feature indices\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.create_header_from_arrays","title":"<code>create_header_from_arrays(array_files, backend=Backend.MEMMAP_V0, version=None)</code>","text":"<p>Create a SCDL header by scanning array files.</p> <p>Parameters:</p> Name Type Description Default <code>array_files</code> <code>List[str]</code> <p>List of array file paths to include</p> required <code>backend</code> <code>Backend</code> <p>Storage backend to use</p> <code>MEMMAP_V0</code> <code>version</code> <code>Optional[SCDLVersion]</code> <p>Schema version (defaults to current)</p> <code>None</code> <p>Returns:</p> Type Description <code>SCDLHeader</code> <p>SCDLHeader with arrays automatically detected</p> Note <p>This function creates placeholder ArrayInfo objects. Real implementations should inspect files to determine actual properties.</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def create_header_from_arrays(\n    array_files: List[str], backend: Backend = Backend.MEMMAP_V0, version: Optional[SCDLVersion] = None\n) -&gt; SCDLHeader:\n    \"\"\"Create a SCDL header by scanning array files.\n\n    Args:\n        array_files: List of array file paths to include\n        backend: Storage backend to use\n        version: Schema version (defaults to current)\n\n    Returns:\n        SCDLHeader with arrays automatically detected\n\n    Note:\n        This function creates placeholder ArrayInfo objects.\n        Real implementations should inspect files to determine actual properties.\n    \"\"\"\n    header = SCDLHeader(version=version, backend=backend)\n\n    for file_path in array_files:\n        path = Path(file_path)\n        array_info = ArrayInfo(\n            name=path.name,\n            length=0,  # Would be determined by inspecting file\n            dtype=ArrayDType.FLOAT32_ARRAY,  # Would be determined by inspecting file\n            shape=None,  # Would be determined by inspecting file\n        )\n        header.add_array(array_info)\n\n    return header\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.merge_headers","title":"<code>merge_headers(header1, header2)</code>","text":"<p>Merge two compatible headers into a single header.</p> <p>Parameters:</p> Name Type Description Default <code>header1</code> <code>SCDLHeader</code> <p>First header</p> required <code>header2</code> <code>SCDLHeader</code> <p>Second header</p> required <p>Returns:</p> Type Description <code>SCDLHeader</code> <p>Merged header</p> <p>Raises:</p> Type Description <code>HeaderSerializationError</code> <p>If headers are incompatible</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def merge_headers(header1: SCDLHeader, header2: SCDLHeader) -&gt; SCDLHeader:\n    \"\"\"Merge two compatible headers into a single header.\n\n    Args:\n        header1: First header\n        header2: Second header\n\n    Returns:\n        Merged header\n\n    Raises:\n        HeaderSerializationError: If headers are incompatible\n    \"\"\"\n    if not validate_header_compatibility(header1, header2):\n        raise HeaderSerializationError(\"Headers are not compatible for merging\")\n\n    # Use the newer version\n    if header1.version.minor &gt;= header2.version.minor:\n        version = header1.version\n    else:\n        version = header2.version\n\n    merged_header = SCDLHeader(\n        version=version,\n        backend=header1.backend,\n        arrays=header1.arrays + header2.arrays,\n        feature_indices=header1.feature_indices + header2.feature_indices,\n    )\n\n    return merged_header\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/header/#bionemo.scdl.schema.header.validate_header_compatibility","title":"<code>validate_header_compatibility(header1, header2)</code>","text":"<p>Check if two headers are compatible for operations like merging.</p> <p>Parameters:</p> Name Type Description Default <code>header1</code> <code>SCDLHeader</code> <p>First header</p> required <code>header2</code> <code>SCDLHeader</code> <p>Second header</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if headers are compatible</p> Source code in <code>bionemo/scdl/schema/header.py</code> <pre><code>def validate_header_compatibility(header1: SCDLHeader, header2: SCDLHeader) -&gt; bool:\n    \"\"\"Check if two headers are compatible for operations like merging.\n\n    Args:\n        header1: First header\n        header2: Second header\n\n    Returns:\n        True if headers are compatible\n    \"\"\"\n    # Check version compatibility (same major version)\n    if header1.version.major != header2.version.major:\n        return False\n\n    # Check backend compatibility\n    if header1.backend != header2.backend:\n        return False\n\n    # Check for conflicting array names\n    names1 = {array.name for array in header1.arrays}\n    names2 = {array.name for array in header2.arrays}\n\n    if names1.intersection(names2):\n        return False\n\n    # Check for conflicting feature index names\n    fi_names1 = {fi.name for fi in header1.feature_indices}\n    fi_names2 = {fi.name for fi in header2.feature_indices}\n\n    if fi_names1.intersection(fi_names2):\n        return False\n\n    # Check for conflicts between arrays and feature indices across headers\n    all_names1 = names1.union(fi_names1)\n    all_names2 = names2.union(fi_names2)\n\n    if all_names1.intersection(all_names2):\n        return False\n\n    return True\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/headerutil/","title":"Headerutil","text":"<p>Cross-platform binary header serialization utilities.</p> <p>This module provides tools for creating fixed-size binary headers that maintain metadata about files in a cross-platform, non-user-readable format.</p>"},{"location":"main/references/API_reference/bionemo/scdl/schema/headerutil/#bionemo.scdl.schema.headerutil.BinaryHeaderCodec","title":"<code>BinaryHeaderCodec</code>","text":"<p>A robust codec for serializing and deserializing fixed-size binary headers.</p> <p>This class provides a clean API for packing and unpacking various data types to/from binary format, with consistent endianness handling and comprehensive error checking. Designed for creating cross-platform file headers in binary form.</p> <p>Parameters:</p> Name Type Description Default <code>endianness</code> <code>Endianness</code> <p>Byte order for serialization (default: NETWORK)</p> <code>NETWORK</code> Example <p>codec = BinaryHeaderCodec(Endianness.NETWORK) data = codec.pack_uint32(42) value = codec.unpack_uint32(data) assert value == 42</p> Source code in <code>bionemo/scdl/schema/headerutil.py</code> <pre><code>class BinaryHeaderCodec:\n    \"\"\"A robust codec for serializing and deserializing fixed-size binary headers.\n\n    This class provides a clean API for packing and unpacking various data types\n    to/from binary format, with consistent endianness handling and comprehensive\n    error checking. Designed for creating cross-platform file headers in binary form.\n\n    Args:\n        endianness: Byte order for serialization (default: NETWORK)\n\n    Example:\n        &gt;&gt;&gt; codec = BinaryHeaderCodec(Endianness.NETWORK)\n        &gt;&gt;&gt; data = codec.pack_uint32(42)\n        &gt;&gt;&gt; value = codec.unpack_uint32(data)\n        &gt;&gt;&gt; assert value == 42\n    \"\"\"\n\n    def __init__(self, endianness: Endianness = Endianness.NETWORK):\n        \"\"\"Initialize the codec with specified byte order.\"\"\"\n        self.endianness = endianness.value\n\n    # Integer packing/unpacking methods\n\n    def pack_uint8(self, value: int) -&gt; bytes:\n        \"\"\"Pack an 8-bit unsigned integer.\n\n        Args:\n            value: Integer value (0-255)\n\n        Returns:\n            1-byte binary representation\n\n        Raises:\n            HeaderSerializationError: If value is out of range\n        \"\"\"\n        self._validate_uint_range(value, 0, 255, \"uint8\")\n        return struct.pack(f\"{self.endianness}B\", value)\n\n    def unpack_uint8(self, data: bytes) -&gt; int:\n        \"\"\"Unpack an 8-bit unsigned integer.\n\n        Args:\n            data: Binary data (must be at least 1 byte)\n\n        Returns:\n            Unpacked integer value\n\n        Raises:\n            HeaderSerializationError: If data is insufficient or invalid\n        \"\"\"\n        self._validate_data_length(data, 1, \"uint8\")\n        return struct.unpack(f\"{self.endianness}B\", data[:1])[0]\n\n    def pack_uint16(self, value: int) -&gt; bytes:\n        \"\"\"Pack a 16-bit unsigned integer.\n\n        Args:\n            value: Integer value (0-65535)\n\n        Returns:\n            2-byte binary representation\n\n        Raises:\n            HeaderSerializationError: If value is out of range\n        \"\"\"\n        self._validate_uint_range(value, 0, 65535, \"uint16\")\n        return struct.pack(f\"{self.endianness}H\", value)\n\n    def unpack_uint16(self, data: bytes) -&gt; int:\n        \"\"\"Unpack a 16-bit unsigned integer.\n\n        Args:\n            data: Binary data (must be at least 2 bytes)\n\n        Returns:\n            Unpacked integer value\n\n        Raises:\n            HeaderSerializationError: If data is insufficient or invalid\n        \"\"\"\n        self._validate_data_length(data, 2, \"uint16\")\n        return struct.unpack(f\"{self.endianness}H\", data[:2])[0]\n\n    def pack_uint32(self, value: int) -&gt; bytes:\n        \"\"\"Pack a 32-bit unsigned integer.\n\n        Args:\n            value: Integer value (0-4294967295)\n\n        Returns:\n            4-byte binary representation\n\n        Raises:\n            HeaderSerializationError: If value is out of range\n        \"\"\"\n        self._validate_uint_range(value, 0, 4294967295, \"uint32\")\n        return struct.pack(f\"{self.endianness}I\", value)\n\n    def unpack_uint32(self, data: bytes) -&gt; int:\n        \"\"\"Unpack a 32-bit unsigned integer.\n\n        Args:\n            data: Binary data (must be at least 4 bytes)\n\n        Returns:\n            Unpacked integer value\n\n        Raises:\n            HeaderSerializationError: If data is insufficient or invalid\n        \"\"\"\n        self._validate_data_length(data, 4, \"uint32\")\n        return struct.unpack(f\"{self.endianness}I\", data[:4])[0]\n\n    def pack_uint64(self, value: int) -&gt; bytes:\n        \"\"\"Pack a 64-bit unsigned integer.\n\n        Args:\n            value: Integer value (0-18446744073709551615)\n\n        Returns:\n            8-byte binary representation\n\n        Raises:\n            HeaderSerializationError: If value is out of range\n        \"\"\"\n        self._validate_uint_range(value, 0, 18446744073709551615, \"uint64\")\n        return struct.pack(f\"{self.endianness}Q\", value)\n\n    def unpack_uint64(self, data: bytes) -&gt; int:\n        \"\"\"Unpack a 64-bit unsigned integer.\n\n        Args:\n            data: Binary data (must be at least 8 bytes)\n\n        Returns:\n            Unpacked integer value\n\n        Raises:\n            HeaderSerializationError: If data is insufficient or invalid\n        \"\"\"\n        self._validate_data_length(data, 8, \"uint64\")\n        return struct.unpack(f\"{self.endianness}Q\", data[:8])[0]\n\n    # Floating point packing/unpacking methods\n\n    def pack_float16(self, value: float) -&gt; bytes:\n        \"\"\"Pack a 16-bit (half-precision) floating point number.\n\n        Args:\n            value: Float value\n\n        Returns:\n            2-byte binary representation\n\n        Raises:\n            HeaderSerializationError: If value cannot be represented\n        \"\"\"\n        try:\n            return struct.pack(f\"{self.endianness}e\", value)\n        except (struct.error, OverflowError) as e:\n            raise HeaderSerializationError(f\"Cannot pack float16 value {value}: {e}\")\n\n    def unpack_float16(self, data: bytes) -&gt; float:\n        \"\"\"Unpack a 16-bit (half-precision) floating point number.\n\n        Args:\n            data: Binary data (must be at least 2 bytes)\n\n        Returns:\n            Unpacked float value\n\n        Raises:\n            HeaderSerializationError: If data is insufficient or invalid\n        \"\"\"\n        self._validate_data_length(data, 2, \"float16\")\n        return struct.unpack(f\"{self.endianness}e\", data[:2])[0]\n\n    def pack_float32(self, value: float) -&gt; bytes:\n        \"\"\"Pack a 32-bit (single-precision) floating point number.\n\n        Args:\n            value: Float value\n\n        Returns:\n            4-byte binary representation\n\n        Raises:\n            HeaderSerializationError: If value cannot be represented\n        \"\"\"\n        try:\n            return struct.pack(f\"{self.endianness}f\", value)\n        except (struct.error, OverflowError) as e:\n            raise HeaderSerializationError(f\"Cannot pack float32 value {value}: {e}\")\n\n    def unpack_float32(self, data: bytes) -&gt; float:\n        \"\"\"Unpack a 32-bit (single-precision) floating point number.\n\n        Args:\n            data: Binary data (must be at least 4 bytes)\n\n        Returns:\n            Unpacked float value\n\n        Raises:\n            HeaderSerializationError: If data is insufficient or invalid\n        \"\"\"\n        self._validate_data_length(data, 4, \"float32\")\n        return struct.unpack(f\"{self.endianness}f\", data[:4])[0]\n\n    # String and array methods (for variable-length data)\n\n    def pack_string(self, value: str, max_length: int | None = None) -&gt; bytes:\n        \"\"\"Pack a UTF-8 string with length prefix.\n\n        Args:\n            value: String to pack\n            max_length: Optional maximum length limit\n\n        Returns:\n            Binary data: 4-byte length + UTF-8 encoded string\n\n        Raises:\n            HeaderSerializationError: If string is too long or encoding fails\n        \"\"\"\n        if not isinstance(value, str):\n            raise HeaderSerializationError(f\"Expected string, got {type(value)}\")\n\n        try:\n            encoded_string = value.encode(\"utf-8\")\n        except UnicodeEncodeError as e:\n            raise HeaderSerializationError(f\"Cannot encode string to UTF-8: {e}\")\n\n        length = len(encoded_string)\n\n        if max_length is not None and length &gt; max_length:\n            raise HeaderSerializationError(f\"String too long: {length} bytes &gt; {max_length} bytes limit\")\n\n        return self.pack_uint32(length) + encoded_string\n\n    def unpack_string(self, data: bytes, max_length: int | None = None) -&gt; Tuple[str, int]:\n        \"\"\"Unpack a UTF-8 string with length prefix.\n\n        Args:\n            data: Binary data starting with 4-byte length prefix\n            max_length: Optional maximum length limit\n\n        Returns:\n            Tuple of (unpacked string, total bytes consumed)\n\n        Raises:\n            HeaderSerializationError: If data is invalid or string too long\n        \"\"\"\n        if len(data) &lt; 4:\n            raise HeaderSerializationError(\"Insufficient data for string length\")\n\n        length = self.unpack_uint32(data[:4])\n\n        if max_length is not None and length &gt; max_length:\n            raise HeaderSerializationError(f\"String too long: {length} bytes &gt; {max_length} bytes limit\")\n\n        if len(data) &lt; 4 + length:\n            raise HeaderSerializationError(f\"Insufficient data for string: need {4 + length} bytes, got {len(data)}\")\n\n        try:\n            string_value = data[4 : 4 + length].decode(\"utf-8\")\n        except UnicodeDecodeError as e:\n            raise HeaderSerializationError(f\"Cannot decode UTF-8 string: {e}\")\n\n        return string_value, 4 + length\n\n    def pack_fixed_string(self, value: str, size: int, padding: bytes = b\"\\x00\") -&gt; bytes:\n        \"\"\"Pack a string into a fixed-size field with padding.\n\n        Useful for creating truly fixed-size headers where string fields\n        have a predetermined maximum size.\n\n        Args:\n            value: String to pack\n            size: Fixed size of the field in bytes\n            padding: Byte value to use for padding (default: null bytes)\n\n        Returns:\n            Fixed-size binary data\n\n        Raises:\n            HeaderSerializationError: If string is too long or parameters invalid\n        \"\"\"\n        if not isinstance(value, str):\n            raise HeaderSerializationError(f\"Expected string, got {type(value)}\")\n\n        if size &lt;= 0:\n            raise HeaderSerializationError(f\"Size must be positive, got {size}\")\n\n        if len(padding) != 1:\n            raise HeaderSerializationError(f\"Padding must be single byte, got {len(padding)} bytes\")\n\n        try:\n            encoded = value.encode(\"utf-8\")\n        except UnicodeEncodeError as e:\n            raise HeaderSerializationError(f\"Cannot encode string to UTF-8: {e}\")\n\n        if len(encoded) &gt; size:\n            raise HeaderSerializationError(f\"String too long: {len(encoded)} bytes &gt; {size} bytes field size\")\n\n        return encoded + padding * (size - len(encoded))\n\n    def unpack_fixed_string(self, data: bytes, size: int, padding: bytes = b\"\\x00\") -&gt; str:\n        \"\"\"Unpack a string from a fixed-size field, removing padding.\n\n        Args:\n            data: Binary data (must be at least size bytes)\n            size: Size of the fixed field in bytes\n            padding: Padding byte to strip (default: null bytes)\n\n        Returns:\n            Unpacked string with padding removed\n\n        Raises:\n            HeaderSerializationError: If data is insufficient or invalid\n        \"\"\"\n        if len(data) &lt; size:\n            raise HeaderSerializationError(f\"Insufficient data: need {size} bytes, got {len(data)}\")\n\n        if len(padding) != 1:\n            raise HeaderSerializationError(f\"Padding must be single byte, got {len(padding)} bytes\")\n\n        field_data = data[:size]\n        # Remove trailing padding\n        string_data = field_data.rstrip(padding)\n\n        try:\n            return string_data.decode(\"utf-8\")\n        except UnicodeDecodeError as e:\n            raise HeaderSerializationError(f\"Cannot decode UTF-8 string: {e}\")\n\n    # Validation helper methods\n\n    def _validate_uint_range(self, value: int, min_val: int, max_val: int, type_name: str) -&gt; None:\n        \"\"\"Validate that an integer value is within the valid range for its type.\"\"\"\n        if not isinstance(value, int):\n            raise HeaderSerializationError(f\"Expected integer for {type_name}, got {type(value)}\")\n\n        if value &lt; min_val or value &gt; max_val:\n            raise HeaderSerializationError(f\"{type_name} value {value} out of range [{min_val}, {max_val}]\")\n\n    def _validate_data_length(self, data: bytes, required_length: int, type_name: str) -&gt; None:\n        \"\"\"Validate that data has sufficient length for unpacking.\"\"\"\n        if not isinstance(data, (bytes, bytearray)):\n            raise HeaderSerializationError(f\"Expected bytes for {type_name}, got {type(data)}\")\n\n        if len(data) &lt; required_length:\n            raise HeaderSerializationError(\n                f\"Insufficient data for {type_name}: need {required_length} bytes, got {len(data)}\"\n            )\n\n    # Utility methods for working with headers\n\n    def calculate_header_size(self, field_specs: List[Tuple[str, Union[int, str]]]) -&gt; int:\n        \"\"\"Calculate the total size of a header given field specifications.\n\n        Args:\n            field_specs: List of (field_type, size) tuples where:\n                - field_type: 'uint8', 'uint16', 'uint32', 'uint64', 'float16', 'float32', 'fixed_string'\n                - size: For fixed_string, the size in bytes; ignored for other types\n\n        Returns:\n            Total header size in bytes\n\n        Example:\n            &gt;&gt;&gt; codec = BinaryHeaderCodec()\n            &gt;&gt;&gt; size = codec.calculate_header_size([\n            ...     ('uint32', None),      # 4 bytes\n            ...     ('uint16', None),      # 2 bytes\n            ...     ('fixed_string', 64),  # 64 bytes\n            ...     ('float32', None)      # 4 bytes\n            ... ])\n            &gt;&gt;&gt; assert size == 74\n        \"\"\"\n        size_map = {\"uint8\": 1, \"uint16\": 2, \"uint32\": 4, \"uint64\": 8, \"float16\": 2, \"float32\": 4}\n\n        total_size = 0\n        for field_type, field_size in field_specs:\n            if field_type == \"fixed_string\":\n                if not isinstance(field_size, int) or field_size &lt;= 0:\n                    raise HeaderSerializationError(f\"fixed_string requires positive integer size, got {field_size}\")\n                total_size += field_size\n            elif field_type in size_map:\n                total_size += size_map[field_type]\n            else:\n                raise HeaderSerializationError(f\"Unknown field type: {field_type}\")\n\n        return total_size\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/headerutil/#bionemo.scdl.schema.headerutil.BinaryHeaderCodec.__init__","title":"<code>__init__(endianness=Endianness.NETWORK)</code>","text":"<p>Initialize the codec with specified byte order.</p> Source code in <code>bionemo/scdl/schema/headerutil.py</code> <pre><code>def __init__(self, endianness: Endianness = Endianness.NETWORK):\n    \"\"\"Initialize the codec with specified byte order.\"\"\"\n    self.endianness = endianness.value\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/headerutil/#bionemo.scdl.schema.headerutil.BinaryHeaderCodec.calculate_header_size","title":"<code>calculate_header_size(field_specs)</code>","text":"<p>Calculate the total size of a header given field specifications.</p> <p>Parameters:</p> Name Type Description Default <code>field_specs</code> <code>List[Tuple[str, Union[int, str]]]</code> <p>List of (field_type, size) tuples where: - field_type: 'uint8', 'uint16', 'uint32', 'uint64', 'float16', 'float32', 'fixed_string' - size: For fixed_string, the size in bytes; ignored for other types</p> required <p>Returns:</p> Type Description <code>int</code> <p>Total header size in bytes</p> Example <p>codec = BinaryHeaderCodec() size = codec.calculate_header_size([ ...     ('uint32', None),      # 4 bytes ...     ('uint16', None),      # 2 bytes ...     ('fixed_string', 64),  # 64 bytes ...     ('float32', None)      # 4 bytes ... ]) assert size == 74</p> Source code in <code>bionemo/scdl/schema/headerutil.py</code> <pre><code>def calculate_header_size(self, field_specs: List[Tuple[str, Union[int, str]]]) -&gt; int:\n    \"\"\"Calculate the total size of a header given field specifications.\n\n    Args:\n        field_specs: List of (field_type, size) tuples where:\n            - field_type: 'uint8', 'uint16', 'uint32', 'uint64', 'float16', 'float32', 'fixed_string'\n            - size: For fixed_string, the size in bytes; ignored for other types\n\n    Returns:\n        Total header size in bytes\n\n    Example:\n        &gt;&gt;&gt; codec = BinaryHeaderCodec()\n        &gt;&gt;&gt; size = codec.calculate_header_size([\n        ...     ('uint32', None),      # 4 bytes\n        ...     ('uint16', None),      # 2 bytes\n        ...     ('fixed_string', 64),  # 64 bytes\n        ...     ('float32', None)      # 4 bytes\n        ... ])\n        &gt;&gt;&gt; assert size == 74\n    \"\"\"\n    size_map = {\"uint8\": 1, \"uint16\": 2, \"uint32\": 4, \"uint64\": 8, \"float16\": 2, \"float32\": 4}\n\n    total_size = 0\n    for field_type, field_size in field_specs:\n        if field_type == \"fixed_string\":\n            if not isinstance(field_size, int) or field_size &lt;= 0:\n                raise HeaderSerializationError(f\"fixed_string requires positive integer size, got {field_size}\")\n            total_size += field_size\n        elif field_type in size_map:\n            total_size += size_map[field_type]\n        else:\n            raise HeaderSerializationError(f\"Unknown field type: {field_type}\")\n\n    return total_size\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/headerutil/#bionemo.scdl.schema.headerutil.BinaryHeaderCodec.pack_fixed_string","title":"<code>pack_fixed_string(value, size, padding=b'\\x00')</code>","text":"<p>Pack a string into a fixed-size field with padding.</p> <p>Useful for creating truly fixed-size headers where string fields have a predetermined maximum size.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>String to pack</p> required <code>size</code> <code>int</code> <p>Fixed size of the field in bytes</p> required <code>padding</code> <code>bytes</code> <p>Byte value to use for padding (default: null bytes)</p> <code>b'\\x00'</code> <p>Returns:</p> Type Description <code>bytes</code> <p>Fixed-size binary data</p> <p>Raises:</p> Type Description <code>HeaderSerializationError</code> <p>If string is too long or parameters invalid</p> Source code in <code>bionemo/scdl/schema/headerutil.py</code> <pre><code>def pack_fixed_string(self, value: str, size: int, padding: bytes = b\"\\x00\") -&gt; bytes:\n    \"\"\"Pack a string into a fixed-size field with padding.\n\n    Useful for creating truly fixed-size headers where string fields\n    have a predetermined maximum size.\n\n    Args:\n        value: String to pack\n        size: Fixed size of the field in bytes\n        padding: Byte value to use for padding (default: null bytes)\n\n    Returns:\n        Fixed-size binary data\n\n    Raises:\n        HeaderSerializationError: If string is too long or parameters invalid\n    \"\"\"\n    if not isinstance(value, str):\n        raise HeaderSerializationError(f\"Expected string, got {type(value)}\")\n\n    if size &lt;= 0:\n        raise HeaderSerializationError(f\"Size must be positive, got {size}\")\n\n    if len(padding) != 1:\n        raise HeaderSerializationError(f\"Padding must be single byte, got {len(padding)} bytes\")\n\n    try:\n        encoded = value.encode(\"utf-8\")\n    except UnicodeEncodeError as e:\n        raise HeaderSerializationError(f\"Cannot encode string to UTF-8: {e}\")\n\n    if len(encoded) &gt; size:\n        raise HeaderSerializationError(f\"String too long: {len(encoded)} bytes &gt; {size} bytes field size\")\n\n    return encoded + padding * (size - len(encoded))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/headerutil/#bionemo.scdl.schema.headerutil.BinaryHeaderCodec.pack_float16","title":"<code>pack_float16(value)</code>","text":"<p>Pack a 16-bit (half-precision) floating point number.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>Float value</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>2-byte binary representation</p> <p>Raises:</p> Type Description <code>HeaderSerializationError</code> <p>If value cannot be represented</p> Source code in <code>bionemo/scdl/schema/headerutil.py</code> <pre><code>def pack_float16(self, value: float) -&gt; bytes:\n    \"\"\"Pack a 16-bit (half-precision) floating point number.\n\n    Args:\n        value: Float value\n\n    Returns:\n        2-byte binary representation\n\n    Raises:\n        HeaderSerializationError: If value cannot be represented\n    \"\"\"\n    try:\n        return struct.pack(f\"{self.endianness}e\", value)\n    except (struct.error, OverflowError) as e:\n        raise HeaderSerializationError(f\"Cannot pack float16 value {value}: {e}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/headerutil/#bionemo.scdl.schema.headerutil.BinaryHeaderCodec.pack_float32","title":"<code>pack_float32(value)</code>","text":"<p>Pack a 32-bit (single-precision) floating point number.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>Float value</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>4-byte binary representation</p> <p>Raises:</p> Type Description <code>HeaderSerializationError</code> <p>If value cannot be represented</p> Source code in <code>bionemo/scdl/schema/headerutil.py</code> <pre><code>def pack_float32(self, value: float) -&gt; bytes:\n    \"\"\"Pack a 32-bit (single-precision) floating point number.\n\n    Args:\n        value: Float value\n\n    Returns:\n        4-byte binary representation\n\n    Raises:\n        HeaderSerializationError: If value cannot be represented\n    \"\"\"\n    try:\n        return struct.pack(f\"{self.endianness}f\", value)\n    except (struct.error, OverflowError) as e:\n        raise HeaderSerializationError(f\"Cannot pack float32 value {value}: {e}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/headerutil/#bionemo.scdl.schema.headerutil.BinaryHeaderCodec.pack_string","title":"<code>pack_string(value, max_length=None)</code>","text":"<p>Pack a UTF-8 string with length prefix.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>String to pack</p> required <code>max_length</code> <code>int | None</code> <p>Optional maximum length limit</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>Binary data: 4-byte length + UTF-8 encoded string</p> <p>Raises:</p> Type Description <code>HeaderSerializationError</code> <p>If string is too long or encoding fails</p> Source code in <code>bionemo/scdl/schema/headerutil.py</code> <pre><code>def pack_string(self, value: str, max_length: int | None = None) -&gt; bytes:\n    \"\"\"Pack a UTF-8 string with length prefix.\n\n    Args:\n        value: String to pack\n        max_length: Optional maximum length limit\n\n    Returns:\n        Binary data: 4-byte length + UTF-8 encoded string\n\n    Raises:\n        HeaderSerializationError: If string is too long or encoding fails\n    \"\"\"\n    if not isinstance(value, str):\n        raise HeaderSerializationError(f\"Expected string, got {type(value)}\")\n\n    try:\n        encoded_string = value.encode(\"utf-8\")\n    except UnicodeEncodeError as e:\n        raise HeaderSerializationError(f\"Cannot encode string to UTF-8: {e}\")\n\n    length = len(encoded_string)\n\n    if max_length is not None and length &gt; max_length:\n        raise HeaderSerializationError(f\"String too long: {length} bytes &gt; {max_length} bytes limit\")\n\n    return self.pack_uint32(length) + encoded_string\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/headerutil/#bionemo.scdl.schema.headerutil.BinaryHeaderCodec.pack_uint16","title":"<code>pack_uint16(value)</code>","text":"<p>Pack a 16-bit unsigned integer.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int</code> <p>Integer value (0-65535)</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>2-byte binary representation</p> <p>Raises:</p> Type Description <code>HeaderSerializationError</code> <p>If value is out of range</p> Source code in <code>bionemo/scdl/schema/headerutil.py</code> <pre><code>def pack_uint16(self, value: int) -&gt; bytes:\n    \"\"\"Pack a 16-bit unsigned integer.\n\n    Args:\n        value: Integer value (0-65535)\n\n    Returns:\n        2-byte binary representation\n\n    Raises:\n        HeaderSerializationError: If value is out of range\n    \"\"\"\n    self._validate_uint_range(value, 0, 65535, \"uint16\")\n    return struct.pack(f\"{self.endianness}H\", value)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/headerutil/#bionemo.scdl.schema.headerutil.BinaryHeaderCodec.pack_uint32","title":"<code>pack_uint32(value)</code>","text":"<p>Pack a 32-bit unsigned integer.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int</code> <p>Integer value (0-4294967295)</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>4-byte binary representation</p> <p>Raises:</p> Type Description <code>HeaderSerializationError</code> <p>If value is out of range</p> Source code in <code>bionemo/scdl/schema/headerutil.py</code> <pre><code>def pack_uint32(self, value: int) -&gt; bytes:\n    \"\"\"Pack a 32-bit unsigned integer.\n\n    Args:\n        value: Integer value (0-4294967295)\n\n    Returns:\n        4-byte binary representation\n\n    Raises:\n        HeaderSerializationError: If value is out of range\n    \"\"\"\n    self._validate_uint_range(value, 0, 4294967295, \"uint32\")\n    return struct.pack(f\"{self.endianness}I\", value)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/headerutil/#bionemo.scdl.schema.headerutil.BinaryHeaderCodec.pack_uint64","title":"<code>pack_uint64(value)</code>","text":"<p>Pack a 64-bit unsigned integer.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int</code> <p>Integer value (0-18446744073709551615)</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>8-byte binary representation</p> <p>Raises:</p> Type Description <code>HeaderSerializationError</code> <p>If value is out of range</p> Source code in <code>bionemo/scdl/schema/headerutil.py</code> <pre><code>def pack_uint64(self, value: int) -&gt; bytes:\n    \"\"\"Pack a 64-bit unsigned integer.\n\n    Args:\n        value: Integer value (0-18446744073709551615)\n\n    Returns:\n        8-byte binary representation\n\n    Raises:\n        HeaderSerializationError: If value is out of range\n    \"\"\"\n    self._validate_uint_range(value, 0, 18446744073709551615, \"uint64\")\n    return struct.pack(f\"{self.endianness}Q\", value)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/headerutil/#bionemo.scdl.schema.headerutil.BinaryHeaderCodec.pack_uint8","title":"<code>pack_uint8(value)</code>","text":"<p>Pack an 8-bit unsigned integer.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int</code> <p>Integer value (0-255)</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>1-byte binary representation</p> <p>Raises:</p> Type Description <code>HeaderSerializationError</code> <p>If value is out of range</p> Source code in <code>bionemo/scdl/schema/headerutil.py</code> <pre><code>def pack_uint8(self, value: int) -&gt; bytes:\n    \"\"\"Pack an 8-bit unsigned integer.\n\n    Args:\n        value: Integer value (0-255)\n\n    Returns:\n        1-byte binary representation\n\n    Raises:\n        HeaderSerializationError: If value is out of range\n    \"\"\"\n    self._validate_uint_range(value, 0, 255, \"uint8\")\n    return struct.pack(f\"{self.endianness}B\", value)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/headerutil/#bionemo.scdl.schema.headerutil.BinaryHeaderCodec.unpack_fixed_string","title":"<code>unpack_fixed_string(data, size, padding=b'\\x00')</code>","text":"<p>Unpack a string from a fixed-size field, removing padding.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Binary data (must be at least size bytes)</p> required <code>size</code> <code>int</code> <p>Size of the fixed field in bytes</p> required <code>padding</code> <code>bytes</code> <p>Padding byte to strip (default: null bytes)</p> <code>b'\\x00'</code> <p>Returns:</p> Type Description <code>str</code> <p>Unpacked string with padding removed</p> <p>Raises:</p> Type Description <code>HeaderSerializationError</code> <p>If data is insufficient or invalid</p> Source code in <code>bionemo/scdl/schema/headerutil.py</code> <pre><code>def unpack_fixed_string(self, data: bytes, size: int, padding: bytes = b\"\\x00\") -&gt; str:\n    \"\"\"Unpack a string from a fixed-size field, removing padding.\n\n    Args:\n        data: Binary data (must be at least size bytes)\n        size: Size of the fixed field in bytes\n        padding: Padding byte to strip (default: null bytes)\n\n    Returns:\n        Unpacked string with padding removed\n\n    Raises:\n        HeaderSerializationError: If data is insufficient or invalid\n    \"\"\"\n    if len(data) &lt; size:\n        raise HeaderSerializationError(f\"Insufficient data: need {size} bytes, got {len(data)}\")\n\n    if len(padding) != 1:\n        raise HeaderSerializationError(f\"Padding must be single byte, got {len(padding)} bytes\")\n\n    field_data = data[:size]\n    # Remove trailing padding\n    string_data = field_data.rstrip(padding)\n\n    try:\n        return string_data.decode(\"utf-8\")\n    except UnicodeDecodeError as e:\n        raise HeaderSerializationError(f\"Cannot decode UTF-8 string: {e}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/headerutil/#bionemo.scdl.schema.headerutil.BinaryHeaderCodec.unpack_float16","title":"<code>unpack_float16(data)</code>","text":"<p>Unpack a 16-bit (half-precision) floating point number.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Binary data (must be at least 2 bytes)</p> required <p>Returns:</p> Type Description <code>float</code> <p>Unpacked float value</p> <p>Raises:</p> Type Description <code>HeaderSerializationError</code> <p>If data is insufficient or invalid</p> Source code in <code>bionemo/scdl/schema/headerutil.py</code> <pre><code>def unpack_float16(self, data: bytes) -&gt; float:\n    \"\"\"Unpack a 16-bit (half-precision) floating point number.\n\n    Args:\n        data: Binary data (must be at least 2 bytes)\n\n    Returns:\n        Unpacked float value\n\n    Raises:\n        HeaderSerializationError: If data is insufficient or invalid\n    \"\"\"\n    self._validate_data_length(data, 2, \"float16\")\n    return struct.unpack(f\"{self.endianness}e\", data[:2])[0]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/headerutil/#bionemo.scdl.schema.headerutil.BinaryHeaderCodec.unpack_float32","title":"<code>unpack_float32(data)</code>","text":"<p>Unpack a 32-bit (single-precision) floating point number.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Binary data (must be at least 4 bytes)</p> required <p>Returns:</p> Type Description <code>float</code> <p>Unpacked float value</p> <p>Raises:</p> Type Description <code>HeaderSerializationError</code> <p>If data is insufficient or invalid</p> Source code in <code>bionemo/scdl/schema/headerutil.py</code> <pre><code>def unpack_float32(self, data: bytes) -&gt; float:\n    \"\"\"Unpack a 32-bit (single-precision) floating point number.\n\n    Args:\n        data: Binary data (must be at least 4 bytes)\n\n    Returns:\n        Unpacked float value\n\n    Raises:\n        HeaderSerializationError: If data is insufficient or invalid\n    \"\"\"\n    self._validate_data_length(data, 4, \"float32\")\n    return struct.unpack(f\"{self.endianness}f\", data[:4])[0]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/headerutil/#bionemo.scdl.schema.headerutil.BinaryHeaderCodec.unpack_string","title":"<code>unpack_string(data, max_length=None)</code>","text":"<p>Unpack a UTF-8 string with length prefix.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Binary data starting with 4-byte length prefix</p> required <code>max_length</code> <code>int | None</code> <p>Optional maximum length limit</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[str, int]</code> <p>Tuple of (unpacked string, total bytes consumed)</p> <p>Raises:</p> Type Description <code>HeaderSerializationError</code> <p>If data is invalid or string too long</p> Source code in <code>bionemo/scdl/schema/headerutil.py</code> <pre><code>def unpack_string(self, data: bytes, max_length: int | None = None) -&gt; Tuple[str, int]:\n    \"\"\"Unpack a UTF-8 string with length prefix.\n\n    Args:\n        data: Binary data starting with 4-byte length prefix\n        max_length: Optional maximum length limit\n\n    Returns:\n        Tuple of (unpacked string, total bytes consumed)\n\n    Raises:\n        HeaderSerializationError: If data is invalid or string too long\n    \"\"\"\n    if len(data) &lt; 4:\n        raise HeaderSerializationError(\"Insufficient data for string length\")\n\n    length = self.unpack_uint32(data[:4])\n\n    if max_length is not None and length &gt; max_length:\n        raise HeaderSerializationError(f\"String too long: {length} bytes &gt; {max_length} bytes limit\")\n\n    if len(data) &lt; 4 + length:\n        raise HeaderSerializationError(f\"Insufficient data for string: need {4 + length} bytes, got {len(data)}\")\n\n    try:\n        string_value = data[4 : 4 + length].decode(\"utf-8\")\n    except UnicodeDecodeError as e:\n        raise HeaderSerializationError(f\"Cannot decode UTF-8 string: {e}\")\n\n    return string_value, 4 + length\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/headerutil/#bionemo.scdl.schema.headerutil.BinaryHeaderCodec.unpack_uint16","title":"<code>unpack_uint16(data)</code>","text":"<p>Unpack a 16-bit unsigned integer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Binary data (must be at least 2 bytes)</p> required <p>Returns:</p> Type Description <code>int</code> <p>Unpacked integer value</p> <p>Raises:</p> Type Description <code>HeaderSerializationError</code> <p>If data is insufficient or invalid</p> Source code in <code>bionemo/scdl/schema/headerutil.py</code> <pre><code>def unpack_uint16(self, data: bytes) -&gt; int:\n    \"\"\"Unpack a 16-bit unsigned integer.\n\n    Args:\n        data: Binary data (must be at least 2 bytes)\n\n    Returns:\n        Unpacked integer value\n\n    Raises:\n        HeaderSerializationError: If data is insufficient or invalid\n    \"\"\"\n    self._validate_data_length(data, 2, \"uint16\")\n    return struct.unpack(f\"{self.endianness}H\", data[:2])[0]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/headerutil/#bionemo.scdl.schema.headerutil.BinaryHeaderCodec.unpack_uint32","title":"<code>unpack_uint32(data)</code>","text":"<p>Unpack a 32-bit unsigned integer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Binary data (must be at least 4 bytes)</p> required <p>Returns:</p> Type Description <code>int</code> <p>Unpacked integer value</p> <p>Raises:</p> Type Description <code>HeaderSerializationError</code> <p>If data is insufficient or invalid</p> Source code in <code>bionemo/scdl/schema/headerutil.py</code> <pre><code>def unpack_uint32(self, data: bytes) -&gt; int:\n    \"\"\"Unpack a 32-bit unsigned integer.\n\n    Args:\n        data: Binary data (must be at least 4 bytes)\n\n    Returns:\n        Unpacked integer value\n\n    Raises:\n        HeaderSerializationError: If data is insufficient or invalid\n    \"\"\"\n    self._validate_data_length(data, 4, \"uint32\")\n    return struct.unpack(f\"{self.endianness}I\", data[:4])[0]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/headerutil/#bionemo.scdl.schema.headerutil.BinaryHeaderCodec.unpack_uint64","title":"<code>unpack_uint64(data)</code>","text":"<p>Unpack a 64-bit unsigned integer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Binary data (must be at least 8 bytes)</p> required <p>Returns:</p> Type Description <code>int</code> <p>Unpacked integer value</p> <p>Raises:</p> Type Description <code>HeaderSerializationError</code> <p>If data is insufficient or invalid</p> Source code in <code>bionemo/scdl/schema/headerutil.py</code> <pre><code>def unpack_uint64(self, data: bytes) -&gt; int:\n    \"\"\"Unpack a 64-bit unsigned integer.\n\n    Args:\n        data: Binary data (must be at least 8 bytes)\n\n    Returns:\n        Unpacked integer value\n\n    Raises:\n        HeaderSerializationError: If data is insufficient or invalid\n    \"\"\"\n    self._validate_data_length(data, 8, \"uint64\")\n    return struct.unpack(f\"{self.endianness}Q\", data[:8])[0]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/headerutil/#bionemo.scdl.schema.headerutil.BinaryHeaderCodec.unpack_uint8","title":"<code>unpack_uint8(data)</code>","text":"<p>Unpack an 8-bit unsigned integer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Binary data (must be at least 1 byte)</p> required <p>Returns:</p> Type Description <code>int</code> <p>Unpacked integer value</p> <p>Raises:</p> Type Description <code>HeaderSerializationError</code> <p>If data is insufficient or invalid</p> Source code in <code>bionemo/scdl/schema/headerutil.py</code> <pre><code>def unpack_uint8(self, data: bytes) -&gt; int:\n    \"\"\"Unpack an 8-bit unsigned integer.\n\n    Args:\n        data: Binary data (must be at least 1 byte)\n\n    Returns:\n        Unpacked integer value\n\n    Raises:\n        HeaderSerializationError: If data is insufficient or invalid\n    \"\"\"\n    self._validate_data_length(data, 1, \"uint8\")\n    return struct.unpack(f\"{self.endianness}B\", data[:1])[0]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/headerutil/#bionemo.scdl.schema.headerutil.Endianness","title":"<code>Endianness</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Byte order specifications for binary data serialization.</p> Source code in <code>bionemo/scdl/schema/headerutil.py</code> <pre><code>class Endianness(Enum):\n    \"\"\"Byte order specifications for binary data serialization.\"\"\"\n\n    NETWORK = (\n        \"!\"  # Network byte order (same as big-endian). This is a good standard, used by Protobuf and other libraries.\n    )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/headerutil/#bionemo.scdl.schema.headerutil.HeaderSerializationError","title":"<code>HeaderSerializationError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when header serialization/deserialization fails.</p> Source code in <code>bionemo/scdl/schema/headerutil.py</code> <pre><code>class HeaderSerializationError(Exception):\n    \"\"\"Raised when header serialization/deserialization fails.\"\"\"\n\n    pass\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/magic/","title":"Magic","text":"<p>SCDL Magic Number Definition.</p> <p>This module defines the magic number for SCDL archives as specified in the schema. The magic number 'SCDL' (0x5343444C) identifies valid SCDL archive headers.</p>"},{"location":"main/references/API_reference/bionemo/scdl/schema/version/","title":"Version","text":""},{"location":"main/references/API_reference/bionemo/scdl/schema/version/#bionemo.scdl.schema.version.CurrentSCDLVersion","title":"<code>CurrentSCDLVersion</code>","text":"<p>               Bases: <code>SCDLVersion</code></p> <p>Current version of the SCDL schema.</p> Source code in <code>bionemo/scdl/schema/version.py</code> <pre><code>class CurrentSCDLVersion(SCDLVersion):\n    \"\"\"Current version of the SCDL schema.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize with the current SCDL schema version: 0.1.0.\"\"\"\n        super().__init__(major=0, minor=1, point=0)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/version/#bionemo.scdl.schema.version.CurrentSCDLVersion.__init__","title":"<code>__init__()</code>","text":"<p>Initialize with the current SCDL schema version: 0.1.0.</p> Source code in <code>bionemo/scdl/schema/version.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize with the current SCDL schema version: 0.1.0.\"\"\"\n    super().__init__(major=0, minor=1, point=0)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/version/#bionemo.scdl.schema.version.SCDLVersion","title":"<code>SCDLVersion</code>","text":"<p>               Bases: <code>Version</code></p> <p>Represent the SCDL schema version.</p> <p>This class models the version of the schema used to store data in an archive.</p> Source code in <code>bionemo/scdl/schema/version.py</code> <pre><code>class SCDLVersion(Version):\n    \"\"\"Represent the SCDL schema version.\n\n    This class models the version of the schema used to store data in an archive.\n    \"\"\"\n\n    def __init__(self, major: int = 0, minor: int = 0, point: int = 0):\n        \"\"\"Initialize an SCDL schema version.\n\n        Args:\n            major (int): Major version number.\n            minor (int): Minor version number.\n            point (int): Patch/point version number.\n        \"\"\"\n        super().__init__(major, minor, point)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the semantic version string.\n\n        Returns:\n            str: Version formatted as \"major.minor.point\".\n        \"\"\"\n        return f\"{self.major}.{self.minor}.{self.point}\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return a developer-friendly representation.\n\n        Returns:\n            str: Representation including field names and values.\n        \"\"\"\n        return f\"SCDLVersion(major={self.major}, minor={self.minor}, point={self.point})\"\n\n    def __eq__(self, other: \"SCDLVersion\") -&gt; bool:\n        \"\"\"Return whether two versions are equal.\n\n        Args:\n            other (SCDLVersion): The version to compare to.\n\n        Returns:\n            bool: True if ``major``, ``minor``, and ``point`` are equal; otherwise False.\n        \"\"\"\n        return self.major == other.major and self.minor == other.minor and self.point == other.point\n\n    def __ne__(self, other: \"SCDLVersion\") -&gt; bool:\n        \"\"\"Return whether two versions are not equal.\n\n        Args:\n            other (SCDLVersion): The version to compare to.\n\n        Returns:\n            bool: True if any of ``major``, ``minor``, or ``point`` differ; otherwise False.\n        \"\"\"\n        return not self == other\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/version/#bionemo.scdl.schema.version.SCDLVersion.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return whether two versions are equal.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SCDLVersion</code> <p>The version to compare to.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if <code>major</code>, <code>minor</code>, and <code>point</code> are equal; otherwise False.</p> Source code in <code>bionemo/scdl/schema/version.py</code> <pre><code>def __eq__(self, other: \"SCDLVersion\") -&gt; bool:\n    \"\"\"Return whether two versions are equal.\n\n    Args:\n        other (SCDLVersion): The version to compare to.\n\n    Returns:\n        bool: True if ``major``, ``minor``, and ``point`` are equal; otherwise False.\n    \"\"\"\n    return self.major == other.major and self.minor == other.minor and self.point == other.point\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/version/#bionemo.scdl.schema.version.SCDLVersion.__init__","title":"<code>__init__(major=0, minor=0, point=0)</code>","text":"<p>Initialize an SCDL schema version.</p> <p>Parameters:</p> Name Type Description Default <code>major</code> <code>int</code> <p>Major version number.</p> <code>0</code> <code>minor</code> <code>int</code> <p>Minor version number.</p> <code>0</code> <code>point</code> <code>int</code> <p>Patch/point version number.</p> <code>0</code> Source code in <code>bionemo/scdl/schema/version.py</code> <pre><code>def __init__(self, major: int = 0, minor: int = 0, point: int = 0):\n    \"\"\"Initialize an SCDL schema version.\n\n    Args:\n        major (int): Major version number.\n        minor (int): Minor version number.\n        point (int): Patch/point version number.\n    \"\"\"\n    super().__init__(major, minor, point)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/version/#bionemo.scdl.schema.version.SCDLVersion.__ne__","title":"<code>__ne__(other)</code>","text":"<p>Return whether two versions are not equal.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SCDLVersion</code> <p>The version to compare to.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if any of <code>major</code>, <code>minor</code>, or <code>point</code> differ; otherwise False.</p> Source code in <code>bionemo/scdl/schema/version.py</code> <pre><code>def __ne__(self, other: \"SCDLVersion\") -&gt; bool:\n    \"\"\"Return whether two versions are not equal.\n\n    Args:\n        other (SCDLVersion): The version to compare to.\n\n    Returns:\n        bool: True if any of ``major``, ``minor``, or ``point`` differ; otherwise False.\n    \"\"\"\n    return not self == other\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/version/#bionemo.scdl.schema.version.SCDLVersion.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a developer-friendly representation.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Representation including field names and values.</p> Source code in <code>bionemo/scdl/schema/version.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return a developer-friendly representation.\n\n    Returns:\n        str: Representation including field names and values.\n    \"\"\"\n    return f\"SCDLVersion(major={self.major}, minor={self.minor}, point={self.point})\"\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/version/#bionemo.scdl.schema.version.SCDLVersion.__str__","title":"<code>__str__()</code>","text":"<p>Return the semantic version string.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Version formatted as \"major.minor.point\".</p> Source code in <code>bionemo/scdl/schema/version.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the semantic version string.\n\n    Returns:\n        str: Version formatted as \"major.minor.point\".\n    \"\"\"\n    return f\"{self.major}.{self.minor}.{self.point}\"\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/version/#bionemo.scdl.schema.version.Version","title":"<code>Version</code>","text":"<p>Generic version class (used throughout SCDL including for new backing implementations).</p> Source code in <code>bionemo/scdl/schema/version.py</code> <pre><code>class Version:\n    \"\"\"Generic version class (used throughout SCDL including for new backing implementations).\"\"\"\n\n    def __init__(self, major: int = 0, minor: int = 0, point: int = 0):\n        \"\"\"Initialize a version.\n\n        Args:\n            major (int): Major version number.\n            minor (int): Minor version number.\n            point (int): Patch/point version number.\n        \"\"\"\n        self.major = major\n        self.minor = minor\n        self.point = point\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/schema/version/#bionemo.scdl.schema.version.Version.__init__","title":"<code>__init__(major=0, minor=0, point=0)</code>","text":"<p>Initialize a version.</p> <p>Parameters:</p> Name Type Description Default <code>major</code> <code>int</code> <p>Major version number.</p> <code>0</code> <code>minor</code> <code>int</code> <p>Minor version number.</p> <code>0</code> <code>point</code> <code>int</code> <p>Patch/point version number.</p> <code>0</code> Source code in <code>bionemo/scdl/schema/version.py</code> <pre><code>def __init__(self, major: int = 0, minor: int = 0, point: int = 0):\n    \"\"\"Initialize a version.\n\n    Args:\n        major (int): Major version number.\n        minor (int): Minor version number.\n        point (int): Patch/point version number.\n    \"\"\"\n    self.major = major\n    self.minor = minor\n    self.point = point\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/scripts/convert_h5ad_to_scdl/","title":"Convert h5ad to scdl","text":""},{"location":"main/references/API_reference/bionemo/scdl/scripts/convert_h5ad_to_scdl/#bionemo.scdl.scripts.convert_h5ad_to_scdl.main","title":"<code>main()</code>","text":"<p>Parse the arguments to process the single cell collection.</p> Source code in <code>bionemo/scdl/scripts/convert_h5ad_to_scdl.py</code> <pre><code>def main():\n    \"\"\"Parse the arguments to process the single cell collection.\"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--num-workers\", type=int, default=4, help=\"The number of AnnData loaders to run in parallel [4].\"\n    )\n    parser.add_argument(\n        \"--use-mp\",\n        action=\"store_true\",\n        default=False,\n        help=\"Use a subprocess for each worker rather than a lightweight OS thread [False].\",\n    )\n    parser.add_argument(\n        \"--data-path\",\n        type=str,\n        required=True,\n        help=\"A path containing AnnData files. Note: These will all be concatenated.\",\n    )\n    parser.add_argument(\n        \"--save-path\", required=True, type=str, help=\"An output path where an SCDataset will be stored.\"\n    )\n\n    parser.add_argument(\n        \"--data-dtype\",\n        type=str,\n        default=None,\n        help=\"The data type to use for the SCDataset. Must be one of 'uint8', 'uint16', 'uint32', 'uint64', 'float32', 'float64'.\",\n    )\n    parser.add_argument(\n        \"--paginated-load-cutoff\",\n        type=int,\n        default=10_000,\n        help=\"The cutoff in MB for paginated loading of the AnnData files.\",\n    )\n    parser.add_argument(\n        \"--load-block-row-size\",\n        type=int,\n        default=1_000_000,\n        help=\"The number of rows to load into memory at a time for paginated loading of the AnnData files.\",\n    )\n    parser.add_argument(\n        \"--use-X-not-raw\",\n        action=\"store_true\",\n        default=False,\n        help=\"Use .X instead of raw.X from the anndata file [False].\",\n    )\n    args = parser.parse_args()\n\n    with tempfile.TemporaryDirectory() as temp_dir:\n        coll = SingleCellCollection(temp_dir)\n        coll.load_h5ad_multi(\n            args.data_path,\n            max_workers=args.num_workers,\n            use_processes=args.use_mp,\n            data_dtype=args.data_dtype,\n            paginated_load_cutoff=args.paginated_load_cutoff,\n            load_block_row_size=args.load_block_row_size,\n            use_X_not_raw=args.use_X_not_raw,\n        )\n        coll.flatten(args.save_path, destroy_on_copy=True)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/util/async_worker_queue/","title":"Async worker queue","text":""},{"location":"main/references/API_reference/bionemo/scdl/util/async_worker_queue/#bionemo.scdl.util.async_worker_queue.AsyncWorkQueue","title":"<code>AsyncWorkQueue</code>","text":"<p>Implements an asynchronous queue.</p> Source code in <code>bionemo/scdl/util/async_worker_queue.py</code> <pre><code>class AsyncWorkQueue:\n    \"\"\"Implements an asynchronous queue.\"\"\"\n\n    def __init__(self, max_workers: int = 5, use_processes: bool = False) -&gt; None:\n        \"\"\"Initialize the AsyncWorkQueue.\n\n        Args:\n            max_workers: The maximum number of worker threads or processes.\n            use_processes: If True, use ProcessPoolExecutor; otherwise, use ThreadPoolExecutor.\n        \"\"\"\n        self.use_processes = use_processes\n        if use_processes:\n            self.executor: Union[concurrent.futures.ThreadPoolExecutor, concurrent.futures.ProcessPoolExecutor] = (\n                concurrent.futures.ProcessPoolExecutor(max_workers=max_workers)\n            )\n        else:\n            self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)\n        self.lock = threading.Lock()\n        self.tasks: List[concurrent.futures.Future] = []\n\n    def submit_task(self, func: Callable[..., Any], *args: Any, **kwargs: Any) -&gt; concurrent.futures.Future:\n        \"\"\"Submit a task to the work queue.\n\n        Args:\n            func: The function to be executed asynchronously.\n            args: Positional arguments to pass to the function.\n            kwargs: Keyword arguments to pass to the function.\n            A Future object representing the execution of the function.\n\n        Returns:\n            Future: placeholder for the asynchronous operation.\n        \"\"\"\n        with self.lock:\n            future = self.executor.submit(func, *args, **kwargs)\n            self.tasks.append(future)\n            return future\n\n    def shutdown(self, wait: bool = True) -&gt; None:\n        \"\"\"Shutdown the executor and wait for the tasks to complete.\n\n        Args:\n            wait: If True, wait for all tasks to complete before shutting down.\n        \"\"\"\n        self.executor.shutdown(wait=wait)\n\n    def get_completed_tasks(self) -&gt; List[concurrent.futures.Future]:\n        \"\"\"Get the list of completed tasks.\n\n        Returns:\n            A list of Future objects that are completed.\n        \"\"\"\n        with self.lock:\n            completed_tasks = [task for task in self.tasks if task.done()]\n            return completed_tasks\n\n    def get_pending_tasks(self) -&gt; List[concurrent.futures.Future]:\n        \"\"\"Get the list of pending tasks.\n\n        Returns:\n            A list of Future objects that are not yet completed.\n        \"\"\"\n        with self.lock:\n            pending_tasks = [task for task in self.tasks if not task.done()]\n            return pending_tasks\n\n    def get_task_results(self) -&gt; List[Any]:\n        \"\"\"Get the results of all completed tasks.\n\n        Returns:\n            A list of results from the completed tasks.\n\n        Raises:\n            Exception: This would be expected if the task fails to complete or\n            if is cancelled.\n        \"\"\"\n        completed_tasks = self.get_completed_tasks()\n        results = []\n        for task in completed_tasks:\n            try:\n                results.append(task.result())\n            except Exception as e:\n                results.append(e)\n        return results\n\n    def wait(self) -&gt; List[Any]:\n        \"\"\"Wait for all submitted tasks to complete and return their results.\n\n        Returns:\n            A list of results from all completed tasks.\n        \"\"\"\n        # Wait for all tasks to complete\n        concurrent.futures.wait(self.tasks)\n\n        # Collect results from all tasks\n        results = []\n        for task in self.tasks:\n            try:\n                results.append(task.result())\n            except Exception as e:\n                results.append(e)\n\n        return results\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/util/async_worker_queue/#bionemo.scdl.util.async_worker_queue.AsyncWorkQueue.__init__","title":"<code>__init__(max_workers=5, use_processes=False)</code>","text":"<p>Initialize the AsyncWorkQueue.</p> <p>Parameters:</p> Name Type Description Default <code>max_workers</code> <code>int</code> <p>The maximum number of worker threads or processes.</p> <code>5</code> <code>use_processes</code> <code>bool</code> <p>If True, use ProcessPoolExecutor; otherwise, use ThreadPoolExecutor.</p> <code>False</code> Source code in <code>bionemo/scdl/util/async_worker_queue.py</code> <pre><code>def __init__(self, max_workers: int = 5, use_processes: bool = False) -&gt; None:\n    \"\"\"Initialize the AsyncWorkQueue.\n\n    Args:\n        max_workers: The maximum number of worker threads or processes.\n        use_processes: If True, use ProcessPoolExecutor; otherwise, use ThreadPoolExecutor.\n    \"\"\"\n    self.use_processes = use_processes\n    if use_processes:\n        self.executor: Union[concurrent.futures.ThreadPoolExecutor, concurrent.futures.ProcessPoolExecutor] = (\n            concurrent.futures.ProcessPoolExecutor(max_workers=max_workers)\n        )\n    else:\n        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)\n    self.lock = threading.Lock()\n    self.tasks: List[concurrent.futures.Future] = []\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/util/async_worker_queue/#bionemo.scdl.util.async_worker_queue.AsyncWorkQueue.get_completed_tasks","title":"<code>get_completed_tasks()</code>","text":"<p>Get the list of completed tasks.</p> <p>Returns:</p> Type Description <code>List[Future]</code> <p>A list of Future objects that are completed.</p> Source code in <code>bionemo/scdl/util/async_worker_queue.py</code> <pre><code>def get_completed_tasks(self) -&gt; List[concurrent.futures.Future]:\n    \"\"\"Get the list of completed tasks.\n\n    Returns:\n        A list of Future objects that are completed.\n    \"\"\"\n    with self.lock:\n        completed_tasks = [task for task in self.tasks if task.done()]\n        return completed_tasks\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/util/async_worker_queue/#bionemo.scdl.util.async_worker_queue.AsyncWorkQueue.get_pending_tasks","title":"<code>get_pending_tasks()</code>","text":"<p>Get the list of pending tasks.</p> <p>Returns:</p> Type Description <code>List[Future]</code> <p>A list of Future objects that are not yet completed.</p> Source code in <code>bionemo/scdl/util/async_worker_queue.py</code> <pre><code>def get_pending_tasks(self) -&gt; List[concurrent.futures.Future]:\n    \"\"\"Get the list of pending tasks.\n\n    Returns:\n        A list of Future objects that are not yet completed.\n    \"\"\"\n    with self.lock:\n        pending_tasks = [task for task in self.tasks if not task.done()]\n        return pending_tasks\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/util/async_worker_queue/#bionemo.scdl.util.async_worker_queue.AsyncWorkQueue.get_task_results","title":"<code>get_task_results()</code>","text":"<p>Get the results of all completed tasks.</p> <p>Returns:</p> Type Description <code>List[Any]</code> <p>A list of results from the completed tasks.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>This would be expected if the task fails to complete or</p> Source code in <code>bionemo/scdl/util/async_worker_queue.py</code> <pre><code>def get_task_results(self) -&gt; List[Any]:\n    \"\"\"Get the results of all completed tasks.\n\n    Returns:\n        A list of results from the completed tasks.\n\n    Raises:\n        Exception: This would be expected if the task fails to complete or\n        if is cancelled.\n    \"\"\"\n    completed_tasks = self.get_completed_tasks()\n    results = []\n    for task in completed_tasks:\n        try:\n            results.append(task.result())\n        except Exception as e:\n            results.append(e)\n    return results\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/util/async_worker_queue/#bionemo.scdl.util.async_worker_queue.AsyncWorkQueue.shutdown","title":"<code>shutdown(wait=True)</code>","text":"<p>Shutdown the executor and wait for the tasks to complete.</p> <p>Parameters:</p> Name Type Description Default <code>wait</code> <code>bool</code> <p>If True, wait for all tasks to complete before shutting down.</p> <code>True</code> Source code in <code>bionemo/scdl/util/async_worker_queue.py</code> <pre><code>def shutdown(self, wait: bool = True) -&gt; None:\n    \"\"\"Shutdown the executor and wait for the tasks to complete.\n\n    Args:\n        wait: If True, wait for all tasks to complete before shutting down.\n    \"\"\"\n    self.executor.shutdown(wait=wait)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/util/async_worker_queue/#bionemo.scdl.util.async_worker_queue.AsyncWorkQueue.submit_task","title":"<code>submit_task(func, *args, **kwargs)</code>","text":"<p>Submit a task to the work queue.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any]</code> <p>The function to be executed asynchronously.</p> required <code>args</code> <code>Any</code> <p>Positional arguments to pass to the function.</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>Keyword arguments to pass to the function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Future</code> <code>Future</code> <p>placeholder for the asynchronous operation.</p> Source code in <code>bionemo/scdl/util/async_worker_queue.py</code> <pre><code>def submit_task(self, func: Callable[..., Any], *args: Any, **kwargs: Any) -&gt; concurrent.futures.Future:\n    \"\"\"Submit a task to the work queue.\n\n    Args:\n        func: The function to be executed asynchronously.\n        args: Positional arguments to pass to the function.\n        kwargs: Keyword arguments to pass to the function.\n        A Future object representing the execution of the function.\n\n    Returns:\n        Future: placeholder for the asynchronous operation.\n    \"\"\"\n    with self.lock:\n        future = self.executor.submit(func, *args, **kwargs)\n        self.tasks.append(future)\n        return future\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/util/async_worker_queue/#bionemo.scdl.util.async_worker_queue.AsyncWorkQueue.wait","title":"<code>wait()</code>","text":"<p>Wait for all submitted tasks to complete and return their results.</p> <p>Returns:</p> Type Description <code>List[Any]</code> <p>A list of results from all completed tasks.</p> Source code in <code>bionemo/scdl/util/async_worker_queue.py</code> <pre><code>def wait(self) -&gt; List[Any]:\n    \"\"\"Wait for all submitted tasks to complete and return their results.\n\n    Returns:\n        A list of results from all completed tasks.\n    \"\"\"\n    # Wait for all tasks to complete\n    concurrent.futures.wait(self.tasks)\n\n    # Collect results from all tasks\n    results = []\n    for task in self.tasks:\n        try:\n            results.append(task.result())\n        except Exception as e:\n            results.append(e)\n\n    return results\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/util/filecopyutil/","title":"Filecopyutil","text":""},{"location":"main/references/API_reference/bionemo/scdl/util/filecopyutil/#bionemo.scdl.util.filecopyutil.extend_files","title":"<code>extend_files(first, second, source_dtype, dest_dtype, elements_per_chunk=10 * 1024 * 1024, delete_file2_on_complete=False, offset=0, add_value=None, allow_downscaling=False)</code>","text":"<p>Concatenates the contents of <code>second</code> into <code>first</code> using memory-efficient operations.</p> <p>Supports optional dtype conversion for upscaling within the same family only: - uint upscaling: uint8 \u2192 uint16 \u2192 uint32 \u2192 uint64 - float upscaling:  float32 \u2192 float64</p> <p>Additionally, supports adding a scalar value to each converted element during copy.</p> <p>Parameters: - first (str): Destination file path (will be extended). - second (str): Source file path (data read from here). - source_dtype (str): Source numpy dtype (e.g., 'uint32', 'float32'). - dest_dtype (str): Destination numpy dtype (e.g., 'uint64', 'float32'). - elements_per_chunk (int): Number of elements to read/write per chunk. - delete_file2_on_complete (bool): Whether to delete the source after completion. - offset (int): Byte offset to start reading within the source file. - add_value (int | None): Optional scalar added to each converted element (after casting). - allow_downscaling (bool): Whether to allow downscaling of the data dtype.</p> <p>Raises: - ValueError: If conversion is not a safe upscaling operation.</p> Source code in <code>bionemo/scdl/util/filecopyutil.py</code> <pre><code>def extend_files(\n    first: str,\n    second: str,\n    source_dtype: str,\n    dest_dtype: str,\n    elements_per_chunk: int = 10 * 1024 * 1024,\n    delete_file2_on_complete: bool = False,\n    offset: int = 0,\n    add_value: int | None = None,\n    allow_downscaling: bool = False,\n):\n    \"\"\"Concatenates the contents of `second` into `first` using memory-efficient operations.\n\n    Supports optional dtype conversion for upscaling within the same family only:\n    - uint upscaling: uint8 \u2192 uint16 \u2192 uint32 \u2192 uint64\n    - float upscaling:  float32 \u2192 float64\n\n    Additionally, supports adding a scalar value to each converted element during copy.\n\n    Parameters:\n    - first (str): Destination file path (will be extended).\n    - second (str): Source file path (data read from here).\n    - source_dtype (str): Source numpy dtype (e.g., 'uint32', 'float32').\n    - dest_dtype (str): Destination numpy dtype (e.g., 'uint64', 'float32').\n    - elements_per_chunk (int): Number of elements to read/write per chunk.\n    - delete_file2_on_complete (bool): Whether to delete the source after completion.\n    - offset (int): Byte offset to start reading within the source file.\n    - add_value (int | None): Optional scalar added to each converted element (after casting).\n    - allow_downscaling (bool): Whether to allow downscaling of the data dtype.\n\n    Raises:\n    - ValueError: If conversion is not a safe upscaling operation.\n\n    \"\"\"\n    if offset &lt; 0 or offset % np.dtype(source_dtype).itemsize != 0:\n        raise ValueError(\n            f\"Offset {offset} must be non-negative and divisible by source dtype size {np.dtype(source_dtype).itemsize}\"\n        )\n    if not allow_downscaling:\n        if source_dtype in INT_ORDER and dest_dtype in INT_ORDER:\n            order = INT_ORDER\n        elif source_dtype in FLOAT_ORDER and dest_dtype in FLOAT_ORDER:\n            order = FLOAT_ORDER\n        else:\n            raise ValueError(\n                f\"Unsupported dtype conversion: {source_dtype} \u2192 {dest_dtype}. Only same-family upscaling allowed.\"\n            )\n        if order.index(dest_dtype) &lt; order.index(source_dtype):\n            raise ValueError(f\"Downscaling not allowed: {source_dtype} \u2192 {dest_dtype}.\")\n\n    # Resolve dtypes once (native endianness) and sizes\n    source_dtype = np.dtype(source_dtype).newbyteorder(\"=\")\n    dest_dtype = np.dtype(dest_dtype).newbyteorder(\"=\")\n    src_item = source_dtype.itemsize\n    dst_item = dest_dtype.itemsize\n    # Pre-cast scalar once to destination dtype for speed\n    add_scalar = None\n    if add_value is not None and add_value != 0:\n        add_scalar = np.array(add_value, dtype=dest_dtype).item()\n\n    # Source sizing\n    size2 = os.path.getsize(second)\n    remaining = size2 - offset\n    if remaining % src_item != 0:\n        raise ValueError(\n            f\"Source size minus offset ({remaining} bytes) not divisible by source dtype size ({src_item}).\"\n        )\n    num_elements = remaining // src_item\n\n    # Pre-extend destination to final size\n    extend_bytes = num_elements * dst_item\n    size1 = os.path.getsize(first)\n    with open(first, \"r+b\") as f_dest:\n        if extend_bytes &gt; 0:\n            f_dest.seek(size1 + extend_bytes - 1)\n            f_dest.write(b\"\\0\")\n\n        write_position = size1\n\n        # Reusable output buffer\n        out_buf = bytearray(elements_per_chunk * dst_item)\n\n        with open(second, \"rb\") as f_source:\n            if offset &gt; 0:\n                f_source.seek(offset)\n\n            elements_processed = 0\n            while elements_processed &lt; num_elements:\n                target_elements = min(elements_per_chunk, num_elements - elements_processed)\n                bytes_to_read = target_elements * src_item\n\n                chunk_bytes = f_source.read(bytes_to_read)\n                if not chunk_bytes:\n                    # Unexpected EOF\n                    raise OSError(f\"Short read at element {elements_processed}: expected {bytes_to_read} bytes, got 0\")\n\n                # Derive actual elements from bytes read to tolerate partial reads\n                actual_elements = len(chunk_bytes) // src_item\n                if actual_elements == 0:\n                    continue\n\n                if source_dtype == dest_dtype and add_scalar is None and len(chunk_bytes) == bytes_to_read:\n                    dst_mv = chunk_bytes\n                else:\n                    src = np.frombuffer(chunk_bytes, dtype=source_dtype, count=actual_elements)\n                    dst_mv = memoryview(out_buf)[: actual_elements * dst_item]\n                    dst = np.frombuffer(dst_mv, dtype=dest_dtype, count=actual_elements)\n                    if add_scalar is not None:\n                        np.add(src.astype(dest_dtype, copy=False), add_scalar, out=dst)\n                    else:\n                        safe_casting = \"unsafe\" if allow_downscaling else \"safe\"\n                        np.copyto(dst, src, casting=safe_casting)\n\n                f_dest.seek(write_position)\n                f_dest.write(dst_mv)\n                write_position += len(dst_mv)\n                elements_processed += actual_elements\n\n    if delete_file2_on_complete:\n        os.remove(second)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/util/memmap_utils/","title":"Memmap utils","text":"<p>Utility functions for memory-mapped dataset operations.</p> <p>This module contains helper functions for: - Data type casting - Sparse array manipulation - Memory-mapped array creation</p>"},{"location":"main/references/API_reference/bionemo/scdl/util/memmap_utils/#bionemo.scdl.util.memmap_utils.determine_dtype","title":"<code>determine_dtype(dtypes)</code>","text":"<p>Choose a common destination dtype by same-family upscaling.</p> <ul> <li>If all source dtypes are unsigned integers: return the widest unsigned int</li> <li>If all source dtypes are floats: return the widest float</li> <li>Otherwise: raise (mixed families not allowed)</li> </ul> Source code in <code>bionemo/scdl/util/memmap_utils.py</code> <pre><code>def determine_dtype(dtypes: Iterable[object]) -&gt; str:\n    \"\"\"Choose a common destination dtype by same-family upscaling.\n\n    - If all source dtypes are unsigned integers: return the widest unsigned int\n    - If all source dtypes are floats: return the widest float\n    - Otherwise: raise (mixed families not allowed)\n    \"\"\"\n    if len(dtypes) == 0:\n        raise ValueError(\"No dtypes provided\")\n    canonical = [np.dtype(dt).name for dt in dtypes]\n    if all(dt in INT_ORDER for dt in canonical):\n        return max(set(canonical), key=lambda dt: INT_ORDER.index(dt))\n    if all(dt in FLOAT_ORDER for dt in canonical):\n        return max(set(canonical), key=lambda dt: FLOAT_ORDER.index(dt))\n    raise ValueError(f\"Mixed float and integer dtype families not allowed: {sorted(set(canonical))}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/util/memmap_utils/#bionemo.scdl.util.memmap_utils.smallest_uint_dtype","title":"<code>smallest_uint_dtype(x)</code>","text":"<p>Returns the smallest unsigned integer dtype that can represent the given number.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>int</code> <p>The number to represent</p> required <p>Returns:</p> Type Description <p>The smallest unsigned integer dtype that can represent the given number</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If x is negative or too large to represent</p> Source code in <code>bionemo/scdl/util/memmap_utils.py</code> <pre><code>def smallest_uint_dtype(x: int):\n    \"\"\"Returns the smallest unsigned integer dtype that can represent the given number.\n\n    Args:\n        x: The number to represent\n\n    Returns:\n        The smallest unsigned integer dtype that can represent the given number\n\n    Raises:\n        ValueError: If x is negative or too large to represent\n    \"\"\"\n    if x &lt; 0:\n        raise ValueError(\"Negative numbers can't be unsigned.\")\n    for dtype, bits in [(\"uint8\", 8), (\"uint16\", 16), (\"uint32\", 32), (\"uint64\", 64)]:\n        if x &lt; (1 &lt;&lt; bits):\n            return dtype\n    raise ValueError(f\"No unsigned integer dtype can represent the given number: {x}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/util/scdl_constants/","title":"Scdl constants","text":"<p>Constants and enums shared across SCDL format specification and implementation.</p> <p>This module provides a single source of truth for: - Array identifiers and their filesystem mappings - Data type specifications - Backend implementations - File and metadata constants</p>"},{"location":"main/references/API_reference/bionemo/scdl/util/scdl_constants/#bionemo.scdl.util.scdl_constants.ArrayDType","title":"<code>ArrayDType</code>","text":"<p>               Bases: <code>IntEnum</code></p> <p>Numpy dtype specification for arrays in SCDL archives.</p> <p>Integer values are used in the binary format for efficient storage.</p> Source code in <code>bionemo/scdl/util/scdl_constants.py</code> <pre><code>class ArrayDType(IntEnum):\n    \"\"\"Numpy dtype specification for arrays in SCDL archives.\n\n    Integer values are used in the binary format for efficient storage.\n    \"\"\"\n\n    UINT8_ARRAY = 1\n    UINT16_ARRAY = 2\n    UINT32_ARRAY = 3\n    UINT64_ARRAY = 4\n    FLOAT16_ARRAY = 5\n    FLOAT32_ARRAY = 6\n    FLOAT64_ARRAY = 7\n    STRING_ARRAY = 8\n    FIXED_STRING_ARRAY = 9\n\n    @property\n    def numpy_dtype_string(self) -&gt; str:\n        \"\"\"Get the corresponding NumPy dtype string.\"\"\"\n        dtype_map = {\n            self.UINT8_ARRAY: \"uint8\",\n            self.UINT16_ARRAY: \"uint16\",\n            self.UINT32_ARRAY: \"uint32\",\n            self.UINT64_ARRAY: \"uint64\",\n            self.FLOAT16_ARRAY: \"float16\",\n            self.FLOAT32_ARRAY: \"float32\",\n            self.FLOAT64_ARRAY: \"float64\",\n            self.STRING_ARRAY: \"string\",\n            self.FIXED_STRING_ARRAY: \"fixed_string\",\n        }\n        return dtype_map[self]\n\n    @classmethod\n    def from_numpy_dtype(cls, dtype) -&gt; \"ArrayDType\":\n        \"\"\"Convert a numpy dtype to ArrayDType enum.\n\n        Args:\n            dtype: numpy dtype object or string representation\n\n        Returns:\n            Corresponding ArrayDType enum value\n\n        Raises:\n            ValueError: If dtype is not supported\n        \"\"\"\n        # Convert dtype object to string if needed\n        if isinstance(dtype, type) and hasattr(dtype, \"__name__\"):\n            # Handle numpy type classes like np.float32, np.uint32\n            dtype_str = dtype.__name__\n        elif hasattr(dtype, \"name\"):\n            # Handle numpy dtype instances\n            dtype_str = dtype.name\n        elif hasattr(dtype, \"dtype\"):\n            dtype_str = dtype.dtype.name\n        else:\n            dtype_str = str(dtype)\n\n        # Map numpy dtype strings to ArrayDType enums\n        dtype_map = {\n            \"uint8\": cls.UINT8_ARRAY,\n            \"uint16\": cls.UINT16_ARRAY,\n            \"uint32\": cls.UINT32_ARRAY,\n            \"uint64\": cls.UINT64_ARRAY,\n            \"float16\": cls.FLOAT16_ARRAY,\n            \"float32\": cls.FLOAT32_ARRAY,\n            \"float64\": cls.FLOAT64_ARRAY,\n            \"object\": cls.STRING_ARRAY,  # Object arrays often contain strings\n            \"str\": cls.STRING_ARRAY,\n            \"&lt;U\": cls.FIXED_STRING_ARRAY,  # Unicode string arrays\n        }\n\n        # Handle variations and aliases\n        if dtype_str.startswith(\"&lt;U\") or dtype_str.startswith(\"&gt;U\"):\n            return cls.FIXED_STRING_ARRAY\n        elif dtype_str.startswith(\"&lt;f\") or dtype_str.startswith(\"&gt;f\"):\n            if \"4\" in dtype_str:\n                return cls.FLOAT32_ARRAY\n            elif \"8\" in dtype_str:\n                return cls.FLOAT64_ARRAY\n            elif \"2\" in dtype_str:\n                return cls.FLOAT16_ARRAY\n        elif dtype_str.startswith((\"&lt;u\", \"&gt;u\")):\n            if \"1\" in dtype_str:\n                return cls.UINT8_ARRAY\n            elif \"2\" in dtype_str:\n                return cls.UINT16_ARRAY\n            elif \"4\" in dtype_str:\n                return cls.UINT32_ARRAY\n            elif \"8\" in dtype_str:\n                return cls.UINT64_ARRAY\n        elif dtype_str.startswith((\"&lt;i\", \"&gt;i\")):\n            raise ValueError(f\"Signed integer dtypes are not supported: {dtype_str}\")\n\n        # Try direct mapping\n        if dtype_str in dtype_map:\n            return dtype_map[dtype_str]\n\n        # Default fallback for common types\n        if \"float32\" in dtype_str or \"f4\" in dtype_str:\n            return cls.FLOAT32_ARRAY\n        elif \"float64\" in dtype_str or \"f8\" in dtype_str:\n            return cls.FLOAT64_ARRAY\n        # Do not silently map signed ints; require explicit handling upstream\n        elif \"int32\" in dtype_str or \"i4\" in dtype_str or \"int64\" in dtype_str or \"i8\" in dtype_str:\n            raise ValueError(f\"Signed integer dtypes are not supported: {dtype_str}\")\n\n        raise ValueError(f\"Unsupported numpy dtype: {dtype_str} (original: {dtype})\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/util/scdl_constants/#bionemo.scdl.util.scdl_constants.ArrayDType.numpy_dtype_string","title":"<code>numpy_dtype_string</code>  <code>property</code>","text":"<p>Get the corresponding NumPy dtype string.</p>"},{"location":"main/references/API_reference/bionemo/scdl/util/scdl_constants/#bionemo.scdl.util.scdl_constants.ArrayDType.from_numpy_dtype","title":"<code>from_numpy_dtype(dtype)</code>  <code>classmethod</code>","text":"<p>Convert a numpy dtype to ArrayDType enum.</p> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <p>numpy dtype object or string representation</p> required <p>Returns:</p> Type Description <code>ArrayDType</code> <p>Corresponding ArrayDType enum value</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dtype is not supported</p> Source code in <code>bionemo/scdl/util/scdl_constants.py</code> <pre><code>@classmethod\ndef from_numpy_dtype(cls, dtype) -&gt; \"ArrayDType\":\n    \"\"\"Convert a numpy dtype to ArrayDType enum.\n\n    Args:\n        dtype: numpy dtype object or string representation\n\n    Returns:\n        Corresponding ArrayDType enum value\n\n    Raises:\n        ValueError: If dtype is not supported\n    \"\"\"\n    # Convert dtype object to string if needed\n    if isinstance(dtype, type) and hasattr(dtype, \"__name__\"):\n        # Handle numpy type classes like np.float32, np.uint32\n        dtype_str = dtype.__name__\n    elif hasattr(dtype, \"name\"):\n        # Handle numpy dtype instances\n        dtype_str = dtype.name\n    elif hasattr(dtype, \"dtype\"):\n        dtype_str = dtype.dtype.name\n    else:\n        dtype_str = str(dtype)\n\n    # Map numpy dtype strings to ArrayDType enums\n    dtype_map = {\n        \"uint8\": cls.UINT8_ARRAY,\n        \"uint16\": cls.UINT16_ARRAY,\n        \"uint32\": cls.UINT32_ARRAY,\n        \"uint64\": cls.UINT64_ARRAY,\n        \"float16\": cls.FLOAT16_ARRAY,\n        \"float32\": cls.FLOAT32_ARRAY,\n        \"float64\": cls.FLOAT64_ARRAY,\n        \"object\": cls.STRING_ARRAY,  # Object arrays often contain strings\n        \"str\": cls.STRING_ARRAY,\n        \"&lt;U\": cls.FIXED_STRING_ARRAY,  # Unicode string arrays\n    }\n\n    # Handle variations and aliases\n    if dtype_str.startswith(\"&lt;U\") or dtype_str.startswith(\"&gt;U\"):\n        return cls.FIXED_STRING_ARRAY\n    elif dtype_str.startswith(\"&lt;f\") or dtype_str.startswith(\"&gt;f\"):\n        if \"4\" in dtype_str:\n            return cls.FLOAT32_ARRAY\n        elif \"8\" in dtype_str:\n            return cls.FLOAT64_ARRAY\n        elif \"2\" in dtype_str:\n            return cls.FLOAT16_ARRAY\n    elif dtype_str.startswith((\"&lt;u\", \"&gt;u\")):\n        if \"1\" in dtype_str:\n            return cls.UINT8_ARRAY\n        elif \"2\" in dtype_str:\n            return cls.UINT16_ARRAY\n        elif \"4\" in dtype_str:\n            return cls.UINT32_ARRAY\n        elif \"8\" in dtype_str:\n            return cls.UINT64_ARRAY\n    elif dtype_str.startswith((\"&lt;i\", \"&gt;i\")):\n        raise ValueError(f\"Signed integer dtypes are not supported: {dtype_str}\")\n\n    # Try direct mapping\n    if dtype_str in dtype_map:\n        return dtype_map[dtype_str]\n\n    # Default fallback for common types\n    if \"float32\" in dtype_str or \"f4\" in dtype_str:\n        return cls.FLOAT32_ARRAY\n    elif \"float64\" in dtype_str or \"f8\" in dtype_str:\n        return cls.FLOAT64_ARRAY\n    # Do not silently map signed ints; require explicit handling upstream\n    elif \"int32\" in dtype_str or \"i4\" in dtype_str or \"int64\" in dtype_str or \"i8\" in dtype_str:\n        raise ValueError(f\"Signed integer dtypes are not supported: {dtype_str}\")\n\n    raise ValueError(f\"Unsupported numpy dtype: {dtype_str} (original: {dtype})\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/util/scdl_constants/#bionemo.scdl.util.scdl_constants.Backend","title":"<code>Backend</code>","text":"<p>               Bases: <code>IntEnum</code></p> <p>Backend implementations for SCDL archives.</p> <p>Defines how array data is stored and accessed.</p> Source code in <code>bionemo/scdl/util/scdl_constants.py</code> <pre><code>class Backend(IntEnum):\n    \"\"\"Backend implementations for SCDL archives.\n\n    Defines how array data is stored and accessed.\n    \"\"\"\n\n    MEMMAP_V0 = 1\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/util/scdl_constants/#bionemo.scdl.util.scdl_constants.FileNames","title":"<code>FileNames</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>All files in SCDL archive.</p> <p>This enum contains both array data files and special metadata files. For arrays, use the <code>array_name</code> property to get the canonical header name.</p> Source code in <code>bionemo/scdl/util/scdl_constants.py</code> <pre><code>class FileNames(str, Enum):\n    \"\"\"All files in SCDL archive.\n\n    This enum contains both array data files and special metadata files.\n    For arrays, use the `array_name` property to get the canonical header name.\n    \"\"\"\n\n    # Array data files\n    DATA = \"data.npy\"\n    ROWPTR = \"row_ptr.npy\"\n    COLPTR = \"col_ptr.npy\"\n    NEIGHBOR_INDICES = \"neighbor_indices.npy\"\n    NEIGHBOR_INDICES_PTR = \"neighbor_indptr.npy\"\n    NEIGHBOR_VALUES = \"neighbor_values.npy\"\n    METADATA = \"metadata.json\"\n    FEATURES = \"features\"\n    VAR_FEATURES = \"var_features\"\n    OBS_FEATURES = \"obs_features\"\n    VERSION = \"version.json\"\n    HEADER = \"header.sch\"\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/util/scdl_constants/#bionemo.scdl.util.scdl_constants.Mode","title":"<code>Mode</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Valid modes for file I/O operations.</p> <p>The write append mode is 'w+' while the read append mode is 'r+'.</p> Source code in <code>bionemo/scdl/util/scdl_constants.py</code> <pre><code>class Mode(str, Enum):\n    \"\"\"Valid modes for file I/O operations.\n\n    The write append mode is 'w+' while the read append mode is 'r+'.\n    \"\"\"\n\n    CREATE_APPEND = \"w+\"\n    READ_APPEND = \"r+\"\n    READ = \"r\"\n    CREATE = \"w\"\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/util/scdl_constants/#bionemo.scdl.util.scdl_constants.NeighborSamplingStrategy","title":"<code>NeighborSamplingStrategy</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Valid sampling strategies for neighbor selection.</p> Source code in <code>bionemo/scdl/util/scdl_constants.py</code> <pre><code>class NeighborSamplingStrategy(str, Enum):\n    \"\"\"Valid sampling strategies for neighbor selection.\"\"\"\n\n    RANDOM = \"random\"\n    FIRST = \"first\"\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/util/torch_dataloader_utils/","title":"Torch dataloader utils","text":""},{"location":"main/references/API_reference/bionemo/scdl/util/torch_dataloader_utils/#bionemo.scdl.util.torch_dataloader_utils.collate_neighbor_sparse_matrix_batch","title":"<code>collate_neighbor_sparse_matrix_batch(batch)</code>","text":"<p>Collates a batch of samples with neighbor data into a single batch.</p> <p>This collation function handles the output format when SingleCellMemMapDataset is used with load_neighbors=True and get_row_with_neighbor() returns tuples.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Dict]</code> <p>List of dictionaries, each containing:    - 'current_cell': Tuple[np.ndarray, np.ndarray] (values, columns)    - 'next_cell': Tuple[np.ndarray, np.ndarray] (values, columns)    - 'current_cell_index': int    - 'next_cell_index': int</p> required <p>Returns:</p> Type Description <code>Dict[str, Union[Tensor, List[int], int]]</code> <p>Dict containing:</p> <code>Dict[str, Union[Tensor, List[int], int]]</code> <ul> <li>'current_cells': Sparse tensor containing all current cells</li> </ul> <code>Dict[str, Union[Tensor, List[int], int]]</code> <ul> <li>'next_cells': Sparse tensor containing all next cells</li> </ul> <code>Dict[str, Union[Tensor, List[int], int]]</code> <ul> <li>'current_cell_indices': List of original indices for current cells</li> </ul> <code>Dict[str, Union[Tensor, List[int], int]]</code> <ul> <li>'next_cell_indices': List of original indices for next cells</li> </ul> <code>Dict[str, Union[Tensor, List[int], int]]</code> <ul> <li>'batch_size': Number of samples in the batch</li> </ul> Source code in <code>bionemo/scdl/util/torch_dataloader_utils.py</code> <pre><code>def collate_neighbor_sparse_matrix_batch(batch: List[Dict]) -&gt; Dict[str, Union[torch.Tensor, List[int], int]]:\n    \"\"\"Collates a batch of samples with neighbor data into a single batch.\n\n    This collation function handles the output format when SingleCellMemMapDataset\n    is used with load_neighbors=True and get_row_with_neighbor() returns tuples.\n\n    Args:\n        batch: List of dictionaries, each containing:\n               - 'current_cell': Tuple[np.ndarray, np.ndarray] (values, columns)\n               - 'next_cell': Tuple[np.ndarray, np.ndarray] (values, columns)\n               - 'current_cell_index': int\n               - 'next_cell_index': int\n\n    Returns:\n        Dict containing:\n        - 'current_cells': Sparse tensor containing all current cells\n        - 'next_cells': Sparse tensor containing all next cells\n        - 'current_cell_indices': List of original indices for current cells\n        - 'next_cell_indices': List of original indices for next cells\n        - 'batch_size': Number of samples in the batch\n    \"\"\"\n    # Extract components\n    current_cells = [item[\"current_cell\"] for item in batch]\n    next_cells = [item[\"next_cell\"] for item in batch]\n    current_indices = [item[\"current_cell_index\"] for item in batch]\n    next_indices = [item[\"next_cell_index\"] for item in batch]\n\n    # Convert tuple format (values, columns) to tensors for collation\n    # Each tensor should be stacked as [values, columns] to match collate_sparse_matrix_batch format\n    current_tensors = [\n        torch.stack([torch.tensor(values, dtype=torch.float32), torch.tensor(columns, dtype=torch.float32)])\n        for values, columns in current_cells\n    ]\n    next_tensors = [\n        torch.stack([torch.tensor(values, dtype=torch.float32), torch.tensor(columns, dtype=torch.float32)])\n        for values, columns in next_cells\n    ]\n\n    # Collate the sparse tensors\n    current_batch = collate_sparse_matrix_batch(current_tensors)\n    next_batch = collate_sparse_matrix_batch(next_tensors)\n\n    return {\n        \"current_cells\": current_batch,\n        \"next_cells\": next_batch,\n        \"current_cell_indices\": current_indices,\n        \"next_cell_indices\": next_indices,\n        \"batch_size\": len(batch),\n    }\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scdl/util/torch_dataloader_utils/#bionemo.scdl.util.torch_dataloader_utils.collate_sparse_matrix_batch","title":"<code>collate_sparse_matrix_batch(batch)</code>","text":"<p>Collate function to create a batch out of sparse tensors.</p> <p>This is necessary to collate sparse matrices of various lengths.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>list[Tensor]</code> <p>A list of Tensors to collate into a batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The tensors collated into a CSR (Compressed Sparse Row) Format.</p> Source code in <code>bionemo/scdl/util/torch_dataloader_utils.py</code> <pre><code>def collate_sparse_matrix_batch(batch: list[torch.Tensor]) -&gt; torch.Tensor:\n    \"\"\"Collate function to create a batch out of sparse tensors.\n\n    This is necessary to collate sparse matrices of various lengths.\n\n    Args:\n        batch: A list of Tensors to collate into a batch.\n\n    Returns:\n        The tensors collated into a CSR (Compressed Sparse Row) Format.\n    \"\"\"\n    batch_rows = torch.cumsum(\n        torch.tensor([0] + [sparse_representation.shape[1] for sparse_representation in batch]), dim=0\n    )\n    batch_cols = torch.cat([sparse_representation[1] for sparse_representation in batch]).to(torch.int32)\n    batch_values = torch.cat([sparse_representation[0] for sparse_representation in batch])\n    if len(batch_cols) == 0:\n        max_pointer = 0\n    else:\n        max_pointer = int(batch_cols.max().item() + 1)\n    batch_sparse_tensor = torch.sparse_csr_tensor(batch_rows, batch_cols, batch_values, size=(len(batch), max_pointer))\n    return batch_sparse_tensor\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scspeedtest/benchmark/","title":"Benchmark","text":""},{"location":"main/references/API_reference/bionemo/scspeedtest/benchmark/#bionemo.scspeedtest.benchmark.BenchmarkConfig","title":"<code>BenchmarkConfig</code>  <code>dataclass</code>","text":"<p>Configuration for benchmarking.</p> <p>This dataclass contains all the configuration parameters needed to run a benchmark. It supports both time-based and batch-based limits, as well as warmup phases.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the benchmark</p> <code>num_epochs</code> <code>int</code> <p>Number of epochs to run</p> <code>max_batches</code> <code>Optional[int]</code> <p>Maximum number of batches to process (None for all)</p> <code>max_time_seconds</code> <code>Optional[float]</code> <p>Maximum time to run benchmark (None for no limit)</p> <code>warmup_batches</code> <code>Optional[int]</code> <p>Number of warmup batches</p> <code>warmup_time_seconds</code> <code>Optional[float]</code> <p>Time to warmup in seconds (overrides warmup_batches if set)</p> <code>data_path</code> <code>Optional[Union[str, Path]]</code> <p>Path to data files (for disk size measurement)</p> Source code in <code>bionemo/scspeedtest/benchmark.py</code> <pre><code>@dataclass\nclass BenchmarkConfig:\n    \"\"\"Configuration for benchmarking.\n\n    This dataclass contains all the configuration parameters needed\n    to run a benchmark. It supports both time-based and batch-based\n    limits, as well as warmup phases.\n\n    Attributes:\n        name: Name of the benchmark\n        num_epochs: Number of epochs to run\n        max_batches: Maximum number of batches to process (None for all)\n        max_time_seconds: Maximum time to run benchmark (None for no limit)\n        warmup_batches: Number of warmup batches\n        warmup_time_seconds: Time to warmup in seconds (overrides warmup_batches if set)\n        data_path: Path to data files (for disk size measurement)\n    \"\"\"\n\n    name: str = \"UnnamedBenchmark\"\n    num_epochs: int = 1\n    max_batches: Optional[int] = None\n    max_time_seconds: Optional[float] = None\n    warmup_batches: Optional[int] = None\n    warmup_time_seconds: Optional[float] = None\n    data_path: Optional[Union[str, Path]] = None\n    shuffle: bool = True\n    num_runs: int = 1\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scspeedtest/benchmark/#bionemo.scspeedtest.benchmark.benchmark_dataloaders_with_configs","title":"<code>benchmark_dataloaders_with_configs(dataloader_configs, shared_dataset_factory=None, output_prefix='consolidated_benchmark_results')</code>","text":"<p>Benchmark multiple dataloader configs with optional shared dataset.</p> <p>Each config can have its own dataset_factory, use the shared_dataset_factory, or have none (dataloader creates everything).</p> <p>Parameters:</p> Name Type Description Default <code>dataloader_configs</code> <code>List[Dict[str, Any]]</code> <p>List of dicts with keys: name, dataloader_factory, dataset_factory (optional), data_path, etc.</p> required <code>shared_dataset_factory</code> <code>Optional[Callable[[], Any]]</code> <p>Optional function that creates a dataset once, then reused across multiple dataloaders</p> <code>None</code> <code>output_prefix</code> <code>str</code> <p>Prefix for the output CSV filename</p> <code>'consolidated_benchmark_results'</code> <p>Returns:</p> Type Description <code>List[BenchmarkResult]</code> <p>List[BenchmarkResult] for multiple dataloader configs.</p> Source code in <code>bionemo/scspeedtest/benchmark.py</code> <pre><code>def benchmark_dataloaders_with_configs(\n    dataloader_configs: List[Dict[str, Any]],\n    shared_dataset_factory: Optional[Callable[[], Any]] = None,\n    output_prefix: str = \"consolidated_benchmark_results\",\n) -&gt; List[BenchmarkResult]:\n    \"\"\"Benchmark multiple dataloader configs with optional shared dataset.\n\n    Each config can have its own dataset_factory, use the shared_dataset_factory, or have none (dataloader creates everything).\n\n    Args:\n        dataloader_configs: List of dicts with keys: name, dataloader_factory, dataset_factory (optional), data_path, etc.\n        shared_dataset_factory: Optional function that creates a dataset once, then reused across multiple dataloaders\n        output_prefix: Prefix for the output CSV filename\n\n    Returns:\n        List[BenchmarkResult] for multiple dataloader configs.\n    \"\"\"\n    results = []\n\n    # Ensure every config has a dataloader_factory\n    for idx, config in enumerate(dataloader_configs):\n        if \"dataloader_factory\" not in config or config[\"dataloader_factory\"] is None:\n            raise ValueError(\n                f\"Config at index {idx} ('{config.get('name', 'UnnamedBenchmark')}') is missing a 'dataloader_factory'.\"\n            )\n\n    _drop_caches()\n\n    # Create shared dataset if factory is provided\n    shared_dataset = None\n    shared_dataset_baseline = None\n    shared_dataset_time = None\n    if shared_dataset_factory is not None:\n        shared_dataset, shared_dataset_baseline, _, _, _, _, shared_dataset_time = measure_peak_memory_full(\n            shared_dataset_factory\n        )\n    for dl_config in dataloader_configs:\n        # Determine which dataset factory to use\n        if \"dataset_factory\" in dl_config:\n            # Config has its own dataset factory\n            config_dataset_factory = dl_config[\"dataset_factory\"]\n        else:\n            # No dataset factory - dataloader factory creates everything\n            config_dataset_factory = None\n\n        config_dataloader_factory = dl_config[\"dataloader_factory\"]\n        if shared_dataset is not None:\n\n            def config_dataloader_from_dataset():\n                return config_dataloader_factory(shared_dataset)\n\n            dataloader_factory = config_dataloader_from_dataset\n        else:\n            dataloader_factory = config_dataloader_factory\n\n        result = benchmark_single_dataloader(\n            dataloader_factory=dataloader_factory,\n            data_path=dl_config.get(\"data_path\", None),\n            name=dl_config.get(\"name\", \"UnnamedBenchmark\"),\n            dataset_factory=config_dataset_factory,\n            num_epochs=dl_config.get(\"num_epochs\", 1),\n            max_batches=dl_config.get(\"max_batches\", None),\n            max_time_seconds=dl_config.get(\"max_time_seconds\", None),\n            warmup_batches=dl_config.get(\"warmup_batches\", 5),\n            warmup_time_seconds=dl_config.get(\"warmup_time_seconds\", None),\n            shuffle=dl_config.get(\"shuffle\", True),\n            num_runs=dl_config.get(\"num_runs\", 1),\n            dataset_baseline=shared_dataset_baseline,\n            output_prefix=output_prefix,\n            dataset_instantiation_time=shared_dataset_time,\n        )\n        # If this hasn't been set, set it to the minimum in the first dataloader\n        if not shared_dataset_baseline:\n            shared_dataset_baseline = result.memory_before_instantiation_mb\n\n        print_results(result)\n        if isinstance(result, list):\n            for r in result:\n                r.dataset_instantiation_time_seconds = shared_dataset_time\n            results.extend(result)\n        else:\n            result.dataset_instantiation_time_seconds = shared_dataset_time\n            results.append(result)\n        _drop_caches()\n    return results\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scspeedtest/benchmark/#bionemo.scspeedtest.benchmark.benchmark_single_dataloader","title":"<code>benchmark_single_dataloader(dataloader_factory, data_path, name='UnnamedBenchmark', dataset_factory=None, num_epochs=1, max_batches=None, max_time_seconds=None, warmup_batches=5, warmup_time_seconds=None, shuffle=False, num_runs=1, dataset_baseline=None, output_prefix='consolidated_benchmark_results', dataset_instantiation_time=None)</code>","text":"<p>Benchmark a single dataloader with optional separate dataset factory.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader_factory</code> <code>Callable[..., Any]</code> <p>Factory function that creates a dataloader. If dataset_factory is provided,                this should accept a dataset parameter. Otherwise, it should create everything internally.</p> required <code>data_path</code> <code>Union[str, Path]</code> <p>Path to the data file</p> required <code>name</code> <code>str</code> <p>Name of the benchmark</p> <code>'UnnamedBenchmark'</code> <code>dataset_factory</code> <code>Optional[Callable[[], Any]]</code> <p>Optional factory function that creates the dataset separately</p> <code>None</code> <code>num_epochs</code> <code>int</code> <p>Number of epochs to run</p> <code>1</code> <code>max_batches</code> <code>Optional[int]</code> <p>Maximum number of batches per epoch (None for unlimited)</p> <code>None</code> <code>max_time_seconds</code> <code>Optional[float]</code> <p>Maximum time to run in seconds (None for unlimited)</p> <code>None</code> <code>warmup_batches</code> <code>int</code> <p>Number of batches for warmup</p> <code>5</code> <code>warmup_time_seconds</code> <code>Optional[float]</code> <p>Time in seconds for warmup</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data</p> <code>False</code> <code>num_runs</code> <code>int</code> <p>Number of runs to perform</p> <code>1</code> <code>dataset_baseline</code> <code>Optional[float]</code> <p>Optional baseline memory usage for the dataset (for dataset reuse with multiple dataloaders)</p> <code>None</code> <code>output_prefix</code> <code>str</code> <p>Prefix for the output CSV filename</p> <code>'consolidated_benchmark_results'</code> <code>dataset_instantiation_time</code> <code>Optional[float]</code> <p>Optional time taken to instantiate the datasets</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[BenchmarkResult, List[BenchmarkResult]]</code> <p>Single BenchmarkResult for num_runs=1, or List[BenchmarkResult] for multiple runs</p> Source code in <code>bionemo/scspeedtest/benchmark.py</code> <pre><code>def benchmark_single_dataloader(\n    dataloader_factory: Callable[..., Any],\n    data_path: Union[str, Path],\n    name: str = \"UnnamedBenchmark\",\n    dataset_factory: Optional[Callable[[], Any]] = None,\n    num_epochs: int = 1,\n    max_batches: Optional[int] = None,\n    max_time_seconds: Optional[float] = None,\n    warmup_batches: int = 5,\n    warmup_time_seconds: Optional[float] = None,\n    shuffle: bool = False,\n    num_runs: int = 1,\n    dataset_baseline: Optional[float] = None,\n    output_prefix: str = \"consolidated_benchmark_results\",\n    dataset_instantiation_time: Optional[float] = None,\n) -&gt; Union[BenchmarkResult, List[BenchmarkResult]]:\n    \"\"\"Benchmark a single dataloader with optional separate dataset factory.\n\n    Args:\n        dataloader_factory: Factory function that creates a dataloader. If dataset_factory is provided,\n                           this should accept a dataset parameter. Otherwise, it should create everything internally.\n        data_path: Path to the data file\n        name: Name of the benchmark\n        dataset_factory: Optional factory function that creates the dataset separately\n        num_epochs: Number of epochs to run\n        max_batches: Maximum number of batches per epoch (None for unlimited)\n        max_time_seconds: Maximum time to run in seconds (None for unlimited)\n        warmup_batches: Number of batches for warmup\n        warmup_time_seconds: Time in seconds for warmup\n        shuffle: Whether to shuffle the data\n        num_runs: Number of runs to perform\n        dataset_baseline: Optional baseline memory usage for the dataset (for dataset reuse with multiple dataloaders)\n        output_prefix: Prefix for the output CSV filename\n        dataset_instantiation_time: Optional time taken to instantiate the datasets\n\n    Returns:\n        Single BenchmarkResult for num_runs=1, or List[BenchmarkResult] for multiple runs\n    \"\"\"\n    if dataset_factory is not None:\n        # Separate dataset and dataloader creation\n        dataset, dataset_baseline_measured, dataset_peak, _, _, dataset_final, dataset_time = measure_peak_memory_full(\n            dataset_factory\n        )\n\n        def dataloader_from_dataset():\n            return dataloader_factory(dataset)\n\n        dataloader, dl_baseline, dl_peak, _, _, dl_final, dl_time = measure_peak_memory_full(dataloader_from_dataset)\n\n        instantiation_metrics = {\n            \"peak_memory_during_instantiation_mb\": max(dl_peak, dataset_peak),\n            \"memory_after_instantiation_mb\": dl_final,\n            \"memory_before_instantiation_mb\": dataset_baseline_measured,\n            \"dataset_instantiation_time_seconds\": dataset_time,\n            \"dataloader_instantiation_time_seconds\": dl_time,\n        }\n\n    else:\n        # Dataloader factory creates everything internally\n        dataloader, dataloader_baseline_measured, peak, _, _, final_mib, setup_time = measure_peak_memory_full(\n            dataloader_factory\n        )\n        instantiation_metrics = {\n            \"peak_memory_during_instantiation_mb\": peak,\n            \"memory_after_instantiation_mb\": final_mib,\n            \"memory_before_instantiation_mb\": dataset_baseline\n            if dataset_baseline is not None\n            else dataloader_baseline_measured,\n            \"dataset_instantiation_time_seconds\": dataset_instantiation_time\n            if dataset_instantiation_time is not None\n            else 0,  # Combined time when no separate dataset factory\n            \"dataloader_instantiation_time_seconds\": setup_time,\n        }\n    disk_size_mb = get_disk_size(data_path)\n\n    results = []\n    for run_idx in range(num_runs):\n        # For single run, use the provided dataloader; for multiple runs, re-instantiate as needed\n        if run_idx == 0:\n            current_dataloader = dataloader\n            run_name_str = name if num_runs == 1 else f\"{name}_run_{run_idx + 1}\"\n        else:\n            if dataset_factory is not None:\n\n                def dataloader_from_dataset():\n                    return dataloader_factory(dataset)\n\n                current_dataloader = dataloader_from_dataset()\n            else:\n                current_dataloader = dataloader_factory()\n            run_name_str = f\"{name}_run_{run_idx + 1}\"\n\n        run_config = BenchmarkConfig(\n            name=run_name_str,\n            num_epochs=num_epochs,\n            max_batches=max_batches,\n            max_time_seconds=max_time_seconds,\n            warmup_batches=warmup_batches,\n            warmup_time_seconds=warmup_time_seconds,\n            data_path=data_path,\n            shuffle=shuffle,\n        )\n        run_result = run_benchmark(current_dataloader, run_config, run_name_str, **instantiation_metrics)\n        del current_dataloader\n        gc.collect()\n        run_result.disk_size_mb = disk_size_mb\n        results.append(run_result)\n\n        export_benchmark_results(run_result, output_prefix=output_prefix)\n        _drop_caches()\n\n    if num_runs == 1:\n        return results[0]\n    else:\n        return results\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scspeedtest/benchmark/#bionemo.scspeedtest.benchmark.print_comparison","title":"<code>print_comparison(results)</code>","text":"<p>Print comparison of multiple benchmark results.</p> Source code in <code>bionemo/scspeedtest/benchmark.py</code> <pre><code>def print_comparison(results: List[BenchmarkResult]) -&gt; None:\n    \"\"\"Print comparison of multiple benchmark results.\"\"\"\n    if not results or len(results) &lt; 2:\n        return\n\n    print(f\"\\nComparison ({len(results)} configurations)\")\n\n    # Show individual results\n    for result in results:\n        print(f\"\\nResult for {result.name}: {result.samples_per_second:.2f} samples/sec\")\n        print(f\"   Memory: {result.peak_memory_mb:.1f} MB\")\n\n    # Find best performers\n    best_samples_per_sec = max(results, key=lambda r: r.samples_per_second)\n    lowest_memory = min(results, key=lambda r: r.peak_memory_mb)\n\n    print(\"\\nBest Performers:\")\n    print(f\"Best speed: {best_samples_per_sec.name} ({best_samples_per_sec.samples_per_second:.2f} samples/sec)\")\n    print(f\"Lowest memory: {lowest_memory.name} ({lowest_memory.peak_memory_mb:.2f} MB)\")\n\n    fastest_instantiation = min(\n        results,\n        key=lambda r: (r.dataset_instantiation_time_seconds or 0) + (r.dataloader_instantiation_time_seconds or 0),\n    )\n    fastest_time = (fastest_instantiation.dataset_instantiation_time_seconds or 0) + (\n        fastest_instantiation.dataloader_instantiation_time_seconds or 0\n    )\n    print(f\"Fastest instantiation: {fastest_instantiation.name} ({fastest_time:.3f} s)\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scspeedtest/benchmark/#bionemo.scspeedtest.benchmark.print_results","title":"<code>print_results(result_or_results)</code>","text":"<p>Print benchmark results in a formatted way. Accepts a single result or a list of results.</p> Source code in <code>bionemo/scspeedtest/benchmark.py</code> <pre><code>def print_results(result_or_results: Union[BenchmarkResult, List[BenchmarkResult]]) -&gt; None:\n    \"\"\"Print benchmark results in a formatted way. Accepts a single result or a list of results.\"\"\"\n    results = result_or_results if isinstance(result_or_results, list) else [result_or_results]\n    for result in results:\n        print(\"=\" * 60)\n        print(f\"Benchmark: {result.name}\")\n        print(f\"Samples/sec: {result.samples_per_second:.2f}\")\n        print(f\"Total samples: {result.total_samples}\")\n        print(f\"Total time: {result.total_time_seconds:.3f}s\")\n        print(f\"Dataset instantiation: {result.dataset_instantiation_time_seconds:.3f}s\")\n        print(f\"Dataloader instantiation: {result.dataloader_instantiation_time_seconds:.3f}s\")\n        print(f\"Peak memory durint iteration: {result.peak_memory_mb:.1f} MB\")\n        print(f\"Peak memory during instantiation: {result.peak_memory_during_instantiation_mb:.1f} MB\")\n        print(f\"Disk size: {result.disk_size_mb:.1f} MB\")\n        print(\"=\" * 60 + \"\\n\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scspeedtest/benchmark/#bionemo.scspeedtest.benchmark.run_benchmark","title":"<code>run_benchmark(dataloader, config, run_name=None, **instantiation_kwargs)</code>","text":"<p>Run the actual benchmark and collect metrics.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader</code> <code>Any</code> <p>The dataloader to benchmark</p> required <code>config</code> <code>BenchmarkConfig</code> <p>Configuration for the benchmark run</p> required <code>run_name</code> <code>Optional[str]</code> <p>Optional name for this run</p> <code>None</code> <code>**instantiation_kwargs</code> <p>Instantiation metrics (dataset_instantiation_time_seconds,                    dataloader_instantiation_time_seconds, peak_memory_during_instantiation_mb,                    memory_before_instantiation_mb, memory_after_instantiation_mb)</p> <code>{}</code> <p>Returns:</p> Type Description <code>BenchmarkResult</code> <p>BenchmarkResult containing all collected data and calculated metrics</p> Source code in <code>bionemo/scspeedtest/benchmark.py</code> <pre><code>def run_benchmark(\n    dataloader: Any,\n    config: BenchmarkConfig,\n    run_name: Optional[str] = None,\n    **instantiation_kwargs,\n) -&gt; BenchmarkResult:\n    \"\"\"Run the actual benchmark and collect metrics.\n\n    Args:\n        dataloader: The dataloader to benchmark\n        config: Configuration for the benchmark run\n        run_name: Optional name for this run\n        **instantiation_kwargs: Instantiation metrics (dataset_instantiation_time_seconds,\n                               dataloader_instantiation_time_seconds, peak_memory_during_instantiation_mb,\n                               memory_before_instantiation_mb, memory_after_instantiation_mb)\n\n    Returns:\n        BenchmarkResult containing all collected data and calculated metrics\n    \"\"\"\n    # Use measure_peak_memory_full to get memory info during benchmark\n    gc.collect()\n\n    def benchmark_iteration_single_epoch(epoch_num, do_warmup):\n        \"\"\"Run a single epoch of benchmarking, with optional warmup.\"\"\"\n        gc.collect()\n\n        update_interval = 10\n        epoch_samples = 0\n        epoch_batches = 0\n        warmup_samples = 0\n        warmup_batches = 0\n        warmup_time = 0.0\n        elapsed = 0.0\n        start_time = None\n\n        pbar = tqdm(desc=f\"{config.name} - Epoch {epoch_num + 1}/{config.num_epochs}\")\n        warm_up_start = time.perf_counter()\n        if not do_warmup or not config.warmup_time_seconds:\n            config.warmup_time_seconds = 0\n        warm_up_end = warm_up_start + config.warmup_time_seconds\n        is_warming_up = True\n\n        for num, batch in enumerate(dataloader):\n            batch_size = get_batch_size(batch)\n\n            current_time = time.perf_counter()\n\n            if is_warming_up:\n                # We're in warm-up period - count samples and batches\n                warmup_samples += batch_size\n                warmup_batches += 1\n\n                if current_time &gt;= warm_up_end:\n                    # Warm-up complete and start the actual timing\n                    warmup_time = current_time - warm_up_start\n\n                    print(f\"Warmup completed: {warmup_samples:,} samples, {warmup_batches:,} batches\")\n\n                    is_warming_up = False\n                    start_time = time.perf_counter()\n                    end_time = start_time + config.max_time_seconds if config.max_time_seconds is not None else None\n                    pbar.set_description(f\"{config.name} - Epoch {epoch_num + 1} (warmup complete)\")\n                else:\n                    if warmup_batches % update_interval == 0:\n                        elapsed_warmup = current_time - warm_up_start\n                        current_warmup_speed = warmup_samples / elapsed_warmup if elapsed_warmup &gt; 0 else 0\n                        pbar.set_description(\n                            f\"{config.name} - Warmup: {elapsed_warmup:.1f}/{config.warmup_time_seconds}s, {current_warmup_speed:.1f} samples/sec\"\n                        )\n                        pbar.update(update_interval)\n                continue\n\n            # Now we're past the warm-up period (or no warmup)\n            epoch_samples += batch_size\n            epoch_batches += 1\n            elapsed = current_time - start_time if start_time else 0\n            if epoch_batches % update_interval == 0:\n                postfix_dict = {\n                    \"epoch\": f\"{epoch_num + 1}/{config.num_epochs}\",\n                    \"samples\": epoch_samples,\n                    \"elapsed\": f\"{elapsed:.2f}s\",\n                }\n\n                pbar.set_postfix(**postfix_dict, refresh=False)\n                pbar.update(update_interval)\n            # Check max_batches limit\n            if (config.max_batches and epoch_batches &gt;= config.max_batches) or (end_time and current_time &gt;= end_time):\n                break\n\n        # If no samples were processed in the epoch, likely because warmup consumed the entire dataset\n        if epoch_samples == 0:\n            import warnings\n\n            warnings.warn(\n                f\"Epoch {epoch_num + 1}: No samples processed after warmup. \"\n                \"Warmup may have consumed the entire dataset. \"\n                \"Consider reducing warmup_batches or warmup_time_seconds.\",\n                RuntimeWarning,\n            )\n\n        # Final progress bar update\n        if epoch_samples &gt; 0 and elapsed &gt; 0:\n            postfix_dict = {\n                \"epoch\": f\"{epoch_num + 1}/{config.num_epochs}\",\n                \"samples\": epoch_samples,\n                \"elapsed\": f\"{elapsed:.2f}s\",\n                \"samples_per_sec\": f\"{epoch_samples / elapsed:.2f}\",\n            }\n            pbar.set_postfix(**postfix_dict, refresh=False)\n\n        pbar.close()\n\n        return epoch_samples, epoch_batches, elapsed, warmup_samples, warmup_batches, warmup_time\n\n    epoch_results = []\n    for epoch in range(config.num_epochs):\n        # Create a modified benchmark_iteration for this epoch\n\n        result_tuple = measure_peak_memory_full(\n            lambda: benchmark_iteration_single_epoch(epoch, epoch == 0), multi_worker=dataloader.num_workers &gt; 0\n        )\n        (\n            (epoch_samples, epoch_batches, elapsed, warmup_samples, warmup_batches, warmup_time),\n            _,\n            peak,\n            avg,\n            _,\n            _,\n            iteration_time,\n        ) = result_tuple\n\n        epoch_results.append(\n            {\n                \"epoch\": epoch + 1,\n                \"samples\": epoch_samples,\n                \"batches\": epoch_batches,\n                \"warmup_samples\": warmup_samples,\n                \"warmup_batches\": warmup_batches,\n                \"peak_memory\": peak,\n                \"avg_memory\": avg,\n                \"iteration_time\": iteration_time,\n                \"elapsed\": elapsed,\n                \"warmup_time\": warmup_time,\n            }\n        )\n\n        print(f\"Epoch {epoch + 1} completed: {epoch_samples:,} samples, {epoch_batches:,} batches\")\n\n    result = BenchmarkResult(\n        name=config.name,\n        data_path=str(config.data_path) if config.data_path else None,\n        max_time_seconds=config.max_time_seconds,\n        shuffle=config.shuffle,\n        num_workers=dataloader.num_workers,\n        # Instantiation metrics passed as kwargs\n        **instantiation_kwargs,\n        epoch_results=epoch_results,\n    )\n    return result\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scspeedtest/common/","title":"Common","text":"<p>Common components shared across the benchmarking framework.</p> <p>This module contains shared dataclasses and utility functions to avoid circular imports between modules. It provides the core data structures and measurement utilities used throughout the benchmarking framework.</p>"},{"location":"main/references/API_reference/bionemo/scspeedtest/common/#bionemo.scspeedtest.common.BenchmarkResult","title":"<code>BenchmarkResult</code>  <code>dataclass</code>","text":"<p>Results from benchmarking a dataloader.</p> <p>This class stores essential metrics and metadata about a dataloader benchmark run for CSV export and analysis.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name/description of the benchmark</p> <code>warmup_time_seconds</code> <code>float</code> <p>Time spent in warmup phase</p> <code>dataset_instantiation_time_seconds</code> <code>float</code> <p>Time to load/create dataset only</p> <code>dataloader_instantiation_time_seconds</code> <code>float</code> <p>Time to wrap dataset in dataloader only</p> <code>madvise_interval</code> <code>Optional[int]</code> <p>Memory advice interval setting used</p> <code>data_path</code> <code>Optional[str]</code> <p>Path to dataset used for benchmarking</p> <code>max_time_seconds</code> <code>Optional[float]</code> <p>Maximum time limit set for the benchmark</p> <code>shuffle</code> <code>Optional[bool]</code> <p>Whether the dataloader was shuffled</p> <code>num_workers</code> <code>Optional[int]</code> <p>Number of worker processes used for data loading</p> <pre><code># Input data\n</code></pre> <code>epoch_results</code> <code>Optional[List[Dict[str, Any]]]</code> <p>List of per-epoch benchmark results</p> Source code in <code>bionemo/scspeedtest/common.py</code> <pre><code>@dataclass\nclass BenchmarkResult:\n    \"\"\"Results from benchmarking a dataloader.\n\n    This class stores essential metrics and metadata about a dataloader benchmark run\n    for CSV export and analysis.\n\n    Attributes:\n        name: Name/description of the benchmark\n        warmup_time_seconds: Time spent in warmup phase\n\n        # Instantiation metrics\n        dataset_instantiation_time_seconds: Time to load/create dataset only\n        dataloader_instantiation_time_seconds: Time to wrap dataset in dataloader only\n\n        # Configuration metadata\n        madvise_interval: Memory advice interval setting used\n        data_path: Path to dataset used for benchmarking\n        max_time_seconds: Maximum time limit set for the benchmark\n        shuffle: Whether the dataloader was shuffled\n        num_workers: Number of worker processes used for data loading\n\n                # Input data\n        epoch_results: List of per-epoch benchmark results\n    \"\"\"\n\n    name: str\n    warmup_time_seconds: float = 0.0\n\n    # Instantiation metrics (always passed explicitly)\n    dataset_instantiation_time_seconds: float = 0.0\n    dataloader_instantiation_time_seconds: float = 0.0\n    peak_memory_during_instantiation_mb: float = 0.0\n    memory_before_instantiation_mb: float = 0.0\n    memory_after_instantiation_mb: float = 0.0\n\n    # Configuration metadata\n    madvise_interval: Optional[int] = None\n    data_path: Optional[str] = None\n    max_time_seconds: Optional[float] = None\n    shuffle: Optional[bool] = None\n    num_workers: Optional[int] = None\n\n    # Input data (always passed explicitly)\n    epoch_results: Optional[List[Dict[str, Any]]] = None\n\n    # Derived metrics\n    samples_per_second: float = 0.0\n    peak_memory_mb: float = 0.0\n    avg_memory_mb: float = 0.0\n    disk_size_mb: float = 0.0\n\n    def __post_init__(self):\n        \"\"\"Calculate derived metrics from epoch results.\"\"\"\n        self.total_samples = sum(r[\"samples\"] for r in self.epoch_results)\n        self.total_time_seconds = sum(r[\"elapsed\"] for r in self.epoch_results)\n        self.samples_per_second = self.total_samples / self.total_time_seconds if self.total_time_seconds &gt; 0 else 0.0\n        self.peak_memory_mb = max(r[\"peak_memory\"] for r in self.epoch_results) - self.memory_before_instantiation_mb\n        self.avg_memory_mb = (\n            sum(r[\"avg_memory\"] for r in self.epoch_results) / len(self.epoch_results)\n            - self.memory_before_instantiation_mb\n        )\n        self.instantiation_time_seconds = (\n            self.dataset_instantiation_time_seconds + self.dataloader_instantiation_time_seconds\n        )\n        self.peak_memory_during_instantiation_mb = (\n            self.peak_memory_during_instantiation_mb - self.memory_before_instantiation_mb\n        )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scspeedtest/common/#bionemo.scspeedtest.common.BenchmarkResult.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Calculate derived metrics from epoch results.</p> Source code in <code>bionemo/scspeedtest/common.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Calculate derived metrics from epoch results.\"\"\"\n    self.total_samples = sum(r[\"samples\"] for r in self.epoch_results)\n    self.total_time_seconds = sum(r[\"elapsed\"] for r in self.epoch_results)\n    self.samples_per_second = self.total_samples / self.total_time_seconds if self.total_time_seconds &gt; 0 else 0.0\n    self.peak_memory_mb = max(r[\"peak_memory\"] for r in self.epoch_results) - self.memory_before_instantiation_mb\n    self.avg_memory_mb = (\n        sum(r[\"avg_memory\"] for r in self.epoch_results) / len(self.epoch_results)\n        - self.memory_before_instantiation_mb\n    )\n    self.instantiation_time_seconds = (\n        self.dataset_instantiation_time_seconds + self.dataloader_instantiation_time_seconds\n    )\n    self.peak_memory_during_instantiation_mb = (\n        self.peak_memory_during_instantiation_mb - self.memory_before_instantiation_mb\n    )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scspeedtest/common/#bionemo.scspeedtest.common.download_example_dataset","title":"<code>download_example_dataset(cache_dir='.', filename='example_data.h5ad')</code>","text":"<p>Download a small example AnnData dataset for testing.</p> <p>Parameters:</p> Name Type Description Default <code>cache_dir</code> <code>str</code> <p>Directory to save the downloaded file (default: current directory)</p> <code>'.'</code> <code>filename</code> <code>str</code> <p>Name of the file to save (default: \"example_data.h5ad\")</p> <code>'example_data.h5ad'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the downloaded file</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If download fails</p> Source code in <code>bionemo/scspeedtest/common.py</code> <pre><code>def download_example_dataset(cache_dir: str = \".\", filename: str = \"example_data.h5ad\") -&gt; str:\n    \"\"\"Download a small example AnnData dataset for testing.\n\n    Args:\n        cache_dir: Directory to save the downloaded file (default: current directory)\n        filename: Name of the file to save (default: \"example_data.h5ad\")\n\n    Returns:\n        str: Path to the downloaded file\n\n    Raises:\n        RuntimeError: If download fails\n    \"\"\"\n    cache_path = Path(cache_dir) / filename\n\n    # If file already exists, return it\n    if cache_path.exists():\n        print(f\"Using cached dataset: {cache_path}\")\n        return str(cache_path)\n\n    # URL for a small example dataset (using 10x genomics pbmc3k dataset)\n    url = \"https://cf.10xgenomics.com/samples/cell/pbmc3k/pbmc3k_filtered_gene_bc_matrices.h5\"\n\n    try:\n        print(f\"Downloading example dataset to {cache_path}...\")\n        os.makedirs(cache_dir, exist_ok=True)\n\n        # Download with progress\n        def progress_hook(block_num, block_size, total_size):\n            if total_size &gt; 0:\n                percent = min(100, (block_num * block_size * 100) // total_size)\n                print(f\"\\rDownload progress: {percent}%\", end=\"\", flush=True)\n\n        urllib.request.urlretrieve(url, cache_path, reporthook=progress_hook)\n        print(\"\\nDownload completed!\")\n\n        return str(cache_path)\n\n    except Exception as e:\n        # Clean up partial download\n        if cache_path.exists():\n            cache_path.unlink()\n        raise RuntimeError(f\"Failed to download example dataset: {e}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scspeedtest/common/#bionemo.scspeedtest.common.export_benchmark_results","title":"<code>export_benchmark_results(results, output_prefix='benchmark_data')</code>","text":"<p>Append benchmark results to detailed breakdown CSV, never overwriting existing data.</p> <p>This function appends benchmark results to an existing CSV file or creates a new one if it doesn't exist. It never overwrites existing files.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>Union[BenchmarkResult, List[BenchmarkResult]]</code> <p>Single BenchmarkResult or list of BenchmarkResults to append</p> required <code>output_prefix</code> <code>str</code> <p>Prefix for the CSV filename</p> <code>'benchmark_data'</code> Source code in <code>bionemo/scspeedtest/common.py</code> <pre><code>def export_benchmark_results(\n    results: Union[BenchmarkResult, List[BenchmarkResult]],\n    output_prefix: str = \"benchmark_data\",\n) -&gt; None:\n    \"\"\"Append benchmark results to detailed breakdown CSV, never overwriting existing data.\n\n    This function appends benchmark results to an existing CSV file or creates\n    a new one if it doesn't exist. It never overwrites existing files.\n\n    Args:\n        results: Single BenchmarkResult or list of BenchmarkResults to append\n        output_prefix: Prefix for the CSV filename\n    \"\"\"\n    # Normalize results to always be a list\n    if isinstance(results, BenchmarkResult):\n        results = [results]\n\n    # Use simple filename in current directory\n    detailed_csv = f\"{output_prefix}_detailed_breakdown.csv\"\n\n    # Always append, only write header if file does not exist\n    file_exists = os.path.exists(detailed_csv)\n    header = not file_exists\n\n    # Build detailed rows\n    detailed_rows = []\n\n    for i, result in enumerate(results, 1):\n        # Handle run numbering for single result vs multiple results\n        if len(results) == 1:\n            # Single result - extract run number from name\n            run_number = 1\n            if \"_run_\" in result.name:\n                try:\n                    run_number = int(result.name.split(\"_run_\")[-1])\n                except ValueError:\n                    pass\n        else:\n            # Multiple results - use enumeration\n            run_number = i\n\n        # Create detailed rows for each epoch\n        for epoch_info in result.epoch_results:\n            detailed_rows.append(\n                {\n                    \"Run_Name\": result.name,\n                    \"Run_Number\": run_number,\n                    \"Epoch\": epoch_info[\"epoch\"],\n                    \"Batches\": epoch_info[\"batches\"],\n                    \"Samples\": epoch_info[\"samples\"],\n                    \"Samples_per_sec\": epoch_info[\"samples\"] / epoch_info[\"elapsed\"]\n                    if epoch_info[\"elapsed\"] &gt; 0\n                    else 0,\n                    \"Peak_Memory_MB\": epoch_info[\"peak_memory\"] - result.memory_before_instantiation_mb,\n                    \"Average_Memory_MB\": epoch_info[\"avg_memory\"] - result.memory_before_instantiation_mb,\n                    \"Total_Time_s\": epoch_info[\"iteration_time\"],\n                    \"Warmup_Time_s\": result.warmup_time_seconds if epoch_info[\"epoch\"] == 1 else 0,\n                    \"Warmup_Samples\": epoch_info[\"warmup_samples\"],\n                    \"Warmup_Batches\": epoch_info[\"warmup_batches\"],\n                    \"Total_Speed_With_Warmup_Samples_per_sec\": (epoch_info[\"samples\"] + epoch_info[\"warmup_samples\"])\n                    / epoch_info[\"iteration_time\"]\n                    if epoch_info[\"iteration_time\"] &gt; 0\n                    else 0,\n                    \"Dataset_Path\": result.data_path,\n                    \"Madvise_Interval\": result.madvise_interval,\n                    \"Max_Time_Seconds\": result.max_time_seconds,\n                    \"Shuffle\": result.shuffle,\n                    \"Dataset_Instantiation_Time_s\": result.dataset_instantiation_time_seconds,\n                    \"Dataloader_Instantiation_Time_s\": result.dataloader_instantiation_time_seconds,\n                    \"Peak_Instantiation_Memory_MB\": result.peak_memory_during_instantiation_mb,\n                    \"Batches_per_sec\": epoch_info[\"batches\"] / epoch_info[\"elapsed\"]\n                    if epoch_info[\"elapsed\"] &gt; 0\n                    else 0,\n                }\n            )\n\n    # Write detailed CSV (always append, never overwrite)\n    pd.DataFrame(detailed_rows).to_csv(detailed_csv, mode=\"a\", header=header, index=False)\n    if not file_exists:\n        print(f\"Created Detailed breakdown CSV: {os.path.abspath(detailed_csv)}\")\n    else:\n        print(f\"Appended to Detailed breakdown CSV: {os.path.abspath(detailed_csv)}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scspeedtest/common/#bionemo.scspeedtest.common.get_batch_size","title":"<code>get_batch_size(batch)</code>","text":"<p>Determine the size of a batch.</p> <p>This function attempts to determine the batch size from various common batch formats including PyTorch tensors, lists, and dictionaries with common keys.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The batch object to measure</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of samples in the batch</p> Source code in <code>bionemo/scspeedtest/common.py</code> <pre><code>def get_batch_size(batch: Any) -&gt; int:\n    \"\"\"Determine the size of a batch.\n\n    This function attempts to determine the batch size from various\n    common batch formats including PyTorch tensors, lists, and\n    dictionaries with common keys.\n\n    Args:\n        batch: The batch object to measure\n\n    Returns:\n        Number of samples in the batch\n    \"\"\"\n    if hasattr(batch, \"X\"):\n        batch_size = batch.X.shape[0]\n    else:\n        batch_size = batch.shape[0] if hasattr(batch, \"shape\") else len(batch)\n    return batch_size\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scspeedtest/common/#bionemo.scspeedtest.common.get_disk_size","title":"<code>get_disk_size(path)</code>","text":"<p>Get disk size of a file or directory in MB.</p> <p>Tested on MacOS and Linux.</p> Source code in <code>bionemo/scspeedtest/common.py</code> <pre><code>def get_disk_size(path: Union[str, Path]) -&gt; float:\n    \"\"\"Get disk size of a file or directory in MB.\n\n    Tested on MacOS and Linux.\n    \"\"\"\n    try:\n        # Use appropriate du command based on platform\n        if platform.system() == \"Darwin\":  # macOS\n            result = subprocess.run([\"du\", \"-s\", str(path)], stdout=subprocess.PIPE, text=True, check=True)\n            size_in_blocks = int(result.stdout.split()[0])\n            size_in_bytes = size_in_blocks * 512  # macOS du uses 512-byte blocks by default\n        else:  # Linux and others\n            result = subprocess.run([\"du\", \"-sb\", str(path)], stdout=subprocess.PIPE, text=True, check=True)\n            size_in_bytes = int(result.stdout.split()[0])\n\n        return size_in_bytes / (1024 * 1024)\n    except (subprocess.CalledProcessError, ValueError, IndexError) as e:\n        raise RuntimeError(f\"Could not determine disk size for {path}: {e}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/scspeedtest/common/#bionemo.scspeedtest.common.measure_peak_memory_full","title":"<code>measure_peak_memory_full(func, *args, sample_interval=0.05, child_refresh_interval=5.0, multi_worker=False, **kwargs)</code>","text":"<p>Measure peak &amp; average memory while running <code>func</code>.</p> <p>If multi_worker=True, uses PSS across the process tree (slower but includes children). Otherwise uses RSS of just the main process (lightweight).</p> <p>Returns:</p> Type Description <p>(result, baseline_mib, peak_mib, avg_mib, delta_mib, final_mib, duration_s)</p> Source code in <code>bionemo/scspeedtest/common.py</code> <pre><code>def measure_peak_memory_full(\n    func,\n    *args,\n    sample_interval: float = 0.05,\n    child_refresh_interval: float = 5.0,\n    multi_worker: bool = False,\n    **kwargs,\n):\n    \"\"\"Measure peak &amp; average memory while running `func`.\n\n    If multi_worker=True, uses PSS across the process tree (slower but includes children).\n    Otherwise uses RSS of just the main process (lightweight).\n\n    Returns:\n      (result,\n       baseline_mib,\n       peak_mib,\n       avg_mib,\n       delta_mib,\n       final_mib,\n       duration_s)\n    \"\"\"\n    parent_pid = os.getpid()\n    stop_event = mp.Event()\n    result_queue = mp.Queue()\n\n    # pick sampler\n    if multi_worker:\n        sampler_proc = mp.Process(\n            target=_fast_pss_sampler,\n            args=(\n                parent_pid,\n                stop_event,\n                result_queue,\n                sample_interval,\n            ),\n        )\n        # baseline via PSS\n        with open(f\"/proc/{parent_pid}/smaps_rollup\") as f:\n            for line in f:\n                if line.startswith(\"Pss:\"):\n                    baseline_kb = int(line.split()[1])\n                    break\n        baseline = baseline_kb * 1024\n    else:\n        sampler_proc = mp.Process(\n            target=_single_rss_sampler,\n            args=(parent_pid, stop_event, result_queue, sample_interval),\n        )\n        # baseline via RSS\n        baseline = psutil.Process(parent_pid).memory_info().rss\n\n    # start sampler\n    sampler_proc.start()\n    gc.collect()\n    start = time.perf_counter()\n\n    try:\n        result = func(*args, **kwargs)\n    finally:\n        stop_event.set()\n        sampler_proc.join()\n\n    duration = time.perf_counter() - start\n\n    # fetch stats\n    try:\n        peak, avg = result_queue.get_nowait()\n    except mp.queues.Empty:\n        peak = avg = baseline\n\n    # final memory\n    if multi_worker:\n        with open(f\"/proc/{parent_pid}/smaps_rollup\") as f:\n            for line in f:\n                if line.startswith(\"Pss:\"):\n                    final_kb = int(line.split()[1])\n                    break\n        final = final_kb * 1024\n    else:\n        final = psutil.Process(parent_pid).memory_info().rss\n\n    # convert to MiB\n    def to_mib(x):\n        return x / 1024**2\n\n    baseline_mib = to_mib(baseline)\n    peak_mib = to_mib(max(peak, final))\n    avg_mib = to_mib(avg)\n    final_mib = to_mib(final)\n    delta_mib = peak_mib - baseline_mib\n    print(\"baseline_mib\", baseline_mib, \"peak_mib\", peak_mib)\n    return (result, baseline_mib, peak_mib, avg_mib, delta_mib, final_mib, duration)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/size_aware_batching/sampler/","title":"Sampler","text":""},{"location":"main/references/API_reference/bionemo/size_aware_batching/sampler/#bionemo.size_aware_batching.sampler.BucketBatchSampler","title":"<code>BucketBatchSampler</code>","text":"<p>               Bases: <code>Sampler[List[int]]</code></p> <p>A batch sampler to create batches with sizes of elements from each pre-defined bucket ranges.</p> <p>Elements of the dataset are first grouped into each bucket based on the bucket ranges and the sizes of elements. Then, a base batch sampler is used for each bucket to create mini-batches.</p> <p>The bucket ranges are specified by <code>bucket_boundaries</code>, which will be first sorted internally and used to create <code>len(bucket_boundaries) - 1</code> left-closed right-open intervals. e.g. if bucket_boundaries tensor is [10, 5, 0, 16], it will be sorted as [0, 5, 10, 16] and 3 buckets will be created with ranges: [0, 5), [5, 10), [10, 16).</p> <p>The base batch sampler will be created by passing the element indices in each bucket as the data source, and <code>base_batch_sampler_shared_kwargs</code> and <code>base_batch_sampler_individual_kwargs</code> to the constructor of the base batch sampler class specified as <code>base_batch_sampler_class</code>. e.g. <code>base_batch_sampler_shared_kwargs = {'drop_last': True}</code> and <code>base_batch_sampler_individual_kwargs = {'batch_size': [8,10,12]}</code> will be used to create 3 batch samplers with drop_last=True and batch_size=8, 10 and 12, and initialized like <code>base_batch_sampler_class(bucket_element_indices[0], batch_size=8, drop_last=True)</code>.</p> <p>In the <code>__iter__</code> method, if <code>shuffle</code> is <code>True</code>, the element indices in each bucket will be shuffled, and a bucket is randomly selected each time to create a mini-batch. If <code>shuffle</code> is <code>False</code>, there is no shuffle on element indices, and the bucket is selected in ascending order of its interval boundaries.</p> <p>This class is used to create homogeneous batches of data for training or evaluation, and reduce the padding necessary to align the shape of elements.</p> <p>Modified from https://github.com/rssrwn/semla-flow/blob/main/semlaflow/data/util.py</p> <p>Examples: <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.size_aware_batching.sampler import BucketBatchSampler\n\n&gt;&gt;&gt; # Define the sizes for a dataset\n&gt;&gt;&gt; sizes = torch.arange(25)\n&gt;&gt;&gt; # Define bucket ranges\n&gt;&gt;&gt; bucket_boundaries = torch.tensor([0, 6, 15, 25])\n\n&gt;&gt;&gt; # Create a bucket batch sampler with torch.utils.data.BatchSampler as base batch sampler\n&gt;&gt;&gt; # As there are 3 buckets, there will be 3 base batch samplers with batch sizes 2, 3, and 5.\n&gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n        sizes=sizes,\n        bucket_boundaries=bucket_boundaries,\n        base_batch_sampler_class=torch.utils.data.BatchSampler,\n        base_batch_sampler_shared_kwargs={'drop_last': False},\n        base_batch_sampler_individual_kwargs={'batch_size': [2,3,5]},\n        shuffle=False,\n    )\n\n&gt;&gt;&gt; # Iterate over batches of indices that lies in the same bucket and with different batch sizes.\n&gt;&gt;&gt; print(list(batch_sampler))\n[[0, 1], [2, 3], [4, 5], [6, 7, 8], [9, 10, 11], [12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24]]\n\n&gt;&gt;&gt; # randomize the dataset and buckets\n&gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n        sizes=sizes,\n        bucket_boundaries=bucket_boundaries,\n        base_batch_sampler_class=torch.utils.data.BatchSampler,\n        base_batch_sampler_shared_kwargs={'drop_last': False},\n        base_batch_sampler_individual_kwargs={'batch_size': [2,3,5]},\n        shuffle=True,\n        generator=torch.Generator().manual_seed(0),\n    )\n&gt;&gt;&gt; print(list(batch_sampler))\n[[24, 17, 16, 22, 19], [2, 5], [12, 10, 11], [3, 0], [15, 18, 20, 21, 23], [7, 13, 6], [14, 9, 8], [1, 4]]\n&gt;&gt;&gt; print(list(batch_sampler))\n[[14, 9, 13], [23, 16, 20, 21, 15], [5, 0], [8, 10, 11], [17, 24, 22, 18, 19], [12, 6, 7], [4, 2], [3, 1]]\n\n&gt;&gt;&gt; # Combine with SizeAwareBatchSampler to control the cost of each batch\n&gt;&gt;&gt; from bionemo.size_aware_batching.sampler import SizeAwareBatchSampler\n&gt;&gt;&gt; item_costs = sizes.tolist()\n&gt;&gt;&gt; def cost_of_element(index):\n        return item_costs[index]\n&gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n        sizes=sizes,\n        bucket_boundaries=bucket_boundaries,\n        base_batch_sampler_class=SizeAwareBatchSampler,\n        base_batch_sampler_shared_kwargs={\"sizeof\": cost_of_element, \"max_total_size\": 40},\n        base_batch_sampler_individual_kwargs={},\n        shuffle=True,\n        generator=torch.Generator().manual_seed(0),\n    )\n&gt;&gt;&gt; print(list(iter(batch_sampler)))\n[[24], [2, 5, 3, 0, 1, 4], [12, 10, 11, 7], [13, 6, 14], [17, 16], [22], [19, 15], [9, 8], [18, 20], [21], [23]]\n</code></pre></p> Source code in <code>bionemo/size_aware_batching/sampler.py</code> <pre><code>class BucketBatchSampler(Sampler[List[int]]):\n    \"\"\"A batch sampler to create batches with sizes of elements from each pre-defined bucket ranges.\n\n    Elements of the dataset are first grouped into each bucket based on the bucket ranges and the sizes of elements.\n    Then, a base batch sampler is used for each bucket to create mini-batches.\n\n    The bucket ranges are specified by `bucket_boundaries`, which will be first sorted internally and used to create\n    `len(bucket_boundaries) - 1` left-closed right-open intervals.\n    e.g. if bucket_boundaries tensor is [10, 5, 0, 16], it will be sorted as [0, 5, 10, 16] and 3 buckets will be created\n    with ranges: [0, 5), [5, 10), [10, 16).\n\n    The base batch sampler will be created by passing the element indices in each bucket as the data source, and\n    `base_batch_sampler_shared_kwargs` and `base_batch_sampler_individual_kwargs`\n    to the constructor of the base batch sampler class specified as `base_batch_sampler_class`.\n    e.g. `base_batch_sampler_shared_kwargs = {'drop_last': True}` and `base_batch_sampler_individual_kwargs = {'batch_size': [8,10,12]}`\n    will be used to create 3 batch samplers with drop_last=True and batch_size=8, 10 and 12, and initialized like\n    `base_batch_sampler_class(bucket_element_indices[0], batch_size=8, drop_last=True)`.\n\n    In the `__iter__` method, if `shuffle` is `True`, the element indices in each bucket will be shuffled, and a bucket\n    is randomly selected each time to create a mini-batch. If `shuffle` is `False`, there is no shuffle on element indices,\n    and the bucket is selected in ascending order of its interval boundaries.\n\n    This class is used to create homogeneous batches of data for training or evaluation, and reduce the padding necessary to align the shape of elements.\n\n    Modified from https://github.com/rssrwn/semla-flow/blob/main/semlaflow/data/util.py\n\n    ---------\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from bionemo.size_aware_batching.sampler import BucketBatchSampler\n\n    &gt;&gt;&gt; # Define the sizes for a dataset\n    &gt;&gt;&gt; sizes = torch.arange(25)\n    &gt;&gt;&gt; # Define bucket ranges\n    &gt;&gt;&gt; bucket_boundaries = torch.tensor([0, 6, 15, 25])\n\n    &gt;&gt;&gt; # Create a bucket batch sampler with torch.utils.data.BatchSampler as base batch sampler\n    &gt;&gt;&gt; # As there are 3 buckets, there will be 3 base batch samplers with batch sizes 2, 3, and 5.\n    &gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n            sizes=sizes,\n            bucket_boundaries=bucket_boundaries,\n            base_batch_sampler_class=torch.utils.data.BatchSampler,\n            base_batch_sampler_shared_kwargs={'drop_last': False},\n            base_batch_sampler_individual_kwargs={'batch_size': [2,3,5]},\n            shuffle=False,\n        )\n\n    &gt;&gt;&gt; # Iterate over batches of indices that lies in the same bucket and with different batch sizes.\n    &gt;&gt;&gt; print(list(batch_sampler))\n    [[0, 1], [2, 3], [4, 5], [6, 7, 8], [9, 10, 11], [12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24]]\n\n    &gt;&gt;&gt; # randomize the dataset and buckets\n    &gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n            sizes=sizes,\n            bucket_boundaries=bucket_boundaries,\n            base_batch_sampler_class=torch.utils.data.BatchSampler,\n            base_batch_sampler_shared_kwargs={'drop_last': False},\n            base_batch_sampler_individual_kwargs={'batch_size': [2,3,5]},\n            shuffle=True,\n            generator=torch.Generator().manual_seed(0),\n        )\n    &gt;&gt;&gt; print(list(batch_sampler))\n    [[24, 17, 16, 22, 19], [2, 5], [12, 10, 11], [3, 0], [15, 18, 20, 21, 23], [7, 13, 6], [14, 9, 8], [1, 4]]\n    &gt;&gt;&gt; print(list(batch_sampler))\n    [[14, 9, 13], [23, 16, 20, 21, 15], [5, 0], [8, 10, 11], [17, 24, 22, 18, 19], [12, 6, 7], [4, 2], [3, 1]]\n\n    &gt;&gt;&gt; # Combine with SizeAwareBatchSampler to control the cost of each batch\n    &gt;&gt;&gt; from bionemo.size_aware_batching.sampler import SizeAwareBatchSampler\n    &gt;&gt;&gt; item_costs = sizes.tolist()\n    &gt;&gt;&gt; def cost_of_element(index):\n            return item_costs[index]\n    &gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n            sizes=sizes,\n            bucket_boundaries=bucket_boundaries,\n            base_batch_sampler_class=SizeAwareBatchSampler,\n            base_batch_sampler_shared_kwargs={\"sizeof\": cost_of_element, \"max_total_size\": 40},\n            base_batch_sampler_individual_kwargs={},\n            shuffle=True,\n            generator=torch.Generator().manual_seed(0),\n        )\n    &gt;&gt;&gt; print(list(iter(batch_sampler)))\n    [[24], [2, 5, 3, 0, 1, 4], [12, 10, 11, 7], [13, 6, 14], [17, 16], [22], [19, 15], [9, 8], [18, 20], [21], [23]]\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        sizes: torch.Tensor,\n        bucket_boundaries: torch.Tensor,\n        base_batch_sampler_class: Type[S],\n        base_batch_sampler_shared_kwargs: Optional[Dict[str, Any]] = None,\n        base_batch_sampler_individual_kwargs: Optional[Dict[str, Iterable]] = None,\n        shuffle: Optional[bool] = True,\n        generator: Optional[torch.Generator] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the BucketBatchSampler.\n\n        Args:\n            sizes: A 1D tensor of real numbers representing the size of each element in the dataset.\n            bucket_boundaries: A 1D tensor of real numbers representing the boundaries of the bucket ranges.\n                It will be first sorted and used to create `len(bucket_boundaries) - 1` left-closed right-open intervals as bucket ranges.\n                It should not contain any duplicate values.\n            base_batch_sampler_class: Base batch sampler class type, which will be used for each bucket, and initialized with the bucket element indices,\n                `base_batch_sampler_shared_kwargs` and the corresponding `base_batch_sampler_individual_kwargs`.\n            base_batch_sampler_shared_kwargs: Shared keyword argument dictionary used to initialize all base batch samplers for all buckets.\n                Sufficient and valid arguments should be provided for `base_batch_sampler_class` with `base_batch_sampler_individual_kwargs`. Default to  {}.\n            base_batch_sampler_individual_kwargs: Keyword argument dictionary used to initialize\n                each bucket batch sampler with the corresponding key value pairs.\n                Length of each value in this dict must be equal to len(bucket_boundaries) - 1 (the number of buckets).\n                Sufficient and valid arguments should be provided for `base_batch_sampler_class` with `base_batch_sampler_shared_kwargs`.\n                Default to  {}.\n            shuffle: A boolean indicating whether to shuffle the dataset and buckets. Defaults to True.\n            generator: Generator used in sampling. Defaults to None.\n\n        Raises:\n            ValueError: If `sizes` is not a 1D tensor of real numbers.\n            ValueError: If `bucket_boundaries` is not a 1D tensor of real numbers.\n            ValueError: If `base_batch_sampler_individual_kwargs` or `base_batch_sampler_individual_kwargs` is not a keyword argument dictionary.\n            ValueError: If the length of values in the dict of `base_batch_sampler_individual_kwargs` must be equal to len(bucket_boundaries) - 1.\n            RuntimeError: If there is no elements with sizes inside the ranges specified by `bucket_boundaries`.\n\n        \"\"\"\n        if not torch.is_tensor(sizes):\n            raise TypeError(f\"sizes should be a torch tensor, but got sizes={sizes}\")\n\n        if sizes.ndim != 1:\n            raise ValueError(f\"sizes should be a 1D tensor, but got sizes with shape {sizes.shape}\")\n\n        if not torch.is_floating_point(sizes) and sizes.dtype not in TorchIntegerDataTypes:\n            raise ValueError(\n                f\"sizes should contain only integers or floating point numbers, but got sizes.dtype={sizes.dtype}\"\n            )\n\n        if not torch.is_tensor(bucket_boundaries):\n            raise TypeError(\n                f\"bucket_boundaries should be a torch tensor, but got bucket_boundaries={bucket_boundaries}\"\n            )\n\n        if bucket_boundaries.ndim != 1:\n            raise ValueError(\n                f\"bucket_boundaries should be a 2D tensor, but got bucket_boundaries with shape {bucket_boundaries.shape}\"\n            )\n\n        if len(bucket_boundaries) &lt; 2:\n            raise ValueError(\n                f\"bucket_boundaries should have at least 2 numbers, but got bucket_boundaries={bucket_boundaries.shape}\"\n            )\n\n        if not torch.is_floating_point(bucket_boundaries) and bucket_boundaries.dtype not in TorchIntegerDataTypes:\n            raise ValueError(\n                f\"bucket_boundaries should contain only integers or floating point numbers, but got bucket_boundaries.dtype={bucket_boundaries.dtype}\"\n            )\n\n        bucket_boundaries = torch.sort(bucket_boundaries)[0]\n\n        if torch.any(bucket_boundaries[:-1] &gt;= bucket_boundaries[1:]):\n            raise ValueError(\n                f\"bucket_boundaries should not have duplicate values, and should specify the lower endpoint of each interval smaller than the upper endpoint, but got sorted bucket_boundaries={bucket_boundaries}\"\n            )\n\n        if not isinstance(shuffle, bool):\n            raise TypeError(f\"shuffle should be a boolean value, but got shuffle={shuffle}\")\n\n        self.sizes = sizes\n        self.bucket_boundaries = bucket_boundaries\n        self.num_buckets = len(bucket_boundaries) - 1\n        self.shuffle = shuffle\n        self.generator = generator\n        if self.shuffle and self.generator is None:\n            self.generator = torch.Generator().manual_seed(int(torch.empty((), dtype=torch.int64).random_().item()))\n\n        if not issubclass(base_batch_sampler_class, Sampler):\n            raise TypeError(\n                f\"base_batch_sampler_class should be a batch sampler class inherited from torch.utils.data.Sampler, but got base_batch_sampler_class={base_batch_sampler_class}\"\n            )\n\n        base_batch_sampler_shared_kwargs = (\n            {} if base_batch_sampler_shared_kwargs is None else base_batch_sampler_shared_kwargs\n        )\n        base_batch_sampler_individual_kwargs = (\n            {} if base_batch_sampler_individual_kwargs is None else base_batch_sampler_individual_kwargs\n        )\n        if not isinstance(base_batch_sampler_shared_kwargs, dict):\n            raise TypeError(\n                f\"base_batch_sampler_shared_kwargs should be a dictionary, but got base_batch_sampler_shared_kwargs={base_batch_sampler_shared_kwargs}\"\n            )\n\n        if not all(isinstance(key, str) for key in base_batch_sampler_shared_kwargs.keys()):\n            raise TypeError(\n                f\"base_batch_sampler_shared_kwargs should have string keys, but got keys={list(base_batch_sampler_shared_kwargs.keys())}\"\n            )\n\n        if not isinstance(base_batch_sampler_individual_kwargs, dict):\n            raise TypeError(\n                f\"base_batch_sampler_individual_kwargs should be a dictionary, but got base_batch_sampler_individual_kwargs={base_batch_sampler_individual_kwargs}\"\n            )\n\n        if not all(isinstance(key, str) for key in base_batch_sampler_individual_kwargs.keys()):\n            raise TypeError(\n                f\"base_batch_sampler_individual_kwargs should have string keys, but got keys={list(base_batch_sampler_individual_kwargs.keys())}\"\n            )\n\n        if not all(len(list(value)) == self.num_buckets for value in base_batch_sampler_individual_kwargs.values()):\n            raise ValueError(\n                f\"Each value in base_batch_sampler_individual_kwargs should have a length of {self.num_buckets}, \"\n                f\"but got lengths {[len(list(value)) for value in base_batch_sampler_individual_kwargs.values()]}\"\n            )\n\n        self.base_batch_sampler_class = base_batch_sampler_class\n        self.base_batch_sampler_shared_kwargs = (\n            {} if base_batch_sampler_shared_kwargs is None else base_batch_sampler_shared_kwargs\n        )\n        base_batch_sampler_individual_kwargs = (\n            {} if base_batch_sampler_individual_kwargs is None else base_batch_sampler_individual_kwargs\n        )\n        self.base_batch_sampler_individual_kwargs = [\n            {key: list(base_batch_sampler_individual_kwargs[key])[k] for key in base_batch_sampler_individual_kwargs}\n            for k in range(self.num_buckets)\n        ]\n\n        self.bucket_sizes: torch.Tensor  # number of elements in each bucket\n        self.bucket_element_indices: List[List[int]]  # List of elements' indices for each bucket\n\n        # bucket index for each element\n        element_bucket_indices = torch.bucketize(sizes, bucket_boundaries, right=True)\n\n        # element indices reordered for each bucket\n        reordered_element_indices = torch.argsort(element_bucket_indices, stable=True)\n\n        # bucket sizes, including the buckets for &lt; bucket_boundaries[0] and &gt;= bucket_boundaries[-1]\n        bucket_sizes = torch.bincount(element_bucket_indices, minlength=len(bucket_boundaries) + 1)\n\n        # bucket segments\n        bucket_segments = torch.cumsum(bucket_sizes, dim=0)[:-1]\n\n        self.bucket_element_indices = []\n        # exclude the buckets for &lt; bucket_boundaries[0] and &gt;= bucket_boundaries[-1]\n        for bucket_idx in range(self.num_buckets):\n            self.bucket_element_indices.append(\n                reordered_element_indices[bucket_segments[bucket_idx] : bucket_segments[bucket_idx + 1]].tolist()\n            )\n        self.bucket_sizes = bucket_sizes[1 : (self.num_buckets + 1)]\n\n        self.num_samples = torch.sum(self.bucket_sizes).item()\n        if self.num_samples == 0:\n            raise RuntimeError(\"The sizes of all elements in the dataset are outside the bucket ranges provided\")\n        if self.num_samples &lt; len(self.sizes):\n            warnings.warn(\n                f\"{len(self.sizes) - self.num_samples} elements are outside the buckets provided and will be skipped\"\n            )\n\n        self.base_batch_samplers: List[Sampler] = self._init_base_batch_samplers()\n\n    def _init_base_batch_samplers(self) -&gt; list[Sampler[List[int]]]:\n        \"\"\"Initialize batch samplers for each bucket.\n\n        Returns:\n            List of batch samplers.\n        \"\"\"\n        base_batch_samplers = []\n        for k in range(self.num_buckets):\n            base_batch_samplers.append(\n                self.base_batch_sampler_class(\n                    self.bucket_element_indices[k],\n                    **self.base_batch_sampler_shared_kwargs,\n                    **self.base_batch_sampler_individual_kwargs[k],\n                )\n            )\n        return base_batch_samplers\n\n    def __len__(self) -&gt; int:\n        \"\"\"Get the number of batches.\n\n        Can only be called if the `base_batch_sampler_class` has __len__() implemented\n\n        Returns:\n            int: Number of batches\n        \"\"\"\n        num_batches = sum(len(sampler) for sampler in self.base_batch_samplers)  # type: ignore\n        return num_batches\n\n    def __iter__(self) -&gt; Iterator[List[int]]:\n        \"\"\"Iterate over batches of indices.\n\n        This function yields batches of indices of elements with sizes from each bucket range.\n\n        Yields:\n            List[int]: A batch of indices of elements with sizes from each bucket range.\n        \"\"\"\n        if self.shuffle:\n            for indices in self.bucket_element_indices:\n                idx = torch.randperm(len(indices), generator=self.generator)\n                indices[:] = torch.tensor(indices)[idx].tolist()\n\n        base_batch_sampler_iters = [iter(batch_sampler) for batch_sampler in self.base_batch_samplers]\n        bucket_remaining_elements = self.bucket_sizes.clone()\n        total_remaining_elements = self.num_samples\n\n        while total_remaining_elements &gt; 0:\n            if self.shuffle:\n                bucket_idx = torch.multinomial(\n                    bucket_remaining_elements / total_remaining_elements, 1, generator=self.generator\n                )\n            else:\n                bucket_idx = torch.argmax((bucket_remaining_elements &gt; 0).to(int))  # type: ignore\n\n            try:\n                batch = next(base_batch_sampler_iters[bucket_idx])\n                bucket_remaining_elements[bucket_idx] -= len(batch)\n                total_remaining_elements -= len(batch)\n                yield batch\n            except StopIteration:\n                bucket_remaining_elements[bucket_idx] = 0\n                total_remaining_elements = torch.sum(bucket_remaining_elements)\n                continue\n</code></pre>"},{"location":"main/references/API_reference/bionemo/size_aware_batching/sampler/#bionemo.size_aware_batching.sampler.BucketBatchSampler.__init__","title":"<code>__init__(sizes, bucket_boundaries, base_batch_sampler_class, base_batch_sampler_shared_kwargs=None, base_batch_sampler_individual_kwargs=None, shuffle=True, generator=None)</code>","text":"<p>Initializes the BucketBatchSampler.</p> <p>Parameters:</p> Name Type Description Default <code>sizes</code> <code>Tensor</code> <p>A 1D tensor of real numbers representing the size of each element in the dataset.</p> required <code>bucket_boundaries</code> <code>Tensor</code> <p>A 1D tensor of real numbers representing the boundaries of the bucket ranges. It will be first sorted and used to create <code>len(bucket_boundaries) - 1</code> left-closed right-open intervals as bucket ranges. It should not contain any duplicate values.</p> required <code>base_batch_sampler_class</code> <code>Type[S]</code> <p>Base batch sampler class type, which will be used for each bucket, and initialized with the bucket element indices, <code>base_batch_sampler_shared_kwargs</code> and the corresponding <code>base_batch_sampler_individual_kwargs</code>.</p> required <code>base_batch_sampler_shared_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Shared keyword argument dictionary used to initialize all base batch samplers for all buckets. Sufficient and valid arguments should be provided for <code>base_batch_sampler_class</code> with <code>base_batch_sampler_individual_kwargs</code>. Default to  {}.</p> <code>None</code> <code>base_batch_sampler_individual_kwargs</code> <code>Optional[Dict[str, Iterable]]</code> <p>Keyword argument dictionary used to initialize each bucket batch sampler with the corresponding key value pairs. Length of each value in this dict must be equal to len(bucket_boundaries) - 1 (the number of buckets). Sufficient and valid arguments should be provided for <code>base_batch_sampler_class</code> with <code>base_batch_sampler_shared_kwargs</code>. Default to  {}.</p> <code>None</code> <code>shuffle</code> <code>Optional[bool]</code> <p>A boolean indicating whether to shuffle the dataset and buckets. Defaults to True.</p> <code>True</code> <code>generator</code> <code>Optional[Generator]</code> <p>Generator used in sampling. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>sizes</code> is not a 1D tensor of real numbers.</p> <code>ValueError</code> <p>If <code>bucket_boundaries</code> is not a 1D tensor of real numbers.</p> <code>ValueError</code> <p>If <code>base_batch_sampler_individual_kwargs</code> or <code>base_batch_sampler_individual_kwargs</code> is not a keyword argument dictionary.</p> <code>ValueError</code> <p>If the length of values in the dict of <code>base_batch_sampler_individual_kwargs</code> must be equal to len(bucket_boundaries) - 1.</p> <code>RuntimeError</code> <p>If there is no elements with sizes inside the ranges specified by <code>bucket_boundaries</code>.</p> Source code in <code>bionemo/size_aware_batching/sampler.py</code> <pre><code>def __init__(\n    self,\n    sizes: torch.Tensor,\n    bucket_boundaries: torch.Tensor,\n    base_batch_sampler_class: Type[S],\n    base_batch_sampler_shared_kwargs: Optional[Dict[str, Any]] = None,\n    base_batch_sampler_individual_kwargs: Optional[Dict[str, Iterable]] = None,\n    shuffle: Optional[bool] = True,\n    generator: Optional[torch.Generator] = None,\n) -&gt; None:\n    \"\"\"Initializes the BucketBatchSampler.\n\n    Args:\n        sizes: A 1D tensor of real numbers representing the size of each element in the dataset.\n        bucket_boundaries: A 1D tensor of real numbers representing the boundaries of the bucket ranges.\n            It will be first sorted and used to create `len(bucket_boundaries) - 1` left-closed right-open intervals as bucket ranges.\n            It should not contain any duplicate values.\n        base_batch_sampler_class: Base batch sampler class type, which will be used for each bucket, and initialized with the bucket element indices,\n            `base_batch_sampler_shared_kwargs` and the corresponding `base_batch_sampler_individual_kwargs`.\n        base_batch_sampler_shared_kwargs: Shared keyword argument dictionary used to initialize all base batch samplers for all buckets.\n            Sufficient and valid arguments should be provided for `base_batch_sampler_class` with `base_batch_sampler_individual_kwargs`. Default to  {}.\n        base_batch_sampler_individual_kwargs: Keyword argument dictionary used to initialize\n            each bucket batch sampler with the corresponding key value pairs.\n            Length of each value in this dict must be equal to len(bucket_boundaries) - 1 (the number of buckets).\n            Sufficient and valid arguments should be provided for `base_batch_sampler_class` with `base_batch_sampler_shared_kwargs`.\n            Default to  {}.\n        shuffle: A boolean indicating whether to shuffle the dataset and buckets. Defaults to True.\n        generator: Generator used in sampling. Defaults to None.\n\n    Raises:\n        ValueError: If `sizes` is not a 1D tensor of real numbers.\n        ValueError: If `bucket_boundaries` is not a 1D tensor of real numbers.\n        ValueError: If `base_batch_sampler_individual_kwargs` or `base_batch_sampler_individual_kwargs` is not a keyword argument dictionary.\n        ValueError: If the length of values in the dict of `base_batch_sampler_individual_kwargs` must be equal to len(bucket_boundaries) - 1.\n        RuntimeError: If there is no elements with sizes inside the ranges specified by `bucket_boundaries`.\n\n    \"\"\"\n    if not torch.is_tensor(sizes):\n        raise TypeError(f\"sizes should be a torch tensor, but got sizes={sizes}\")\n\n    if sizes.ndim != 1:\n        raise ValueError(f\"sizes should be a 1D tensor, but got sizes with shape {sizes.shape}\")\n\n    if not torch.is_floating_point(sizes) and sizes.dtype not in TorchIntegerDataTypes:\n        raise ValueError(\n            f\"sizes should contain only integers or floating point numbers, but got sizes.dtype={sizes.dtype}\"\n        )\n\n    if not torch.is_tensor(bucket_boundaries):\n        raise TypeError(\n            f\"bucket_boundaries should be a torch tensor, but got bucket_boundaries={bucket_boundaries}\"\n        )\n\n    if bucket_boundaries.ndim != 1:\n        raise ValueError(\n            f\"bucket_boundaries should be a 2D tensor, but got bucket_boundaries with shape {bucket_boundaries.shape}\"\n        )\n\n    if len(bucket_boundaries) &lt; 2:\n        raise ValueError(\n            f\"bucket_boundaries should have at least 2 numbers, but got bucket_boundaries={bucket_boundaries.shape}\"\n        )\n\n    if not torch.is_floating_point(bucket_boundaries) and bucket_boundaries.dtype not in TorchIntegerDataTypes:\n        raise ValueError(\n            f\"bucket_boundaries should contain only integers or floating point numbers, but got bucket_boundaries.dtype={bucket_boundaries.dtype}\"\n        )\n\n    bucket_boundaries = torch.sort(bucket_boundaries)[0]\n\n    if torch.any(bucket_boundaries[:-1] &gt;= bucket_boundaries[1:]):\n        raise ValueError(\n            f\"bucket_boundaries should not have duplicate values, and should specify the lower endpoint of each interval smaller than the upper endpoint, but got sorted bucket_boundaries={bucket_boundaries}\"\n        )\n\n    if not isinstance(shuffle, bool):\n        raise TypeError(f\"shuffle should be a boolean value, but got shuffle={shuffle}\")\n\n    self.sizes = sizes\n    self.bucket_boundaries = bucket_boundaries\n    self.num_buckets = len(bucket_boundaries) - 1\n    self.shuffle = shuffle\n    self.generator = generator\n    if self.shuffle and self.generator is None:\n        self.generator = torch.Generator().manual_seed(int(torch.empty((), dtype=torch.int64).random_().item()))\n\n    if not issubclass(base_batch_sampler_class, Sampler):\n        raise TypeError(\n            f\"base_batch_sampler_class should be a batch sampler class inherited from torch.utils.data.Sampler, but got base_batch_sampler_class={base_batch_sampler_class}\"\n        )\n\n    base_batch_sampler_shared_kwargs = (\n        {} if base_batch_sampler_shared_kwargs is None else base_batch_sampler_shared_kwargs\n    )\n    base_batch_sampler_individual_kwargs = (\n        {} if base_batch_sampler_individual_kwargs is None else base_batch_sampler_individual_kwargs\n    )\n    if not isinstance(base_batch_sampler_shared_kwargs, dict):\n        raise TypeError(\n            f\"base_batch_sampler_shared_kwargs should be a dictionary, but got base_batch_sampler_shared_kwargs={base_batch_sampler_shared_kwargs}\"\n        )\n\n    if not all(isinstance(key, str) for key in base_batch_sampler_shared_kwargs.keys()):\n        raise TypeError(\n            f\"base_batch_sampler_shared_kwargs should have string keys, but got keys={list(base_batch_sampler_shared_kwargs.keys())}\"\n        )\n\n    if not isinstance(base_batch_sampler_individual_kwargs, dict):\n        raise TypeError(\n            f\"base_batch_sampler_individual_kwargs should be a dictionary, but got base_batch_sampler_individual_kwargs={base_batch_sampler_individual_kwargs}\"\n        )\n\n    if not all(isinstance(key, str) for key in base_batch_sampler_individual_kwargs.keys()):\n        raise TypeError(\n            f\"base_batch_sampler_individual_kwargs should have string keys, but got keys={list(base_batch_sampler_individual_kwargs.keys())}\"\n        )\n\n    if not all(len(list(value)) == self.num_buckets for value in base_batch_sampler_individual_kwargs.values()):\n        raise ValueError(\n            f\"Each value in base_batch_sampler_individual_kwargs should have a length of {self.num_buckets}, \"\n            f\"but got lengths {[len(list(value)) for value in base_batch_sampler_individual_kwargs.values()]}\"\n        )\n\n    self.base_batch_sampler_class = base_batch_sampler_class\n    self.base_batch_sampler_shared_kwargs = (\n        {} if base_batch_sampler_shared_kwargs is None else base_batch_sampler_shared_kwargs\n    )\n    base_batch_sampler_individual_kwargs = (\n        {} if base_batch_sampler_individual_kwargs is None else base_batch_sampler_individual_kwargs\n    )\n    self.base_batch_sampler_individual_kwargs = [\n        {key: list(base_batch_sampler_individual_kwargs[key])[k] for key in base_batch_sampler_individual_kwargs}\n        for k in range(self.num_buckets)\n    ]\n\n    self.bucket_sizes: torch.Tensor  # number of elements in each bucket\n    self.bucket_element_indices: List[List[int]]  # List of elements' indices for each bucket\n\n    # bucket index for each element\n    element_bucket_indices = torch.bucketize(sizes, bucket_boundaries, right=True)\n\n    # element indices reordered for each bucket\n    reordered_element_indices = torch.argsort(element_bucket_indices, stable=True)\n\n    # bucket sizes, including the buckets for &lt; bucket_boundaries[0] and &gt;= bucket_boundaries[-1]\n    bucket_sizes = torch.bincount(element_bucket_indices, minlength=len(bucket_boundaries) + 1)\n\n    # bucket segments\n    bucket_segments = torch.cumsum(bucket_sizes, dim=0)[:-1]\n\n    self.bucket_element_indices = []\n    # exclude the buckets for &lt; bucket_boundaries[0] and &gt;= bucket_boundaries[-1]\n    for bucket_idx in range(self.num_buckets):\n        self.bucket_element_indices.append(\n            reordered_element_indices[bucket_segments[bucket_idx] : bucket_segments[bucket_idx + 1]].tolist()\n        )\n    self.bucket_sizes = bucket_sizes[1 : (self.num_buckets + 1)]\n\n    self.num_samples = torch.sum(self.bucket_sizes).item()\n    if self.num_samples == 0:\n        raise RuntimeError(\"The sizes of all elements in the dataset are outside the bucket ranges provided\")\n    if self.num_samples &lt; len(self.sizes):\n        warnings.warn(\n            f\"{len(self.sizes) - self.num_samples} elements are outside the buckets provided and will be skipped\"\n        )\n\n    self.base_batch_samplers: List[Sampler] = self._init_base_batch_samplers()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/size_aware_batching/sampler/#bionemo.size_aware_batching.sampler.BucketBatchSampler.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over batches of indices.</p> <p>This function yields batches of indices of elements with sizes from each bucket range.</p> <p>Yields:</p> Type Description <code>List[int]</code> <p>List[int]: A batch of indices of elements with sizes from each bucket range.</p> Source code in <code>bionemo/size_aware_batching/sampler.py</code> <pre><code>def __iter__(self) -&gt; Iterator[List[int]]:\n    \"\"\"Iterate over batches of indices.\n\n    This function yields batches of indices of elements with sizes from each bucket range.\n\n    Yields:\n        List[int]: A batch of indices of elements with sizes from each bucket range.\n    \"\"\"\n    if self.shuffle:\n        for indices in self.bucket_element_indices:\n            idx = torch.randperm(len(indices), generator=self.generator)\n            indices[:] = torch.tensor(indices)[idx].tolist()\n\n    base_batch_sampler_iters = [iter(batch_sampler) for batch_sampler in self.base_batch_samplers]\n    bucket_remaining_elements = self.bucket_sizes.clone()\n    total_remaining_elements = self.num_samples\n\n    while total_remaining_elements &gt; 0:\n        if self.shuffle:\n            bucket_idx = torch.multinomial(\n                bucket_remaining_elements / total_remaining_elements, 1, generator=self.generator\n            )\n        else:\n            bucket_idx = torch.argmax((bucket_remaining_elements &gt; 0).to(int))  # type: ignore\n\n        try:\n            batch = next(base_batch_sampler_iters[bucket_idx])\n            bucket_remaining_elements[bucket_idx] -= len(batch)\n            total_remaining_elements -= len(batch)\n            yield batch\n        except StopIteration:\n            bucket_remaining_elements[bucket_idx] = 0\n            total_remaining_elements = torch.sum(bucket_remaining_elements)\n            continue\n</code></pre>"},{"location":"main/references/API_reference/bionemo/size_aware_batching/sampler/#bionemo.size_aware_batching.sampler.BucketBatchSampler.__len__","title":"<code>__len__()</code>","text":"<p>Get the number of batches.</p> <p>Can only be called if the <code>base_batch_sampler_class</code> has len() implemented</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of batches</p> Source code in <code>bionemo/size_aware_batching/sampler.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get the number of batches.\n\n    Can only be called if the `base_batch_sampler_class` has __len__() implemented\n\n    Returns:\n        int: Number of batches\n    \"\"\"\n    num_batches = sum(len(sampler) for sampler in self.base_batch_samplers)  # type: ignore\n    return num_batches\n</code></pre>"},{"location":"main/references/API_reference/bionemo/size_aware_batching/sampler/#bionemo.size_aware_batching.sampler.SizeAwareBatchSampler","title":"<code>SizeAwareBatchSampler</code>","text":"<p>               Bases: <code>Sampler[List[int]]</code></p> <p>Varriying-size batching data sampler class that ensures batch size doesn't exceed maximum.</p> <p>A sampler that batches elements of varying sizes while ensuring that the total size of each batch does not exceed a specified maximum.</p> <p>This is useful when dealing with datasets where each element has a different size, such as graphs or sequences of varying lengths. The sampler uses a provided <code>sizeof</code> function to determine the size of each element in the dataset and ensures that the total size of each batch does not exceed the specified <code>max_total_size</code>.</p> <p>Examples: <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.size_aware_batching.sampler import SizeAwareBatchSampler\n\n\n&gt;&gt;&gt; # Define a sample dataset with torch.tensor\n&gt;&gt;&gt; dataset = [torch.tensor([1, 2]), torch.tensor([3, 4]), torch.tensor([5, 6]),\n...            torch.tensor([7, 8]), torch.tensor([9, 10])]\n\n\n&gt;&gt;&gt; # Define a function that returns the size of each element in the dataset.\n&gt;&gt;&gt; def sizeof(index):\n...     return dataset[index].numel()\n\n\n&gt;&gt;&gt; # Create a SizeAwareBatchSampler with a maximum total batch size of 10.\n&gt;&gt;&gt; batch_sampler = SizeAwareBatchSampler(\n...     sampler=torch.utils.data.SequentialSampler(dataset),\n...     sizeof=sizeof,\n...     max_total_size=4\n... )\n\n\n&gt;&gt;&gt; # Iterate over batches of indices that do not exceed the maximum total size.\n&gt;&gt;&gt; print(list(batch_sampler))\n    [[0, 1], [2, 3], [4]]\n</code></pre></p> Source code in <code>bionemo/size_aware_batching/sampler.py</code> <pre><code>class SizeAwareBatchSampler(Sampler[List[int]]):\n    \"\"\"Varriying-size batching data sampler class that ensures batch size doesn't exceed maximum.\n\n    A sampler that batches elements of varying sizes while ensuring\n    that the total size of each batch does not exceed a specified maximum.\n\n    This is useful when dealing with datasets where each element has a\n    different size, such as graphs or sequences of varying lengths.\n    The sampler uses a provided `sizeof` function to determine the size\n    of each element in the dataset and ensures that the total size of\n    each batch does not exceed the specified `max_total_size`.\n\n    ---------\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from bionemo.size_aware_batching.sampler import SizeAwareBatchSampler\n\n\n    &gt;&gt;&gt; # Define a sample dataset with torch.tensor\n    &gt;&gt;&gt; dataset = [torch.tensor([1, 2]), torch.tensor([3, 4]), torch.tensor([5, 6]),\n    ...            torch.tensor([7, 8]), torch.tensor([9, 10])]\n\n\n    &gt;&gt;&gt; # Define a function that returns the size of each element in the dataset.\n    &gt;&gt;&gt; def sizeof(index):\n    ...     return dataset[index].numel()\n\n\n    &gt;&gt;&gt; # Create a SizeAwareBatchSampler with a maximum total batch size of 10.\n    &gt;&gt;&gt; batch_sampler = SizeAwareBatchSampler(\n    ...     sampler=torch.utils.data.SequentialSampler(dataset),\n    ...     sizeof=sizeof,\n    ...     max_total_size=4\n    ... )\n\n\n    &gt;&gt;&gt; # Iterate over batches of indices that do not exceed the maximum total size.\n    &gt;&gt;&gt; print(list(batch_sampler))\n        [[0, 1], [2, 3], [4]]\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        sampler: Union[Sampler[List[int]], Iterable[int]],\n        sizeof: Callable[[int], Real],\n        max_total_size: Real,\n        info_logger: Optional[Callable[[str], None]] = None,\n        warn_logger: Optional[Callable[[str], None]] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the SizeAwareBatchSampler.\n\n        Args:\n            sampler: The underlying sampler.\n            sizeof: A function that returns the size at each index. E.g., this can used to\n                determine how much memory an element consumes. Its return type must be\n                comparable with `max_total_size` and it must be addable (operator `+`).\n            max_total_size: The maximum total size of a mini-batch. The semantics of \"size\"\n                is defined by the `sizeof` argument. The type of this value must be comparable\n                with the return type of sizeof, i.e., the operator `&lt;` and `==` must be meaningful.\n            info_logger: A function to log info. Defaults to None.\n            warn_logger: A function to log warnings. Defaults None.\n\n        Raises:\n            TypeError: If sampler is not an instance of Sampler or Iterable, or if sizeof is not a callable, dictionary, or sequence container.\n            ValueError: If max_total_size is not a positive number.\n\n        \"\"\"\n        if not (isinstance(sampler, Sampler) or (isinstance(sampler, Iterable) and not isinstance(sampler, str))):\n            raise TypeError(\"sampler should be an instance of torch.utils.data.Sampler or Iterable\")\n\n        if not isinstance(max_total_size, Real):\n            raise ValueError(f\"max_total_size should be int or float but got {type(max_total_size)}\")\n\n        self._info_logger = info_logger\n        self._warn_logger = warn_logger\n\n        self._is_sizeof_callable = callable(sizeof)\n\n        if not self._is_sizeof_callable:\n            raise TypeError(\"sizeof must be a callable\")\n\n        self._sampler = sampler\n        self._sizeof = sizeof\n        self._max_total_size = max_total_size\n\n    def __iter__(self) -&gt; Iterator[List[int]]:\n        \"\"\"Iterate over batches of indices.\n\n        This function yields batches of indices that do not exceed the maximum total size.\n\n        Yields:\n            A batch of indices that do not exceed the maximum total size.\n        \"\"\"\n        return size_aware_batching(\n            self._sampler,\n            self._sizeof,\n            self._max_total_size,\n            collate_fn=None,\n            info_logger=self._info_logger,\n            warn_logger=self._warn_logger,\n        )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/size_aware_batching/sampler/#bionemo.size_aware_batching.sampler.SizeAwareBatchSampler.__init__","title":"<code>__init__(sampler, sizeof, max_total_size, info_logger=None, warn_logger=None)</code>","text":"<p>Initializes the SizeAwareBatchSampler.</p> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[Sampler[List[int]], Iterable[int]]</code> <p>The underlying sampler.</p> required <code>sizeof</code> <code>Callable[[int], Real]</code> <p>A function that returns the size at each index. E.g., this can used to determine how much memory an element consumes. Its return type must be comparable with <code>max_total_size</code> and it must be addable (operator <code>+</code>).</p> required <code>max_total_size</code> <code>Real</code> <p>The maximum total size of a mini-batch. The semantics of \"size\" is defined by the <code>sizeof</code> argument. The type of this value must be comparable with the return type of sizeof, i.e., the operator <code>&lt;</code> and <code>==</code> must be meaningful.</p> required <code>info_logger</code> <code>Optional[Callable[[str], None]]</code> <p>A function to log info. Defaults to None.</p> <code>None</code> <code>warn_logger</code> <code>Optional[Callable[[str], None]]</code> <p>A function to log warnings. Defaults None.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If sampler is not an instance of Sampler or Iterable, or if sizeof is not a callable, dictionary, or sequence container.</p> <code>ValueError</code> <p>If max_total_size is not a positive number.</p> Source code in <code>bionemo/size_aware_batching/sampler.py</code> <pre><code>def __init__(\n    self,\n    sampler: Union[Sampler[List[int]], Iterable[int]],\n    sizeof: Callable[[int], Real],\n    max_total_size: Real,\n    info_logger: Optional[Callable[[str], None]] = None,\n    warn_logger: Optional[Callable[[str], None]] = None,\n) -&gt; None:\n    \"\"\"Initializes the SizeAwareBatchSampler.\n\n    Args:\n        sampler: The underlying sampler.\n        sizeof: A function that returns the size at each index. E.g., this can used to\n            determine how much memory an element consumes. Its return type must be\n            comparable with `max_total_size` and it must be addable (operator `+`).\n        max_total_size: The maximum total size of a mini-batch. The semantics of \"size\"\n            is defined by the `sizeof` argument. The type of this value must be comparable\n            with the return type of sizeof, i.e., the operator `&lt;` and `==` must be meaningful.\n        info_logger: A function to log info. Defaults to None.\n        warn_logger: A function to log warnings. Defaults None.\n\n    Raises:\n        TypeError: If sampler is not an instance of Sampler or Iterable, or if sizeof is not a callable, dictionary, or sequence container.\n        ValueError: If max_total_size is not a positive number.\n\n    \"\"\"\n    if not (isinstance(sampler, Sampler) or (isinstance(sampler, Iterable) and not isinstance(sampler, str))):\n        raise TypeError(\"sampler should be an instance of torch.utils.data.Sampler or Iterable\")\n\n    if not isinstance(max_total_size, Real):\n        raise ValueError(f\"max_total_size should be int or float but got {type(max_total_size)}\")\n\n    self._info_logger = info_logger\n    self._warn_logger = warn_logger\n\n    self._is_sizeof_callable = callable(sizeof)\n\n    if not self._is_sizeof_callable:\n        raise TypeError(\"sizeof must be a callable\")\n\n    self._sampler = sampler\n    self._sizeof = sizeof\n    self._max_total_size = max_total_size\n</code></pre>"},{"location":"main/references/API_reference/bionemo/size_aware_batching/sampler/#bionemo.size_aware_batching.sampler.SizeAwareBatchSampler.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over batches of indices.</p> <p>This function yields batches of indices that do not exceed the maximum total size.</p> <p>Yields:</p> Type Description <code>List[int]</code> <p>A batch of indices that do not exceed the maximum total size.</p> Source code in <code>bionemo/size_aware_batching/sampler.py</code> <pre><code>def __iter__(self) -&gt; Iterator[List[int]]:\n    \"\"\"Iterate over batches of indices.\n\n    This function yields batches of indices that do not exceed the maximum total size.\n\n    Yields:\n        A batch of indices that do not exceed the maximum total size.\n    \"\"\"\n    return size_aware_batching(\n        self._sampler,\n        self._sizeof,\n        self._max_total_size,\n        collate_fn=None,\n        info_logger=self._info_logger,\n        warn_logger=self._warn_logger,\n    )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/size_aware_batching/sampler/#bionemo.size_aware_batching.sampler.size_aware_batching","title":"<code>size_aware_batching(dataset, sizeof, max_total_size, collate_fn=None, info_logger=None, warn_logger=None)</code>","text":"<p>Creates a batching iterator where each batch size varries (within a max limit) according to memory consumption.</p> <p>A generator that batches elements from an iterable while ensuring that the total size of each batch does not exceed a specified maximum. Here the size can be a measurement of memory consumption of the elements in the batch. This can be useful for both indexible data or non-indexible but iterable data.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Iterable[Data]</code> <p>The input iterable.</p> required <code>sizeof</code> <code>Callable[[Data], Real]</code> <p>A function or mapping that returns the \"size\" of each element in <code>dataset</code>. E.g., this can used to determine how much memory an element consumes. Its return type must be comparable with <code>max_total_size</code> and it must be addable (operator <code>+</code>).</p> required <code>max_total_size</code> <code>Real</code> <p>The maximum total \"size\" of each batch. The semantics of \"size\" is defined by the <code>sizeof</code> argument. The type of this value must be comparable with the return type of sizeof, i.e., the operator <code>&lt;</code> and <code>==</code> must be meaningful.</p> required <code>collate_fn</code> <code>Optional[Callable[[Iterable[Data]], BatchCollated]]</code> <p>An optional function to collate batches. Defaults to None, in which case each batch is a list of elements from the input dataset</p> <code>None</code> <code>info_logger</code> <code>Optional[Callable[[str], None]]</code> <p>A function to log info. Defaults to None.</p> <code>None</code> <code>warn_logger</code> <code>Optional[Callable[[str], None]]</code> <p>A function to log warnings. Defaults to None.</p> <code>None</code> <p>Yields:</p> Type Description <code>Union[List[Data], BatchCollated]</code> <p>A generator that yields batches from <code>dataset</code>.</p> <p>Assumptions 1. Linear complexity. This function consumes the given Iterable of data (<code>dataset</code>) once,    by going over the data item one by one to build a batch and yield it as soon as the    addition of the next data item to the batch would exceed <code>max_total_size</code> or if the    batch is the last one (end of iteration) 2. Additive size measurement. For the general usage case of building mini-batches with    a threshold of the batch's memory consumption, it assumes that the size of the batch is    the sum of all elements in the batch (additive property). 3. Comparable type of <code>max_total_size</code> and <code>sizeof</code>'s return. <code>sizeof</code>'s return values    must be compared with <code>max_total_size</code> to threshold the size of batches</p> <p>Caveat 1: The generated batch sizes may have large variance    - how to workaround: filter the output of this generator using a batch size threshold 2: The number of batches may vary a lot across different epochs.    - how to workaround: increase the number of steps that compose an epoch,      e.g., in the Lightning training/validation loop, which effectively increases the input      dataset size per epoch</p> <p>Example: <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch.utils.data import default_collate\n&gt;&gt;&gt; from bionemo.size_aware_batching.sampler import size_aware_batching\n\n&gt;&gt;&gt; # Define a sample dataset with torch.tensor\n&gt;&gt;&gt; dataset = [torch.tensor([1, 2]), torch.tensor([3, 4]), torch.tensor([5, 6]),\n...            torch.tensor([7, 8]), torch.tensor([9, 10])]\n\n&gt;&gt;&gt; # Define a sizeof function that returns the size of each tensor\n&gt;&gt;&gt; def sizeof(x):\n...     return x.numel()\n\n&gt;&gt;&gt; # Create a generator with max_total_size=4 and default_collate_fn\n&gt;&gt;&gt; gen = size_aware_batching(dataset, sizeof, 4, collate_fn=default_collate)\n&gt;&gt;&gt; batches = list(gen)\n&gt;&gt;&gt; print(batches)\n    [tensor([[1, 2], [3, 4]]), tensor([[5, 6], [7, 8]]), tensor([[9, 10]])]\n</code></pre></p> Source code in <code>bionemo/size_aware_batching/sampler.py</code> <pre><code>def size_aware_batching(\n    dataset: Iterable[Data],\n    sizeof: Callable[[Data], Real],\n    max_total_size: Real,\n    collate_fn: Optional[Callable[[Iterable[Data]], BatchCollated]] = None,\n    info_logger: Optional[Callable[[str], None]] = None,\n    warn_logger: Optional[Callable[[str], None]] = None,\n) -&gt; Iterator[Union[List[Data], BatchCollated]]:\n    \"\"\"Creates a batching iterator where each batch size varries (within a max limit) according to memory consumption.\n\n    A generator that batches elements from an iterable while ensuring that the\n    total size of each batch does not exceed a specified maximum. Here the size\n    can be a measurement of memory consumption of the elements in the batch.\n    This can be useful for both indexible data or non-indexible but iterable data.\n\n    Args:\n        dataset: The input iterable.\n        sizeof: A function or mapping that returns the \"size\" of each element in `dataset`.\n            E.g., this can used to determine how much memory an element consumes. Its return\n            type must be comparable with `max_total_size` and it must be addable (operator `+`).\n        max_total_size: The maximum total \"size\" of each batch. The semantics of \"size\"\n            is defined by the `sizeof` argument. The type of this value must be comparable\n            with the return type of sizeof, i.e., the operator `&lt;` and `==` must be meaningful.\n        collate_fn: An optional function to collate batches. Defaults to None, in which case\n            each batch is a list of elements from the input dataset\n        info_logger: A function to log info. Defaults to None.\n        warn_logger: A function to log warnings. Defaults to None.\n\n    Yields:\n        A generator that yields batches from `dataset`.\n\n    -----------\n    Assumptions\n    1. Linear complexity. This function consumes the given Iterable of data (`dataset`) once,\n       by going over the data item one by one to build a batch and yield it as soon as the\n       addition of the next data item to the batch would exceed `max_total_size` or if the\n       batch is the last one (end of iteration)\n    2. Additive size measurement. For the general usage case of building mini-batches with\n       a threshold of the batch's memory consumption, it assumes that the size of the batch is\n       the sum of all elements in the batch (additive property).\n    3. Comparable type of `max_total_size` and `sizeof`'s return. `sizeof`'s return values\n       must be compared with `max_total_size` to threshold the size of batches\n\n\n    ------\n    Caveat\n    1: The generated batch sizes may have large variance\n       - how to workaround: filter the output of this generator using a batch size threshold\n    2: The number of batches may vary a lot across different epochs.\n       - how to workaround: increase the number of steps that compose an epoch,\n         e.g., in the Lightning training/validation loop, which effectively increases the input\n         dataset size per epoch\n\n\n    -------\n\n    Example:\n    ```python\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from torch.utils.data import default_collate\n    &gt;&gt;&gt; from bionemo.size_aware_batching.sampler import size_aware_batching\n\n    &gt;&gt;&gt; # Define a sample dataset with torch.tensor\n    &gt;&gt;&gt; dataset = [torch.tensor([1, 2]), torch.tensor([3, 4]), torch.tensor([5, 6]),\n    ...            torch.tensor([7, 8]), torch.tensor([9, 10])]\n\n    &gt;&gt;&gt; # Define a sizeof function that returns the size of each tensor\n    &gt;&gt;&gt; def sizeof(x):\n    ...     return x.numel()\n\n    &gt;&gt;&gt; # Create a generator with max_total_size=4 and default_collate_fn\n    &gt;&gt;&gt; gen = size_aware_batching(dataset, sizeof, 4, collate_fn=default_collate)\n    &gt;&gt;&gt; batches = list(gen)\n    &gt;&gt;&gt; print(batches)\n        [tensor([[1, 2], [3, 4]]), tensor([[5, 6], [7, 8]]), tensor([[9, 10]])]\n    ```\n\n    \"\"\"\n    is_sizeof_callable = callable(sizeof)\n    has_collate_fn = collate_fn is not None and callable(collate_fn)\n\n    if not is_sizeof_callable:\n        raise TypeError(\"sizeof must be a callable\")\n\n    batch_total_size = 0\n    batch = []\n    n_samples = 0\n    n_samples_batched = 0\n    n_batches = 0\n    for data in dataset:\n        n_samples += 1\n        try:\n            new_size = sizeof(data)\n        except Exception as e:\n            raise RuntimeError(f\"sizeof raises error at data={data}: {e}\") from e\n        if new_size &gt; max_total_size:\n            if warn_logger is not None:\n                warn_logger(f\"Size of element {data} exceeds max_total_size ({new_size} &gt; {max_total_size}), skipping\")\n            continue\n        if new_size + batch_total_size &gt; max_total_size:\n            n_batches += 1\n            if has_collate_fn:\n                yield collate_fn(batch)\n            else:\n                yield batch\n            batch_total_size = 0\n            batch = []\n        batch.append(data)\n        n_samples_batched += 1\n        batch_total_size += new_size\n\n    # return the remaining batch if there is\n    if len(batch) &gt; 0:\n        n_batches += 1\n        if has_collate_fn:\n            yield collate_fn(batch)\n        else:\n            yield batch\n\n    if warn_logger is not None and n_samples_batched &lt; n_samples:\n        warn_logger(\n            f\"{n_samples_batched} samples were batched from {n_samples} \"\n            f\"of the input data. Missing samples are due to exceeding max_total_size={max_total_size})\"\n        )\n\n    if info_logger is not None:\n        info_logger(\n            f\"Batched {n_samples_batched} samples into {n_batches} batches. \"\n            f\"If this doesn't match the your expectation, consider adjusting \"\n            f\"max_total_size or the sizeof functor\"\n        )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/size_aware_batching/utils/","title":"Utils","text":""},{"location":"main/references/API_reference/bionemo/size_aware_batching/utils/#bionemo.size_aware_batching.utils.Buckets","title":"<code>Buckets</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A container for storing bucket boundaries and sizes.</p> <p>Attributes:</p> Name Type Description <code>bucket_boundaries</code> <code>Tensor</code> <p>A 1D tensor with the boundaries of all the bucket.</p> <code>bucket_sizes</code> <code>Tensor</code> <p>The number of elements in each bucket.</p> Source code in <code>bionemo/size_aware_batching/utils.py</code> <pre><code>class Buckets(NamedTuple):\n    \"\"\"A container for storing bucket boundaries and sizes.\n\n    Attributes:\n        bucket_boundaries (torch.Tensor): A 1D tensor with the boundaries of all the bucket.\n        bucket_sizes (torch.Tensor): The number of elements in each bucket.\n    \"\"\"\n\n    bucket_boundaries: torch.Tensor\n    bucket_sizes: torch.Tensor\n</code></pre>"},{"location":"main/references/API_reference/bionemo/size_aware_batching/utils/#bionemo.size_aware_batching.utils.collect_cuda_peak_alloc","title":"<code>collect_cuda_peak_alloc(dataset, work, device, cleanup=None)</code>","text":"<p>Collects CUDA peak memory allocation statistics for a given workflow.</p> <p>This function iterates through the provided dataset, applies the given feature function to each data point, and records the peak CUDA memory allocation during this process. The features extracted from the data points are collected along with their corresponding memory usage statistics.</p> <p>Note that the first few iterations of the workflow might result in smaller memory allocations due to uninitialized data (e.g., internal PyTorch buffers). Therefore, users may want to skip these initial data points when analyzing the results.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Iterable[Data]</code> <p>An iterable containing the input data.</p> required <code>work</code> <code>Callable[[Data], Feature]</code> <p>A function that takes a data point and returns its corresponding feature. This is where the main computation happens and memory allocations are tracked.</p> required <code>device</code> <code>device</code> <p>The target Torch CUDA device.</p> required <code>cleanup</code> <code>Optional[Callable[[], None]]</code> <p>A function that is called after each iteration to perform any necessary cleanup.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[List[Feature], List[int]]</code> <p>A tuple containing the collected features and their corresponding memory usage statistics.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided device is not a CUDA device.</p> <p>Examples: <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.size_aware_batching.utils import collect_cuda_peak_alloc\n\n\n&gt;&gt;&gt; # prepare dataset, model and other components of a workflow\n&gt;&gt;&gt; # for which the user want to collect CUDA peak memory allocation statistics\n&gt;&gt;&gt; dataset, model, optimizer = ...\n&gt;&gt;&gt; # Set the target Torch CUDA device.\n&gt;&gt;&gt; device = torch.device(\"cuda:0\")\n&gt;&gt;&gt; model = model.to(device)\n\n&gt;&gt;&gt; # Define a function that takes an element of the dataset as input and\n&gt;&gt;&gt; # do a training step\n&gt;&gt;&gt; def work(data):\n...     # example body of a training loop\n...     optimizer.zero_grad()\n...     output = model(data.to(device))\n...     loss = compute_loss(output)\n...     loss.backward()\n...     optimizer.step()\n...     # extract the feature for later to be modeled or analyzed\n...     return featurize(data)\n\n&gt;&gt;&gt; # can optionally use a cleanup function to release the references\n&gt;&gt;&gt; # hold during the work(). This cleanup function will be called\n&gt;&gt;&gt; # at the end of each step before garbage collection and memory allocations measurement\n&gt;&gt;&gt; def cleanup():\n...     model.zero_grad(set_to_none=True)\n\n&gt;&gt;&gt; # Collect features (i.e., model outputs) and memory usage statistics for the workflow.\n&gt;&gt;&gt; features, alloc_peaks = collect_cuda_peak_alloc(\n...     dataset=batches,\n...     work=work,\n...     device=device,\n...     cleanup=cleanup,\n... )\n\n\n&gt;&gt;&gt; # use features and alloc_peaks as needed, e.g., fit a model\n&gt;&gt;&gt; # that can use these statistics to predict memory usage\n&gt;&gt;&gt; memory_model = ...\n&gt;&gt;&gt; memory_model.fit(features, alloc_peaks)\n</code></pre></p> Source code in <code>bionemo/size_aware_batching/utils.py</code> <pre><code>def collect_cuda_peak_alloc(\n    dataset: Iterable[Data],\n    work: Callable[[Data], Feature],\n    device: torch.device,\n    cleanup: Optional[Callable[[], None]] = None,\n) -&gt; Tuple[List[Feature], List[int]]:\n    \"\"\"Collects CUDA peak memory allocation statistics for a given workflow.\n\n    This function iterates through the provided dataset, applies the given feature function to each data point,\n    and records the peak CUDA memory allocation during this process. The features extracted from the data points\n    are collected along with their corresponding memory usage statistics.\n\n    Note that the first few iterations of the workflow might result in smaller memory allocations due to uninitialized\n    data (e.g., internal PyTorch buffers). Therefore, users may want to skip these initial data points when analyzing the results.\n\n    Args:\n        dataset: An iterable containing the input data.\n        work: A function that takes a data point and returns its corresponding feature. This is where\n            the main computation happens and memory allocations are tracked.\n        device: The target Torch CUDA device.\n        cleanup: A function that is called after each iteration to perform any necessary cleanup.\n\n    Returns:\n        A tuple containing the collected features and their corresponding memory usage statistics.\n\n    Raises:\n        ValueError: If the provided device is not a CUDA device.\n\n    -------\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from bionemo.size_aware_batching.utils import collect_cuda_peak_alloc\n\n\n    &gt;&gt;&gt; # prepare dataset, model and other components of a workflow\n    &gt;&gt;&gt; # for which the user want to collect CUDA peak memory allocation statistics\n    &gt;&gt;&gt; dataset, model, optimizer = ...\n    &gt;&gt;&gt; # Set the target Torch CUDA device.\n    &gt;&gt;&gt; device = torch.device(\"cuda:0\")\n    &gt;&gt;&gt; model = model.to(device)\n\n    &gt;&gt;&gt; # Define a function that takes an element of the dataset as input and\n    &gt;&gt;&gt; # do a training step\n    &gt;&gt;&gt; def work(data):\n    ...     # example body of a training loop\n    ...     optimizer.zero_grad()\n    ...     output = model(data.to(device))\n    ...     loss = compute_loss(output)\n    ...     loss.backward()\n    ...     optimizer.step()\n    ...     # extract the feature for later to be modeled or analyzed\n    ...     return featurize(data)\n\n    &gt;&gt;&gt; # can optionally use a cleanup function to release the references\n    &gt;&gt;&gt; # hold during the work(). This cleanup function will be called\n    &gt;&gt;&gt; # at the end of each step before garbage collection and memory allocations measurement\n    &gt;&gt;&gt; def cleanup():\n    ...     model.zero_grad(set_to_none=True)\n\n    &gt;&gt;&gt; # Collect features (i.e., model outputs) and memory usage statistics for the workflow.\n    &gt;&gt;&gt; features, alloc_peaks = collect_cuda_peak_alloc(\n    ...     dataset=batches,\n    ...     work=work,\n    ...     device=device,\n    ...     cleanup=cleanup,\n    ... )\n\n\n    &gt;&gt;&gt; # use features and alloc_peaks as needed, e.g., fit a model\n    &gt;&gt;&gt; # that can use these statistics to predict memory usage\n    &gt;&gt;&gt; memory_model = ...\n    &gt;&gt;&gt; memory_model.fit(features, alloc_peaks)\n    ```\n\n\n    \"\"\"\n    if device.type != \"cuda\":\n        raise ValueError(\"This function is intended for CUDA devices only.\")\n\n    features = []\n    alloc_peaks = []\n\n    for data in dataset:\n        try:\n            torch.cuda.reset_peak_memory_stats(device)\n            feature = work(data)\n            alloc_peak = torch.cuda.memory_stats(device)[\"allocated_bytes.all.peak\"]\n            alloc_peaks.append(alloc_peak)\n            features.append(feature)\n        except torch.cuda.OutOfMemoryError:\n            print(\"Encounter CUDA out-of-memory error. Skipping sample\", file=sys.stderr, flush=True)\n            continue\n        finally:\n            # ensures cleanup is done next round even in case of exception\n            del data\n            if \"feature\" in locals():\n                del feature\n            if cleanup is not None:\n                cleanup()\n            gc.collect()\n            torch.cuda.empty_cache()\n            torch.cuda.reset_peak_memory_stats(device)\n    return features, alloc_peaks\n</code></pre>"},{"location":"main/references/API_reference/bionemo/size_aware_batching/utils/#bionemo.size_aware_batching.utils.create_buckets","title":"<code>create_buckets(sizes, max_width, min_bucket_count)</code>","text":"<p>Create buckets for a list of integers with pre-defined maximal width of interval and minimal bucket count.</p> <p>It will return a named tuple containing the bucket boundaries and the actual bucket sizes. e.g. torch.tensor([0, 5, 7]), torch.tensor([3,2]): specifies 2 buckets: one with range 0&lt;= sizes &lt; 5, width=5 and 3 elements and the other one with range 5 &lt;= sizes &lt; 7, width=2 and 2 elements.</p> <p>Parameters:</p> Name Type Description Default <code>sizes</code> <code>Tensor</code> <p>An 1D tensor of integers.</p> required <code>max_width</code> <code>int</code> <p>The maximum width of a bucket, should be a positive integer.</p> required <code>min_bucket_count</code> <code>int</code> <p>The minimum count of a bucket, should be a positive integer. Bucket size may be smaller than min_bucket_count if its width reaches max_width.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided sizes is empty, or not integers.</p> <code>ValueError</code> <p>If max_width is not a positive integer or min_bucket_count is not a positive integer.</p> <p>Returns:</p> Type Description <code>Buckets</code> <p>A namedtuple containing bucket boundaries in ascending order and the number of elements in each bucket.</p> <p>Examples: <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.size_aware_batching.utils import create_buckets\n\n&gt;&gt;&gt; sizes = torch.tensor([1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 22, 22, 22, 22])\n&gt;&gt;&gt; buckets = create_buckets(sizes, max_width=5, min_bucket_count=10)\n&gt;&gt;&gt; # 5 buckets: 1 &lt;= sizes &lt; 6, 6 &lt;= sizes &lt; 11, 11 &lt;= sizes &lt; 16, 16 &lt;= sizes &lt; 21, 21 &lt;= sizes &lt; 23\n&gt;&gt;&gt; print(buckets.bucket_boundaries)\ntensor([ 1,  6, 11, 16, 21, 23])\n\n&gt;&gt;&gt; # each with 12, 0, 0, 0, 4 elements respectively.\n&gt;&gt;&gt; print(buckets.bucket_sizes)\ntensor([12,  0,  0,  0,  4])\n\n&gt;&gt;&gt; sizes = torch.arange(20)\n&gt;&gt;&gt; # min_bucket_count is used to control bucket size\n&gt;&gt;&gt; buckets = create_buckets(sizes, max_width=10, min_bucket_count=5)\n&gt;&gt;&gt; print(buckets.bucket_boundaries)\ntensor([ 0,  5, 10, 15, 20])\n\n&gt;&gt;&gt; print(buckets.bucket_sizes)\ntensor([5, 5, 5, 5])\n</code></pre></p> Source code in <code>bionemo/size_aware_batching/utils.py</code> <pre><code>def create_buckets(sizes: torch.Tensor, max_width: int, min_bucket_count: int) -&gt; Buckets:\n    \"\"\"Create buckets for a list of integers with pre-defined maximal width of interval and minimal bucket count.\n\n    It will return a named tuple containing the bucket boundaries and the actual bucket sizes.\n    e.g. torch.tensor([0, 5, 7]), torch.tensor([3,2]): specifies 2 buckets: one with range 0&lt;= sizes &lt; 5, width=5 and 3 elements\n    and the other one with range 5 &lt;= sizes &lt; 7, width=2 and 2 elements.\n\n\n    Args:\n        sizes: An 1D tensor of integers.\n        max_width: The maximum width of a bucket, should be a positive integer.\n        min_bucket_count: The minimum count of a bucket, should be a positive integer.\n            Bucket size may be smaller than min_bucket_count if its width reaches max_width.\n\n    Raises:\n        ValueError: If the provided sizes is empty, or not integers.\n        ValueError: If max_width is not a positive integer or min_bucket_count is not a positive integer.\n\n    Returns:\n        A namedtuple containing bucket boundaries in ascending order and the number of elements in each bucket.\n\n    ---------\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from bionemo.size_aware_batching.utils import create_buckets\n\n    &gt;&gt;&gt; sizes = torch.tensor([1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 22, 22, 22, 22])\n    &gt;&gt;&gt; buckets = create_buckets(sizes, max_width=5, min_bucket_count=10)\n    &gt;&gt;&gt; # 5 buckets: 1 &lt;= sizes &lt; 6, 6 &lt;= sizes &lt; 11, 11 &lt;= sizes &lt; 16, 16 &lt;= sizes &lt; 21, 21 &lt;= sizes &lt; 23\n    &gt;&gt;&gt; print(buckets.bucket_boundaries)\n    tensor([ 1,  6, 11, 16, 21, 23])\n\n    &gt;&gt;&gt; # each with 12, 0, 0, 0, 4 elements respectively.\n    &gt;&gt;&gt; print(buckets.bucket_sizes)\n    tensor([12,  0,  0,  0,  4])\n\n    &gt;&gt;&gt; sizes = torch.arange(20)\n    &gt;&gt;&gt; # min_bucket_count is used to control bucket size\n    &gt;&gt;&gt; buckets = create_buckets(sizes, max_width=10, min_bucket_count=5)\n    &gt;&gt;&gt; print(buckets.bucket_boundaries)\n    tensor([ 0,  5, 10, 15, 20])\n\n    &gt;&gt;&gt; print(buckets.bucket_sizes)\n    tensor([5, 5, 5, 5])\n    ```\n\n    \"\"\"\n    if not torch.is_tensor(sizes):\n        raise TypeError(f\"sizes should be a torch tensor, but got sizes={sizes}\")\n\n    if sizes.ndim != 1:\n        raise ValueError(f\"sizes should be a 1D tensor, but got sizes with shape {sizes.shape}\")\n\n    if sizes.dtype not in TorchIntegerDataTypes:\n        raise ValueError(f\"sizes should contain only integers, but got sizes.dtype={sizes.dtype}\")\n\n    if len(sizes) == 0:\n        raise ValueError(\"sizes should not be empty\")\n\n    if not isinstance(max_width, int) or max_width &lt;= 0:\n        raise ValueError(f\"max_width should be a positive integer but got max_width={max_width}\")\n\n    if not isinstance(min_bucket_count, int) or min_bucket_count &lt;= 0:\n        raise ValueError(f\"min_bucket_count should be a positive integer but got min_bucket_count={min_bucket_count}\")\n\n    unique_values, counts = torch.unique(sizes, return_counts=True, sorted=True)\n\n    bucket_boundaries = [unique_values[0]]\n    bucket_sizes = []\n    start = 0\n    end = 0\n    upper_bound = unique_values[0] + 1\n    bucket_count = 0\n\n    while start &lt; len(unique_values):\n        while (\n            end &lt; len(unique_values)\n            and bucket_count &lt; min_bucket_count\n            and unique_values[end] - bucket_boundaries[-1] &lt; max_width\n        ):\n            bucket_count += counts[end]\n            end += 1\n\n        bucket_sizes.append(sum(counts[start:end]))\n        if end == len(unique_values):\n            upper_bound = unique_values[-1] + 1\n        else:\n            upper_bound = unique_values[end]\n\n        # Adjust the end of the range to ensure that no width exceeds 'max_width'\n        n_empty_buckets = (upper_bound - bucket_boundaries[-1]) // max_width\n        if n_empty_buckets &gt; 0:\n            bucket_boundaries.extend(\n                list(\n                    range(\n                        bucket_boundaries[-1] + max_width,\n                        bucket_boundaries[-1] + max_width * (n_empty_buckets + 1),\n                        max_width,\n                    )\n                )\n            )\n            bucket_sizes.extend([0] * (n_empty_buckets - 1))\n        else:\n            bucket_boundaries.append(upper_bound)\n\n        start = end\n        end = start + 1\n        # index start may be out of bounds\n        bucket_count = counts[start:end].sum()\n\n    bucket_boundaries = torch.tensor(bucket_boundaries)\n    bucket_sizes = torch.tensor(bucket_sizes)\n\n    return Buckets(bucket_boundaries, bucket_sizes)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/assert_optimizer_grads_match/","title":"Assert optimizer grads match","text":""},{"location":"main/references/API_reference/bionemo/testing/assert_optimizer_grads_match/#bionemo.testing.assert_optimizer_grads_match.assert_grads_close","title":"<code>assert_grads_close(left, right)</code>","text":"<p>Assert that two gradient tensors are close using theorem 5.3 of https://www.arxiv.org/pdf/2506.09280.</p> Source code in <code>bionemo/testing/assert_optimizer_grads_match.py</code> <pre><code>def assert_grads_close(left: torch.Tensor, right: torch.Tensor):\n    \"\"\"Assert that two gradient tensors are close using theorem 5.3 of https://www.arxiv.org/pdf/2506.09280.\"\"\"\n    # Implement theorem 5.3 of https://www.arxiv.org/pdf/2506.09280\n\n    # This is the real test:\n    rel, bnd, ok = check_gradient(\n        left, right, l=0, dtype=torch.bfloat16\n    )  # hard code to layer 0 since that's the most permissive\n\n    # If the real test above fails, run an assert close for the useful diagnostics and raise either way.\n    if not ok:\n        rel_shuff, _, ok_shuff = check_gradient(left, torch.roll(right, shifts=-1, dims=-1), l=0, dtype=torch.bfloat16)\n\n        try:\n            torch.testing.assert_close(left, right)\n            msg = (\n                \"AssertionError on relative norm magnitude \"\n                f\"(rel={rel}, bnd={bnd}, ok={ok}, rel_shuff={rel_shuff}, ok_shuff={ok_shuff}) \"\n                \"but torch.testing.assert_close(left, right) passes. \\n\"\n                f\"Left: {left.shape}/{left.dtype} {left}\\n\"\n                f\"Right: {right.shape}/{right.dtype} {right}\"\n            )\n        except AssertionError as e:\n            msg = (\n                \"AssertionError on relative norm magnitude \"\n                f\"(rel={rel}, bnd={bnd}, ok={ok}, rel_shuff={rel_shuff}, ok_shuff={ok_shuff}): {e}\\n\"\n                f\"Left: {left.shape}/{left.dtype} {left}\\n\"\n                f\"Right: {right.shape}/{right.dtype} {right}\"\n            )\n        raise AssertionError(msg)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/assert_optimizer_grads_match/#bionemo.testing.assert_optimizer_grads_match.assert_optimizer_states_match","title":"<code>assert_optimizer_states_match(checkpoint_dirs)</code>","text":"<p>Compare optimizer state across provided torch_dist checkpoints.</p> <ul> <li>Keys: ensure the set of optimizer tensor keys match across checkpoints</li> <li>Values: ensure corresponding tensors are equal (allclose)</li> <li>Structure (non-tensor common state): ensure common optimizer structures match</li> </ul> Source code in <code>bionemo/testing/assert_optimizer_grads_match.py</code> <pre><code>def assert_optimizer_states_match(checkpoint_dirs):\n    \"\"\"Compare optimizer state across provided torch_dist checkpoints.\n\n    - Keys: ensure the set of optimizer tensor keys match across checkpoints\n    - Values: ensure corresponding tensors are equal (allclose)\n    - Structure (non-tensor common state): ensure common optimizer structures match\n    \"\"\"\n    assert len(checkpoint_dirs) &gt; 1, \"This test requires 2 or more checkpoints &lt;dir1&gt; [&lt;dir2&gt; ...].\"\n\n    base_dir = checkpoint_dirs[0]\n\n    # Compare optimizer tensors\n    base_plain = load_dist_checkpoint_pt(base_dir)\n    base_empty = load_dist_checkpoint_pt(base_dir, return_full_empty=True, device=\"meta\")\n    base_opt_tensors = _filter_optimizer_tensors(base_plain)\n    assert base_opt_tensors, f\"No optimizer tensors found in checkpoint: {base_dir}\"\n    assertions = []\n    for other_dir in checkpoint_dirs[1:]:\n        try:\n            other_plain = load_dist_checkpoint_pt(other_dir)\n            other_empty = load_dist_checkpoint_pt(other_dir, return_full_empty=True, device=\"meta\")\n            other_opt_tensors = _filter_optimizer_tensors(other_plain)\n            assert other_opt_tensors, f\"No optimizer tensors found in checkpoint: {other_dir}\"\n            _assert_optimizer_tensors_equal(base_opt_tensors, other_opt_tensors, base_empty, other_empty)\n            print(f\"Optimizer tensors match for {base_dir} and {other_dir}\")\n            del other_plain\n            del other_opt_tensors\n        except AssertionError as e:\n            msg = f\"AssertionError comparing {base_dir} to {other_dir}:\\n{e}\"\n            print(f\"Optimizer tensors mismatch for {base_dir} and {other_dir}:\\n{msg}\")\n            assertions.append(AssertionError(msg))\n    assert not assertions, f\"AssertionErrors comparing {checkpoint_dirs}:\\n{assertions}\"\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/assert_optimizer_grads_match/#bionemo.testing.assert_optimizer_grads_match.check_gradient","title":"<code>check_gradient(g_hat, g_ref, l, *, L=32, C=1.03, dtype=None, k=4.0)</code>","text":"<p>Compute (rel_error, bound, ok) for layer l.</p> <ul> <li>If dtype is None, infer from g_ref (or g_hat if needed).</li> </ul>"},{"location":"main/references/API_reference/bionemo/testing/assert_optimizer_grads_match/#bionemo.testing.assert_optimizer_grads_match.check_gradient--see-httpswwwarxivorgpdf250609280-theorem-53","title":"See https://www.arxiv.org/pdf/2506.09280 theorem 5.3","text":"Source code in <code>bionemo/testing/assert_optimizer_grads_match.py</code> <pre><code>def check_gradient(\n    g_hat: TensorLike,\n    g_ref: TensorLike,\n    l: int,\n    *,\n    L: int = 32,\n    C: float = 1.03,\n    dtype: Optional[torch.dtype] = None,\n    k: float = 4.0,\n) -&gt; Tuple[float, float, bool]:\n    \"\"\"Compute (rel_error, bound, ok) for layer l.\n\n    - If dtype is None, infer from g_ref (or g_hat if needed).\n    # See https://www.arxiv.org/pdf/2506.09280 theorem 5.3\n    \"\"\"\n    # Infer dtype if not provided\n    if dtype is None:\n        gr_list = list(_as_iter(g_ref))\n        if gr_list:\n            dtype = gr_list[0].dtype\n        else:\n            dtype = torch.bfloat16  # fallback\n    rel = relative_grad_diff(g_hat, g_ref)\n    bnd = expected_rel_bound(l, L=L, C=C, dtype=dtype, k=k)\n    return rel, bnd, (rel &lt;= bnd)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/assert_optimizer_grads_match/#bionemo.testing.assert_optimizer_grads_match.expected_rel_bound","title":"<code>expected_rel_bound(l, *, L=32, C=1.03, dtype=torch.bfloat16, k=4.0)</code>","text":"<p>Bound ~ k * (C ** (L + 1 - l)) * eps_mch, with 1-based layer index l.</p> <ul> <li>L is hard-coded default to 32 per your request.</li> <li>C is 'close to 1'; 1.01-1.05 are reasonable defaults.</li> <li>k absorbs the hidden constant in big-O; 2-8 are common choices.</li> <li>dtype controls eps_mch; for FP8 use BF16 epsilon (see https://www.arxiv.org/pdf/2506.09280 theorem 5.3).</li> </ul> Source code in <code>bionemo/testing/assert_optimizer_grads_match.py</code> <pre><code>def expected_rel_bound(\n    l: int,\n    *,\n    L: int = 32,\n    C: float = 1.03,\n    dtype: Optional[torch.dtype] = torch.bfloat16,\n    k: float = 4.0,\n) -&gt; float:\n    \"\"\"Bound ~ k * (C ** (L + 1 - l)) * eps_mch, with 1-based layer index l.\n\n    - L is hard-coded default to 32 per your request.\n    - C is 'close to 1'; 1.01-1.05 are reasonable defaults.\n    - k absorbs the hidden constant in big-O; 2-8 are common choices.\n    - dtype controls eps_mch; for FP8 use BF16 epsilon (see https://www.arxiv.org/pdf/2506.09280 theorem 5.3).\n    \"\"\"\n    eps_mch = machine_epsilon_for_dtype(dtype or torch.bfloat16)\n    depth = L + 1 - l  # 1-based depth from the top (as in the theorem)\n    depth = max(depth, 0)\n    return float(k * (C**depth) * eps_mch)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/assert_optimizer_grads_match/#bionemo.testing.assert_optimizer_grads_match.load_dist_checkpoint_pt","title":"<code>load_dist_checkpoint_pt(ckpt_dir, metadata_ckpt_dir=None, pattern='optimizer', device='cpu', return_full_empty=False)</code>","text":"<p>Return {full_key: tensor} for every tensor whose key matches pattern.</p> Source code in <code>bionemo/testing/assert_optimizer_grads_match.py</code> <pre><code>def load_dist_checkpoint_pt(\n    ckpt_dir,\n    metadata_ckpt_dir=None,\n    pattern=r\"optimizer\",\n    device=\"cpu\",\n    return_full_empty: bool = False,\n):\n    \"\"\"Return {full_key: tensor} for every tensor whose key matches *pattern*.\"\"\"\n    meta_ckpt_dir = Path(metadata_ckpt_dir or ckpt_dir)\n    meta_reader = FileSystemReader(str(meta_ckpt_dir))\n\n    # --- fast metadata pass (no tensor data yet) -----------------------------\n    meta = meta_reader.read_metadata()  # tiny JSON read\n    tmeta = meta.state_dict_metadata  # key \u279c TensorMetadata\n    if return_full_empty:\n        wanted = [k for k in tmeta if hasattr(tmeta[k], \"size\")]\n    else:\n        wanted = [k for k in tmeta if re.search(pattern, k) and hasattr(tmeta[k], \"size\")]\n    if not wanted:\n        raise ValueError(f\"No keys matching /{pattern}/ in {ckpt_dir}\")\n\n    # --- build \"empty\" placeholders -----------------------------------------\n    placeholders = {\n        k: torch.empty(tuple(tmeta[k].size), dtype=tmeta[k].properties.dtype, device=device) for k in wanted\n    }\n    if return_full_empty:\n        return placeholders\n    # --- stream just those tensors (no process-group needed) -----------------\n    data_reader = FileSystemReader(str(ckpt_dir))\n\n    load(\n        state_dict=placeholders,\n        storage_reader=data_reader,\n        no_dist=True,  # switches off all collectives\n    )\n    return placeholders  # dict[str, Tensor]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/assert_optimizer_grads_match/#bionemo.testing.assert_optimizer_grads_match.machine_epsilon_for_dtype","title":"<code>machine_epsilon_for_dtype(dtype)</code>","text":"<p>Return machine epsilon for dtype. For FP8, use BF16 epsilon per paper.</p> Source code in <code>bionemo/testing/assert_optimizer_grads_match.py</code> <pre><code>def machine_epsilon_for_dtype(dtype: torch.dtype) -&gt; float:\n    \"\"\"Return machine epsilon for dtype. For FP8, use BF16 epsilon per paper.\"\"\"\n    # Standard types\n    if dtype in (torch.float32, torch.float16, torch.bfloat16):\n        return float(torch.finfo(dtype).eps)\n    # FP8 recipes: accum/store typically BF16/FP32; use BF16 epsilon\n    if hasattr(torch, \"float8_e4m3fn\") and dtype in (\n        torch.float8_e4m3fn,\n        getattr(torch, \"float8_e5m2fn\", None),\n    ):\n        return float(torch.finfo(torch.bfloat16).eps)\n    # Fallback\n    return float(torch.finfo(torch.float32).eps)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/assert_optimizer_grads_match/#bionemo.testing.assert_optimizer_grads_match.main","title":"<code>main()</code>","text":"<p>Main entry point for comparing optimizer states across multiple checkpoints.</p> Source code in <code>bionemo/testing/assert_optimizer_grads_match.py</code> <pre><code>def main():\n    \"\"\"Main entry point for comparing optimizer states across multiple checkpoints.\"\"\"\n    parser = ArgumentParser(\n        description=\"Given checkpoints saved with adam b1,b2=0 trained for one step, \"\n        \"we can check that the gradients match under different training configurations. \"\n        \"Currently this test script has some hard-coded assumptions for GPT style models, \"\n        \"namely which layers are RowParallel and require different unsharding logic.\"\n    )\n    parser.add_argument(\"checkpoints\", nargs=\"+\", type=Path, help=\"Path to the checkpoints to compare\")\n    args = parser.parse_args()\n    assert_optimizer_states_match(args.checkpoints)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/assert_optimizer_grads_match/#bionemo.testing.assert_optimizer_grads_match.relative_grad_diff","title":"<code>relative_grad_diff(g_hat, g_ref, eps_den=1e-30)</code>","text":"<p>Relative difference ||g_hat - g_ref||_F / ||g_ref||_F.</p> <p>Accepts a single tensor or an iterable of shards for each argument.</p> Source code in <code>bionemo/testing/assert_optimizer_grads_match.py</code> <pre><code>def relative_grad_diff(g_hat: TensorLike, g_ref: TensorLike, eps_den: float = 1e-30) -&gt; float:\n    \"\"\"Relative difference ||g_hat - g_ref||_F / ||g_ref||_F.\n\n    Accepts a single tensor or an iterable of shards for each argument.\n    \"\"\"\n    # Convert to lists to avoid iterator consumption issues\n    gh_list = list(_as_iter(g_hat))\n    gr_list = list(_as_iter(g_ref))\n\n    if len(gh_list) != len(gr_list):\n        raise ValueError(f\"Shard count mismatch: {len(gh_list)} vs {len(gr_list)}\")\n\n    if not gh_list:\n        return 0.0\n\n    num_sq = torch.tensor(0.0, device=gh_list[0].device)\n    for a, b in zip(gh_list, gr_list):\n        num_sq = num_sq + (a.float() - b.float()).pow(2).sum()\n    num = torch.sqrt(num_sq)\n    den = _fro_norm(g_ref)\n    return float(num / (den + eps_den))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/assert_optimizer_grads_match/#bionemo.testing.assert_optimizer_grads_match.unshard_row_parallel_state","title":"<code>unshard_row_parallel_state(saved_state, out_features, in_features, tp)</code>","text":"<p>Unshard row-parallel state tensor from sharded format to full format.</p> <p>saved_state: [..., tp, out_features * (in_features // tp)]</p> Source code in <code>bionemo/testing/assert_optimizer_grads_match.py</code> <pre><code>def unshard_row_parallel_state(saved_state, out_features, in_features, tp):\n    \"\"\"Unshard row-parallel state tensor from sharded format to full format.\n\n    saved_state: [..., tp, out_features * (in_features // tp)]\n    \"\"\"\n    prefix = saved_state.shape[:-2]\n    per = in_features // tp\n    x = saved_state.view(*prefix, tp, out_features, per)  # [..., tp, O, I_shard]\n    x = x.permute(*range(len(prefix)), -2, -3, -1)  # [..., O, tp, I_shard]\n    x = x.reshape(*prefix, out_features, in_features)  # [..., O, I]\n    return x\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/callbacks/","title":"Callbacks","text":""},{"location":"main/references/API_reference/bionemo/testing/lightning/","title":"Lightning","text":""},{"location":"main/references/API_reference/bionemo/testing/lightning/#bionemo.testing.lightning.extract_global_steps_from_log","title":"<code>extract_global_steps_from_log(log_string)</code>","text":"<p>Extract global steps from a Pytorch lightening log string.</p> Source code in <code>bionemo/testing/lightning.py</code> <pre><code>def extract_global_steps_from_log(log_string: str) -&gt; List[int]:\n    \"\"\"Extract global steps from a Pytorch lightening log string.\"\"\"\n    pattern = r\"\\| global_step: (\\d+) \\|\"\n    matches = re.findall(pattern, log_string)\n    return [int(step) for step in matches]\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/lightning/#bionemo.testing.lightning.get_random_microbatch","title":"<code>get_random_microbatch(microbatch_size, max_sequence_length, vocab_size, seed, mask_index=MLM_LOSS_IGNORE_INDEX)</code>","text":"<p>Generate random microbatches for testing.</p> <p>Note that this follows the convention that token_logits are s,b, while other fields are b,s.</p> Source code in <code>bionemo/testing/lightning.py</code> <pre><code>def get_random_microbatch(\n    microbatch_size: int,\n    max_sequence_length: int,\n    vocab_size: int,\n    seed: int,\n    mask_index: int = MLM_LOSS_IGNORE_INDEX,\n) -&gt; Dict[str, Dict[str, torch.Tensor]]:\n    \"\"\"Generate random microbatches for testing.\n\n    Note that this follows the convention that token_logits are s,b, while other fields are b,s.\n    \"\"\"\n    generator = torch.Generator(device=torch.cuda.current_device()).manual_seed(seed)\n    labels = torch.randint(\n        low=0,\n        high=vocab_size,\n        size=(microbatch_size, max_sequence_length),\n        generator=generator,\n        device=torch.cuda.current_device(),\n    )  # [b s]\n    loss_mask = torch.randint(\n        low=1,\n        high=1 + 1,\n        size=(microbatch_size, max_sequence_length),\n        dtype=torch.long,\n        device=torch.cuda.current_device(),\n        generator=generator,\n    )  # [b s]\n    token_logits = torch.rand(\n        max_sequence_length, microbatch_size, vocab_size, device=torch.cuda.current_device(), generator=generator\n    )  # [s b v]\n    labels[loss_mask == 0] = mask_index  # propagate masking to labels\n    microbatch_output = {\n        \"batch\": {\"labels\": labels, \"loss_mask\": loss_mask},\n        \"forward_out\": {\"token_logits\": token_logits},\n    }\n    return microbatch_output\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/megatron_dataset_compatibility/","title":"Megatron dataset compatibility","text":""},{"location":"main/references/API_reference/bionemo/testing/megatron_dataset_compatibility/#bionemo.testing.megatron_dataset_compatibility.DatasetDistributedNondeterministic","title":"<code>DatasetDistributedNondeterministic</code>","text":"<p>               Bases: <code>AssertionError</code></p> <p>Datasets are not locally deterministic.</p> Source code in <code>bionemo/testing/megatron_dataset_compatibility.py</code> <pre><code>class DatasetDistributedNondeterministic(AssertionError):\n    \"\"\"Datasets are not locally deterministic.\"\"\"\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/megatron_dataset_compatibility/#bionemo.testing.megatron_dataset_compatibility.DatasetLocallyNondeterministic","title":"<code>DatasetLocallyNondeterministic</code>","text":"<p>               Bases: <code>AssertionError</code></p> <p>Datasets are not locally deterministic.</p> Source code in <code>bionemo/testing/megatron_dataset_compatibility.py</code> <pre><code>class DatasetLocallyNondeterministic(AssertionError):\n    \"\"\"Datasets are not locally deterministic.\"\"\"\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/megatron_dataset_compatibility/#bionemo.testing.megatron_dataset_compatibility.assert_dataset_compatible_with_megatron","title":"<code>assert_dataset_compatible_with_megatron(dataset, index=0, assert_elements_equal=assert_dict_tensors_approx_equal)</code>","text":"<p>Make sure that a dataset passes some basic sanity checks for megatron determinism constraints.</p> Constraints tested <ul> <li>dataset[i] returns the same element regardless of device</li> <li>dataset[i] doesn't make calls to known problematic randomization procedures (currently <code>torch.manual_seed</code>).</li> </ul> <p>As more constraints are discovered, they should be added to this test.</p> Source code in <code>bionemo/testing/megatron_dataset_compatibility.py</code> <pre><code>def assert_dataset_compatible_with_megatron(\n    dataset: torch.utils.data.Dataset[TensorCollectionOrTensor],\n    index: Index = 0,\n    assert_elements_equal: Callable[\n        [TensorCollectionOrTensor, TensorCollectionOrTensor], None\n    ] = assert_dict_tensors_approx_equal,\n):\n    \"\"\"Make sure that a dataset passes some basic sanity checks for megatron determinism constraints.\n\n    Constraints tested:\n        * dataset[i] returns the same element regardless of device\n        * dataset[i] doesn't make calls to known problematic randomization procedures (currently `torch.manual_seed`).\n\n    As more constraints are discovered, they should be added to this test.\n    \"\"\"\n    # 1. Make sure the dataset is deterministic when you ask for the same elements.\n    n_elements = len(dataset)  # type: ignore\n    assert n_elements &gt; 0, \"Need one element or more to test\"\n    try:\n        assert_elements_equal(dataset[index], dataset[index])\n    except AssertionError as e_0:\n        raise DatasetLocallyNondeterministic(e_0)\n    with (\n        patch(\"torch.manual_seed\") as mock_manual_seed,\n        patch(\"torch.cuda.manual_seed\") as mock_cuda_manual_seed,\n        patch(\"torch.cuda.manual_seed_all\") as mock_cuda_manual_seed_all,\n    ):\n        _ = dataset[index]\n    if mock_manual_seed.call_count &gt; 0 or mock_cuda_manual_seed.call_count &gt; 0 or mock_cuda_manual_seed_all.call_count:\n        raise DatasetDistributedNondeterministic(\n            \"You cannot safely use torch.manual_seed in a cluster with model parallelism. Use torch.Generator directly.\"\n            \" See https://github.com/NVIDIA/Megatron-LM/blob/dddecd19/megatron/core/tensor_parallel/random.py#L198-L199\"\n        )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/megatron_dataset_compatibility/#bionemo.testing.megatron_dataset_compatibility.assert_dataset_elements_not_equal","title":"<code>assert_dataset_elements_not_equal(dataset, index_a=0, index_b=1, assert_elements_equal=assert_dict_tensors_approx_equal)</code>","text":"<p>Test the case where two indices return different elements on datasets that employ randomness, like masking.</p> <p>NOTE: if you have a dataset without any kinds of randomness, just use the <code>assert_dataset_compatible_with_megatron</code> test and skip this one. This test is for the case when you want to test that a dataset that applies a random transform to your elements as a function of index actually does so with two different indices that map to the same underlying object. This test also runs <code>assert_dataset_compatible_with_megatron</code> behind the scenes so if you do this you do not need to also do the other.</p> <p>With epoch upsampling approaches, some underlying index, say index=0, will be called multiple times by some wrapping dataset object. For example if you have a dataset of length 1, and you wrap it in an up-sampler that maps it to length 2 by mapping index 0 to 0 and 1 to 0, then in that wrapper we apply randomness to the result and we expect different masks to be used for each call, even though the underlying object is the same. Again this test only applies to a dataset that employs randomness. Another approach some of our datasets take is to use a special index that captures both the underlying index, and the epoch index. This tuple of indices is used internally to seed the mask. If that kind of dataset is used, then index_a could be (epoch=0, idx=0) and index_b could be (epoch=1, idx=0), for example. We expect those to return different random features.</p> <p>The idea for using this test effectively is to identify cases where you have two indices that return the same underlying object, but where you expect different randomization to be applied to each by the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset[TensorCollectionOrTensor]</code> <p>dataset object with randomness (eg masking) to test.</p> required <code>index_a</code> <code>Index</code> <p>index for some element. Defaults to 0.</p> <code>0</code> <code>index_b</code> <code>Index</code> <p>index for a different element. Defaults to 1.</p> <code>1</code> <code>assert_elements_equal</code> <code>Callable[[TensorCollectionOrTensor, TensorCollectionOrTensor], None]</code> <p>Function to compare two returned batch elements. Defaults to <code>assert_dict_tensors_approx_equal</code> which works for both tensors and dictionaries of tensors.</p> <code>assert_dict_tensors_approx_equal</code> Source code in <code>bionemo/testing/megatron_dataset_compatibility.py</code> <pre><code>def assert_dataset_elements_not_equal(\n    dataset: torch.utils.data.Dataset[TensorCollectionOrTensor],\n    index_a: Index = 0,\n    index_b: Index = 1,\n    assert_elements_equal: Callable[\n        [TensorCollectionOrTensor, TensorCollectionOrTensor], None\n    ] = assert_dict_tensors_approx_equal,\n):\n    \"\"\"Test the case where two indices return different elements on datasets that employ randomness, like masking.\n\n    NOTE: if you have a dataset without any kinds of randomness, just use the `assert_dataset_compatible_with_megatron`\n    test and skip this one. This test is for the case when you want to test that a dataset that applies a random\n    transform to your elements as a function of index actually does so with two different indices that map to the same\n    underlying object. This test also runs `assert_dataset_compatible_with_megatron` behind the scenes so if you\n    do this you do not need to also do the other.\n\n    With epoch upsampling approaches, some underlying index, say index=0, will be called multiple times by some wrapping\n    dataset object. For example if you have a dataset of length 1, and you wrap it in an up-sampler that maps it to\n    length 2 by mapping index 0 to 0 and 1 to 0, then in that wrapper we apply randomness to the result and we expect\n    different masks to be used for each call, even though the underlying object is the same. Again this test only\n    applies to a dataset that employs randomness. Another approach some of our datasets take is to use a special index\n    that captures both the underlying index, and the epoch index. This tuple of indices is used internally to seed the\n    mask. If that kind of dataset is used, then index_a could be (epoch=0, idx=0) and index_b could be (epoch=1, idx=0),\n    for example. We expect those to return different random features.\n\n    The idea for using this test effectively is to identify cases where you have two indices that return the same\n    underlying object, but where you expect different randomization to be applied to each by the dataset.\n\n    Args:\n        dataset: dataset object with randomness (eg masking) to test.\n        index_a: index for some element. Defaults to 0.\n        index_b: index for a different element. Defaults to 1.\n        assert_elements_equal: Function to compare two returned batch elements. Defaults to\n            `assert_dict_tensors_approx_equal` which works for both tensors and dictionaries of tensors.\n    \"\"\"\n    # 0, first sanity check for determinism/compatibility on idx0 and idx1\n    assert_dataset_compatible_with_megatron(dataset, index=index_a, assert_elements_equal=assert_elements_equal)\n    assert_dataset_compatible_with_megatron(dataset, index=index_b, assert_elements_equal=assert_elements_equal)\n    # 1, now check that index_a != index_b\n    with pytest.raises(AssertionError):\n        assert_elements_equal(dataset[index_a], dataset[index_b])\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/megatron_dataset_compatibility/#bionemo.testing.megatron_dataset_compatibility.assert_dict_tensors_approx_equal","title":"<code>assert_dict_tensors_approx_equal(actual, expected)</code>","text":"<p>Assert that two tensors are equal.</p> Source code in <code>bionemo/testing/megatron_dataset_compatibility.py</code> <pre><code>def assert_dict_tensors_approx_equal(actual: TensorCollectionOrTensor, expected: TensorCollectionOrTensor) -&gt; None:\n    \"\"\"Assert that two tensors are equal.\"\"\"\n    if isinstance(actual, dict) and isinstance(expected, dict):\n        a_keys, b_keys = actual.keys(), expected.keys()\n        assert a_keys == b_keys\n        for key in a_keys:\n            torch.testing.assert_close(actual=actual[key], expected=expected[key])\n    else:\n        torch.testing.assert_close(actual=actual, expected=expected)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/megatron_parallel_state_utils/","title":"Megatron parallel state utils","text":"<p>This package contains utilities for managing the state of distributed model parallelism in Megatron and Apex.</p> <p>In general you should just use the context manager <code>distributed_model_parallel_state</code> to manage the state of your test. This context manager will handle the setup and teardown of the distributed model parallel state for you.</p> <p>Example usage: <pre><code>from bionemo.testing import megatron_parallel_state_utils\n\ndef my_test():\n    with megatron_parallel_state_utils.distributed_model_parallel_state():\n        # your test code that requires megatron/apex parallel state to be set up here\n</code></pre></p>"},{"location":"main/references/API_reference/bionemo/testing/megatron_parallel_state_utils/#bionemo.testing.megatron_parallel_state_utils.clean_parallel_state_context","title":"<code>clean_parallel_state_context()</code>","text":"<p>Puts you into a clean parallel state, and again tears it down at the end.</p> Source code in <code>bionemo/testing/megatron_parallel_state_utils.py</code> <pre><code>@contextmanager\ndef clean_parallel_state_context():\n    \"\"\"Puts you into a clean parallel state, and again tears it down at the end.\"\"\"\n    try:\n        clean_up_distributed_and_parallel_states()\n        yield\n    finally:\n        clean_up_distributed_and_parallel_states()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/megatron_parallel_state_utils/#bionemo.testing.megatron_parallel_state_utils.clean_up_distributed_and_parallel_states","title":"<code>clean_up_distributed_and_parallel_states(verify_distributed_state=False)</code>","text":"<p>Clean up parallel states, torch.distributed and torch cuda cache.</p> Source code in <code>bionemo/testing/megatron_parallel_state_utils.py</code> <pre><code>def clean_up_distributed_and_parallel_states(verify_distributed_state=False):\n    \"\"\"Clean up parallel states, torch.distributed and torch cuda cache.\"\"\"\n    _reset_microbatch_calculator()\n    # Destroy Megatron distributed/parallel state environment.\n    parallel_state.destroy_model_parallel()\n    # Destroy the torch default / world process group.\n    if torch.distributed.is_initialized():\n        torch.distributed.destroy_process_group()\n    # Clear torch.compile/dynamo cache\n    try:\n        if hasattr(torch, \"_dynamo\"):\n            torch._dynamo.reset()\n        if hasattr(torch, \"compiler\"):\n            torch.compiler.reset()\n    except Exception as e:\n        print(f\"Failed to reset torch compile: {e}\")\n    # Free unused CPU memory.\n    gc.collect()\n    # Free reserved / cached GPU memory allocated by Torch / CUDA.\n    torch.cuda.empty_cache()\n    if verify_distributed_state:\n        # Utilize to debug OOM or orphaned processes in GPU.\n        allocated_vram = torch.cuda.memory_allocated() / 1024**3\n        reserved_vram = torch.cuda.memory_reserved() / 1024**3\n        print(\n            \"\\n--------------------------------\\n\"\n            f\"Memory Profile for Device: {torch.cuda.current_device()}\\n\"\n            f\"Allocated: {allocated_vram} GB\\n\"\n            f\"Reserved: {reserved_vram} GB\\n\"\n            f\"GPU Processes:\\n{torch.cuda.list_gpu_processes()}\\n\"\n            \"--------------------------------\\n\"\n        )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/megatron_parallel_state_utils/#bionemo.testing.megatron_parallel_state_utils.distributed_model_parallel_state","title":"<code>distributed_model_parallel_state(seed=42, rank=0, world_size=1, backend='nccl', **initialize_model_parallel_kwargs)</code>","text":"<p>Context manager for torch distributed and parallel state testing.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>random seed to be passed into tensor_parallel.random (https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/tensor_parallel/random.py). default to 42.</p> <code>42</code> <code>rank</code> <code>int</code> <p>global rank of the current cuda device. default to 0.</p> <code>0</code> <code>world_size</code> <code>int</code> <p>world size or number of devices. default to 1.</p> <code>1</code> <code>backend</code> <code>str</code> <p>backend to torch.distributed.init_process_group. default to 'nccl'.</p> <code>'nccl'</code> <code>**initialize_model_parallel_kwargs</code> <p>kwargs to be passed into initialize_model_parallel (https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/parallel_state.py).</p> <code>{}</code> Source code in <code>bionemo/testing/megatron_parallel_state_utils.py</code> <pre><code>@contextmanager\ndef distributed_model_parallel_state(\n    seed: int = 42,\n    rank: int = 0,\n    world_size: int = 1,\n    backend: str = \"nccl\",\n    **initialize_model_parallel_kwargs,\n):\n    \"\"\"Context manager for torch distributed and parallel state testing.\n\n    Args:\n        seed (int): random seed to be passed into tensor_parallel.random (https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/tensor_parallel/random.py). default to 42.\n        rank (int): global rank of the current cuda device. default to 0.\n        world_size (int): world size or number of devices. default to 1.\n        backend (str): backend to torch.distributed.init_process_group. default to 'nccl'.\n        **initialize_model_parallel_kwargs: kwargs to be passed into initialize_model_parallel (https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/parallel_state.py).\n    \"\"\"\n    with MonkeyPatch.context() as context:\n        initial_states = None\n        try:\n            clean_up_distributed_and_parallel_states()\n\n            # distributed and parallel state set up\n            if not os.environ.get(\"MASTER_ADDR\", None):\n                context.setenv(\"MASTER_ADDR\", DEFAULT_MASTER_ADDR)\n            if not os.environ.get(\"MASTER_PORT\", None):\n                network_address, free_network_port = find_free_network_port(address=DEFAULT_MASTER_ADDR)\n                context.setenv(\n                    \"MASTER_PORT\", free_network_port if free_network_port is not None else DEFAULT_MASTER_PORT\n                )\n            if not os.environ.get(\"NCCL_TIMEOUT\", None):\n                context.setenv(\"NCCL_TIMEOUT\", DEFAULT_NCCL_TIMEOUT)\n            context.setenv(\"RANK\", str(rank))\n\n            torch.distributed.init_process_group(backend=backend, world_size=world_size)\n            parallel_state.initialize_model_parallel(**initialize_model_parallel_kwargs)\n\n            # tensor parallel random seed set up\n            # do not call torch.cuda.manual_seed after so!\n            if tp_random.get_cuda_rng_tracker().is_initialized():\n                initial_states = tp_random.get_cuda_rng_tracker().get_states()\n            if seed is not None:\n                tp_random.model_parallel_cuda_manual_seed(seed)\n\n            yield\n        finally:\n            # restore/unset tensor parallel random seed\n            if initial_states is not None:\n                tp_random.get_cuda_rng_tracker().set_states(initial_states)\n            else:\n                # Reset to the unset state\n                tp_random.get_cuda_rng_tracker().reset()\n\n            clean_up_distributed_and_parallel_states()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/megatron_parallel_state_utils/#bionemo.testing.megatron_parallel_state_utils.find_free_network_port","title":"<code>find_free_network_port(address='localhost')</code>","text":"<p>Finds a free port for the specified address. Defaults to localhost.</p> Source code in <code>bionemo/testing/megatron_parallel_state_utils.py</code> <pre><code>def find_free_network_port(address: str = \"localhost\") -&gt; int:\n    \"\"\"Finds a free port for the specified address. Defaults to localhost.\"\"\"\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.bind((address, 0))\n    addr_port = s.getsockname()\n    s.close()\n    if addr_port is None:\n        # Could not find any free port.\n        return None, None\n    return addr_port\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/megatron_parallel_state_utils/#bionemo.testing.megatron_parallel_state_utils.mock_distributed_parallel_state","title":"<code>mock_distributed_parallel_state(world_size=8, rank=0, tensor_model_parallel_size=1, pipeline_model_parallel_size=1, virtual_pipeline_model_parallel_size=None, context_parallel_size=1, expert_model_parallel_size=1, seed=42)</code>","text":"<p>A context manager that facilitates easy mocking of torch.distributed for an arbitrary GPU in a simulated cluster.</p> Key functions that are mocked <ul> <li><code>torch.distributed.new_group</code> when <code>backend=\"gloo\"</code> which doesn't support a <code>backend=\"fake\"</code></li> <li><code>torch.distributed.destroy_process_group</code> when <code>backend=\"gloo\"</code> since new \"gloo\" groups are not actually made</li> <li><code>torch._C._cuda_setDevice</code> which changes the current device behind the scenes. We assign devices round-robin     to support <code>world_size &gt; torch.cuda.device_count()</code>.</li> </ul> <p>Outside of this mocking, a fake cluster is initialized using <code>backend=\"fake\"</code> in <code>torch.distributed</code>. This sets up     enough global state and environment for megatron to think that it is initializing a larger cluster with some     settings where the current context has some user defined rank. You can then test the megatron state on a     hypothetical rank in some large world size.</p> <p>Parameters:</p> Name Type Description Default <code>world_size</code> <code>int</code> <p>The world size (cluster size). Defaults to 8.</p> <code>8</code> <code>rank</code> <code>int</code> <p>the GPU number globally in the cluster. Defaults to 0.</p> <code>0</code> <code>tensor_model_parallel_size</code> <code>int</code> <p>tensor model parallel setting for megatron. Defaults to 1.</p> <code>1</code> <code>pipeline_model_parallel_size</code> <code>int</code> <p>pipeline model parallel setting for megatron. Defaults to 1.</p> <code>1</code> <code>virtual_pipeline_model_parallel_size</code> <code>Optional[int]</code> <p>virtual pipeline model parallel size for megatron. Defaults to None.</p> <code>None</code> <code>context_parallel_size</code> <code>int</code> <p>context parallel size. Defaults to 1.</p> <code>1</code> <code>expert_model_parallel_size</code> <code>int</code> <p>expert model parallel size. Defaults to 1.</p> <code>1</code> <code>seed</code> <code>int | None</code> <p>seed for RNG state. Defaults to 42.</p> <code>42</code> Source code in <code>bionemo/testing/megatron_parallel_state_utils.py</code> <pre><code>@contextmanager\ndef mock_distributed_parallel_state(\n    world_size: int = 8,\n    rank: int = 0,\n    tensor_model_parallel_size: int = 1,\n    pipeline_model_parallel_size: int = 1,\n    virtual_pipeline_model_parallel_size: Optional[int] = None,\n    context_parallel_size: int = 1,\n    expert_model_parallel_size: int = 1,\n    seed: int | None = 42,\n):\n    \"\"\"A context manager that facilitates easy mocking of torch.distributed for an arbitrary GPU in a simulated cluster.\n\n    Key functions that are mocked:\n        * `torch.distributed.new_group` when `backend=\"gloo\"` which doesn't support a `backend=\"fake\"`\n        * `torch.distributed.destroy_process_group` when `backend=\"gloo\"` since new \"gloo\" groups are not actually made\n        * `torch._C._cuda_setDevice` which changes the current device behind the scenes. We assign devices round-robin\n            to support `world_size &gt; torch.cuda.device_count()`.\n\n    Outside of this mocking, a fake cluster is initialized using `backend=\"fake\"` in `torch.distributed`. This sets up\n        enough global state and environment for megatron to think that it is initializing a larger cluster with some\n        settings where the current context has some user defined rank. You can then test the megatron state on a\n        hypothetical rank in some large world size.\n\n    Args:\n        world_size: The world size (cluster size). Defaults to 8.\n        rank: the GPU number globally in the cluster. Defaults to 0.\n        tensor_model_parallel_size: tensor model parallel setting for megatron. Defaults to 1.\n        pipeline_model_parallel_size: pipeline model parallel setting for megatron. Defaults to 1.\n        virtual_pipeline_model_parallel_size: virtual pipeline model parallel size for megatron. Defaults to None.\n        context_parallel_size: context parallel size. Defaults to 1.\n        expert_model_parallel_size: expert model parallel size. Defaults to 1.\n        seed: seed for RNG state. Defaults to 42.\n    \"\"\"\n    # First set up mocks for torch.distributed state/info\n    ori_device_count = torch.cuda.device_count()\n    # Conditionally mock torch.distributed.new_group based on backend argument\n    ori_dist_new_group = torch.distributed.new_group\n\n    def mock_new_group(*args, **kwargs):\n        if kwargs.get(\"backend\") == \"gloo\":\n            # Return a specific mock if backend is 'gloo'\n            return MagicMock(name=\"gloo_group\")\n        else:\n            # Return another mock or a different behavior for other backends\n            return ori_dist_new_group(*args, **kwargs)\n\n    ori_destroy_pg = torch.distributed.destroy_process_group\n\n    def mock_destroy_gloo_group(pg=None):\n        if isinstance(pg, MagicMock):\n            return None\n        ori_destroy_pg(pg)\n\n    # The next mock is required to \"set the device\" to one that is greater than the number of actual GPUs\n    #  the consequence of this mock is that the device is always dev 0\n    ori_set_device = torch._C._cuda_setDevice\n\n    def mock_set_device(device):\n        if ori_device_count &gt; 0:\n            ori_set_device(device % ori_device_count)  # wrap around the request\n\n    with (\n        mock.patch(\"torch.distributed.new_group\", side_effect=mock_new_group),\n        mock.patch(\"torch.distributed.destroy_process_group\", side_effect=mock_destroy_gloo_group),\n        mock.patch(\"torch._C._cuda_setDevice\", side_effect=mock_set_device),\n    ):\n        # Next set up state etc\n        state_util = _MockMegatronParallelStateSingleton()  # static singleton class\n        state_util.world_size = world_size\n        state_util.rank = rank\n        initial_states: Optional[Any] = None\n        try:\n            state_util.set_world_size(world_size=world_size, rank=rank)\n            state_util.initialize_model_parallel(\n                tensor_model_parallel_size=tensor_model_parallel_size,\n                pipeline_model_parallel_size=pipeline_model_parallel_size,\n                virtual_pipeline_model_parallel_size=virtual_pipeline_model_parallel_size,\n                context_parallel_size=context_parallel_size,\n                expert_model_parallel_size=expert_model_parallel_size,\n            )\n            # Our goal is to set required state on entry, and then restore current state on exit for the RNGs.\n            #  there are two possibilities that are handled below:\n            # 1. If the RNG state is not initialized, we need to set it up and then\n            #     unset it on exit to restore the current state. We track that this is the case when `initial_states` is `None`.\n            # 2. If the RNG state is initialized, we need to track this state and reset it on exit to be what it was on entry.\n            #    We track that this is the case when `initial_states` is not `None`.\n            if tp_random.get_cuda_rng_tracker().is_initialized():\n                initial_states = tp_random.get_cuda_rng_tracker().get_states()\n            if seed is not None:\n                # Set the seed if provided, this case is valid whether or not the RNG had state previously.\n                #  on exit the RNG state will be restored to what it was on entry.\n                tp_random.model_parallel_cuda_manual_seed(seed)\n            else:\n                # This is the case where the RNG state is not initialized and no seed was provided.\n                #  We need to raise an error in this case, as we cannot restore the RNG state on exit and we need a seed\n                #  to initialize the RNG state to. This only happens if the user overrides the default seed and sets it\n                #  to None, and additionally if the RNG state was not initialized externally, as there is a default seed of 42.\n                if initial_states is None:\n                    raise ValueError(\n                        \"You must provide a seed if the initial parallel state is unset. \"\n                        \"Either provide a seed or leave the default seed (rather setting to None) \"\n                        \"or initialize the RNG state externally.\"\n                    )\n            yield\n        finally:\n            if initial_states is not None:\n                tp_random.get_cuda_rng_tracker().set_states(initial_states)\n            else:\n                # Reset to the unset state\n                tp_random.get_cuda_rng_tracker().reset()\n            state_util.destroy_model_parallel()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/subprocess_utils/","title":"Subprocess utils","text":""},{"location":"main/references/API_reference/bionemo/testing/subprocess_utils/#bionemo.testing.subprocess_utils.run_command_in_subprocess","title":"<code>run_command_in_subprocess(command, path, timeout=3600)</code>","text":"<p>Run a command in a subprocess and return the output.</p> Source code in <code>bionemo/testing/subprocess_utils.py</code> <pre><code>def run_command_in_subprocess(command: str, path: str, timeout: int = 3600) -&gt; str:\n    \"\"\"Run a command in a subprocess and return the output.\"\"\"\n    open_port = find_free_network_port()\n    # a local copy of the environment\n    env = dict(**os.environ)\n    env[\"MASTER_PORT\"] = str(open_port)\n\n    result = run_command_with_timeout(\n        command=command,\n        path=path,\n        env=env,\n        timeout=timeout,  # Set an appropriate timeout in seconds\n    )\n\n    # For debugging purposes, print the output if the test fails.\n    if result.returncode != 0:\n        sys.stderr.write(\"STDOUT:\\n\" + result.stdout + \"\\n\")\n        sys.stderr.write(\"STDERR:\\n\" + result.stderr + \"\\n\")\n\n    assert result.returncode == 0, f\"Command failed: {command}\"\n\n    return result.stdout\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/subprocess_utils/#bionemo.testing.subprocess_utils.run_command_with_timeout","title":"<code>run_command_with_timeout(command, path, env, timeout=3600)</code>","text":"<p>Run command with timeout and incremental output processing to prevent hanging.</p> Source code in <code>bionemo/testing/subprocess_utils.py</code> <pre><code>def run_command_with_timeout(command, path, env, timeout=3600):\n    \"\"\"Run command with timeout and incremental output processing to prevent hanging.\"\"\"\n    # Start process without capturing output in the main process\n    process = subprocess.Popen(\n        command,\n        shell=True,\n        cwd=path,\n        env=env,\n        text=True,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        bufsize=1,  # Line buffered\n    )\n\n    stdout_data = []\n    stderr_data = []\n    start_time = time.time()\n\n    try:\n        # Use select to handle output in a non-blocking way\n        import select\n\n        # Get file descriptors for stdout and stderr\n        stdout_fd = process.stdout.fileno()\n        stderr_fd = process.stderr.fileno()\n\n        # Set up select lists\n        read_fds = [stdout_fd, stderr_fd]\n\n        # Process output incrementally\n        while read_fds and process.poll() is None:\n            # Check for timeout\n            if timeout and time.time() - start_time &gt; timeout:\n                process.terminate()\n                time.sleep(0.5)\n                if process.poll() is None:\n                    process.kill()\n                raise subprocess.TimeoutExpired(command, timeout)\n\n            # Wait for output with a short timeout to allow checking process status\n            ready_fds, _, _ = select.select(read_fds, [], [], 1.0)\n\n            for fd in ready_fds:\n                if fd == stdout_fd:\n                    line = process.stdout.readline()\n                    if not line:\n                        read_fds.remove(stdout_fd)\n                        continue\n                    stdout_data.append(line)\n                    # Optionally process/print output incrementally\n                    # print(f\"STDOUT: {line.strip()}\")\n\n                if fd == stderr_fd:\n                    line = process.stderr.readline()\n                    if not line:\n                        read_fds.remove(stderr_fd)\n                        continue\n                    stderr_data.append(line)\n                    # Optionally process/print error output incrementally\n                    # print(f\"STDERR: {line.strip()}\")\n\n        # Get any remaining output\n        remaining_stdout, remaining_stderr = process.communicate()\n        if remaining_stdout:\n            stdout_data.append(remaining_stdout)\n        if remaining_stderr:\n            stderr_data.append(remaining_stderr)\n\n        # Create result object similar to subprocess.run\n        result = subprocess.CompletedProcess(\n            args=command, returncode=process.returncode, stdout=\"\".join(stdout_data), stderr=\"\".join(stderr_data)\n        )\n        return result\n\n    except Exception as e:\n        # Make sure we don't leave zombie processes\n        if process.poll() is None:\n            process.terminate()\n            time.sleep(0.5)\n            if process.poll() is None:\n                process.kill()\n        raise e\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/tensorboard/","title":"Tensorboard","text":""},{"location":"main/references/API_reference/bionemo/testing/tensorboard/#bionemo.testing.tensorboard.verify_tensorboard_logs","title":"<code>verify_tensorboard_logs(tb_log_dir, expected_metrics, min_steps=1)</code>","text":"<p>Verify that TensorBoard logs exist and contain expected metrics.</p> <p>Parameters:</p> Name Type Description Default <code>tb_log_dir</code> <code>Path</code> <p>Path to the TensorBoard log directory</p> required <code>expected_metrics</code> <code>list[str]</code> <p>List of metric names expected in the logs</p> required <code>min_steps</code> <code>int</code> <p>Minimum number of steps expected in the logs</p> <code>1</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>None if verification succeeds, error message string if it fails</p> Source code in <code>bionemo/testing/tensorboard.py</code> <pre><code>def verify_tensorboard_logs(tb_log_dir: Path, expected_metrics: list[str], min_steps: int = 1) -&gt; Optional[str]:\n    \"\"\"Verify that TensorBoard logs exist and contain expected metrics.\n\n    Args:\n        tb_log_dir: Path to the TensorBoard log directory\n        expected_metrics: List of metric names expected in the logs\n        min_steps: Minimum number of steps expected in the logs\n\n    Returns:\n        None if verification succeeds, error message string if it fails\n    \"\"\"\n    # Find event files in the log directory\n    event_files = list(tb_log_dir.glob(\"events.out.tfevents.*\"))\n    if len(event_files) == 0:\n        return f\"No TensorBoard event files found in {tb_log_dir}\"\n\n    # Load the event file\n    event_acc = EventAccumulator(str(tb_log_dir))\n    event_acc.Reload()\n\n    # Get available scalar tags\n    scalar_tags = event_acc.Tags()[\"scalars\"]\n\n    # Check that expected metrics are present\n    for metric in expected_metrics:\n        # Check if metric exists in any form (might have prefixes like \"train/\" or suffixes)\n        metric_found = any(metric in tag for tag in scalar_tags)\n        if not metric_found:\n            return f\"Expected metric '{metric}' not found in TensorBoard logs. Available tags: {scalar_tags}\"\n\n    # Verify we have logged data for at least min_steps\n    if scalar_tags:\n        # Get the first available metric to check step count\n        first_metric = scalar_tags[0]\n        events = event_acc.Scalars(first_metric)\n        if len(events) &lt; min_steps:\n            return f\"Expected at least {min_steps} steps logged, but found {len(events)}\"\n\n    return None\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/","title":"Testing callbacks","text":""},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.AbstractStopAndGoCallback","title":"<code>AbstractStopAndGoCallback</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseInterruptedVsContinuousCallback</code></p> <p>Abstract base class for stop-and-go callback to compare metadata before pausing and after resuming training.</p> <p>This base class provides utility methods to help streamline stop and go comparison.</p> Provided methods <ul> <li>init: initializes the callback with the given mode.</li> <li>get_metadata: abstract method that should be overridden to get metadata from the trainer and pl_module.</li> </ul> Default behaviors <ul> <li>in stop mode, metadata is gotten and compared on_validation_epoch_end.</li> <li>in go mode, metadata is gotten and saved on_train_epoch_start.</li> </ul> <p>Override these behaviors if necessary.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class AbstractStopAndGoCallback(ABC, BaseInterruptedVsContinuousCallback):\n    \"\"\"Abstract base class for stop-and-go callback to compare metadata before pausing and after resuming training.\n\n    This base class provides utility methods to help streamline stop and go comparison.\n\n    Provided methods:\n        - __init__: initializes the callback with the given mode.\n        - get_metadata: abstract method that should be overridden to get metadata from the trainer and pl_module.\n\n    Default behaviors:\n        - in stop mode, metadata is gotten and compared on_validation_epoch_end.\n        - in go mode, metadata is gotten and saved on_train_epoch_start.\n\n    Override these behaviors if necessary.\n    \"\"\"\n\n    def __init__(self, mode: Mode = Mode.STOP):\n        \"\"\"Initialize StopAndGoCallback.\n\n        Args:\n            mode (str, optional): Mode to run in. Must be either Mode.STOP or Mode.RESUME. Defaults to Mode.STOP.\n\n        Notes:\n            User must override get_metadata to get metadata from the trainer and pl_module.\n        \"\"\"\n        if mode not in [Mode.STOP, Mode.RESUME]:\n            raise ValueError(f\"mode must be 'stop' or 'go', got {mode}\")\n        self.mode = mode\n        super().__init__()\n\n    @abstractmethod\n    def get_metadata(self, trainer: Trainer, pl_module: LightningModule) -&gt; Any:\n        \"\"\"Get metadata from trainer and pl_module.\"\"\"\n        raise NotImplementedError\n\n    def on_train_epoch_start(self, trainer: Trainer, pl_module: LightningModule):  # noqa: D102\n        if self.mode == Mode.RESUME:\n            self.data = self.get_metadata(trainer, pl_module)\n\n    def on_validation_epoch_end(self, trainer: Trainer, pl_module: LightningModule):  # noqa: D102\n        if not trainer.sanity_checking and self.mode == Mode.STOP:\n            self.data = self.get_metadata(trainer, pl_module)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.AbstractStopAndGoCallback.__init__","title":"<code>__init__(mode=Mode.STOP)</code>","text":"<p>Initialize StopAndGoCallback.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Mode to run in. Must be either Mode.STOP or Mode.RESUME. Defaults to Mode.STOP.</p> <code>STOP</code> Notes <p>User must override get_metadata to get metadata from the trainer and pl_module.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def __init__(self, mode: Mode = Mode.STOP):\n    \"\"\"Initialize StopAndGoCallback.\n\n    Args:\n        mode (str, optional): Mode to run in. Must be either Mode.STOP or Mode.RESUME. Defaults to Mode.STOP.\n\n    Notes:\n        User must override get_metadata to get metadata from the trainer and pl_module.\n    \"\"\"\n    if mode not in [Mode.STOP, Mode.RESUME]:\n        raise ValueError(f\"mode must be 'stop' or 'go', got {mode}\")\n    self.mode = mode\n    super().__init__()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.AbstractStopAndGoCallback.get_metadata","title":"<code>get_metadata(trainer, pl_module)</code>  <code>abstractmethod</code>","text":"<p>Get metadata from trainer and pl_module.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>@abstractmethod\ndef get_metadata(self, trainer: Trainer, pl_module: LightningModule) -&gt; Any:\n    \"\"\"Get metadata from trainer and pl_module.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.BaseInterruptedVsContinuousCallback","title":"<code>BaseInterruptedVsContinuousCallback</code>","text":"<p>               Bases: <code>Callback</code>, <code>CallbackMethods</code>, <code>IOMixin</code></p> <p>Base class for serializable stop-and-go callback to compare continuous to interrupted training.</p> <p>This class is used by extending a callback and collecting data into the <code>self.data</code> attribute. This data is then compared between continuous and interrupted training.</p> <p>See nemo.lightning.megatron_parallel.CallbackMethods for the available callback methods.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class BaseInterruptedVsContinuousCallback(Callback, CallbackMethods, io.IOMixin):\n    \"\"\"Base class for serializable stop-and-go callback to compare continuous to interrupted training.\n\n    This class is used by extending a callback and collecting data into the `self.data` attribute. This data is then\n    compared between continuous and interrupted training.\n\n    See nemo.lightning.megatron_parallel.CallbackMethods for the available callback methods.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initializes the callback.\"\"\"\n        self.data = []\n\n    def __deepcopy__(self, memo):\n        \"\"\"Don't actually attempt to copy this data when this callback is being serialized.\"\"\"\n        ...\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.BaseInterruptedVsContinuousCallback.__deepcopy__","title":"<code>__deepcopy__(memo)</code>","text":"<p>Don't actually attempt to copy this data when this callback is being serialized.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def __deepcopy__(self, memo):\n    \"\"\"Don't actually attempt to copy this data when this callback is being serialized.\"\"\"\n    ...\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.BaseInterruptedVsContinuousCallback.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the callback.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes the callback.\"\"\"\n    self.data = []\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.ConsumedSamplesCallback","title":"<code>ConsumedSamplesCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Stop-and-go callback to check consumed samples before pausing and after resuming training.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class ConsumedSamplesCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Stop-and-go callback to check consumed samples before pausing and after resuming training.\"\"\"\n\n    def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n        \"\"\"Get consumed samples as metadata.\"\"\"\n        if step.trainer.training:\n            data_sampler = step.trainer.datamodule.data_sampler\n            consumed_samples = data_sampler.compute_consumed_samples(\n                step.trainer.global_step - step.trainer.datamodule.init_global_step\n            )\n            self.data.append(np.array(consumed_samples))\n        return step\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.ConsumedSamplesCallback.on_megatron_step_start","title":"<code>on_megatron_step_start(step)</code>","text":"<p>Get consumed samples as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n    \"\"\"Get consumed samples as metadata.\"\"\"\n    if step.trainer.training:\n        data_sampler = step.trainer.datamodule.data_sampler\n        consumed_samples = data_sampler.compute_consumed_samples(\n            step.trainer.global_step - step.trainer.datamodule.init_global_step\n        )\n        self.data.append(np.array(consumed_samples))\n    return step\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.GlobalStepStateCallback","title":"<code>GlobalStepStateCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Stop-and-go callback for global_step before pausing and after resuming training.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class GlobalStepStateCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Stop-and-go callback for global_step before pausing and after resuming training.\"\"\"\n\n    def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n        \"\"\"Get learning rate as metadata.\"\"\"\n        if step.trainer.training:\n            self.data.append(np.array(step.trainer.global_step))\n        return step\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.GlobalStepStateCallback.on_megatron_step_start","title":"<code>on_megatron_step_start(step)</code>","text":"<p>Get learning rate as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n    \"\"\"Get learning rate as metadata.\"\"\"\n    if step.trainer.training:\n        self.data.append(np.array(step.trainer.global_step))\n    return step\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.LearningRateCallback","title":"<code>LearningRateCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Stop-and-go callback for learning rate before pausing and after resuming training.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class LearningRateCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Stop-and-go callback for learning rate before pausing and after resuming training.\"\"\"\n\n    def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n        \"\"\"Get learning rate as metadata.\"\"\"\n        if step.trainer.training:\n            self.data.append(np.array(step.trainer.optimizers[0].param_groups[0][\"lr\"]))\n        return step\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.LearningRateCallback.on_megatron_step_start","title":"<code>on_megatron_step_start(step)</code>","text":"<p>Get learning rate as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n    \"\"\"Get learning rate as metadata.\"\"\"\n    if step.trainer.training:\n        self.data.append(np.array(step.trainer.optimizers[0].param_groups[0][\"lr\"]))\n    return step\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.OptimizerStateCallback","title":"<code>OptimizerStateCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Stop-and-go callback to check optimizer states before pausing and after resuming training.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class OptimizerStateCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Stop-and-go callback to check optimizer states before pausing and after resuming training.\"\"\"\n\n    def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n        \"\"\"Get optimizer states as metadata.\"\"\"\n        if step.trainer.training:\n            self.data.append(\n                recursive_detach(\n                    [\n                        optimizer.mcore_optimizer.optimizer.state_dict()[\"state\"]\n                        for optimizer in step.trainer.optimizers\n                    ]\n                )\n            )\n        return step\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.OptimizerStateCallback.on_megatron_step_start","title":"<code>on_megatron_step_start(step)</code>","text":"<p>Get optimizer states as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n    \"\"\"Get optimizer states as metadata.\"\"\"\n    if step.trainer.training:\n        self.data.append(\n            recursive_detach(\n                [\n                    optimizer.mcore_optimizer.optimizer.state_dict()[\"state\"]\n                    for optimizer in step.trainer.optimizers\n                ]\n            )\n        )\n    return step\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.SignalAfterGivenStepCallback","title":"<code>SignalAfterGivenStepCallback</code>","text":"<p>               Bases: <code>Callback</code>, <code>CallbackMethods</code></p> <p>A callback that emits a given signal to the current process at the defined step.</p> <p>Use this callback for pytest based Stop and go tests.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class SignalAfterGivenStepCallback(Callback, CallbackMethods):\n    \"\"\"A callback that emits a given signal to the current process at the defined step.\n\n    Use this callback for pytest based Stop and go tests.\n    \"\"\"\n\n    def __init__(\n        self,\n        stop_step: int,\n        signal_: signal.Signals = signal.SIGUSR2,\n        use_trainer_should_stop: bool = False,\n        stop_before_step: bool = False,\n    ):\n        \"\"\"Initializes the callback with the given stop_step.\"\"\"\n        # Note that the stop step will be one less than the requested step if stop_before_step is True.\n        #  this is because the first step is 0 so you get i+1 steps normally.\n        if stop_before_step:\n            self.stop_step = stop_step - 1\n        else:\n            self.stop_step = stop_step\n        self.signal = signal_\n        # If True, ask the trainer to stop by setting should_stop to True rather than emitting a kill signal.\n        self.use_trainer_should_stop = use_trainer_should_stop\n\n    def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n        \"\"\"Stop training if the global step is greater than or equal to the stop_step.\"\"\"\n        if step.trainer.global_step &gt;= self.stop_step:\n            if self.use_trainer_should_stop:\n                # Ask the trainer to stop by setting should_stop to True rather than emitting a kill signal.\n                step.trainer.should_stop = True\n            else:\n                os.kill(os.getpid(), self.signal)\n        return step\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.SignalAfterGivenStepCallback.__init__","title":"<code>__init__(stop_step, signal_=signal.SIGUSR2, use_trainer_should_stop=False, stop_before_step=False)</code>","text":"<p>Initializes the callback with the given stop_step.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def __init__(\n    self,\n    stop_step: int,\n    signal_: signal.Signals = signal.SIGUSR2,\n    use_trainer_should_stop: bool = False,\n    stop_before_step: bool = False,\n):\n    \"\"\"Initializes the callback with the given stop_step.\"\"\"\n    # Note that the stop step will be one less than the requested step if stop_before_step is True.\n    #  this is because the first step is 0 so you get i+1 steps normally.\n    if stop_before_step:\n        self.stop_step = stop_step - 1\n    else:\n        self.stop_step = stop_step\n    self.signal = signal_\n    # If True, ask the trainer to stop by setting should_stop to True rather than emitting a kill signal.\n    self.use_trainer_should_stop = use_trainer_should_stop\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.SignalAfterGivenStepCallback.on_megatron_step_start","title":"<code>on_megatron_step_start(step)</code>","text":"<p>Stop training if the global step is greater than or equal to the stop_step.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n    \"\"\"Stop training if the global step is greater than or equal to the stop_step.\"\"\"\n    if step.trainer.global_step &gt;= self.stop_step:\n        if self.use_trainer_should_stop:\n            # Ask the trainer to stop by setting should_stop to True rather than emitting a kill signal.\n            step.trainer.should_stop = True\n        else:\n            os.kill(os.getpid(), self.signal)\n    return step\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.StopAfterValidEpochEndCallback","title":"<code>StopAfterValidEpochEndCallback</code>","text":"<p>               Bases: <code>Callback</code>, <code>CallbackMethods</code></p> <p>A callback that stops training after the validation epoch.</p> <p>Use this callback for pytest based Stop and go tests.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class StopAfterValidEpochEndCallback(Callback, CallbackMethods):\n    \"\"\"A callback that stops training after the validation epoch.\n\n    Use this callback for pytest based Stop and go tests.\n    \"\"\"\n\n    def on_validation_epoch_end(self, trainer: Trainer, pl_module: LightningModule):  # noqa: D102\n        if trainer.sanity_checking:\n            return\n        trainer.should_stop = True\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.TrainInputCallback","title":"<code>TrainInputCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Collect training input samples for comparison.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class TrainInputCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Collect training input samples for comparison.\"\"\"\n\n    def on_megatron_microbatch_end(\n        self,\n        step: MegatronStep,\n        batch: DataT,\n        forward_callback: \"MegatronLossReduction\",\n        output: Any,\n    ) -&gt; None:\n        \"\"\"Get consumed samples as metadata.\"\"\"\n        if step.trainer.training:\n            self.data.append(recursive_detach(batch))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.TrainInputCallback.on_megatron_microbatch_end","title":"<code>on_megatron_microbatch_end(step, batch, forward_callback, output)</code>","text":"<p>Get consumed samples as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_microbatch_end(\n    self,\n    step: MegatronStep,\n    batch: DataT,\n    forward_callback: \"MegatronLossReduction\",\n    output: Any,\n) -&gt; None:\n    \"\"\"Get consumed samples as metadata.\"\"\"\n    if step.trainer.training:\n        self.data.append(recursive_detach(batch))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.TrainLossCallback","title":"<code>TrainLossCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Collect training loss samples for comparison.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class TrainLossCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Collect training loss samples for comparison.\"\"\"\n\n    def on_megatron_step_end(\n        self,\n        step: MegatronStep,\n        microbatch_outputs: List[Any],\n        reduced: Optional[Union[torch.Tensor, Dict[str, torch.Tensor]]] = None,\n    ) -&gt; None:\n        \"\"\"Get consumed samples as metadata.\"\"\"\n        if step.trainer.training:\n            self.data.append(recursive_detach(reduced))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.TrainLossCallback.on_megatron_step_end","title":"<code>on_megatron_step_end(step, microbatch_outputs, reduced=None)</code>","text":"<p>Get consumed samples as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_step_end(\n    self,\n    step: MegatronStep,\n    microbatch_outputs: List[Any],\n    reduced: Optional[Union[torch.Tensor, Dict[str, torch.Tensor]]] = None,\n) -&gt; None:\n    \"\"\"Get consumed samples as metadata.\"\"\"\n    if step.trainer.training:\n        self.data.append(recursive_detach(reduced))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.TrainOutputCallback","title":"<code>TrainOutputCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Collect training output samples for comparison.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class TrainOutputCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Collect training output samples for comparison.\"\"\"\n\n    def on_megatron_microbatch_end(\n        self,\n        step: MegatronStep,\n        batch: DataT,\n        forward_callback: \"MegatronLossReduction\",\n        output: Any,\n    ) -&gt; None:\n        \"\"\"Get consumed samples as metadata.\"\"\"\n        if step.trainer.training:\n            self.data.append(recursive_detach(output))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.TrainOutputCallback.on_megatron_microbatch_end","title":"<code>on_megatron_microbatch_end(step, batch, forward_callback, output)</code>","text":"<p>Get consumed samples as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_microbatch_end(\n    self,\n    step: MegatronStep,\n    batch: DataT,\n    forward_callback: \"MegatronLossReduction\",\n    output: Any,\n) -&gt; None:\n    \"\"\"Get consumed samples as metadata.\"\"\"\n    if step.trainer.training:\n        self.data.append(recursive_detach(output))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.TrainValInitConsumedSamplesStopAndGoCallback","title":"<code>TrainValInitConsumedSamplesStopAndGoCallback</code>","text":"<p>               Bases: <code>AbstractStopAndGoCallback</code></p> <p>Stop-and-go callback to check consumed samples before pausing and after resuming training.</p> <p>This is currently the only callback that doesn't fit with the new pattern of directly comparing continuous and interrupted training, since the dataloaders don't track their consumed_samples before and after checkpoint resumption.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class TrainValInitConsumedSamplesStopAndGoCallback(AbstractStopAndGoCallback):\n    \"\"\"Stop-and-go callback to check consumed samples before pausing and after resuming training.\n\n    This is currently the only callback that doesn't fit with the new pattern of directly comparing continuous and\n    interrupted training, since the dataloaders don't track their consumed_samples before and after checkpoint\n    resumption.\n    \"\"\"\n\n    @override\n    def get_metadata(self, trainer: Trainer, pl_module: LightningModule) -&gt; Any:\n        \"\"\"Get consumed samples as metadata.\"\"\"\n        # return trainer.datamodule.state_dict()[\"consumed_samples\"]  # TODO why state_dict can be empty despite working lines below\n        train_data_sampler: MegatronPretrainingSampler = trainer.train_dataloader.batch_sampler\n        val_data_sampler: MegatronPretrainingSampler = trainer.val_dataloaders.batch_sampler\n        return train_data_sampler.consumed_samples, val_data_sampler.consumed_samples\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.TrainValInitConsumedSamplesStopAndGoCallback.get_metadata","title":"<code>get_metadata(trainer, pl_module)</code>","text":"<p>Get consumed samples as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>@override\ndef get_metadata(self, trainer: Trainer, pl_module: LightningModule) -&gt; Any:\n    \"\"\"Get consumed samples as metadata.\"\"\"\n    # return trainer.datamodule.state_dict()[\"consumed_samples\"]  # TODO why state_dict can be empty despite working lines below\n    train_data_sampler: MegatronPretrainingSampler = trainer.train_dataloader.batch_sampler\n    val_data_sampler: MegatronPretrainingSampler = trainer.val_dataloaders.batch_sampler\n    return train_data_sampler.consumed_samples, val_data_sampler.consumed_samples\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.ValidInputCallback","title":"<code>ValidInputCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Collect validation input samples for comparison.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class ValidInputCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Collect validation input samples for comparison.\"\"\"\n\n    def on_megatron_microbatch_end(\n        self,\n        step: MegatronStep,\n        batch: DataT,\n        forward_callback: \"MegatronLossReduction\",\n        output: Any,\n    ) -&gt; None:\n        \"\"\"Get consumed samples as metadata.\"\"\"\n        if step.trainer.validating:\n            self.data.append(recursive_detach(batch))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.ValidInputCallback.on_megatron_microbatch_end","title":"<code>on_megatron_microbatch_end(step, batch, forward_callback, output)</code>","text":"<p>Get consumed samples as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_microbatch_end(\n    self,\n    step: MegatronStep,\n    batch: DataT,\n    forward_callback: \"MegatronLossReduction\",\n    output: Any,\n) -&gt; None:\n    \"\"\"Get consumed samples as metadata.\"\"\"\n    if step.trainer.validating:\n        self.data.append(recursive_detach(batch))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.ValidLossCallback","title":"<code>ValidLossCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Collect training loss samples for comparison.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class ValidLossCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Collect training loss samples for comparison.\"\"\"\n\n    def on_megatron_step_end(\n        self,\n        step: MegatronStep,\n        microbatch_outputs: List[Any],\n        reduced: Optional[Union[torch.Tensor, Dict[str, torch.Tensor]]] = None,\n    ) -&gt; None:\n        \"\"\"Get consumed samples as metadata.\"\"\"\n        if step.trainer.validating:\n            self.data.append(recursive_detach(reduced))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.ValidLossCallback.on_megatron_step_end","title":"<code>on_megatron_step_end(step, microbatch_outputs, reduced=None)</code>","text":"<p>Get consumed samples as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_step_end(\n    self,\n    step: MegatronStep,\n    microbatch_outputs: List[Any],\n    reduced: Optional[Union[torch.Tensor, Dict[str, torch.Tensor]]] = None,\n) -&gt; None:\n    \"\"\"Get consumed samples as metadata.\"\"\"\n    if step.trainer.validating:\n        self.data.append(recursive_detach(reduced))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.ValidOutputCallback","title":"<code>ValidOutputCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Collect validation output samples for comparison.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class ValidOutputCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Collect validation output samples for comparison.\"\"\"\n\n    def on_megatron_microbatch_end(\n        self,\n        step: MegatronStep,\n        batch: DataT,\n        forward_callback: \"MegatronLossReduction\",\n        output: Any,\n    ) -&gt; None:\n        \"\"\"Get consumed samples as metadata.\"\"\"\n        if step.trainer.validating:\n            self.data.append(recursive_detach(output))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.ValidOutputCallback.on_megatron_microbatch_end","title":"<code>on_megatron_microbatch_end(step, batch, forward_callback, output)</code>","text":"<p>Get consumed samples as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_microbatch_end(\n    self,\n    step: MegatronStep,\n    batch: DataT,\n    forward_callback: \"MegatronLossReduction\",\n    output: Any,\n) -&gt; None:\n    \"\"\"Get consumed samples as metadata.\"\"\"\n    if step.trainer.validating:\n        self.data.append(recursive_detach(output))\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/torch/","title":"Torch","text":""},{"location":"main/references/API_reference/bionemo/testing/torch/#bionemo.testing.torch.check_fp8_support","title":"<code>check_fp8_support(device_id=0)</code>","text":"<p>Check if FP8 is supported on the current GPU.</p> <p>FP8 requires compute capability 8.9+ (Ada Lovelace/Hopper architecture or newer).</p> Source code in <code>bionemo/testing/torch.py</code> <pre><code>def check_fp8_support(device_id: int = 0) -&gt; tuple[bool, str, str]:\n    \"\"\"Check if FP8 is supported on the current GPU.\n\n    FP8 requires compute capability 8.9+ (Ada Lovelace/Hopper architecture or newer).\n    \"\"\"\n    if not torch.cuda.is_available():\n        return False, \"0.0\", \"CUDA not available\"\n    device_props = torch.cuda.get_device_properties(device_id)\n    compute_capability = f\"{device_props.major}.{device_props.minor}\"\n    device_name = device_props.name\n    # FP8 is supported on compute capability 8.9+ (Ada Lovelace/Hopper architecture)\n    is_supported = (device_props.major &gt; 8) or (device_props.major == 8 and device_props.minor &gt;= 9)\n    return is_supported, compute_capability, f\"Device: {device_name}, Compute Capability: {compute_capability}\"\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/torch/#bionemo.testing.torch.get_device_and_memory_allocated","title":"<code>get_device_and_memory_allocated()</code>","text":"<p>Get the current device index, name, and memory usage.</p> Source code in <code>bionemo/testing/torch.py</code> <pre><code>def get_device_and_memory_allocated() -&gt; str:\n    \"\"\"Get the current device index, name, and memory usage.\"\"\"\n    current_device_index = torch.cuda.current_device()\n    props = torch.cuda.get_device_properties(current_device_index)\n    message = f\"\"\"\n        current device index: {current_device_index}\n        current device uuid: {props.uuid}\n        current device name: {props.name}\n        memory, total on device: {torch.cuda.mem_get_info()[1] / 1024**3:.3f} GB\n        memory, available on device: {torch.cuda.mem_get_info()[0] / 1024**3:.3f} GB\n        memory allocated for tensors etc: {torch.cuda.memory_allocated() / 1024**3:.3f} GB\n        max memory reserved for tensors etc: {torch.cuda.max_memory_allocated() / 1024**3:.3f} GB\n        \"\"\"\n    return message\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/torch/#bionemo.testing.torch.recursive_assert_approx_equal","title":"<code>recursive_assert_approx_equal(x, y, atol=0.0001, rtol=0.0001)</code>","text":"<p>Assert that all tensors in a nested structure are approximately equal.</p> Source code in <code>bionemo/testing/torch.py</code> <pre><code>def recursive_assert_approx_equal(x, y, atol=1e-4, rtol=1e-4):\n    \"\"\"Assert that all tensors in a nested structure are approximately equal.\"\"\"\n    if isinstance(x, torch.Tensor):\n        torch.testing.assert_close(x, y, atol=atol, rtol=rtol)\n    elif isinstance(x, np.ndarray):\n        np.testing.assert_allclose(x, y, atol=atol, rtol=rtol)\n    elif isinstance(x, (list, tuple)):\n        assert len(x) == len(y), f\"Length mismatch: {len(x)} vs {len(y)}\"\n        for x_item, y_item in zip(x, y):\n            recursive_assert_approx_equal(x_item, y_item, atol=atol, rtol=rtol)\n    elif isinstance(x, dict):\n        assert x.keys() == y.keys()\n        for key in x:\n            recursive_assert_approx_equal(x[key], y[key], atol=atol, rtol=rtol)\n    else:\n        assert x == y\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/torch/#bionemo.testing.torch.recursive_detach","title":"<code>recursive_detach(x)</code>","text":"<p>Detach all tensors in a nested structure.</p> Source code in <code>bionemo/testing/torch.py</code> <pre><code>def recursive_detach(x):\n    \"\"\"Detach all tensors in a nested structure.\"\"\"\n    if isinstance(x, torch.Tensor):\n        return x.detach().cpu()\n    elif isinstance(x, (list, tuple)):\n        return type(x)(recursive_detach(item) for item in x)\n    elif isinstance(x, dict):\n        return {key: recursive_detach(value) for key, value in x.items()}\n    else:\n        return x\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/utils/","title":"Utils","text":""},{"location":"main/references/API_reference/bionemo/testing/utils/#bionemo.testing.utils.assert_matrix_correlation_above_value","title":"<code>assert_matrix_correlation_above_value(actual, expected, mask=None, min_correlation=0.95, msg='')</code>","text":"<p>Assert that two tensors are close with a root mean squared error (RMSE)     relative to the scaled root mean square values for each matrix. This tells     you if the RMSE implies that the two matrices are more similar to eachother     as-is than would be the case if values were randomly permuted.</p> <p>Parameters:</p> Name Type Description Default <code>actual</code> <code>Tensor</code> <p>The actual tensor.</p> required <code>expected</code> <code>Tensor</code> <p>The expected tensor.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>If there are only some values you want to compare, apply this mask and RMSE will be computed on the unmasked items only.</p> <code>None</code> <code>min_relative_rmse</code> <p>The relative tolerance parameter.</p> required Source code in <code>bionemo/testing/utils.py</code> <pre><code>def assert_matrix_correlation_above_value(  # noqa: D417\n    actual: torch.Tensor,\n    expected: torch.Tensor,\n    mask: Optional[torch.Tensor] = None,\n    min_correlation: float = 0.95,\n    msg: str = \"\",\n) -&gt; None:\n    \"\"\"Assert that two tensors are close with a root mean squared error (RMSE)\n        relative to the scaled root mean square values for each matrix. This tells\n        you if the RMSE implies that the two matrices are more similar to eachother\n        as-is than would be the case if values were randomly permuted.\n\n    Args:\n        actual: The actual tensor.\n        expected: The expected tensor.\n        mask: If there are only some values you want to compare,\n            apply this mask and RMSE will be computed on the unmasked items only.\n        min_relative_rmse: The relative tolerance parameter.\n    \"\"\"  # noqa: D205\n    if mask is None:\n        mask = torch.ones_like(actual)\n    else:\n        if len(mask.shape) &lt; len(actual.shape):\n            mask = mask[..., None]\n    masked_actual = actual[mask.expand_as(actual).to(bool)]\n    masked_expected = expected[mask.expand_as(expected).to(bool)]\n    corr = torch.corrcoef(torch.stack([masked_actual, masked_expected]))[0, 1]\n    if corr &lt; min_correlation:\n        raise AssertionError(f\"Correlation below threshold: {corr} &lt; {min_correlation}. {msg}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/utils/#bionemo.testing.utils.assert_matrix_mape_below_value","title":"<code>assert_matrix_mape_below_value(actual, expected, mask=None, max_mape=0.1, eps=0.001, msg='')</code>","text":"<p>Assert that two tensors are close with a root mean squared error (RMSE)     relative to the scaled root mean square values for each matrix. This tells     you if the RMSE implies that the two matrices are more similar to eachother     as-is than would be the case if values were randomly permuted.</p> <p>Parameters:</p> Name Type Description Default <code>actual</code> <code>Tensor</code> <p>The actual tensor.</p> required <code>expected</code> <code>Tensor</code> <p>The expected tensor.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>If there are only some values you want to compare, apply this mask and RMSE will be computed on the unmasked items only.</p> <code>None</code> <code>min_relative_rmse</code> <p>The relative tolerance parameter.</p> required Source code in <code>bionemo/testing/utils.py</code> <pre><code>def assert_matrix_mape_below_value(  # noqa: D417\n    actual: torch.Tensor,\n    expected: torch.Tensor,\n    mask: Optional[torch.Tensor] = None,\n    max_mape: float = 0.1,\n    eps: float = 1e-3,\n    msg: str = \"\",\n) -&gt; None:\n    \"\"\"Assert that two tensors are close with a root mean squared error (RMSE)\n        relative to the scaled root mean square values for each matrix. This tells\n        you if the RMSE implies that the two matrices are more similar to eachother\n        as-is than would be the case if values were randomly permuted.\n\n    Args:\n        actual: The actual tensor.\n        expected: The expected tensor.\n        mask: If there are only some values you want to compare,\n            apply this mask and RMSE will be computed on the unmasked items only.\n        min_relative_rmse: The relative tolerance parameter.\n    \"\"\"  # noqa: D205\n    if mask is None:\n        mask = torch.ones_like(actual)\n    else:\n        if len(mask.shape) &lt; len(actual.shape):\n            mask = mask[..., None]\n    masked_actual = actual[mask.expand_as(actual).to(bool)]\n    masked_expected = expected[mask.expand_as(expected).to(bool)]\n    mape = (\n        torch.mean(\n            torch.abs(masked_actual - masked_expected)\n            / torch.maximum(torch.abs(masked_expected), torch.zeros_like(masked_expected) + eps)\n        )\n        * 100.0\n    )\n    if mape &gt; max_mape:\n        raise AssertionError(f\"MAPE below threshold: {mape} &gt; {max_mape}. {msg}\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/data/esm2/","title":"Esm2","text":""},{"location":"main/references/API_reference/bionemo/testing/data/esm2/#bionemo.testing.data.esm2.create_mock_parquet_train_val_inputs","title":"<code>create_mock_parquet_train_val_inputs(tmp_path)</code>","text":"<p>Create a mock protein train and val cluster parquet.</p> Source code in <code>bionemo/testing/data/esm2.py</code> <pre><code>def create_mock_parquet_train_val_inputs(tmp_path):\n    \"\"\"Create a mock protein train and val cluster parquet.\"\"\"\n    train_cluster_path = tmp_path / \"train_clusters.parquet\"\n    train_clusters = pd.DataFrame(\n        {\n            \"ur90_id\": [[\"UniRef90_A\"], [\"UniRef90_B\", \"UniRef90_C\"]],\n        }\n    )\n    train_clusters.to_parquet(train_cluster_path)\n\n    valid_cluster_path = tmp_path / \"valid_clusters.parquet\"\n    valid_clusters = pd.DataFrame(\n        {\n            \"ur50_id\": [\"UniRef50_A\", \"UniRef50_B\", \"UniRef90_A\", \"UniRef90_B\"],\n        }\n    )\n    valid_clusters.to_parquet(valid_cluster_path)\n    return train_cluster_path, valid_cluster_path\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/data/esm2/#bionemo.testing.data.esm2.create_mock_protein_dataset","title":"<code>create_mock_protein_dataset(tmp_path)</code>","text":"<p>Create a mock protein dataset.</p> Source code in <code>bionemo/testing/data/esm2.py</code> <pre><code>def create_mock_protein_dataset(tmp_path):\n    \"\"\"Create a mock protein dataset.\"\"\"\n    db_file = tmp_path / \"protein_dataset.db\"\n    conn = sqlite3.connect(str(db_file))\n    cursor = conn.cursor()\n\n    cursor.execute(\n        \"\"\"\n        CREATE TABLE protein (\n            id TEXT PRIMARY KEY,\n            sequence TEXT\n        )\n    \"\"\"\n    )\n\n    proteins = [\n        (\"UniRef90_A\", \"ACDEFGHIKLMNPQRSTVWY\"),\n        (\"UniRef90_B\", \"DEFGHIKLMNPQRSTVWYAC\"),\n        (\"UniRef90_C\", \"MGHIKLMNPQRSTVWYACDE\"),\n        (\"UniRef50_A\", \"MKTVRQERLKSIVRI\"),\n        (\"UniRef50_B\", \"MRILERSKEPVSGAQLA\"),\n    ]\n    cursor.executemany(\"INSERT INTO protein VALUES (?, ?)\", proteins)\n\n    conn.commit()\n    conn.close()\n\n    return db_file\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/data/fasta/","title":"Fasta","text":""},{"location":"main/references/API_reference/bionemo/testing/data/fasta/#bionemo.testing.data.fasta.create_fasta_file","title":"<code>create_fasta_file(fasta_file_path, num_sequences, sequence_length=None, sequence_lengths=None, repeating_dna_pattern=ALU_SEQUENCE, max_line_length=80)</code>","text":"<p>Creates a fasta file with the given number of sequences, sequence length, and repeating dna pattern. Each contig uses a shifted version of the repeating pattern.</p> Source code in <code>bionemo/testing/data/fasta.py</code> <pre><code>def create_fasta_file(\n    fasta_file_path: Path,\n    num_sequences: int,\n    sequence_length: int | None = None,\n    sequence_lengths: list[int] | None = None,\n    repeating_dna_pattern: str = ALU_SEQUENCE,\n    max_line_length: int = 80,\n) -&gt; Path:\n    \"\"\"Creates a fasta file with the given number of sequences, sequence length, and repeating dna pattern. Each contig uses a shifted version of the repeating pattern.\"\"\"\n    assert sequence_length is not None or sequence_lengths is not None\n    with open(fasta_file_path, \"w\") as f:\n        if sequence_lengths is not None:\n            assert len(sequence_lengths) == num_sequences\n        else:\n            assert sequence_length is not None\n            sequence_lengths: list[int] = [sequence_length] * num_sequences\n        for i in range(num_sequences):\n            # get the repeating pattern shifted by i for this contig\n            repeat_pattern_for_contig = repeating_dna_pattern[i:] + repeating_dna_pattern[:i]\n            # repeat the pattern enough times to reach the desired sequence length\n            if sequence_lengths[i] &lt;= len(repeat_pattern_for_contig):\n                contig_output = repeat_pattern_for_contig[: sequence_lengths[i]]\n            else:\n                # Calculate how many complete repeats we need\n                num_repeats = sequence_lengths[i] // len(repeat_pattern_for_contig)\n                remainder = sequence_lengths[i] % len(repeat_pattern_for_contig)\n                contig_output = repeat_pattern_for_contig * num_repeats + repeat_pattern_for_contig[:remainder]\n            # verify the length of the contig is as expected\n            assert len(contig_output) == sequence_lengths[i]\n            # Fold the contig output into lines of max_line_length\n            contig_output = \"\\n\".join(\n                contig_output[i : i + max_line_length] for i in range(0, sequence_lengths[i], max_line_length)\n            )\n            # write to the fasta file with the actual contig_output, not the repeating pattern\n            f.write(f\"&gt;contig_{i}\\n{contig_output}\\n\")\n    return fasta_file_path\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/data/load/","title":"Load","text":""},{"location":"main/references/API_reference/bionemo/testing/data/load/#bionemo.testing.data.load.default_ngc_client","title":"<code>default_ngc_client(use_guest_if_api_key_invalid=True)</code>","text":"<p>Create a default NGC client.</p> <p>This should load the NGC API key from ~/.ngc/config, or from environment variables passed to the docker container.</p> Source code in <code>bionemo/core/data/load.py</code> <pre><code>def default_ngc_client(use_guest_if_api_key_invalid: bool = True) -&gt; \"ngcsdk.Client\":\n    \"\"\"Create a default NGC client.\n\n    This should load the NGC API key from ~/.ngc/config, or from environment variables passed to the docker container.\n    \"\"\"\n    import ngcsdk\n\n    client = ngcsdk.Client()\n\n    try:\n        client.configure()\n\n    except ValueError as e:\n        if use_guest_if_api_key_invalid:\n            logger.error(f\"Error configuring NGC client: {e}, signing in as guest.\")\n            client = ngcsdk.Client(\"no-apikey\")\n            client.configure(\n                api_key=\"no-apikey\",  # pragma: allowlist secret\n                org_name=\"no-org\",\n                team_name=\"no-team\",\n                ace_name=\"no-ace\",\n            )\n\n        else:\n            raise\n\n    return client\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/data/load/#bionemo.testing.data.load.default_pbss_client","title":"<code>default_pbss_client()</code>","text":"<p>Create a default S3 client for PBSS.</p> Source code in <code>bionemo/core/data/load.py</code> <pre><code>def default_pbss_client():\n    \"\"\"Create a default S3 client for PBSS.\"\"\"\n    try:\n        import boto3\n    except ImportError:\n        raise ImportError(\"boto3 is required to download from PBSS.\")\n\n    retry_config = Config(retries={\"max_attempts\": 10, \"mode\": \"standard\"})\n    return boto3.client(\"s3\", endpoint_url=\"https://pbss.s8k.io\", config=retry_config)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/data/load/#bionemo.testing.data.load.load","title":"<code>load(model_or_data_tag, source=DEFAULT_SOURCE, resources=None, cache_dir=None)</code>","text":"<p>Download a resource from PBSS or NGC.</p> <p>Parameters:</p> Name Type Description Default <code>model_or_data_tag</code> <code>str</code> <p>A pointer to the desired resource. Must be a key in the resources dictionary.</p> required <code>source</code> <code>SourceOptions</code> <p>Either \"pbss\" (NVIDIA-internal download) or \"ngc\" (NVIDIA GPU Cloud). Defaults to \"pbss\".</p> <code>DEFAULT_SOURCE</code> <code>resources</code> <code>dict[str, Resource] | None</code> <p>A custom dictionary of resources. If None, the default resources will be used. (Mostly for testing.)</p> <code>None</code> <code>cache_dir</code> <code>Path | None</code> <p>The directory to store downloaded files. Defaults to BIONEMO_CACHE_DIR. (Mostly for testing.)</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the desired tag was not found, or if an NGC url was requested but not provided.</p> <p>Returns:</p> Type Description <code>Path</code> <p>A Path object pointing either at the downloaded file, or at a decompressed folder containing the</p> <code>Path</code> <p>file(s).</p> <p>Examples:</p> <p>For a resource specified in 'filename.yaml' with tag 'tag', the following will download the file:</p> <pre><code>&gt;&gt;&gt; load(\"filename/tag\")\nPosixPath(/tmp/bionemo/downloaded-file-name)\n</code></pre> Source code in <code>bionemo/core/data/load.py</code> <pre><code>def load(\n    model_or_data_tag: str,\n    source: SourceOptions = DEFAULT_SOURCE,\n    resources: dict[str, Resource] | None = None,\n    cache_dir: Path | None = None,\n) -&gt; Path:\n    \"\"\"Download a resource from PBSS or NGC.\n\n    Args:\n        model_or_data_tag: A pointer to the desired resource. Must be a key in the resources dictionary.\n        source: Either \"pbss\" (NVIDIA-internal download) or \"ngc\" (NVIDIA GPU Cloud). Defaults to \"pbss\".\n        resources: A custom dictionary of resources. If None, the default resources will be used. (Mostly for testing.)\n        cache_dir: The directory to store downloaded files. Defaults to BIONEMO_CACHE_DIR. (Mostly for testing.)\n\n    Raises:\n        ValueError: If the desired tag was not found, or if an NGC url was requested but not provided.\n\n    Returns:\n        A Path object pointing either at the downloaded file, or at a decompressed folder containing the\n        file(s).\n\n    Examples:\n        For a resource specified in 'filename.yaml' with tag 'tag', the following will download the file:\n        &gt;&gt;&gt; load(\"filename/tag\")\n        PosixPath(/tmp/bionemo/downloaded-file-name)\n    \"\"\"\n    if resources is None:\n        resources = get_all_resources()\n\n    if cache_dir is None:\n        cache_dir = BIONEMO_CACHE_DIR\n\n    if model_or_data_tag not in resources:\n        raise ValueError(f\"Resource '{model_or_data_tag}' not found.\")\n\n    if source == \"ngc\" and resources[model_or_data_tag].ngc is None:\n        raise ValueError(f\"Resource '{model_or_data_tag}' does not have an NGC URL.\")\n\n    resource = resources[model_or_data_tag]\n    filename = str(resource.pbss).split(\"/\")[-1]\n\n    extension = \"\".join(Path(filename).suffixes)\n    processor = _get_processor(extension, resource.unpack, resource.decompress)\n\n    if source == \"pbss\":\n        download_fn = _s3_download\n        url = resource.pbss\n\n    elif source == \"ngc\":\n        assert resource.ngc_registry is not None\n        download_fn = NGCDownloader(filename=filename, ngc_registry=resource.ngc_registry)\n        url = resource.ngc\n\n    else:\n        raise ValueError(f\"Source '{source}' not supported.\")\n\n    # Pooch will keep checking hashes and unpacking archives for each call,\n    # which is very time-consuming for large checkpoints. Instead, we make it\n    # do it only once by marking the resource as fully checked.\n    fname = f\"{resource.sha256}-{filename}\"\n    checked = cache_dir / (fname + \".checked\")\n    if checked.exists():\n        path = checked.read_text()\n        logger.debug(f\"Using cached {path=} from {checked=}\")\n        return Path(path)\n\n    download = pooch.retrieve(\n        url=str(url),\n        fname=fname,\n        known_hash=resource.sha256,\n        path=cache_dir,\n        downloader=download_fn,\n        processor=processor,\n    )\n\n    # Pooch by default returns a list of unpacked files if they unpack a zipped or tarred directory. Instead of that, we\n    # just want the unpacked, parent folder.\n    if isinstance(download, list):\n        path = Path(processor.extract_dir)  # type: ignore\n    else:\n        path = Path(download)\n\n    checked.write_text(str(path))\n    return path\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/data/resource/","title":"Resource","text":""},{"location":"main/references/API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource","title":"<code>Resource</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class that represents a remote resource for downloading and caching test data.</p> Source code in <code>bionemo/core/data/resource.py</code> <pre><code>class Resource(pydantic.BaseModel):\n    \"\"\"Class that represents a remote resource for downloading and caching test data.\"\"\"\n\n    model_config = pydantic.ConfigDict(use_attribute_docstrings=True)\n\n    tag: Annotated[str, pydantic.StringConstraints(pattern=r\"^[^/]*/[^/]*$\")]  # Only slash between filename and tag.\n    \"\"\"A unique identifier for the resource. The file(s) will be accessible via load(\"filename/tag\").\"\"\"\n\n    ngc: Annotated[str, pydantic.AfterValidator(_validate_ngc_resource)] | None = None\n    \"\"\"The NGC URL for the resource.\n\n    Should be in format [org/[team/]]name[:version]. If None, the resource is not available on NGC.\n    \"\"\"\n\n    ngc_registry: Literal[\"model\", \"resource\"] | None = None\n    \"\"\"The NGC resource type (model or resource) for the data. Must be provided if ngc is not None.\"\"\"\n\n    pbss: Annotated[pydantic.AnyUrl, pydantic.UrlConstraints(allowed_schemes=[\"s3\"])]\n    \"\"\"The PBSS (NVIDIA-internal) URL of the resource.\"\"\"\n\n    sha256: str | None\n    \"\"\"The SHA256 checksum of the resource. If None, the SHA will not be checked on download (not recommended).\"\"\"\n\n    owner: pydantic.NameEmail\n    \"\"\"The owner or primary point of contact for the resource, in the format \"Name &lt;email&gt;\".\"\"\"\n\n    description: str | None = None\n    \"\"\"A description of the file(s).\"\"\"\n\n    unpack: Literal[False, None] = None\n    \"\"\"Whether the resource should be unpacked after download. If None, will defer to the file extension.\"\"\"\n\n    decompress: Literal[False, None] = None\n    \"\"\"Whether the resource should be decompressed after download. If None, will defer to the file extension.\"\"\"\n\n    @pydantic.model_validator(mode=\"after\")\n    def _validate_ngc_registry(self):\n        if self.ngc and not self.ngc_registry:\n            raise ValueError(f\"ngc_registry must be provided if ngc is not None: {self.tag}\")\n        return self\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.decompress","title":"<code>decompress = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the resource should be decompressed after download. If None, will defer to the file extension.</p>"},{"location":"main/references/API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.description","title":"<code>description = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A description of the file(s).</p>"},{"location":"main/references/API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.ngc","title":"<code>ngc = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The NGC URL for the resource.</p> <p>Should be in format [org/[team/]]name[:version]. If None, the resource is not available on NGC.</p>"},{"location":"main/references/API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.ngc_registry","title":"<code>ngc_registry = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The NGC resource type (model or resource) for the data. Must be provided if ngc is not None.</p>"},{"location":"main/references/API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.owner","title":"<code>owner</code>  <code>instance-attribute</code>","text":"<p>The owner or primary point of contact for the resource, in the format \"Name \"."},{"location":"main/references/API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.pbss","title":"<code>pbss</code>  <code>instance-attribute</code>","text":"<p>The PBSS (NVIDIA-internal) URL of the resource.</p>"},{"location":"main/references/API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.sha256","title":"<code>sha256</code>  <code>instance-attribute</code>","text":"<p>The SHA256 checksum of the resource. If None, the SHA will not be checked on download (not recommended).</p>"},{"location":"main/references/API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.tag","title":"<code>tag</code>  <code>instance-attribute</code>","text":"<p>A unique identifier for the resource. The file(s) will be accessible via load(\"filename/tag\").</p>"},{"location":"main/references/API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.unpack","title":"<code>unpack = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the resource should be unpacked after download. If None, will defer to the file extension.</p>"},{"location":"main/references/API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.get_all_resources","title":"<code>get_all_resources(resource_path=None)</code>  <code>cached</code>","text":"<p>Return a dictionary of all resources.</p> Source code in <code>bionemo/core/data/resource.py</code> <pre><code>@functools.cache\ndef get_all_resources(resource_path: Path | None = None) -&gt; dict[str, Resource]:\n    \"\"\"Return a dictionary of all resources.\"\"\"\n    if not resource_path:\n        resource_path = Path(files(\"bionemo.core.data\").joinpath(\"resources\"))  # type: ignore\n\n    resources_files = itertools.chain(resource_path.glob(\"*.yaml\"), resource_path.glob(\"*.yml\"))\n\n    all_resources = [resource for file in resources_files for resource in _parse_resource_file(file)]\n\n    resource_list = pydantic.TypeAdapter(list[Resource]).validate_python(all_resources)\n    resource_dict = {resource.tag: resource for resource in resource_list}\n\n    if len(resource_dict) != len(resource_list):\n        # Show the # of and which ones are duplicated so that a user can begin debugging and resolve the issue.\n        tag_counts = Counter([resource.tag for resource in resource_list])\n        raise ValueError(f\"Duplicate resource tags found!: {[tag for tag, count in tag_counts.items() if count &gt; 1]}\")\n\n    return resource_dict\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/harnesses/mode/","title":"Mode","text":""},{"location":"main/references/API_reference/bionemo/testing/harnesses/mode/#bionemo.testing.harnesses.mode.Mode","title":"<code>Mode</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Mode for stop-go testing.</p> Source code in <code>bionemo/testing/harnesses/mode.py</code> <pre><code>class Mode(Enum):\n    \"\"\"Mode for stop-go testing.\"\"\"\n\n    STOP = auto()\n    RESUME = auto()\n    CONTINUOUS = auto()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/harnesses/stop_and_go/","title":"Stop and go","text":""},{"location":"main/references/API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness","title":"<code>StopAndGoHarness</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for testing consistency between interrupted and continuous training.</p> <p>Users should override cls.setup_model and update cls.setup_class to customize the downstream test cases. Metadata are collected through callbacks and users can add new unit tests by comparing the metadata for the interrupted and continuous cases.</p> <p>By default, learning rate, global step, optimizer state, consumed samples, input and output tensors, and loss are compared. Users can add additional metrics by adding new callbacks to <code>cls.callbacks</code> and associated test functions.</p> Stop and go tests act as follows <ul> <li>setup a clean model for a brief training run, set callbacks to track.</li> <li>interrupt training via the StopAndGoException in the callback Raise.</li> <li>train the model resumed from the checkpoint with the same set of callbacks.</li> <li>train the model continuously without interruption with a new set of the same callbacks.</li> <li>compare each pair of interrupted and continuous callbacks to check for equality.</li> </ul> Considerations when implementing this class <ul> <li>The derived test name should start with <code>Test</code>, and test methods should start with <code>test_</code> to enable pytest   discovery.</li> <li>devices, pipeline_model_parallel, and tensor_model_parallel may impact the setup of DataModule. Certain     datasets expect a known global batch size, which depends on the number of devices and conditional tensor     model parallel/ pipeline model parallel settings. By default, we are testing only on single device without     parallelism.</li> <li>'mode' is useful in some cases, but not in all cases. Implement conditions based on these when useful. As an     example, it may be useful to implement a test that stops and resumes.<ul> <li>changing callbacks to test metadata integrity (core feature of stop-and-go tests).</li> <li>changing the model construction to use different hyperparameters.</li> <li>... etc Each of the above tests cases may be useful for automated testing of various expected behavior.</li> </ul> </li> <li>stop(), resume(), continuous() or collectively run_stop_and_go() are provided methods which execute the actual   tests, leveraging the conditions in the various setup methods, respecting 'mode' where necessary.</li> </ul> <p>Attributes:</p> Name Type Description <code>root_dir</code> <p>The root directory.</p> <code>val_check_interval</code> <code>int</code> <p>The validation check interval. Stored as an attribute to ensure consistency.</p> <code>exp_name</code> <code>str</code> <p>The experiment name.</p> <code>extra_metrics_dict</code> <code>str</code> <p>A dictionary of metrics and their corresponding functions.</p> <p>See Also: bionemo.testing.callbacks.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>class StopAndGoHarness(ABC):\n    \"\"\"Abstract base class for testing consistency between interrupted and continuous training.\n\n    Users should override cls.setup_model and update cls.setup_class to customize the downstream test cases. Metadata\n    are collected through callbacks and users can add new unit tests by comparing the metadata for the interrupted and\n    continuous cases.\n\n    By default, learning rate, global step, optimizer state, consumed samples, input and output tensors, and loss are\n    compared. Users can add additional metrics by adding new callbacks to `cls.callbacks` and associated test functions.\n\n    Stop and go tests act as follows:\n        - setup a clean model for a brief training run, set callbacks to track.\n        - interrupt training via the StopAndGoException in the callback Raise.\n        - train the model resumed from the checkpoint with the same set of callbacks.\n        - train the model continuously without interruption with a new set of the same callbacks.\n        - compare each pair of interrupted and continuous callbacks to check for equality.\n\n    Considerations when implementing this class:\n        - The derived test name should start with `Test`, and test methods should start with `test_` to enable pytest\n          discovery.\n        - devices, pipeline_model_parallel, and tensor_model_parallel may impact the setup of DataModule. Certain\n            datasets expect a known global batch size, which depends on the number of devices and conditional tensor\n            model parallel/ pipeline model parallel settings. By default, we are testing only on single device without\n            parallelism.\n        - 'mode' is useful in some cases, but not in all cases. Implement conditions based on these when useful. As an\n            example, it may be useful to implement a test that stops and resumes.\n            - changing callbacks to test metadata integrity (core feature of stop-and-go tests).\n            - changing the model construction to use different hyperparameters.\n            - ... etc\n            Each of the above tests cases may be useful for automated testing of various expected behavior.\n        - stop(), resume(), continuous() or collectively run_stop_and_go() are provided methods which execute the actual\n          tests, leveraging the conditions in the various setup methods, respecting 'mode' where necessary.\n\n    Attributes:\n        root_dir: The root directory.\n        val_check_interval: The validation check interval. Stored as an attribute to ensure consistency.\n        exp_name: The experiment name.\n        extra_metrics_dict: A dictionary of metrics and their corresponding functions.\n\n    See Also: bionemo.testing.callbacks.\n    \"\"\"\n\n    # class variables that need to be overridden\n    num_steps: int\n    val_check_interval: int\n    limit_val_batches: int\n    lr: float = 1e-4\n    precision: Literal[\"16-mixed\", \"bf16-mixed\", \"32\"]\n    output_tensor_atol: float = 1e-3  # Absolute tolerance for model precision between output tensors.\n    output_tensor_rtol: float = 1e-4  # Relative tolerance for model precision between output tensors.\n\n    # class variables that will be setup in setUpClass\n    tempdir: tempfile.TemporaryDirectory\n    metadata_dir: pathlib.Path\n    exp_name: str\n    callbacks: CallbackDict\n    nemo_logger: NeMoLogger\n\n    @classmethod\n    def setup_class(cls) -&gt; None:\n        \"\"\"Sets up the class by creating a temporary directory, metadata_dir, exp_name and callbacks.\"\"\"\n        cls.tempdir = tempfile.TemporaryDirectory()\n        cls.metadata_dir = pathlib.Path(cls.tempdir.name) / \"metadata\"\n        cls.exp_name = cls.__name__\n\n        cls.callbacks = cls.get_default_callbacks()\n\n        cls.nemo_logger = NeMoLogger(\n            log_dir=cls.tempdir.name,\n            name=cls.exp_name,\n            use_datetime_version=False,\n            version=None,\n            tensorboard=None,\n            wandb=None,\n            ckpt=None,\n        )\n\n    @classmethod\n    def teardown_class(cls) -&gt; None:\n        \"\"\"Tears down the class by cleaning up the temporary directory.\"\"\"\n        cls.tempdir.cleanup()\n\n    @classmethod\n    @abstractmethod\n    def setup_model(cls, mode: Mode) -&gt; tuple[pl.LightningModule, pl.LightningDataModule, nl.MegatronOptimizerModule]:\n        \"\"\"Constructs the model, data, and optimizer for the test harness.\n\n        Optionally supports separate code paths for 'stop'/'resume'/'continuous', although implementors are encouraged\n        to use the same code path for both.\n\n        Args:\n            mode: The mode indicating whether to stop or go.\n\n        Returns:\n            tuple: A tuple containing the model, data, and optimizer.\n        \"\"\"\n        raise NotImplementedError()\n\n    @classmethod\n    def setup_trainer(\n        cls,\n        mode: Mode,\n    ) -&gt; nl.Trainer:\n        \"\"\"Setup trainer by passing stop, resume, or continuous callbacks according to mode.\n\n        Args:\n            mode (Mode): The mode indicating whether to stop, resume, or train continuously.\n\n        Returns:\n            (nl.Trainer): NeMo Lightning trainer object.\n        \"\"\"\n        strategy = MegatronStrategy(\n            ddp=\"megatron\",\n            find_unused_parameters=True,\n            ckpt_include_optimizer=True,\n            ckpt_async_save=False,\n        )\n\n        trainer = nl.Trainer(\n            devices=1,\n            max_steps=cls.num_steps,\n            accelerator=\"gpu\",\n            strategy=strategy,\n            limit_val_batches=cls.limit_val_batches,\n            val_check_interval=cls.val_check_interval,\n            log_every_n_steps=cls.val_check_interval,\n            num_nodes=1,\n            callbacks=list(cls.callbacks[mode].values()),\n            plugins=nl.MegatronMixedPrecision(precision=cls.precision),\n        )\n        return trainer\n\n    @classmethod\n    def get_default_callbacks(cls) -&gt; CallbackDict:\n        \"\"\"Returns a list of callbacks based on the specified mode. Base implementation provides reasonable defaults.\n\n        To extend this method, call the super and append to the callbacks, depending on which mode you are in:\n\n        ```python\n        callbacks = super().get_callbacks()\n        callbacks[mode][\"MyCustomCallback\"] = MyCustomCallback()\n        return callbacks\n        ```\n\n        Returns:\n            A dictionary of callbacks based on the specified mode, each of which maps a callback name to a callback\n            object.\n        \"\"\"\n        callbacks: CallbackDict = {}\n\n        def make_callbacks() -&gt; Dict[Type[pl.Callback], pl.Callback]:\n            return {\n                testing_callbacks.LearningRateCallback: testing_callbacks.LearningRateCallback(),\n                testing_callbacks.GlobalStepStateCallback: testing_callbacks.GlobalStepStateCallback(),\n                testing_callbacks.ConsumedSamplesCallback: testing_callbacks.ConsumedSamplesCallback(),\n                testing_callbacks.OptimizerStateCallback: testing_callbacks.OptimizerStateCallback(),\n                testing_callbacks.TrainInputCallback: testing_callbacks.TrainInputCallback(),\n                testing_callbacks.TrainOutputCallback: testing_callbacks.TrainOutputCallback(),\n                testing_callbacks.TrainLossCallback: testing_callbacks.TrainLossCallback(),\n                testing_callbacks.ValidInputCallback: testing_callbacks.ValidInputCallback(),\n                testing_callbacks.ValidOutputCallback: testing_callbacks.ValidOutputCallback(),\n                testing_callbacks.ValidLossCallback: testing_callbacks.ValidLossCallback(),\n            }\n\n        interrupted_callbacks = make_callbacks()\n        callbacks[Mode.CONTINUOUS] = make_callbacks()\n\n        for mode in [Mode.STOP, Mode.RESUME]:\n            consumed_samples_cls = testing_callbacks.TrainValInitConsumedSamplesStopAndGoCallback\n            callbacks[mode] = {\n                consumed_samples_cls: consumed_samples_cls(mode=mode),\n                **interrupted_callbacks,\n            }\n\n        callbacks[Mode.STOP].update(\n            {\n                testing_callbacks.StopAfterValidEpochEndCallback: testing_callbacks.StopAfterValidEpochEndCallback(),\n                nl_callbacks.ModelCheckpoint: nl_callbacks.ModelCheckpoint(\n                    save_last=True,\n                    monitor=\"val_loss\",\n                    save_top_k=2,\n                    always_save_context=True,\n                    filename=\"{epoch}-{step}-{val_loss:.2f}\",\n                ),\n            }\n        )\n\n        return callbacks\n\n    # stop() and resume() are provided methods and run the requisite methods with the appropriate mode.\n    @classmethod\n    def stop(cls) -&gt; None:\n        \"\"\"Runs pre-training and 'stops' after the first checkpoint is saved.\n\n        This method sets up the model, data, and optimizer for the Mode.STOP mode.\n        It then sets up the trainer and strategy for the Mode.STOP mode with the given metrics.\n        The training process is executed using the `llm.train` function, passing the model, data, trainer, logger, optimizer, and resume options.\n        If a `testing_callbacks.StopAndGoException` is raised during training, it is caught and no action is taken.\n\n        Raises:\n            testing_callbacks.StopAndGoException: If a stop and go exception occurs during training.\n        \"\"\"\n        logging.info(\"Running stop()...\")\n\n        model, data, opt = cls.setup_model(mode=Mode.STOP)\n        trainer = cls.setup_trainer(Mode.STOP)\n        with distributed_model_parallel_state():\n            llm.train(\n                model=model,\n                data=data,\n                trainer=trainer,\n                log=cls.nemo_logger,\n                optim=opt,\n                resume=resume.AutoResume(\n                    resume_if_exists=False,  # Looks for the -last checkpoint to continue training.\n                    resume_ignore_no_checkpoint=True,  # When false this will throw an error with no existing checkpoint.\n                ),\n            )\n\n    @classmethod\n    def resume(cls) -&gt; None:\n        \"\"\"Resumes the model from the checkpoint saved at the end of `stop()` and verifies the metadata integrity.\"\"\"\n        logging.info(\"Running resume()...\")\n\n        model, data, opt = cls.setup_model(mode=Mode.RESUME)\n        trainer = cls.setup_trainer(Mode.RESUME)\n        with distributed_model_parallel_state():\n            llm.train(\n                model=model,\n                data=data,\n                trainer=trainer,\n                log=cls.nemo_logger,\n                optim=opt,\n                resume=resume.AutoResume(\n                    resume_if_exists=True,  # Looks for the -last checkpoint to continue training.\n                    resume_ignore_no_checkpoint=False,  # When false this will throw an error with no existing checkpoint.\n                ),\n            )\n\n    @classmethod\n    def continuous(cls) -&gt; None:\n        \"\"\"Trains the model in one continuous path without stopping.\"\"\"\n        logging.info(\"Running continuous()...\")\n\n        model, data, opt = cls.setup_model(mode=Mode.CONTINUOUS)\n        trainer = cls.setup_trainer(Mode.CONTINUOUS)\n        with distributed_model_parallel_state():\n            llm.train(model=model, data=data, trainer=trainer, log=cls.nemo_logger, optim=opt)\n\n    @classmethod\n    def run_stop_and_go(cls):\n        \"\"\"Executes training both continuously and with a checkpoint interruption.\"\"\"\n        # Interrupted model training\n        cls.stop()\n        cls.resume()\n\n        # Cleanup and reinitialize the temporary directory so we don't conflict with a previous checkpoint.\n        cls.tempdir.cleanup()\n        cls.tempdir = tempfile.TemporaryDirectory()\n\n        # Continuous model training.\n        cls.continuous()\n\n    @pytest.mark.parametrize(\n        \"callback_type\",\n        [\n            testing_callbacks.LearningRateCallback,\n            testing_callbacks.GlobalStepStateCallback,\n            testing_callbacks.ConsumedSamplesCallback,\n            testing_callbacks.OptimizerStateCallback,\n            testing_callbacks.TrainInputCallback,\n            testing_callbacks.TrainOutputCallback,\n            testing_callbacks.TrainLossCallback,\n            testing_callbacks.ValidInputCallback,\n            testing_callbacks.ValidOutputCallback,\n            testing_callbacks.ValidLossCallback,\n        ],\n    )\n    def test_stop_and_go_consistency(self, callback_type):\n        \"\"\"Tests the consistency of the callback data between the interrupted and continuous checks.\"\"\"\n        interrupted_callback = get_callback(self.callbacks, Mode.RESUME, callback_type)\n        continuous_callback = get_callback(self.callbacks, Mode.CONTINUOUS, callback_type)\n        assert interrupted_callback.data, f\"No data found for {callback_type}\"\n\n        if callback_type in {testing_callbacks.TrainOutputCallback, testing_callbacks.ValidOutputCallback}:\n            atol, rtol = self.output_tensor_atol, self.output_tensor_rtol\n        else:\n            atol, rtol = 1e-4, 1e-4\n\n        recursive_assert_approx_equal(\n            interrupted_callback.data,\n            continuous_callback.data,\n            atol=atol,\n            rtol=rtol,\n        )\n\n    def test_train_val_init_consumed_samples(self):\n        \"\"\"Tests the initial consumed samples in stop-and-go scenario.\"\"\"\n        train_consumed_stop, val_consumed_stop = get_callback(\n            self.callbacks, Mode.STOP, testing_callbacks.TrainValInitConsumedSamplesStopAndGoCallback\n        ).data\n        train_consumed_go, val_consumed_go = get_callback(\n            self.callbacks, Mode.RESUME, testing_callbacks.TrainValInitConsumedSamplesStopAndGoCallback\n        ).data\n\n        assert val_consumed_stop == 0\n        assert val_consumed_go == 0\n        assert train_consumed_stop == 0\n        assert train_consumed_go &gt; 0\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.continuous","title":"<code>continuous()</code>  <code>classmethod</code>","text":"<p>Trains the model in one continuous path without stopping.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\ndef continuous(cls) -&gt; None:\n    \"\"\"Trains the model in one continuous path without stopping.\"\"\"\n    logging.info(\"Running continuous()...\")\n\n    model, data, opt = cls.setup_model(mode=Mode.CONTINUOUS)\n    trainer = cls.setup_trainer(Mode.CONTINUOUS)\n    with distributed_model_parallel_state():\n        llm.train(model=model, data=data, trainer=trainer, log=cls.nemo_logger, optim=opt)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.get_default_callbacks","title":"<code>get_default_callbacks()</code>  <code>classmethod</code>","text":"<p>Returns a list of callbacks based on the specified mode. Base implementation provides reasonable defaults.</p> <p>To extend this method, call the super and append to the callbacks, depending on which mode you are in:</p> <pre><code>callbacks = super().get_callbacks()\ncallbacks[mode][\"MyCustomCallback\"] = MyCustomCallback()\nreturn callbacks\n</code></pre> <p>Returns:</p> Type Description <code>CallbackDict</code> <p>A dictionary of callbacks based on the specified mode, each of which maps a callback name to a callback</p> <code>CallbackDict</code> <p>object.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\ndef get_default_callbacks(cls) -&gt; CallbackDict:\n    \"\"\"Returns a list of callbacks based on the specified mode. Base implementation provides reasonable defaults.\n\n    To extend this method, call the super and append to the callbacks, depending on which mode you are in:\n\n    ```python\n    callbacks = super().get_callbacks()\n    callbacks[mode][\"MyCustomCallback\"] = MyCustomCallback()\n    return callbacks\n    ```\n\n    Returns:\n        A dictionary of callbacks based on the specified mode, each of which maps a callback name to a callback\n        object.\n    \"\"\"\n    callbacks: CallbackDict = {}\n\n    def make_callbacks() -&gt; Dict[Type[pl.Callback], pl.Callback]:\n        return {\n            testing_callbacks.LearningRateCallback: testing_callbacks.LearningRateCallback(),\n            testing_callbacks.GlobalStepStateCallback: testing_callbacks.GlobalStepStateCallback(),\n            testing_callbacks.ConsumedSamplesCallback: testing_callbacks.ConsumedSamplesCallback(),\n            testing_callbacks.OptimizerStateCallback: testing_callbacks.OptimizerStateCallback(),\n            testing_callbacks.TrainInputCallback: testing_callbacks.TrainInputCallback(),\n            testing_callbacks.TrainOutputCallback: testing_callbacks.TrainOutputCallback(),\n            testing_callbacks.TrainLossCallback: testing_callbacks.TrainLossCallback(),\n            testing_callbacks.ValidInputCallback: testing_callbacks.ValidInputCallback(),\n            testing_callbacks.ValidOutputCallback: testing_callbacks.ValidOutputCallback(),\n            testing_callbacks.ValidLossCallback: testing_callbacks.ValidLossCallback(),\n        }\n\n    interrupted_callbacks = make_callbacks()\n    callbacks[Mode.CONTINUOUS] = make_callbacks()\n\n    for mode in [Mode.STOP, Mode.RESUME]:\n        consumed_samples_cls = testing_callbacks.TrainValInitConsumedSamplesStopAndGoCallback\n        callbacks[mode] = {\n            consumed_samples_cls: consumed_samples_cls(mode=mode),\n            **interrupted_callbacks,\n        }\n\n    callbacks[Mode.STOP].update(\n        {\n            testing_callbacks.StopAfterValidEpochEndCallback: testing_callbacks.StopAfterValidEpochEndCallback(),\n            nl_callbacks.ModelCheckpoint: nl_callbacks.ModelCheckpoint(\n                save_last=True,\n                monitor=\"val_loss\",\n                save_top_k=2,\n                always_save_context=True,\n                filename=\"{epoch}-{step}-{val_loss:.2f}\",\n            ),\n        }\n    )\n\n    return callbacks\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.resume","title":"<code>resume()</code>  <code>classmethod</code>","text":"<p>Resumes the model from the checkpoint saved at the end of <code>stop()</code> and verifies the metadata integrity.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\ndef resume(cls) -&gt; None:\n    \"\"\"Resumes the model from the checkpoint saved at the end of `stop()` and verifies the metadata integrity.\"\"\"\n    logging.info(\"Running resume()...\")\n\n    model, data, opt = cls.setup_model(mode=Mode.RESUME)\n    trainer = cls.setup_trainer(Mode.RESUME)\n    with distributed_model_parallel_state():\n        llm.train(\n            model=model,\n            data=data,\n            trainer=trainer,\n            log=cls.nemo_logger,\n            optim=opt,\n            resume=resume.AutoResume(\n                resume_if_exists=True,  # Looks for the -last checkpoint to continue training.\n                resume_ignore_no_checkpoint=False,  # When false this will throw an error with no existing checkpoint.\n            ),\n        )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.run_stop_and_go","title":"<code>run_stop_and_go()</code>  <code>classmethod</code>","text":"<p>Executes training both continuously and with a checkpoint interruption.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\ndef run_stop_and_go(cls):\n    \"\"\"Executes training both continuously and with a checkpoint interruption.\"\"\"\n    # Interrupted model training\n    cls.stop()\n    cls.resume()\n\n    # Cleanup and reinitialize the temporary directory so we don't conflict with a previous checkpoint.\n    cls.tempdir.cleanup()\n    cls.tempdir = tempfile.TemporaryDirectory()\n\n    # Continuous model training.\n    cls.continuous()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.setup_class","title":"<code>setup_class()</code>  <code>classmethod</code>","text":"<p>Sets up the class by creating a temporary directory, metadata_dir, exp_name and callbacks.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\ndef setup_class(cls) -&gt; None:\n    \"\"\"Sets up the class by creating a temporary directory, metadata_dir, exp_name and callbacks.\"\"\"\n    cls.tempdir = tempfile.TemporaryDirectory()\n    cls.metadata_dir = pathlib.Path(cls.tempdir.name) / \"metadata\"\n    cls.exp_name = cls.__name__\n\n    cls.callbacks = cls.get_default_callbacks()\n\n    cls.nemo_logger = NeMoLogger(\n        log_dir=cls.tempdir.name,\n        name=cls.exp_name,\n        use_datetime_version=False,\n        version=None,\n        tensorboard=None,\n        wandb=None,\n        ckpt=None,\n    )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.setup_model","title":"<code>setup_model(mode)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Constructs the model, data, and optimizer for the test harness.</p> <p>Optionally supports separate code paths for 'stop'/'resume'/'continuous', although implementors are encouraged to use the same code path for both.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Mode</code> <p>The mode indicating whether to stop or go.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[LightningModule, LightningDataModule, MegatronOptimizerModule]</code> <p>A tuple containing the model, data, and optimizer.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\n@abstractmethod\ndef setup_model(cls, mode: Mode) -&gt; tuple[pl.LightningModule, pl.LightningDataModule, nl.MegatronOptimizerModule]:\n    \"\"\"Constructs the model, data, and optimizer for the test harness.\n\n    Optionally supports separate code paths for 'stop'/'resume'/'continuous', although implementors are encouraged\n    to use the same code path for both.\n\n    Args:\n        mode: The mode indicating whether to stop or go.\n\n    Returns:\n        tuple: A tuple containing the model, data, and optimizer.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.setup_trainer","title":"<code>setup_trainer(mode)</code>  <code>classmethod</code>","text":"<p>Setup trainer by passing stop, resume, or continuous callbacks according to mode.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Mode</code> <p>The mode indicating whether to stop, resume, or train continuously.</p> required <p>Returns:</p> Type Description <code>Trainer</code> <p>NeMo Lightning trainer object.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\ndef setup_trainer(\n    cls,\n    mode: Mode,\n) -&gt; nl.Trainer:\n    \"\"\"Setup trainer by passing stop, resume, or continuous callbacks according to mode.\n\n    Args:\n        mode (Mode): The mode indicating whether to stop, resume, or train continuously.\n\n    Returns:\n        (nl.Trainer): NeMo Lightning trainer object.\n    \"\"\"\n    strategy = MegatronStrategy(\n        ddp=\"megatron\",\n        find_unused_parameters=True,\n        ckpt_include_optimizer=True,\n        ckpt_async_save=False,\n    )\n\n    trainer = nl.Trainer(\n        devices=1,\n        max_steps=cls.num_steps,\n        accelerator=\"gpu\",\n        strategy=strategy,\n        limit_val_batches=cls.limit_val_batches,\n        val_check_interval=cls.val_check_interval,\n        log_every_n_steps=cls.val_check_interval,\n        num_nodes=1,\n        callbacks=list(cls.callbacks[mode].values()),\n        plugins=nl.MegatronMixedPrecision(precision=cls.precision),\n    )\n    return trainer\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.stop","title":"<code>stop()</code>  <code>classmethod</code>","text":"<p>Runs pre-training and 'stops' after the first checkpoint is saved.</p> <p>This method sets up the model, data, and optimizer for the Mode.STOP mode. It then sets up the trainer and strategy for the Mode.STOP mode with the given metrics. The training process is executed using the <code>llm.train</code> function, passing the model, data, trainer, logger, optimizer, and resume options. If a <code>testing_callbacks.StopAndGoException</code> is raised during training, it is caught and no action is taken.</p> <p>Raises:</p> Type Description <code>StopAndGoException</code> <p>If a stop and go exception occurs during training.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\ndef stop(cls) -&gt; None:\n    \"\"\"Runs pre-training and 'stops' after the first checkpoint is saved.\n\n    This method sets up the model, data, and optimizer for the Mode.STOP mode.\n    It then sets up the trainer and strategy for the Mode.STOP mode with the given metrics.\n    The training process is executed using the `llm.train` function, passing the model, data, trainer, logger, optimizer, and resume options.\n    If a `testing_callbacks.StopAndGoException` is raised during training, it is caught and no action is taken.\n\n    Raises:\n        testing_callbacks.StopAndGoException: If a stop and go exception occurs during training.\n    \"\"\"\n    logging.info(\"Running stop()...\")\n\n    model, data, opt = cls.setup_model(mode=Mode.STOP)\n    trainer = cls.setup_trainer(Mode.STOP)\n    with distributed_model_parallel_state():\n        llm.train(\n            model=model,\n            data=data,\n            trainer=trainer,\n            log=cls.nemo_logger,\n            optim=opt,\n            resume=resume.AutoResume(\n                resume_if_exists=False,  # Looks for the -last checkpoint to continue training.\n                resume_ignore_no_checkpoint=True,  # When false this will throw an error with no existing checkpoint.\n            ),\n        )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.teardown_class","title":"<code>teardown_class()</code>  <code>classmethod</code>","text":"<p>Tears down the class by cleaning up the temporary directory.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\ndef teardown_class(cls) -&gt; None:\n    \"\"\"Tears down the class by cleaning up the temporary directory.\"\"\"\n    cls.tempdir.cleanup()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.test_stop_and_go_consistency","title":"<code>test_stop_and_go_consistency(callback_type)</code>","text":"<p>Tests the consistency of the callback data between the interrupted and continuous checks.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@pytest.mark.parametrize(\n    \"callback_type\",\n    [\n        testing_callbacks.LearningRateCallback,\n        testing_callbacks.GlobalStepStateCallback,\n        testing_callbacks.ConsumedSamplesCallback,\n        testing_callbacks.OptimizerStateCallback,\n        testing_callbacks.TrainInputCallback,\n        testing_callbacks.TrainOutputCallback,\n        testing_callbacks.TrainLossCallback,\n        testing_callbacks.ValidInputCallback,\n        testing_callbacks.ValidOutputCallback,\n        testing_callbacks.ValidLossCallback,\n    ],\n)\ndef test_stop_and_go_consistency(self, callback_type):\n    \"\"\"Tests the consistency of the callback data between the interrupted and continuous checks.\"\"\"\n    interrupted_callback = get_callback(self.callbacks, Mode.RESUME, callback_type)\n    continuous_callback = get_callback(self.callbacks, Mode.CONTINUOUS, callback_type)\n    assert interrupted_callback.data, f\"No data found for {callback_type}\"\n\n    if callback_type in {testing_callbacks.TrainOutputCallback, testing_callbacks.ValidOutputCallback}:\n        atol, rtol = self.output_tensor_atol, self.output_tensor_rtol\n    else:\n        atol, rtol = 1e-4, 1e-4\n\n    recursive_assert_approx_equal(\n        interrupted_callback.data,\n        continuous_callback.data,\n        atol=atol,\n        rtol=rtol,\n    )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.test_train_val_init_consumed_samples","title":"<code>test_train_val_init_consumed_samples()</code>","text":"<p>Tests the initial consumed samples in stop-and-go scenario.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>def test_train_val_init_consumed_samples(self):\n    \"\"\"Tests the initial consumed samples in stop-and-go scenario.\"\"\"\n    train_consumed_stop, val_consumed_stop = get_callback(\n        self.callbacks, Mode.STOP, testing_callbacks.TrainValInitConsumedSamplesStopAndGoCallback\n    ).data\n    train_consumed_go, val_consumed_go = get_callback(\n        self.callbacks, Mode.RESUME, testing_callbacks.TrainValInitConsumedSamplesStopAndGoCallback\n    ).data\n\n    assert val_consumed_stop == 0\n    assert val_consumed_go == 0\n    assert train_consumed_stop == 0\n    assert train_consumed_go &gt; 0\n</code></pre>"},{"location":"main/references/API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.get_callback","title":"<code>get_callback(callbacks, mode, callback_type)</code>","text":"<p>Returns the callback with the given name and mode.</p> <p>Convenience function to make type hinting easier.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>CallbackDict</code> <p>The dictionary of callbacks.</p> required <code>mode</code> <code>Mode</code> <p>The mode indicating whether to stop or go.</p> required <code>callback_type</code> <code>Type[Callback]</code> <p>The type of the callback.</p> required <p>Returns:</p> Type Description <code>Callback</code> <p>pl.Callback: The callback with the given name and mode.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>def get_callback(callbacks: CallbackDict, mode: Mode, callback_type: Type[Callback]) -&gt; Callback:\n    \"\"\"Returns the callback with the given name and mode.\n\n    Convenience function to make type hinting easier.\n\n    Args:\n        callbacks: The dictionary of callbacks.\n        mode: The mode indicating whether to stop or go.\n        callback_type: The type of the callback.\n\n    Returns:\n        pl.Callback: The callback with the given name and mode.\n    \"\"\"\n    return callbacks[mode][callback_type]  # type: ignore\n</code></pre>"},{"location":"main/references/API_reference/bionemo/webdatamodule/datamodule/","title":"Datamodule","text":""},{"location":"main/references/API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.PickledDataWDS","title":"<code>PickledDataWDS</code>","text":"<p>               Bases: <code>WebDataModule</code></p> <p>A LightningDataModule to process pickled data into webdataset tar files.</p> <p><code>PickledDataWDS</code> is a LightningDataModule to process pickled data into webdataset tar files and setup dataset and dataloader. This inherits the webdataset setup from its parent module <code>WebDataModule</code>. This data module takes a directory of pickled data files, data filename prefixes for train/val/test splits, data filename suffixes and prepare webdataset tar files by globbing the specific pickle data files <code>{dir_pickles}/{name_subset[split]}.{suffix_pickles}</code> and outputing to webdataset tar file with the dict structure: <pre><code>    {\"__key__\" : name.replace(\".\", \"-\"),\n     suffix_pickles : pickled.dumps(data) }\n</code></pre> NOTE: this assumes only one pickled file is processed for each sample. In its setup() function, it creates the webdataset object chaining up the input <code>pipeline_wds</code> workflow. In its train/val/test_dataloader(), it creates the WebLoader object chaining up the <code>pipeline_prebatch_wld</code> workflow.</p>"},{"location":"main/references/API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.PickledDataWDS--examples","title":"Examples:","text":"<ol> <li>create the data module with a directory of pickle files and the file name prefix thereof for different splits to used by <code>Lightning.Trainer.fit()</code></li> </ol> <pre><code>&gt;&gt;&gt; from bionemo.core.data.datamodule import Split, PickledDataWDS\n\n&gt;&gt;&gt; dir_pickles = \"/path/to/my/pickles/dir\"\n\n&gt;&gt;&gt; # the following will use `sample1.mydata.pt` and `sample2.mydata.pt` as the\n&gt;&gt;&gt; # training dataset and `sample4.mydata.pt` and `sample5.mydata.pt` as the\n&gt;&gt;&gt; # validation dataset\n\n&gt;&gt;&gt; suffix_pickles = \"mydata.pt\"\n\n&gt;&gt;&gt; names_subset = {\n&gt;&gt;&gt;     Split.train: [sample1, sample2],\n&gt;&gt;&gt;     Split.val: [sample4, sample5],\n&gt;&gt;&gt; }\n\n&gt;&gt;&gt; # the following setting will attempt to create at least 5 tar files in\n&gt;&gt;&gt; # `/path/to/output/tars/dir/myshards-00000{0-5}.tar`\n\n&gt;&gt;&gt; n_tars_wds = 5\n&gt;&gt;&gt; prefix_tars_wds = \"myshards\"\n&gt;&gt;&gt; output_dir_tar_files = {\n        Split.train : \"/path/to/output/tars/dir-train\",\n        Split.val : \"/path/to/output/tars/dir-val\",\n        Split.test : \"/path/to/output/tars/dir-test\",\n    }\n\n&gt;&gt;&gt; # user can optionally customize the data processing routines and kwargs used\n&gt;&gt;&gt; # in the WebDataset and WebLoader (see the examples in `WebDataModule`)\n\n&gt;&gt;&gt; pipeline_wds = { Split.train: ... }\n\n&gt;&gt;&gt; pipeline_prebatch_wld = { Split.train: ... }\n\n&gt;&gt;&gt; kwargs_wds = { Split.train: ..., Split.val: ... }\n\n&gt;&gt;&gt; kwargs_wld = { Split.train: ..., Split.val: ... }\n\n&gt;&gt;&gt; invoke_wds = { Split.train: ..., Split.val: ... }\n\n&gt;&gt;&gt; invoke_wld = { Split.train: ..., Split.val: ... }\n\n&gt;&gt;&gt; # create the data module\n&gt;&gt;&gt; data_module = PickledDataWDS(\n&gt;&gt;&gt;     dir_pickles,\n&gt;&gt;&gt;     names_subset,\n&gt;&gt;&gt;     suffix_pickles, # `WebDataModule` args\n&gt;&gt;&gt;     output_dir_tar_files, # `WebDataModule` args\n&gt;&gt;&gt;     n_tars_wds=n_tars_wds,\n&gt;&gt;&gt;     prefix_tars_wds=prefix_tars_wds, # `WebDataModule` kwargs\n&gt;&gt;&gt;     pipeline_wds=pipeline_wds, # `WebDataModule` kwargs\n&gt;&gt;&gt;     pipeline_prebatch_wld=pipelines_wdl_batch, # `WebDataModule` kwargs\n&gt;&gt;&gt;     kwargs_wds=kwargs_wds, # `WebDataModule` kwargs\n&gt;&gt;&gt;     kwargs_wld=kwargs_wld, # `WebDataModule` kwargs\n&gt;&gt;&gt;     invoke_wds=invoke_wds, # `WebDataModule` kwargs\n&gt;&gt;&gt;     invoke_wld=invoke_wld, # `WebDataModule` kwargs\n&gt;&gt;&gt; )\n</code></pre> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>class PickledDataWDS(WebDataModule):\n    \"\"\"A LightningDataModule to process pickled data into webdataset tar files.\n\n    `PickledDataWDS` is a LightningDataModule to process pickled data into webdataset tar files\n    and setup dataset and dataloader. This inherits the webdataset setup from its parent module\n    `WebDataModule`. This data module takes a directory of pickled data files, data filename\n    prefixes for train/val/test splits, data filename suffixes and prepare webdataset tar files\n    by globbing the specific pickle data files `{dir_pickles}/{name_subset[split]}.{suffix_pickles}`\n    and outputing to webdataset tar file with the dict structure:\n    ```\n        {\"__key__\" : name.replace(\".\", \"-\"),\n         suffix_pickles : pickled.dumps(data) }\n    ```\n    NOTE: this assumes only one pickled file is processed for each sample. In\n    its setup() function, it creates the webdataset object chaining up the input\n    `pipeline_wds` workflow. In its train/val/test_dataloader(), it creates the\n    WebLoader object chaining up the `pipeline_prebatch_wld` workflow.\n\n    Examples:\n    --------\n    1. create the data module with a directory of pickle files and the file name\n    prefix thereof for different splits to used by `Lightning.Trainer.fit()`\n\n    ```python\n    &gt;&gt;&gt; from bionemo.core.data.datamodule import Split, PickledDataWDS\n\n    &gt;&gt;&gt; dir_pickles = \"/path/to/my/pickles/dir\"\n\n    &gt;&gt;&gt; # the following will use `sample1.mydata.pt` and `sample2.mydata.pt` as the\n    &gt;&gt;&gt; # training dataset and `sample4.mydata.pt` and `sample5.mydata.pt` as the\n    &gt;&gt;&gt; # validation dataset\n\n    &gt;&gt;&gt; suffix_pickles = \"mydata.pt\"\n\n    &gt;&gt;&gt; names_subset = {\n    &gt;&gt;&gt;     Split.train: [sample1, sample2],\n    &gt;&gt;&gt;     Split.val: [sample4, sample5],\n    &gt;&gt;&gt; }\n\n    &gt;&gt;&gt; # the following setting will attempt to create at least 5 tar files in\n    &gt;&gt;&gt; # `/path/to/output/tars/dir/myshards-00000{0-5}.tar`\n\n    &gt;&gt;&gt; n_tars_wds = 5\n    &gt;&gt;&gt; prefix_tars_wds = \"myshards\"\n    &gt;&gt;&gt; output_dir_tar_files = {\n            Split.train : \"/path/to/output/tars/dir-train\",\n            Split.val : \"/path/to/output/tars/dir-val\",\n            Split.test : \"/path/to/output/tars/dir-test\",\n        }\n\n    &gt;&gt;&gt; # user can optionally customize the data processing routines and kwargs used\n    &gt;&gt;&gt; # in the WebDataset and WebLoader (see the examples in `WebDataModule`)\n\n    &gt;&gt;&gt; pipeline_wds = { Split.train: ... }\n\n    &gt;&gt;&gt; pipeline_prebatch_wld = { Split.train: ... }\n\n    &gt;&gt;&gt; kwargs_wds = { Split.train: ..., Split.val: ... }\n\n    &gt;&gt;&gt; kwargs_wld = { Split.train: ..., Split.val: ... }\n\n    &gt;&gt;&gt; invoke_wds = { Split.train: ..., Split.val: ... }\n\n    &gt;&gt;&gt; invoke_wld = { Split.train: ..., Split.val: ... }\n\n    &gt;&gt;&gt; # create the data module\n    &gt;&gt;&gt; data_module = PickledDataWDS(\n    &gt;&gt;&gt;     dir_pickles,\n    &gt;&gt;&gt;     names_subset,\n    &gt;&gt;&gt;     suffix_pickles, # `WebDataModule` args\n    &gt;&gt;&gt;     output_dir_tar_files, # `WebDataModule` args\n    &gt;&gt;&gt;     n_tars_wds=n_tars_wds,\n    &gt;&gt;&gt;     prefix_tars_wds=prefix_tars_wds, # `WebDataModule` kwargs\n    &gt;&gt;&gt;     pipeline_wds=pipeline_wds, # `WebDataModule` kwargs\n    &gt;&gt;&gt;     pipeline_prebatch_wld=pipelines_wdl_batch, # `WebDataModule` kwargs\n    &gt;&gt;&gt;     kwargs_wds=kwargs_wds, # `WebDataModule` kwargs\n    &gt;&gt;&gt;     kwargs_wld=kwargs_wld, # `WebDataModule` kwargs\n    &gt;&gt;&gt;     invoke_wds=invoke_wds, # `WebDataModule` kwargs\n    &gt;&gt;&gt;     invoke_wld=invoke_wld, # `WebDataModule` kwargs\n    &gt;&gt;&gt; )\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        dir_pickles: str,\n        names_subset: Dict[Split, List[str]],\n        *args,\n        n_tars_wds: Optional[int] = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Constructor.\n\n        Args:\n            dir_pickles: input directory of pickled data files\n            names_subset: list of filename prefix of\n                the data samples to be loaded in the dataset and dataloader for\n                each of the split\n            *args: arguments passed to the parent WebDataModule\n            n_tars_wds: attempt to create at least this number of\n                webdataset shards\n            **kwargs: arguments passed to the parent WebDataModule\n        \"\"\"\n        super().__init__(\n            *args,\n            **kwargs,\n        )\n\n        self._dir_pickles = dir_pickles\n\n        self._names_subset = names_subset\n\n        self._n_tars_wds = n_tars_wds\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"This is called only by the main process by the Lightning workflow.\n\n        Do not rely on this data module object's state update here as there is no\n        way to communicate the state update to other subprocesses. The nesting\n        `pickles_to_tars` function goes through the data name prefixes in the\n        different splits, read the corresponding pickled file and output a\n        webdataset tar archive with the dict structure: {\"__key__\" :\n        name.replace(\".\", \"-\"), suffix_pickles : pickled.dumps(data) }.\n        \"\"\"\n        for split in self._names_subset.keys():\n            # create wds shards (tar files) for train set\n            pickles_to_tars(\n                self._dir_pickles,\n                self._names_subset[split],\n                self._suffix_keys_wds,\n                self._dirs_tars_wds[split],\n                self._prefix_tars_wds,\n                min_num_shards=self._n_tars_wds,\n            )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.PickledDataWDS.__init__","title":"<code>__init__(dir_pickles, names_subset, *args, n_tars_wds=None, **kwargs)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>dir_pickles</code> <code>str</code> <p>input directory of pickled data files</p> required <code>names_subset</code> <code>Dict[Split, List[str]]</code> <p>list of filename prefix of the data samples to be loaded in the dataset and dataloader for each of the split</p> required <code>*args</code> <p>arguments passed to the parent WebDataModule</p> <code>()</code> <code>n_tars_wds</code> <code>Optional[int]</code> <p>attempt to create at least this number of webdataset shards</p> <code>None</code> <code>**kwargs</code> <p>arguments passed to the parent WebDataModule</p> <code>{}</code> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def __init__(\n    self,\n    dir_pickles: str,\n    names_subset: Dict[Split, List[str]],\n    *args,\n    n_tars_wds: Optional[int] = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Constructor.\n\n    Args:\n        dir_pickles: input directory of pickled data files\n        names_subset: list of filename prefix of\n            the data samples to be loaded in the dataset and dataloader for\n            each of the split\n        *args: arguments passed to the parent WebDataModule\n        n_tars_wds: attempt to create at least this number of\n            webdataset shards\n        **kwargs: arguments passed to the parent WebDataModule\n    \"\"\"\n    super().__init__(\n        *args,\n        **kwargs,\n    )\n\n    self._dir_pickles = dir_pickles\n\n    self._names_subset = names_subset\n\n    self._n_tars_wds = n_tars_wds\n</code></pre>"},{"location":"main/references/API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.PickledDataWDS.prepare_data","title":"<code>prepare_data()</code>","text":"<p>This is called only by the main process by the Lightning workflow.</p> <p>Do not rely on this data module object's state update here as there is no way to communicate the state update to other subprocesses. The nesting <code>pickles_to_tars</code> function goes through the data name prefixes in the different splits, read the corresponding pickled file and output a webdataset tar archive with the dict structure: {\"key\" : name.replace(\".\", \"-\"), suffix_pickles : pickled.dumps(data) }.</p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"This is called only by the main process by the Lightning workflow.\n\n    Do not rely on this data module object's state update here as there is no\n    way to communicate the state update to other subprocesses. The nesting\n    `pickles_to_tars` function goes through the data name prefixes in the\n    different splits, read the corresponding pickled file and output a\n    webdataset tar archive with the dict structure: {\"__key__\" :\n    name.replace(\".\", \"-\"), suffix_pickles : pickled.dumps(data) }.\n    \"\"\"\n    for split in self._names_subset.keys():\n        # create wds shards (tar files) for train set\n        pickles_to_tars(\n            self._dir_pickles,\n            self._names_subset[split],\n            self._suffix_keys_wds,\n            self._dirs_tars_wds[split],\n            self._prefix_tars_wds,\n            min_num_shards=self._n_tars_wds,\n        )\n</code></pre>"},{"location":"main/references/API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.Split","title":"<code>Split</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Names for each data split.</p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>class Split(Enum):\n    \"\"\"Names for each data split.\"\"\"\n\n    train = auto()\n    val = auto()\n    test = auto()\n</code></pre>"},{"location":"main/references/API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule","title":"<code>WebDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>A LightningDataModule for using webdataset tar files.</p> <p><code>WebDataModule</code> is a <code>LightningDataModule</code> for using webdataset tar files to setup PyTorch datasets and dataloaders. This data module takes as input a dictionary: Split -&gt; tar file directory and vaiours webdataset config settings. In its setup() function, it creates the webdataset object chaining up the input <code>pipeline_wds</code> workflow. In its train/val/test_dataloader(), it creates the WebLoader object chaining up the <code>pipeline_prebatch_wld</code> workflow.</p>"},{"location":"main/references/API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule--examples","title":"Examples:","text":"<ol> <li> <p>create the data module with input directory to webdataset tar files. Depending on which of the downstream Lightning.Trainer methods are called, e.g., <code>Trainer.fit()</code>, <code>Trainer.validate()</code>, <code>Trainer.test()</code> or <code>Trainer.predict()</code>, only a subset of the train, val and test splits need to be specified in the various input options to the data module:</p> </li> <li> <p><code>Trainer.fit()</code> requires the <code>train</code> and <code>val</code> splits</p> </li> <li><code>Trainer.validate()</code> requires the <code>val</code> split</li> <li><code>Trainer.test()</code> requires the <code>test</code> splits</li> <li><code>Trainer.predict()</code> requires the <code>test</code> splits</li> </ol> <p>Here is an example of constructing the data module for <code>Trainer.fit()</code>: <pre><code>&gt;&gt;&gt; from bionemo.webdatamodule.datamodule import Split, WebDataModule\n&gt;&gt;&gt;\n&gt;&gt;&gt; tar_file_prefix = \"shards\"\n&gt;&gt;&gt;\n&gt;&gt;&gt; dirs_of_tar_files = {\n&gt;&gt;&gt;     Split.train: \"/path/to/train/split/tars\",\n&gt;&gt;&gt;     Split.val: \"/path/to/val/split/tars\",\n&gt;&gt;&gt; }\n&gt;&gt;&gt;\n&gt;&gt;&gt; n_samples {\n&gt;&gt;&gt;     Split.train: 1000,\n&gt;&gt;&gt;     Split.val: 100,\n&gt;&gt;&gt; }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # this is the string to retrieve the corresponding data object from the\n&gt;&gt;&gt; # webdataset file (see\n&gt;&gt;&gt; # https://github.com/webdataset/webdataset?tab=readme-ov-file#the-webdataset-format\n&gt;&gt;&gt; # for details)\n&gt;&gt;&gt; suffix_keys_wds = \"tensor.pyd\"\n&gt;&gt;&gt;\n&gt;&gt;&gt; seed = 27193781\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Specify the routines to process the samples in the WebDataset object.\n&gt;&gt;&gt; # The routine is a generator of an Iterable of generators that are chained\n&gt;&gt;&gt; # together by nested function calling. The following is equivalent of\n&gt;&gt;&gt; # defining a overall generator of `shuffle(untuple(...))` which\n&gt;&gt;&gt; # untuples the samples and shuffles them. See webdataset's Documentation\n&gt;&gt;&gt; # for details.\n&gt;&gt;&gt; # NOTE: the `untuple` is almost always necessary due to the webdataset's\n&gt;&gt;&gt; # file parsing rule.\n&gt;&gt;&gt;\n&gt;&gt;&gt; untuple = lambda source : (sample for (sample,) in source)\n&gt;&gt;&gt;\n&gt;&gt;&gt; from webdatast import shuffle\n&gt;&gt;&gt; pipeline_wds = {\n&gt;&gt;&gt;     Split.train : [untuple, shuffle(n_samples[Split.train],\n&gt;&gt;&gt;                                     rng=random.Random(seed_rng_shfl))],\n&gt;&gt;&gt;     Split.val: untuple\n&gt;&gt;&gt; }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Similarly the user can optionally define the processing routine on the\n&gt;&gt;&gt; # WebLoader (the dataloader of webdataset).\n&gt;&gt;&gt; # NOTE: these routines by default take unbatched sample as input so the\n&gt;&gt;&gt; # user can customize their batching routines here\n&gt;&gt;&gt;\n&gt;&gt;&gt; batch = batched(local_batch_size, collation_fn=lambda\n                    list_samples : torch.vstack(list_samples))\n&gt;&gt;&gt; pipeline_prebatch_wld = {\n        Split.train: [shuffle(n_samples[Split.train],\n                              rng=random.Random(seed_rng_shfl)), batch],\n        Split.val : batch,\n        Split.test : batch\n    }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # the user can optionally specify the kwargs for WebDataset and\n&gt;&gt;&gt; # WebLoader\n&gt;&gt;&gt;\n&gt;&gt;&gt; kwargs_wds = {\n&gt;&gt;&gt;     split : {'shardshuffle' : split == Split.train,\n&gt;&gt;&gt;              'nodesplitter' : wds.split_by_node,\n&gt;&gt;&gt;              'seed' : seed_rng_shfl}\n&gt;&gt;&gt;     for split in Split\n&gt;&gt;&gt;     }\n&gt;&gt;&gt;\n&gt;&gt;&gt; kwargs_wld = {\n&gt;&gt;&gt;     split : {\"num_workers\": 2} for split in Split\n&gt;&gt;&gt;     }\n&gt;&gt;&gt;\n&gt;&gt;&gt; invoke_wds = {\n&gt;&gt;&gt;     split: [(\"with_epoch\", {\"nbatches\" : 5})] for split in Split\n&gt;&gt;&gt;     }\n&gt;&gt;&gt;\n&gt;&gt;&gt; invoke_wld = {\n&gt;&gt;&gt;     split: [(\"with_epoch\", {\"nbatches\" : 5}] for split in Split\n&gt;&gt;&gt;     }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # construct the data module\n&gt;&gt;&gt; data_module = WebDataModule(suffix_keys_wds,\n                                dirs_of_tar_files,\n                                prefix_tars_wds=tar_file_prefix,\n                                pipeline_wds=pipeline_wds,\n                                pipeline_prebatch_wld=pipeline_prebatch_wld,\n                                kwargs_wds=kwargs_wds,\n                                kwargs_wld=kwargs_wld,\n                                invoke_wds=invoke_wds,\n                                invoke_wld=invoke_wld,\n                                )\n</code></pre></p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>class WebDataModule(L.LightningDataModule):\n    \"\"\"A LightningDataModule for using webdataset tar files.\n\n    `WebDataModule` is a `LightningDataModule` for using webdataset tar files to setup PyTorch\n    datasets and dataloaders. This data module takes as input a dictionary: Split -&gt; tar file\n    directory and vaiours webdataset config settings. In its setup() function, it creates the\n    webdataset object chaining up the input `pipeline_wds` workflow. In its train/val/test_dataloader(),\n    it creates the WebLoader object chaining up the `pipeline_prebatch_wld` workflow.\n\n    Examples:\n    --------\n    1. create the data module with input directory to webdataset tar files.\n    Depending on which of the downstream Lightning.Trainer methods are called,\n    e.g., `Trainer.fit()`, `Trainer.validate()`, `Trainer.test()` or\n    `Trainer.predict()`, only a subset of the train, val and test splits need to\n    be specified in the various input options to the data module:\n\n    - `Trainer.fit()` requires the `train` and `val` splits\n    - `Trainer.validate()` requires the `val` split\n    - `Trainer.test()` requires the `test` splits\n    - `Trainer.predict()` requires the `test` splits\n\n    Here is an example of constructing the data module for `Trainer.fit()`:\n    ```python\n    &gt;&gt;&gt; from bionemo.webdatamodule.datamodule import Split, WebDataModule\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; tar_file_prefix = \"shards\"\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; dirs_of_tar_files = {\n    &gt;&gt;&gt;     Split.train: \"/path/to/train/split/tars\",\n    &gt;&gt;&gt;     Split.val: \"/path/to/val/split/tars\",\n    &gt;&gt;&gt; }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; n_samples {\n    &gt;&gt;&gt;     Split.train: 1000,\n    &gt;&gt;&gt;     Split.val: 100,\n    &gt;&gt;&gt; }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # this is the string to retrieve the corresponding data object from the\n    &gt;&gt;&gt; # webdataset file (see\n    &gt;&gt;&gt; # https://github.com/webdataset/webdataset?tab=readme-ov-file#the-webdataset-format\n    &gt;&gt;&gt; # for details)\n    &gt;&gt;&gt; suffix_keys_wds = \"tensor.pyd\"\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; seed = 27193781\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Specify the routines to process the samples in the WebDataset object.\n    &gt;&gt;&gt; # The routine is a generator of an Iterable of generators that are chained\n    &gt;&gt;&gt; # together by nested function calling. The following is equivalent of\n    &gt;&gt;&gt; # defining a overall generator of `shuffle(untuple(...))` which\n    &gt;&gt;&gt; # untuples the samples and shuffles them. See webdataset's Documentation\n    &gt;&gt;&gt; # for details.\n    &gt;&gt;&gt; # NOTE: the `untuple` is almost always necessary due to the webdataset's\n    &gt;&gt;&gt; # file parsing rule.\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; untuple = lambda source : (sample for (sample,) in source)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; from webdatast import shuffle\n    &gt;&gt;&gt; pipeline_wds = {\n    &gt;&gt;&gt;     Split.train : [untuple, shuffle(n_samples[Split.train],\n    &gt;&gt;&gt;                                     rng=random.Random(seed_rng_shfl))],\n    &gt;&gt;&gt;     Split.val: untuple\n    &gt;&gt;&gt; }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Similarly the user can optionally define the processing routine on the\n    &gt;&gt;&gt; # WebLoader (the dataloader of webdataset).\n    &gt;&gt;&gt; # NOTE: these routines by default take unbatched sample as input so the\n    &gt;&gt;&gt; # user can customize their batching routines here\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; batch = batched(local_batch_size, collation_fn=lambda\n                        list_samples : torch.vstack(list_samples))\n    &gt;&gt;&gt; pipeline_prebatch_wld = {\n            Split.train: [shuffle(n_samples[Split.train],\n                                  rng=random.Random(seed_rng_shfl)), batch],\n            Split.val : batch,\n            Split.test : batch\n        }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # the user can optionally specify the kwargs for WebDataset and\n    &gt;&gt;&gt; # WebLoader\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; kwargs_wds = {\n    &gt;&gt;&gt;     split : {'shardshuffle' : split == Split.train,\n    &gt;&gt;&gt;              'nodesplitter' : wds.split_by_node,\n    &gt;&gt;&gt;              'seed' : seed_rng_shfl}\n    &gt;&gt;&gt;     for split in Split\n    &gt;&gt;&gt;     }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; kwargs_wld = {\n    &gt;&gt;&gt;     split : {\"num_workers\": 2} for split in Split\n    &gt;&gt;&gt;     }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; invoke_wds = {\n    &gt;&gt;&gt;     split: [(\"with_epoch\", {\"nbatches\" : 5})] for split in Split\n    &gt;&gt;&gt;     }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; invoke_wld = {\n    &gt;&gt;&gt;     split: [(\"with_epoch\", {\"nbatches\" : 5}] for split in Split\n    &gt;&gt;&gt;     }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # construct the data module\n    &gt;&gt;&gt; data_module = WebDataModule(suffix_keys_wds,\n                                    dirs_of_tar_files,\n                                    prefix_tars_wds=tar_file_prefix,\n                                    pipeline_wds=pipeline_wds,\n                                    pipeline_prebatch_wld=pipeline_prebatch_wld,\n                                    kwargs_wds=kwargs_wds,\n                                    kwargs_wld=kwargs_wld,\n                                    invoke_wds=invoke_wds,\n                                    invoke_wld=invoke_wld,\n                                    )\n    ```\n\n    \"\"\"\n\n    def __init__(\n        self,\n        suffix_keys_wds: Union[str, Iterable[str]],\n        dirs_tars_wds: Dict[Split, str],\n        prefix_tars_wds: str = \"wdshards\",\n        pipeline_wds: Optional[Dict[Split, Union[Iterable[Iterable[Any]], Iterable[Any]]]] = None,\n        pipeline_prebatch_wld: Optional[Dict[Split, Union[Iterable[Iterable[Any]], Iterable[Any]]]] = None,\n        kwargs_wds: Optional[Dict[Split, Dict[str, Any]]] = None,\n        kwargs_wld: Optional[Dict[Split, Dict[str, Any]]] = None,\n        invoke_wds: Optional[Dict[Split, List[Tuple[str, Dict[str, Any]]]]] = None,\n        invoke_wld: Optional[Dict[Split, List[Tuple[str, Dict[str, Any]]]]] = None,\n    ):\n        \"\"\"Constructor.\n\n        Args:\n            suffix_keys_wds: a set of keys each\n                corresponding to a data object in the webdataset tar file\n                dictionary. The data objects of these keys will be extracted and\n                tupled for each sample in the tar files\n            dirs_tars_wds: input dictionary: Split -&gt; tar file\n                directory that contains the webdataset tar files for each split\n        Kwargs:\n            prefix_tars_wds: name prefix of the input webdataset tar\n                files. The input tar files are globbed by\n                \"{dirs_tars_wds[split]}/{prefix_tars_wds}-*.tar\"\n            pipeline_wds: a dictionary of webdatast composable, i.e.,\n                functor that maps a iterator to another iterator that\n                transforms the data sample yield from the dataset object, for\n                different splits, or an iterable to such a sequence of such\n                iterators. For example, this can be used to transform the\n                sample in the worker before sending it to the main process of\n                the dataloader\n            pipeline_prebatch_wld: a dictionary\n                of webloader composable, i.e., functor that maps a iterator to\n                another iterator that transforms the data sample yield from the\n                WebLoader object, for different splits, or an iterable to a\n                seuqnence of such iterators. For example, this can be used for\n                batching the samples. NOTE: this is applied before batching is\n                yield from the WebLoader\n            kwargs_wds: kwargs for the WebDataset.__init__()\n            kwargs_wld : kwargs for the WebLoader.__init__(), e.g., num_workers, of each split\n            invoke_wds: a dictionary of WebDataset methods to be called upon WebDataset\n                construction. These methods must return the WebDataset object itself. Examples\n                are .with_length() and .with_epoch(). These methods will be applied towards\n                the end of returning the WebDataset object, i.e., after the pipline_wds\n                have been applied. The inner list of tuples each has its first element as the\n                method name and the second element as the corresponding method's kwargs.\n            invoke_wld: a dictionary of WebLoader methods to be called upon WebLoader\n                construction. These methods must return the WebLoader object itself. Examples\n                are .with_length() and .with_epoch(). These methods will be applied towards\n                the end of returning the WebLoader object, i.e., after the pipelin_prebatch_wld\n                have been applied. The inner list of tuples each has its first element as the\n                method name and the second element as the corresponding method's kwargs.\n        \"\"\"\n        super().__init__()\n\n        self._dirs_tars_wds = dirs_tars_wds\n\n        if not isinstance(suffix_keys_wds, get_args(Union[str, Iterable])):\n            raise TypeError(\"suffix_keys_wds can only be str or Iterable[str]\")\n\n        self._suffix_keys_wds = suffix_keys_wds\n\n        self._prefix_tars_wds = prefix_tars_wds\n        self._pipeline_wds = pipeline_wds\n        self._pipeline_prebatch_wld = pipeline_prebatch_wld\n\n        self._kwargs_wld = kwargs_wld\n\n        self._kwargs_wds = kwargs_wds\n\n        self._invoke_wds = invoke_wds\n        self._invoke_wld = invoke_wld\n\n        # to be created later in setup\n        self._dataset = {}\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"This is called only by the main process by the Lightning workflow.\n\n        Do not rely on this data module object's state update here as there is no\n        way to communicate the state update to other subprocesses. Is a **no-op**.\n        \"\"\"\n        pass\n\n    def _setup_wds(self, split: Split) -&gt; wds.WebDataset:\n        \"\"\"Setup webdataset and webloader. This is called by setup().\n\n        Args:\n            split (Split): train, val or test split\n\n        Returns:\n            WebDataset\n\n        \"\"\"\n        if split not in self._dirs_tars_wds.keys():\n            raise RuntimeError(f\"_setup_wds() is called with {split} split that doesn't have the input tar dir\")\n        urls = sorted(glob.glob(f\"{self._dirs_tars_wds[split]}/{self._prefix_tars_wds}-*.tar\"))\n        kwargs = self._kwargs_wds[split] if self._kwargs_wds is not None else None\n        dataset = wds.WebDataset(urls, **(kwargs if kwargs is not None else {})).decode()\n        if isinstance(self._suffix_keys_wds, str):\n            dataset = dataset.extract_keys(f\"*.{self._suffix_keys_wds}\")\n        else:\n            dataset = dataset.extract_keys(*[f\"*.{key}\" for key in self._suffix_keys_wds])\n\n        if self._pipeline_wds is not None and self._pipeline_wds[split] is not None:\n            if isinstance(self._pipeline_wds[split], Iterable):\n                dataset = dataset.compose(*self._pipeline_wds[split])\n            else:\n                dataset = dataset.compose(self._pipeline_wds[split])\n\n        if self._invoke_wds is not None and self._invoke_wds[split] is not None:\n            for method in self._invoke_wds[split]:\n                name_method, kwargs_method = method\n                dataset = getattr(dataset, name_method)(**kwargs_method)\n        return dataset\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"This is called on all Lightning-managed nodes in a multi-node training session.\n\n        Args:\n            stage: \"fit\", \"test\" or \"predict\"\n        \"\"\"\n        if stage == \"fit\":\n            self._dataset[Split.train] = self._setup_wds(Split.train)\n            self._dataset[Split.val] = self._setup_wds(Split.val)\n        elif stage == \"validate\":\n            self._dataset[Split.val] = self._setup_wds(Split.val)\n        elif stage == \"test\":\n            self._dataset[Split.test] = self._setup_wds(Split.test)\n        elif stage == \"predict\":\n            self._dataset[Split.test] = self._setup_wds(Split.test)\n        else:\n            raise NotImplementedError(f\"Data setup with {stage=} is not implemented.\")\n\n    def _setup_dataloader(self, split: Split) -&gt; wds.WebLoader:\n        \"\"\"Setup the dataloader for the input dataset split.\n\n        Args:\n            split (Split): input split type\n\n        Returns:\n             WebLoader object\n\n        Raises:\n            ValueError if `split` doesn't correspond to a known dataset.\n        \"\"\"\n        if self._dataset[split] is None:\n            raise ValueError(\n                f\"_setup_dataloader() is called with {split} split without setting up the corresponding dataset.\"\n            )\n        dataset = self._dataset[split]\n        kwargs = self._kwargs_wld[split] if self._kwargs_wld is not None else None\n        loader = wds.WebLoader(dataset, **(kwargs if kwargs is not None else {}))\n\n        if self._pipeline_prebatch_wld is not None and self._pipeline_prebatch_wld[split] is not None:\n            if isinstance(self._pipeline_prebatch_wld[split], Iterable):\n                loader = loader.compose(*self._pipeline_prebatch_wld[split])\n            else:\n                loader = loader.compose(self._pipeline_prebatch_wld[split])\n\n        if self._invoke_wld is not None and self._invoke_wld[split] is not None:\n            for method in self._invoke_wld[split]:\n                name_method, kwargs_method = method\n                loader = getattr(loader, name_method)(**kwargs_method)\n\n        return loader\n\n    def train_dataloader(self) -&gt; wds.WebLoader:\n        \"\"\"Webdataset for the training data.\"\"\"\n        return self._setup_dataloader(Split.train)\n\n    def val_dataloader(self) -&gt; wds.WebLoader:\n        \"\"\"Webdataset for the validation data.\"\"\"\n        return self._setup_dataloader(Split.val)\n\n    def test_dataloader(self) -&gt; wds.WebLoader:\n        \"\"\"Webdataset for the test data.\"\"\"\n        return self._setup_dataloader(Split.test)\n\n    def predict_dataloader(self) -&gt; wds.WebLoader:\n        \"\"\"Alias for :func:`test_dataloader`.\"\"\"\n        return self._setup_dataloader(Split.test)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule.__init__","title":"<code>__init__(suffix_keys_wds, dirs_tars_wds, prefix_tars_wds='wdshards', pipeline_wds=None, pipeline_prebatch_wld=None, kwargs_wds=None, kwargs_wld=None, invoke_wds=None, invoke_wld=None)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>suffix_keys_wds</code> <code>Union[str, Iterable[str]]</code> <p>a set of keys each corresponding to a data object in the webdataset tar file dictionary. The data objects of these keys will be extracted and tupled for each sample in the tar files</p> required <code>dirs_tars_wds</code> <code>Dict[Split, str]</code> <p>input dictionary: Split -&gt; tar file directory that contains the webdataset tar files for each split</p> required <p>Kwargs:     prefix_tars_wds: name prefix of the input webdataset tar         files. The input tar files are globbed by         \"{dirs_tars_wds[split]}/{prefix_tars_wds}-*.tar\"     pipeline_wds: a dictionary of webdatast composable, i.e.,         functor that maps a iterator to another iterator that         transforms the data sample yield from the dataset object, for         different splits, or an iterable to such a sequence of such         iterators. For example, this can be used to transform the         sample in the worker before sending it to the main process of         the dataloader     pipeline_prebatch_wld: a dictionary         of webloader composable, i.e., functor that maps a iterator to         another iterator that transforms the data sample yield from the         WebLoader object, for different splits, or an iterable to a         seuqnence of such iterators. For example, this can be used for         batching the samples. NOTE: this is applied before batching is         yield from the WebLoader     kwargs_wds: kwargs for the WebDataset.init()     kwargs_wld : kwargs for the WebLoader.init(), e.g., num_workers, of each split     invoke_wds: a dictionary of WebDataset methods to be called upon WebDataset         construction. These methods must return the WebDataset object itself. Examples         are .with_length() and .with_epoch(). These methods will be applied towards         the end of returning the WebDataset object, i.e., after the pipline_wds         have been applied. The inner list of tuples each has its first element as the         method name and the second element as the corresponding method's kwargs.     invoke_wld: a dictionary of WebLoader methods to be called upon WebLoader         construction. These methods must return the WebLoader object itself. Examples         are .with_length() and .with_epoch(). These methods will be applied towards         the end of returning the WebLoader object, i.e., after the pipelin_prebatch_wld         have been applied. The inner list of tuples each has its first element as the         method name and the second element as the corresponding method's kwargs.</p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def __init__(\n    self,\n    suffix_keys_wds: Union[str, Iterable[str]],\n    dirs_tars_wds: Dict[Split, str],\n    prefix_tars_wds: str = \"wdshards\",\n    pipeline_wds: Optional[Dict[Split, Union[Iterable[Iterable[Any]], Iterable[Any]]]] = None,\n    pipeline_prebatch_wld: Optional[Dict[Split, Union[Iterable[Iterable[Any]], Iterable[Any]]]] = None,\n    kwargs_wds: Optional[Dict[Split, Dict[str, Any]]] = None,\n    kwargs_wld: Optional[Dict[Split, Dict[str, Any]]] = None,\n    invoke_wds: Optional[Dict[Split, List[Tuple[str, Dict[str, Any]]]]] = None,\n    invoke_wld: Optional[Dict[Split, List[Tuple[str, Dict[str, Any]]]]] = None,\n):\n    \"\"\"Constructor.\n\n    Args:\n        suffix_keys_wds: a set of keys each\n            corresponding to a data object in the webdataset tar file\n            dictionary. The data objects of these keys will be extracted and\n            tupled for each sample in the tar files\n        dirs_tars_wds: input dictionary: Split -&gt; tar file\n            directory that contains the webdataset tar files for each split\n    Kwargs:\n        prefix_tars_wds: name prefix of the input webdataset tar\n            files. The input tar files are globbed by\n            \"{dirs_tars_wds[split]}/{prefix_tars_wds}-*.tar\"\n        pipeline_wds: a dictionary of webdatast composable, i.e.,\n            functor that maps a iterator to another iterator that\n            transforms the data sample yield from the dataset object, for\n            different splits, or an iterable to such a sequence of such\n            iterators. For example, this can be used to transform the\n            sample in the worker before sending it to the main process of\n            the dataloader\n        pipeline_prebatch_wld: a dictionary\n            of webloader composable, i.e., functor that maps a iterator to\n            another iterator that transforms the data sample yield from the\n            WebLoader object, for different splits, or an iterable to a\n            seuqnence of such iterators. For example, this can be used for\n            batching the samples. NOTE: this is applied before batching is\n            yield from the WebLoader\n        kwargs_wds: kwargs for the WebDataset.__init__()\n        kwargs_wld : kwargs for the WebLoader.__init__(), e.g., num_workers, of each split\n        invoke_wds: a dictionary of WebDataset methods to be called upon WebDataset\n            construction. These methods must return the WebDataset object itself. Examples\n            are .with_length() and .with_epoch(). These methods will be applied towards\n            the end of returning the WebDataset object, i.e., after the pipline_wds\n            have been applied. The inner list of tuples each has its first element as the\n            method name and the second element as the corresponding method's kwargs.\n        invoke_wld: a dictionary of WebLoader methods to be called upon WebLoader\n            construction. These methods must return the WebLoader object itself. Examples\n            are .with_length() and .with_epoch(). These methods will be applied towards\n            the end of returning the WebLoader object, i.e., after the pipelin_prebatch_wld\n            have been applied. The inner list of tuples each has its first element as the\n            method name and the second element as the corresponding method's kwargs.\n    \"\"\"\n    super().__init__()\n\n    self._dirs_tars_wds = dirs_tars_wds\n\n    if not isinstance(suffix_keys_wds, get_args(Union[str, Iterable])):\n        raise TypeError(\"suffix_keys_wds can only be str or Iterable[str]\")\n\n    self._suffix_keys_wds = suffix_keys_wds\n\n    self._prefix_tars_wds = prefix_tars_wds\n    self._pipeline_wds = pipeline_wds\n    self._pipeline_prebatch_wld = pipeline_prebatch_wld\n\n    self._kwargs_wld = kwargs_wld\n\n    self._kwargs_wds = kwargs_wds\n\n    self._invoke_wds = invoke_wds\n    self._invoke_wld = invoke_wld\n\n    # to be created later in setup\n    self._dataset = {}\n</code></pre>"},{"location":"main/references/API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Alias for :func:<code>test_dataloader</code>.</p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def predict_dataloader(self) -&gt; wds.WebLoader:\n    \"\"\"Alias for :func:`test_dataloader`.\"\"\"\n    return self._setup_dataloader(Split.test)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule.prepare_data","title":"<code>prepare_data()</code>","text":"<p>This is called only by the main process by the Lightning workflow.</p> <p>Do not rely on this data module object's state update here as there is no way to communicate the state update to other subprocesses. Is a no-op.</p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"This is called only by the main process by the Lightning workflow.\n\n    Do not rely on this data module object's state update here as there is no\n    way to communicate the state update to other subprocesses. Is a **no-op**.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"main/references/API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>This is called on all Lightning-managed nodes in a multi-node training session.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>\"fit\", \"test\" or \"predict\"</p> required Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"This is called on all Lightning-managed nodes in a multi-node training session.\n\n    Args:\n        stage: \"fit\", \"test\" or \"predict\"\n    \"\"\"\n    if stage == \"fit\":\n        self._dataset[Split.train] = self._setup_wds(Split.train)\n        self._dataset[Split.val] = self._setup_wds(Split.val)\n    elif stage == \"validate\":\n        self._dataset[Split.val] = self._setup_wds(Split.val)\n    elif stage == \"test\":\n        self._dataset[Split.test] = self._setup_wds(Split.test)\n    elif stage == \"predict\":\n        self._dataset[Split.test] = self._setup_wds(Split.test)\n    else:\n        raise NotImplementedError(f\"Data setup with {stage=} is not implemented.\")\n</code></pre>"},{"location":"main/references/API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Webdataset for the test data.</p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def test_dataloader(self) -&gt; wds.WebLoader:\n    \"\"\"Webdataset for the test data.\"\"\"\n    return self._setup_dataloader(Split.test)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Webdataset for the training data.</p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def train_dataloader(self) -&gt; wds.WebLoader:\n    \"\"\"Webdataset for the training data.\"\"\"\n    return self._setup_dataloader(Split.train)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Webdataset for the validation data.</p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def val_dataloader(self) -&gt; wds.WebLoader:\n    \"\"\"Webdataset for the validation data.\"\"\"\n    return self._setup_dataloader(Split.val)\n</code></pre>"},{"location":"main/references/API_reference/bionemo/webdatamodule/utils/","title":"Utils","text":""},{"location":"main/references/API_reference/bionemo/webdatamodule/utils/#bionemo.webdatamodule.utils.pickles_to_tars","title":"<code>pickles_to_tars(dir_input, input_prefix_subset, input_suffix, dir_output, output_prefix, func_output_data=lambda prefix, suffix_to_data: {'__key__': prefix, None: suffix_to_data}, min_num_shards=None)</code>","text":"<p>Convert a subset of pickle files from a directory to Webdataset tar files.</p> <p>Input path and name pattern for sample 0: f\"{dir_input}/{input_prefix_subset[0]}.{input_suffix[0]}\" f\"{dir_input}/{input_prefix_subset[0]}.{input_suffix[1]}\" Input path and name pattern for sample 1: f\"{dir_input}/{input_prefix_subset[1]}.{input_suffix[0]}\" f\"{dir_input}/{input_prefix_subset[1]}.{input_suffix[1]}\" ... Output path and name pattern: f\"{dir_output}/{output_prefix}-%06d.tar\".</p> <p>The webdataset tar archive is specified by the dictionary: {     \"key\" : sample_filename_preifx,     sample_filename_suffix_1 : data_1,     sample_filename_suffix_2 : data_2,     ... } so that parsing the tar archive is equivalent of reading {sample_filename_preifx}.{sample_filename_suffix_1} etc.</p> <p>Here, each sample data get its name prefix from one element of <code>input_prefix_subset</code> and its name suffixes from the list <code>input_suffix</code>. Per the webdataset file format specification, the <code>sample_filename_preifx</code> can't contain dots '.' so this function removes it for the user by calling .replace(\".\", \"-\") on the elements of <code>input_prefix_subset</code></p> <p>Parameters:</p> Name Type Description Default <code>dir_input</code> <code>str</code> <p>Input directory</p> required <code>input_prefix_subset</code> <code>List[str]</code> <p>Input subset of pickle files' prefix</p> required <code>input_suffix</code> <code>Union[str, Iterable[str]]</code> <p>Input pickle file name suffixes, each for one type of data object, for all the samples</p> required <code>dir_output</code> <code>str</code> <p>Output directory</p> required <code>output_prefix</code> <code>str</code> <p>Output tar file name prefix</p> required <code>func_output_data</code> <code>Callable[[str, Dict[str, Any]], Dict[str, Any]]</code> <p>function that maps the name prefix, name suffix and data object to a webdataset tar archive dictionary. Refer to the webdataset github repo for the archive file format specification.</p> <code>lambda prefix, suffix_to_data: {'__key__': prefix, None: suffix_to_data}</code> <code>min_num_shards </code> <p>create at least this number of tar files. WebDataset has bugs when reading small number of tar files in a multi-node lightening + DDP setting so this option can be used to guarantee the tar file counts</p> required Source code in <code>bionemo/webdatamodule/utils.py</code> <pre><code>def pickles_to_tars(\n    dir_input: str,\n    input_prefix_subset: List[str],\n    input_suffix: Union[str, Iterable[str]],\n    dir_output: str,\n    output_prefix: str,\n    func_output_data: Callable[[str, Dict[str, Any]], Dict[str, Any]] = lambda prefix, suffix_to_data: {\n        \"__key__\": prefix,\n        **suffix_to_data,\n    },\n    min_num_shards: Optional[int] = None,\n) -&gt; None:\n    \"\"\"Convert a subset of pickle files from a directory to Webdataset tar files.\n\n    Input path and name pattern for sample 0:\n    f\"{dir_input}/{input_prefix_subset[0]}.{input_suffix[0]}\"\n    f\"{dir_input}/{input_prefix_subset[0]}.{input_suffix[1]}\"\n    Input path and name pattern for sample 1:\n    f\"{dir_input}/{input_prefix_subset[1]}.{input_suffix[0]}\"\n    f\"{dir_input}/{input_prefix_subset[1]}.{input_suffix[1]}\"\n    ...\n    Output path and name pattern:\n    f\"{dir_output}/{output_prefix}-%06d.tar\".\n\n    The webdataset tar archive is specified by the dictionary:\n    {\n        \"__key__\" : sample_filename_preifx,\n        sample_filename_suffix_1 : data_1,\n        sample_filename_suffix_2 : data_2,\n        ...\n    }\n    so that parsing the tar archive is equivalent of reading\n    {sample_filename_preifx}.{sample_filename_suffix_1} etc.\n\n    Here, each sample data get its name prefix from one element of\n    `input_prefix_subset` and its name suffixes from the list `input_suffix`.\n    Per the webdataset file format specification, the `sample_filename_preifx`\n    can't contain dots '.' so this function removes it for the user by calling\n    .replace(\".\", \"-\") on the elements of `input_prefix_subset`\n\n    Args:\n        dir_input: Input directory\n        input_prefix_subset: Input subset of pickle files' prefix\n        input_suffix: Input pickle file name\n            suffixes, each for one type of data object, for all the samples\n        dir_output: Output directory\n        output_prefix: Output tar file name prefix\n        func_output_data: function that maps the name prefix, name suffix and\n            data object to a webdataset tar archive dictionary. Refer to the webdataset\n            github repo for the archive file format specification.\n        min_num_shards : create at least this number of tar files.\n            WebDataset has bugs when reading small number of tar files in a\n            multi-node lightening + DDP setting so this option can be used to\n            guarantee the tar file counts\n    \"\"\"\n    if not isinstance(input_suffix, get_args(Union[str, Iterable])):\n        raise TypeError(\"input_suffix can only be str or Iterable[str]\")\n    os.makedirs(dir_output, exist_ok=True)\n    wd_subset_pattern = os.path.join(dir_output, f\"{output_prefix}-%06d.tar\")\n    n_samples_per_shard_max = 100000\n    if min_num_shards is not None:\n        if min_num_shards &lt;= 0:\n            raise ValueError(f\"Invalid min_num_shards = {min_num_shards} &lt;= 0\")\n        n_samples_per_shard_max = len(input_prefix_subset) // min_num_shards\n    with wds.ShardWriter(\n        wd_subset_pattern,\n        encoder=False,\n        maxcount=n_samples_per_shard_max,\n        compress=False,\n        mode=0o777,\n    ) as sink:\n        for name in input_prefix_subset:\n            try:\n                if isinstance(input_suffix, str):\n                    suffix_to_data = {input_suffix: (Path(dir_input) / f\"{name}.{input_suffix}\").read_bytes()}\n                else:\n                    suffix_to_data = {\n                        suffix: (Path(dir_input) / f\"{name}.{suffix}\").read_bytes() for suffix in input_suffix\n                    }\n                # the prefix name shouldn't contain any \".\" per webdataset's\n                # specification\n                sample = func_output_data(name.replace(\".\", \"-\"), suffix_to_data)\n                sink.write(sample)\n            except ModuleNotFoundError as e:\n                raise RuntimeError(\n                    \"Can't process pickle file due to\\\n                                   missing dependencies\"\n                ) from e\n            except Exception as e:\n                raise RuntimeError(f\"Failed to write {name} into tar files.\") from e\n</code></pre>"},{"location":"models/","title":"BioNeMo Framework: Available Models","text":"<p>State-of-the-art models are continually integrated into the BioNeMo Framework. The BioNeMo Framework currently offers the following pre-trained models:</p> Model Modality Uses Training Location AMPLIFY Protein Representation Learning bionemo-recipes ESM-2 Protein Representation Learning bionemo-recipes Evo2 DNA Generative AI sub-packages (5D parallel) Geneformer Single Cell Representation Learning bionemo-recipes <p>For more information about the models included in BioNeMo Framework, refer to the Model Cards linked in the table above or the original publications referenced in the respective model descriptions.</p>"},{"location":"models/amplify/","title":"Amplify","text":""},{"location":"models/amplify/#model-overview","title":"Model Overview","text":""},{"location":"models/amplify/#description","title":"Description","text":"<p>A NeMo and Megatron-LM compatible version of the AMPLIFY model, a protein language model variant of ESM-2 with modified layer structure and dataset construction. The model is designed for protein sequence understanding and prediction tasks, with variants available at 120M and 350M parameter sizes.</p> <p>This model is ready for commercial use.</p>"},{"location":"models/amplify/#third-party-community-consideration","title":"Third-Party Community Consideration","text":"<p>This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party's requirements for this application and use case; see the Chandar Lab website.</p>"},{"location":"models/amplify/#references","title":"References","text":"<p>[1] Protein Language Models: Is Scaling Necessary? Quentin Fournier, Robert M. Vernon, Almer van der Sloot, Benjamin Schulz, Sarath Chandar, Christopher James Langmead bioRxiv 2024.09.23.614603; doi: https://doi.org/10.1101/2024.09.23.614603</p>"},{"location":"models/amplify/#model-architecture","title":"Model Architecture","text":"<p>Architecture Type: Transformer</p> <p>Network Architecture: ESM-2 variant with modified layer structure</p>"},{"location":"models/amplify/#input","title":"Input","text":"<p>Input Type(s): Text (Protein Sequences)</p> <p>Input Format(s): String</p> <p>Input Parameters: 1D</p> <p>Other Properties Related to Input: Protein sequence represented as a string of canonical amino acids.</p>"},{"location":"models/amplify/#output","title":"Output","text":"<p>Output Type(s): Embeddings (Amino acid and sequence-level)</p> <p>Output Format: Numeric vector</p> <p>Output Parameters: 1D</p> <p>Other Properties Related to Output: Numeric vector with floating-point values corresponding to an embedding for each amino acid in the input protein sequence.</p>"},{"location":"models/amplify/#software-integration","title":"Software Integration","text":"<p>Runtime Engine(s):</p> <ul> <li>BioNeMo, NeMo, Megatron-LM</li> </ul> <p>Supported Hardware Microarchitecture Compatibility:</p> <ul> <li>NVIDIA Ampere</li> <li>NVIDIA Hopper</li> </ul> <p>[Preferred/Supported] Operating System(s):</p> <ul> <li>Linux</li> </ul>"},{"location":"models/amplify/#model-versions","title":"Model Versions","text":"<p>The model is fully compatible with weights distributed via HuggingFace, i.e., chandar-lab/AMPLIFY_120M. To initialize a NeMo version of AMPLIFY from a HuggingFace tag, use the provided <code>HFAMPLIFYImporter</code>:</p> <pre><code>module = biobert_lightning_module(config=AMPLIFYConfig())\nio.import_ckpt(module, f\"hf://chandar-lab/AMPLIFY_120M\", tmp_path / \"nemo_checkpoint\")\n</code></pre>"},{"location":"models/amplify/#training-evaluation","title":"Training &amp; Evaluation","text":""},{"location":"models/amplify/#training-dataset","title":"Training Dataset","text":"<p>The model was trained on a curated dataset of protein sequences following similar principles to ESM-2's training data. For more details on the training dataset, see the original AMPLIFY paper.</p>"},{"location":"models/amplify/#inference","title":"Inference","text":"<p>Engine: BioNeMo, NeMo</p> <p>Test Hardware:</p> <ul> <li>NVIDIA H100</li> </ul>"},{"location":"models/amplify/#license","title":"License","text":"<p>AMPLIFY is provided under the Apache 2.0 license.</p>"},{"location":"models/amplify/#pre-training-performance","title":"Pre-training Performance","text":"Example pre-training commands <pre><code>=== \"120M\"\n\n    ```\n    python /workspace/bionemo-framework/sub-packages/bionemo-amplify/src/bionemo/amplify/train_amplify.py \\\n        ...\n        --num-nodes=2 \\\n        --devices=8 \\\n        --min-seq-length 512 \\\n        --max-seq-length 512 \\\n        --num-layers 24 \\\n        --num-attention-heads 10 \\\n        --hidden-size 640 \\\n        --ffn-hidden-size 2560 \\\n        --micro-batch-size 256\n    ```\n\n=== \"350M\"\n\n    ```\n    python /workspace/bionemo-framework/sub-packages/bionemo-amplify/src/bionemo/amplify/train_amplify.py \\\n        ...\n        --num-nodes=4 \\\n        --devices=8 \\\n        --min-seq-length 512 \\\n        --max-seq-length 512 \\\n        --num-layers 32 \\\n        --num-attention-heads 15 \\\n        --hidden-size 960 \\\n        --ffn-hidden-size 3840 \\\n        --micro-batch-size 128\n    ```\n</code></pre> Model Size GPUs Batch Size (per GPU) Training Step Time (s) 120M 16 x NVIDIA H100 256 0.461 350M 32 x NVIDIA H100 128 0.525"},{"location":"models/amplify/#model-convergence","title":"Model Convergence","text":"<p>Model convergence curves are shown below for the 120M and 350M models, trained on the chandar-lab/UR100P dataset for 1M steps.</p> <p></p> <p></p> <p></p>"},{"location":"models/amplify/#final-perplexities-by-model-size","title":"Final Perplexities by Model Size","text":"Model Size Perplexity at 1M Steps 120M 4.23 350M 3.05"},{"location":"models/evo2/","title":"Evo2","text":""},{"location":"models/evo2/#model-overview","title":"Model Overview","text":""},{"location":"models/evo2/#description","title":"Description:","text":"<p>Evo 2 is a genomic foundation model that enables prediction and generation tasks from the molecular to genome scale. At 40 billion parameters, the model understands the genetic code for all domains of life and is the largest AI model for biology to date. Evo 2 was trained on a dataset of nearly 9 trillion nucleotides.</p> <p>This model is ready for commercial use.</p>"},{"location":"models/evo2/#third-party-community-consideration","title":"Third-Party Community Consideration","text":"<p>This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party's requirements for this application and use case; see Arc Institute website.</p>"},{"location":"models/evo2/#licenseterms-of-use","title":"License/Terms of Use:","text":"<p>GOVERNING TERMS: The NIM container is governed by the NVIDIA Software License Agreement and Product-Specific Terms for AI Products. Use of this model is governed by the NVIDIA Open Model License Agreement. ADDITIONAL INFORMATION: Apache 2.0 License.</p> <p>You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws.</p>"},{"location":"models/evo2/#deployment-geography","title":"Deployment Geography:","text":"<p>Global</p>"},{"location":"models/evo2/#use-case","title":"Use Case:","text":"<p>Evo is able to perform zero-shot function prediction for genes. Evo also can perform multi-element generation tasks, such as generating synthetic CRISPR-Cas molecular complexes. Evo 2 can also predict gene essentiality at nucleotide resolution and can generate coding-rich sequences up to 1 million in length. Advances in multi-modal and multi-scale learning with Evo provides a promising path toward improving our understanding and control of biology across multiple levels of complexity.</p>"},{"location":"models/evo2/#release-date","title":"Release Date:","text":"<p>2/19/2025</p>"},{"location":"models/evo2/#acknowledgements","title":"Acknowledgements:","text":"<pre><code>@article{nguyen2024sequence,\n   author = {Eric Nguyen and Michael Poli and Matthew G. Durrant and Brian Kang and Dhruva Katrekar and David B. Li and Liam J. Bartie and Armin W. Thomas and Samuel H. King and Garyk Brixi and Jeremy Sullivan and Madelena Y. Ng and Ashley Lewis and Aaron Lou and Stefano Ermon and Stephen A. Baccus and Tina Hernandez-Boussard and Christopher R\u00e9 and Patrick D. Hsu and Brian L. Hie },\n   title = {Sequence modeling and design from molecular to genome scale with Evo},\n   journal = {Science},\n   volume = {386},\n   number = {6723},\n   pages = {eado9336},\n   year = {2024},\n   doi = {10.1126/science.ado9336},\n   URL = {https://www.science.org/doi/abs/10.1126/science.ado9336},\n}\n</code></pre> <pre><code>@article {merchant2024semantic,\n   author = {Merchant, Aditi T and King, Samuel H and Nguyen, Eric and Hie, Brian L},\n   title = {Semantic mining of functional de novo genes from a genomic language model},\n   year = {2024},\n   doi = {10.1101/2024.12.17.628962},\n   publisher = {Cold Spring Harbor Laboratory},\n   URL = {https://www.biorxiv.org/content/early/2024/12/18/2024.12.17.628962},\n   journal = {bioRxiv}\n}\n</code></pre>"},{"location":"models/evo2/#model-architecture","title":"Model Architecture:","text":"<p>Architecture Type: Generative Neural Network</p> <p>Network Architecture: StripedHyena</p>"},{"location":"models/evo2/#input","title":"Input:","text":"<p>Input Type(s): Text</p> <p>Input Format(s): DNA Sequence (String)</p> <p>Input Parameters: One-Dimensional (1D)</p>"},{"location":"models/evo2/#output","title":"Output:","text":"<p>Output Type(s): Text</p> <p>Output Format: DNA Sequence (String)</p> <p>Output Parameters: One-Dimensional (1D)</p>"},{"location":"models/evo2/#software-integration","title":"Software Integration:","text":"<p>Runtime Engine(s):</p> <ul> <li>PyTorch</li> <li>Transformer Engine</li> </ul> <p>Supported Hardware Microarchitecture Compatibility:</p> <ul> <li>NVIDIA Hopper</li> </ul> <p>[Preferred/Supported] Operating System(s):</p> <ul> <li>Linux</li> </ul>"},{"location":"models/evo2/#model-versions","title":"Model Versions:","text":"<p>The following model versions are available to download through our CLI:</p> <ul> <li><code>evo2/1b-8k:1.0</code> is a NeMo2 format model converted from arcinstitute/savanna_evo2_1b_base which is a 1B parameter   Evo2 model pre-trained on 8K context genome data.</li> <li><code>evo2/1b-8k-bf16:1.0</code> is a fine-tuned variant of <code>evo2/1b-8k:1.0</code> that performs well with BF16 precision.</li> <li><code>evo2/7b-8k:1.0</code> is a NeMo2 format model converted from arcinstitute/savanna_evo2_7b_base   which is a 7B parameter Evo2 model pre-trained on 8K context genome data.</li> <li><code>evo2/7b-1m:1.0</code> is a NeMo2 format model converted from arcinstitute/savanna_evo2_7b which is a   7B parameter Evo2 model further fine-tuned from arcinstitute/savanna_evo2_7b_base to support 1M context   lengths.</li> </ul> <p>The following Savanna format checkpoints are also available on HuggingFace and may be converted to NeMo2 format following the steps in our Evo2 fine-tuning example notebook:</p> <ul> <li>arcinstitute/savanna_evo2_40b_base - A 40B model trained   on 8K context data.</li> <li>arcinstitute/savanna_evo2_40b - A 40B model fine-tuned from arcinstitute/savanna_evo2_40b_base   to support 1M context data.</li> </ul>"},{"location":"models/evo2/#training-testing-and-evaluation-datasets","title":"Training, Testing, and Evaluation Datasets:","text":"<p>Multiple datasets were used for training, testing and evaluation (see details below).</p> <p>OpenGenome Link: Sequence modeling and design from molecular to genome scale with Evo Data Collection Method: Automatic/Sensors/Human Labeling Method by dataset: Automatic The previously published OpenGenome dataset was used in its entirety as part of the training data for this study. This included representative prokaryotic genomes available through GTDB release v214.1, and curated phage and plasmid sequences retrieved through IMG/VR and IMG/PR.</p> <p>Updated GTDB prokaryotic genomes Link: GTDB: an ongoing census of bacterial and archaeal diversity through a phylogenetically consistent, rank normalized and complete genome-based taxonomy Data Collection Method: Automatic/Sensors/Human Labeling Method by dataset: Automatic New prokaryotic reference genomes made available through the GTDB release 220.0 update were added to the training data for this study. New genomes were identified by selecting all species' reference genomes that had no previously published (release 214.1) genomes within their species cluster, resulting in 28,174 additional prokaryotic genomes.</p> <p>NCBI Eukaryotic reference genomes Link: Mash: fast genome and metagenome distance estimation using MinHash Data Collection Method: Automatic/Sensors/Human Labeling Method by dataset: Automatic All available eukaryotic reference genomes were downloaded from NCBI on 05/32/2024, excluding atypical genomes, metagenome-assembled genomes, and genomes from large multi-isolate projects. This resulted in 16,704 genomes including an estimated ~10.7 trillion nucleotides. Only contigs that were annotated as 'Primary Assembly', 'non-nuclear', or 'aGasCar1.hap1' (an aberrant annotation that applied only to GCA_027917425.1) were retained. Mash sketch was run on each individual genome with the flag \"-s 10000\" and the mash distance was calculated between all genomes as an estimate for their pairwise 1-ANI (average nucleotide identity). All genomes with a mash distance &lt; 0.01 were joined with edges in a graph, and clusters were identified by finding connected components. One representative genome per cluster was chosen, prioritizing genomes with a higher assembly level and genomes with longer total sequence length. This clustering resulted in 15,148 candidate genomes. Genomes were further filtered by removing ambiguous nucleotides at the termini of each contig, by removing regions annotated as \"centromere\" in an available GFF file, and by removing contigs that were less than 10 kb in total length. Finally, contigs that were composed of more than 5% ambiguous nucleotides were removed. This final filtered set included 15,032 genomes and 6.98 trillion nucleotides.</p> <p>Bridge Metagenomic Data Link: Bridge RNAs direct programmable recombination of target and donor DNA Data Collection Method: Automatic/Sensors/Human Labeling Method by dataset: Automatic A previously described metagenomics dataset was further curated as part of the training data. This included 41,253 metagenomes and metagenome-assembled genomes from NCBI, JGI IMG, MGnify, MG-RAST, Tara Oceans samples, and Youngblut et al. animal gut metagenomes. All contigs were split at consecutive stretches of ambiguous nucleotides of length 5 bp or longer, the split contigs were filtered by a minimum sequence length of 1 kb, and only contigs with at least one open reading frame as predicted by prodigal were kept. Contig-encoded proteins were previously clustered at 90% identity using MMseqs. To further remove redundant sequences, contigs were sorted by descending length, and each contig was only retained if at least 90% of its respective protein clusters were not already in the sequence collection (determined using a bloom filter).</p> <p>NCBI Organelle Link: NCBI Organelle Genome Data Package Data Collection Method: Automatic/Sensors/Human Labeling Method by dataset: Automatic Eukaryotic organelle genomes: (at the time of data query) 33,457 organelle genomes were identified and downloaded using the \"NCBI Organelle\" web resource. Ambiguous nucleotides at the terminal ends of the organelle genome sequences were removed. Sequences that had over 25 ambiguous nucleotides were removed. This resulted in 32,241 organelle genomes that were used for training, including 1,613 mitochondria, 12,856 chloroplasts, 1,751 plastids, 18 apicoplasts, 1 cyanelle, and 1 kinetoplast.</p>"},{"location":"models/evo2/#inference","title":"Inference:","text":"<p>Engine: PyTorch, Transformer Engine</p> <p>Test Hardware:</p> <p>Evo2 NIM:</p> <ul> <li>H200 (1 and 2 GPU configurations, 144 GB each)</li> <li>H100 (2 GPU configuration, 80 GB each)</li> </ul> <p>BioNeMo Framework:</p> <ul> <li>A100 (1, 8, ..., 1024 GPU configurations)</li> <li>H100 (1, 8, ..., 2048 GPU configurations)</li> <li>A6000 (1, 2 GPU configurations (standard developer environment))</li> <li>A5880 (1, 2 GPU configurations (alternative developer environment))</li> <li>L40S (1 GPU configuration (brev.dev standard configuration))</li> </ul>"},{"location":"models/evo2/#ethical-considerations","title":"Ethical Considerations:","text":"<p>NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.</p> <p>Users are responsible for ensuring the physical properties of model-generated molecules are appropriately evaluated and comply with applicable safety regulations and ethical standards.</p> <p>Please report security vulnerabilities or NVIDIA AI Concerns here.</p>"},{"location":"models/evo2/#benchmarking","title":"Benchmarking","text":""},{"location":"models/evo2/#performance-vs-context-length","title":"Performance vs Context Length","text":"<p>With the current implementation of Evo2, we do not have the heavily optimized kernels in place for convolution operators like we do for attention layers in a model like Llama 2. Even with this shortcoming, we see that the benefit from including more convolutional layers makes up for the earlier stage of optimization at around the 64K context length. Beyond that point we see an improvement in performance even compared to a highly optimized transformer model.</p> <p></p> <p>Comparing model sizes, our benchmarks show the 7B variant processes approximately 4.9x more tokens per step than the 40B variant across tested configurations. When scaling from 8K to 1M sequence length, throughput decreases as expected, with the 7B model processing 9.7x fewer tokens per step and the 40B model processing 8.9x fewer tokens per step at the longer context length:</p> <p></p>"},{"location":"models/evo2/#performance-vs-cluster-size","title":"Performance vs Cluster Size","text":"<p>Performance scales linearly with a very small overhead on a cluster with fast interconnects. </p>"},{"location":"models/evo2/#accuracy","title":"Accuracy","text":""},{"location":"models/evo2/#zeroshot-brca1-vep","title":"Zeroshot BRCA1 VEP","text":"<p>To evaluate Evo 2's accuracy, we replicated Arc's zero-shot variant effect prediction experiment on the BRCA1 gene using the Findlay et al. (2018) dataset of 3,893 SNVs. The experiment tests the model's ability to predict if single nucleotide variants disrupt protein function (potentially increasing cancer risk) by analyzing experimentally determined function scores that categorize variants as LOF, INT, or FUNC based on their degree of functional disruption.</p> <p>Evo 2 is used to score the likelihood probabilities of both reference and variant sequences for each single nucleotide variant:</p> <p></p> <p>Performance evaluation across multiple Evo 2 model variants was conducted by computing likelihood scores for reference and variant sequences of each single nucleotide variant (SNV), with AUROC scores shown in the following table:</p> Model AUROC Arc Evo 2 1B 0.73 BioNeMo Evo 2 1B 0.76 BioNeMo Evo 2 7B 0.87"},{"location":"models/evo2/#training-diagnostics","title":"Training diagnostics","text":""},{"location":"models/evo2/#7b-training-equivalence-with-nv-model-variant","title":"7b training equivalence with NV model variant","text":"<p>For this test we demonstrate that our NV model variant has similar architecture, but uses gelus activations in the hyena layers as was originally intended, as well as convolutional bias in the short hyena convolutions as well as the medium and long layers. These changes result in a model that has similar training dynamics early in the process, but may have improved stability. </p> <p>As a baseline we compared to the original training run of Evo2 7b in the Savanna codebase here on W&amp;B </p>"},{"location":"models/evo2/#1b-training-equivalence-same-setup","title":"1b training equivalence (same setup)","text":"<p>We trained a 1b model with the same configuration as was used by savanna. We achieve a largely similar training curve for the first 6950 steps.  As a baseline we compared to a to the original training run of Evo2 1b in the Savanna codebase here on W&amp;B. Around step 6950 this run had a loss of between 1.18 and 1.2. </p>"},{"location":"models/geneformer/","title":"Geneformer","text":"<p>Training Code Moved to bionemo-recipes</p> <p>The 5D parallel training implementation for Geneformer has been migrated to a simplified TransformerEngine + FSDP implementation in bionemo-recipes. For training Geneformer models, please refer to the recipe in <code>bionemo-recipes/recipes/geneformer_native_te_mfsdp_fp8/</code>. This page contains model card information, checkpoint details, and benchmark results.</p>"},{"location":"models/geneformer/#model-overview","title":"Model Overview","text":""},{"location":"models/geneformer/#description","title":"Description:","text":"<p>Geneformer generates a dense representation of a scRNA cell by learning co-expression patterns within single cells. Geneformer is a tabular count model trained on scRNA from the Chan Zuckerberg CELLxGENE census. Geneformer computes a complete embedding for each cell over the top 1024 expressed genes. The embeddings are used as features for a variety of predictive tasks. This model is ready for both commercial and academic use.</p>"},{"location":"models/geneformer/#references","title":"References:","text":"<ul> <li>Geneformer, reference foundation model for single-cell RNA: Transfer learning enables predictions in network biology | Nature</li> <li>scGPT, alternative foundation model for single-cell RNA: scGPT: toward building a foundation model for single-cell multi-omics using generative AI | Nature Methods</li> <li>scBERT, alternative foundation model for single-cell RNA: scBERT as a large-scale pretrained deep language model for cell type annotation of single-cell RNA-seq data | Nature Machine Intelligence</li> <li>scFoundation, alternative foundation model for single-cell RNA: Large Scale Foundation Model on Single-cell Transcriptomics | bioRxiv   CELLxGENE census, public repository for scRNA experiments: CZ CELLxGENE Discover - Cellular Visualization Tool (cziscience.com)</li> </ul>"},{"location":"models/geneformer/#model-architecture","title":"Model Architecture:","text":"<p>Architecture Type: Bidirectional Encoder Representations from Transformers (BERT) Network Architecture: Geneformer </p>"},{"location":"models/geneformer/#input","title":"Input:","text":"<p>Input Type(s): Number (Row represents cell, containing gene names and single cell expression counts)  Input Format(s): Array AnnData Input Parameters: 1D </p>"},{"location":"models/geneformer/#output","title":"Output:","text":"<p>Output Type(s): Vector (Dense Embedding Predictions)embeddings.  Output Format: NumPy  Output Parameters: 1D  Other Properties Related to Output: Numeric floating point vector (fp16, bf16, or fp32); geneformer-10M-240530 outputs 256 dimensional embeddings; geneformer-106M-240530 outputs 768 dimensional embeddings </p>"},{"location":"models/geneformer/#software-integration","title":"Software Integration:","text":"<p>Runtime Engine(s):</p> <ul> <li>BioNeMo, NeMo 1.2 </li> </ul> <p>Supported Hardware Microarchitecture Compatibility: </p> <ul> <li>Ampere </li> <li>Hopper </li> <li>Volta </li> </ul> <p>[Preferred/Supported] Operating System(s): </p> <ul> <li>Linux </li> </ul>"},{"location":"models/geneformer/#model-versions","title":"Model Versions:","text":"<ul> <li>geneformer-10M-240530</li> <li>10.3M parameter Geneformer variant</li> <li>25429 ensemble ID based gene tokens</li> <li>256 hidden dimensions with 4 heads, 6 layers and a 512 dimensional FFN</li> <li>ReLU activation</li> <li>1e-12 EPS layernorm</li> <li>bf16 mixed precision training with 32 bit residual connections</li> <li>2% hidden dropout, 10% attention dropout</li> <li>geneformer-106M-240530</li> <li>106M parameter Geneformer variant</li> <li>25429 ensemble ID based gene tokens</li> <li>768 hidden dimensions with 12 heads, 12 layers and a 3072 dimensional FFN</li> <li>ReLU activation</li> <li>1e-12 EPS layernorm</li> <li>bf16 mixed precision training with 32 bit residual connections</li> <li>2% hidden dropout, 10% attention dropout</li> </ul>"},{"location":"models/geneformer/#training-evaluation","title":"Training &amp; Evaluation:","text":""},{"location":"models/geneformer/#training-dataset","title":"Training Dataset:","text":"<p>Single cell expression counts from CELLxGENE Census used for the direct download of data matching similar criteria to those described in the Geneformer publication. Limiting cell data to organism=\"Homo sapiens\", with a non \"na\" suspension_type, is_primary_data=True, and disease=\"normal\" to limit to non-diseased tissues that are also the primary data source per cell to make sure that cells are only included once in the download. We tracked metadata including \"assay\", \"sex\", \"development_stage\", \"tissue_general\", \"dataset_id\" and \"self_reported_ethnicity\". The metadata \"assay\", \"tissue_general\", and \"dataset_id\" were used to construct dataset splits into train, validation, and test sets.</p> <p>The training set represented 99% of the downloaded cells. We partitioned the data by dataset_id into a train set (99%) and a hold-out set (1%), to make sure that the hold-out datasets were independently collected single cell experiments, which helps evaluate generalizability to new future datasets.</p> <p>In this training split, we made sure that all \"assay\" and \"tissue_general\" labels were present in the training set so that our model would have maximal visibility into different tissues and assay biases.</p> <p>The 1% hold-out evaluation set was split further into a validation and test set. This final split was mostly done randomly by cell; however, we set aside a full dataset into the test split so that we could evaluate performance after training on a completely unseen dataset, including when monitoring the validation loss during training.</p> <p>Link: Datasets downloaded from CZ CELLxGENE Discover - Cellular Visualization Tool (cziscience.com) Data Collection Method by dataset</p> <ul> <li>[Human] </li> </ul> <p>Labeling Method by dataset</p> <ul> <li>Hybrid: Automated, Human </li> </ul> <p>Properties (Quantity, Dataset Descriptions, Sensor(s)): 23.64 million non-diseased and human-derived single cells were chosen from the CZI CELLxGENE census, which is characterized as follows: </p> <ul> <li>Assay Bias:</li> <li>The vast majority of the dataset is one of the 10x genomics assays. Approximately 20M of the 26M cells are genomic assays, 4M are sci-RNA-seq, while remaining assays (microwell-seq, drop-seq, bd rhapsody, smart-seq, seq-well, and MARS-seq) represent small fractions of the full datasets.</li> <li>Sex:</li> <li>12.5M are male-derived cells; 10M are female derived cells. The remaining cells are not annotated.</li> <li>Self-Reported Ethnicity:</li> <li>Approximately 12M cells are not annotated; 9M are annotated as \"European.\" .5M are annotated as \"Han Chinese.\" followed by \"African American\".</li> <li>Age Bias:</li> <li>The dataset is heavily biased toward donors less than one year. The next highest group would be the segment that includes ages 21-30.</li> <li>Tissue Type Bias:</li> <li>9M cells are \"brain\" derived. 4M are blood derived, followed by \"lung\", \"breast\", \"heart\" and \"eye\" at approximately 1M cells each.</li> </ul> <p>Dataset was derived from a limited number of public sources where methods and protocols may not represent sufficiently diverse sources to capture the full scope of gene expression.</p>"},{"location":"models/geneformer/#evaluation-dataset","title":"Evaluation Dataset:","text":"<p>Adamson et al 2016 PERTURB-seq dataset, accessed by Harvard dataverse. Link: adamson.zip - Harvard Dataverse Data Collection Method by dataset</p> <ul> <li>Human </li> </ul> <p>Labeling Method by dataset</p> <ul> <li>Automated - Molecular Barcoding </li> </ul> <p>Properties (Quantity, Dataset Descriptions, Sensor(s)): There are ~20k single cells, half of which represent unperturbed control samples, and the other half which contain an additional datatable containing the CRISPR knock-out targets for each cell.</p> <p>Link: CZ CELLxGENE Discover - Cellular Visualization Tool (cziscience.com) Data Collection Method by dataset</p> <ul> <li>Human </li> </ul> <p>Labeling Method by dataset</p> <ul> <li>Hybrid: Automated, Human </li> </ul> <p>Properties (Quantity, Dataset Descriptions, Sensor(s)):</p> <ul> <li>240,000 single cells were chosen from the CZI CELLxGENE census such that they did not share a <code>dataset_id</code> with any cell in the training data described previously.</li> </ul>"},{"location":"models/geneformer/#inference","title":"Inference:","text":"<p>Engine: BioNeMo, NeMo  Test Hardware: </p> <ul> <li>Ampere </li> <li>Hopper </li> <li>Volta </li> </ul> <p>*Additional description content may be included here</p>"},{"location":"models/geneformer/#ethical-considerations","title":"Ethical Considerations:","text":"<p>NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see our documentation on Explainability, Bias, Safety &amp; Security, and Privacy. Please report security vulnerabilities or NVIDIA AI Concerns here.</p>"},{"location":"models/geneformer/#training-diagnostics","title":"Training Diagnostics","text":""},{"location":"models/geneformer/#geneformer-10m","title":"geneformer-10M","text":"<p>Training was performed on 8 servers with 8 A100 GPUs each for a total of 81485 steps using the CELLxGENE split with a per-GPU micro batch size 32 and global batch size of 2048. Training took a total of 4 days, 8 hours of wallclock time. As can be seen in the following images, training and validation curves both decreased fairly smoothly throughout the course of training.</p> <p> </p>"},{"location":"models/geneformer/#geneformer-106m","title":"geneformer-106M","text":"<p>This checkpoint was trained for approximately 35,650 steps using the CELLxGENE split. Training was performed on 16 servers with 8 A100 GPUs each for a total of 35,650 steps using the CELLxGENE split with a per-GPU micro batch size 16 and global batch size of 2,048. Training took a total of 8 hours of wallclock time. As can be seen in the following image, training and validation curves both decreased fairly smoothly throughout the course of training.</p> <p> </p>"},{"location":"models/geneformer/#benchmarking","title":"Benchmarking","text":""},{"location":"models/geneformer/#accuracy-benchmarks","title":"Accuracy Benchmarks","text":""},{"location":"models/geneformer/#masked-language-model-mlm-loss","title":"Masked Language Model (MLM) Loss","text":"<p>The following describes the BERT MLM token loss. Like in the original BERT paper, and the Geneformer paper, 15% of all tokens are included in the loss. Of the included tokens, 80% are <code>\"[MASK]\"</code> token, 2% are a random gene token, and 18% are the correct output token. Note that this was an unintentional deviation from the original publication, but so far it seems to be working well. In the future, we will test the intended 80%/10%/10% mixture proposed in the paper. The token loss in the following table is the mean cross entropy loss of the 15% of tokens included in the loss mask averaged across cells. As a baseline, Geneformer was downloaded from the ctheodoris/Geneformer page on HuggingFace on 2024/11/04 and applied to the same masking/unmasking problem on this dataset, but with model-specific cell representations due to the updated tokenizer and medians dictionary used to train, and the update from training with 2048 tokens to 4096 tokens per cell. The held-out <code>test</code> dataset from our training splits described previously was used, and it should be noted that some of these cells may have been involved in training the baseline Geneformer.</p> Model Description Token Loss (lower is better) Baseline Geneformer 3.206* geneformer-10M-240530 3.18 geneformer-106M-240530 2.89 <p>Baseline Geneformer was recently updated on HuggingFace making loss comparisons challenging.</p> <pre><code>[Geneformer](https://huggingface.co/ctheodoris/Geneformer) was recently updated on HuggingFace to a new version.\nIn a future release we will make checkpoint conversion scripts available so that the public model can be ran\ndirectly. Some key differences follow:\n\n * Trained on a much larger 95M cell dataset. Our current checkpoints were trained with 23M cells.\n * The new 12 layer baseline Geneformer variant sits between our 10M and 106M parameter models in parameter count with\n  approximately 38M parameters.\n * The model is trained with a 4096 context rather than a 2048 context. When forcing the model to make predictions\n  with a 2048 context, the MLM loss drops to *2.76*, which is probably unfair because this may be \"out of domain\" for\n  training. It is really hard to compare these loss numbers directly is the only take-home here.\n * The model was trained on a set of 20,275 genes, rather than the older set of 25,426 genes. This would also be\n  expected to give a boost in loss since there are fewer tokens to choose from.\n</code></pre>"},{"location":"models/geneformer/#downstream-task-accuracy","title":"Downstream Task Accuracy","text":"<p>Here we benchmark four models, with two baselines. These models are tasked with cell type classification, using the Crohn's disease small intestine dataset from Elmentaite et al. (2020), Developmental Cell. This dataset contains approximately 22,500 single cells from both healthy children aged 4-13 and children with Crohn's disease. This dataset contains 31 unique cell types which we assume to be annotated accurately. This dataset was held out of our pre-training dataset as all diseased samples were removed.</p> <ul> <li>Baseline 1) scRNA workflow: this model uses PCA with 10 components and random forest on normalized and log transformed expression counts to produce a result.</li> <li>Baseline 2) geneformer-qa, a model trained for approximately 100 steps with approximately random weights. We expect this model to perform no differently than working on counts directly.</li> <li>geneformer-10M-240530 and geneformer-106M-240530 as described above.</li> </ul> <p>For more details see the example notebook titled Geneformer-celltype-classification-example.ipynb</p> <p> </p>"},{"location":"models/geneformer/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>The 106M parameter variant of Geneformer achieves over 50 TFLOPS per GPU during training. This is consistent whether trained with 1 or 8 A100s.</p> <p></p> <p>Performance will increase if the <code>num_dataset_workers</code> and the <code>micro_batch_size</code> are set appropriately. For the above metrics, we set <code>num_dataset_workers=8</code>. For the 10m model, set <code>micro_batch_size=120</code> and for the 106m model set the <code>micro_batch_size=16</code>. This will enable you to achieve similar performance results.</p>"},{"location":"models/ESM-2/","title":"ESM-2","text":""},{"location":"models/ESM-2/#model-overview","title":"Model Overview","text":""},{"location":"models/ESM-2/#description","title":"Description","text":"<p>ESM-2 is a pre-trained, bi-directional encoder (BERT-style model) over amino acid sequences. ESM-2 models provide embeddings for amino acids that have led to state-of-the-art performance on downstream tasks such as structure and function prediction. ESM-2 has been trained at a number of different model sizes. BioNeMo2 includes converted checkpoints for the 650M and 3B parameter variants. The 650M model has 33 layers, 20 attention heads, and a hidden space dimension of 1280. The 3B model has 36 layers, 40 attention heads, and a hidden space dimension of 2,560.</p> <p>These models are ready for commercial use.</p>"},{"location":"models/ESM-2/#third-party-community-consideration","title":"Third-Party Community Consideration","text":"<p>This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party's requirements for this application and use case [1]; see link to Non-NVIDIA Model Card for ESM-2 3B model and Non-NVIDIA Model Card for ESM-2 650M model</p>"},{"location":"models/ESM-2/#references","title":"References","text":"<p>[1] Lin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., Smetanin, N., Verkuil, R., Kabeli, O., Shmueli, Y. and dos Santos Costa, A., 2023. Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, 379(6637), pp.1123-1130.</p> <p>[2] \"UniProt: the universal protein knowledgebase in 2021.\" Nucleic acids research 49, no. D1 (2021): D480-D489.</p> <p>[3] Devlin, J., Chang, M.W., Lee, K. and Toutanova, K., 2018. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>"},{"location":"models/ESM-2/#model-architecture","title":"Model Architecture","text":"<p>Architecture Type: BERT</p> <p>Network Architecture: ESM-2</p>"},{"location":"models/ESM-2/#input","title":"Input","text":"<p>Input Type(s): Text (Protein Sequences)</p> <p>Input Parameters: 1D</p> <p>Other Properties Related to Input: Protein sequence represented as a string of canonical amino acids, of maximum length 1022. Longer sequences are automatically truncated to this length.</p>"},{"location":"models/ESM-2/#output","title":"Output","text":"<p>Output Type(s): Embeddings (Amino acid and sequence-level)</p> <p>Output Parameters: 1D</p> <p>Other Properties Related to Output: Numeric vector with floating-point values corresponding to an embedding for each amino acid in the input protein sequence. Maximum output length is 1022 embeddings - one embedding vector per amino acid.</p>"},{"location":"models/ESM-2/#software-integration","title":"Software Integration","text":"<p>Runtime Engine(s)</p> <ul> <li>BioNeMo, NeMo, Megatron, TransformerEngine</li> </ul> <p>Supported Hardware Microarchitecture Compatibility</p> <ul> <li>NVIDIA Ampere</li> <li>NVIDIA Hopper</li> <li>NVIDIA Volta</li> </ul> <p>[Preferred/Supported] Operating System(s)</p> <ul> <li>Linux</li> </ul>"},{"location":"models/ESM-2/#model-versions","title":"Model Versions","text":"<ul> <li>esm2/650m:2.0</li> <li>esm2/3b:2.0</li> </ul>"},{"location":"models/ESM-2/#training-evaluation","title":"Training &amp; Evaluation","text":""},{"location":"models/ESM-2/#training-dataset","title":"Training Dataset","text":"<p>Original ESM-2 checkpoints from HuggingFace were trained with the UniProt 2021_04 sequence database. For more details on the training dataset, see Lin et al. 2023. The train/test splits used by the original authors were not distributed. A pre-training database compiled by NVIDIA following a similar approach is described in UniProt Dataset.</p>"},{"location":"models/ESM-2/#inference","title":"Inference","text":"<p>Engine: BioNeMo, NeMo</p> <p>Test Hardware</p> <ul> <li>NVIDIA Ampere</li> <li>NVIDIA Hopper</li> <li>NVIDIA Volta</li> </ul>"},{"location":"models/ESM-2/#license","title":"License","text":"<p>ESM-2 is provided under the Apache 2.0 license.</p>"},{"location":"models/ESM-2/#competitive-benchmarking","title":"Competitive Benchmarking","text":""},{"location":"models/ESM-2/#accuracy","title":"Accuracy","text":"<p>A validation set of 328,360 UniRef50 representative sequences were randomly selected from UniRef 2024_03 (see UniProt Dataset). This validation set was used to ensure that the output of BioNeMo-converted checkpoints is consistent with their outputs when evaluated with the HuggingFace Transformers library.</p> Checkpoint HuggingFace BioNeMo2 Lin et al. 2023 650M 7.001 7.002 6.95  3B 6.003 6.004 6.49  <p>Different Validation Sets</p> <pre><code>The HuggingFace and converted BioNeMo2 checkpoints were evaluated on a newly curated validation set. Perplexities\nfrom Lin *et al.* 2023 are reported for comparison, but the original train/test splits are not available.\n</code></pre>"},{"location":"models/ESM-2/#training-performance","title":"Training Performance","text":""},{"location":"models/ESM-2/#single-node-training-performance","title":"Single-node Training Performance","text":"<p>The pure-PyTorch baseline (compiled with <code>torch.compile()</code>) raised an out-of-memory error for batch sizes larger than 16 at the ESM2-650M model size. The <code>bionemo2</code> model could handle batch sizes of 46, reaching a model FLOPs utilization of 59.2% on an NVIDIA A100.</p>"},{"location":"models/ESM-2/#model-scaling","title":"Model Scaling","text":"<p>Training ESM-2 at the 650M, 3B, and 15B model variants show improved performance with the BioNeMo2 framework over the pure-PyTorch baseline. These experiments were conducted on 16x NVIDIA A100 or 16x NVIDIA H100 GPUs split across two nodes. <sup>*</sup>Note: 15B model variants were trained on 64 GPUs with the BioNeMo2 framework.</p>"},{"location":"models/ESM-2/#device-scaling","title":"Device Scaling","text":"<p>Training ESM-3B on 256 NVIDIA A100s on 32 nodes achieved 96.85% of the theoretical linear throughput expected from extrapolating single-node (8 GPU) performance, representing a model flops utilization of 60.6% at 256 devices.</p>"},{"location":"models/ESM-2/#lora-fine-tuning-performace","title":"LoRA Fine-tuning Performace","text":"<p>Fine-tuning ESM-3B and ESM-650M with LoRA achieves improvements in GPU utilization and training time over fine-tuning a full ESM2 model. In models with LoRA, the encoder and embedding layers are replaced with LoRA modules.</p>"},{"location":"models/ESM-2/#lora-gpu-memory-usage","title":"LoRA GPU Memory Usage","text":"<p>GPU memory usage decreases by a factor of 2.5 - 4 in a model fine-tuned with LoRA.</p> <p></p>"},{"location":"models/ESM-2/#lora-scaling","title":"LoRA Scaling","text":"<p>The number of tokens processed per second increases by 25-80%.</p> <p></p>"},{"location":"models/ESM-2/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Model Overview</li> <li>Pre-trained Checkpoints</li> </ul>"},{"location":"models/ESM-2/pre-training/","title":"Pre-training ESM-2","text":"<p>Pre-trained checkpoints for ESM-2 are available at the 8M, 650M, and 3B model sizes. These models were trained by the BioNeMo Framework team to reproduce the original training results from Lin et al., Science (2023), with more recent UniProt data and leveraging the BioNeMo training infrastructure. The full pre-training data and train/test splits are available.</p>"},{"location":"models/ESM-2/pre-training/#model-convergence","title":"Model Convergence","text":"<p>Validation perplexity evaluated on the NVIDIA validation set.</p> <p></p> Model Size Perplexity at 500K Updates 8M 10.26 650M 7.14 3B 6.42"},{"location":"models/ESM-2/pre-training/#pre-training-recipes","title":"Pre-training Recipes","text":"8M <pre><code>```python\nesm2_8m_ckpt_path = load(\"esm2/8m:2.0\")\n```\n\n### Training Script\n\n| Training Parameters     | Value  |\n| ----------------------- | ------ |\n| # of GPUs               | 32     |\n| GPU Type                | A100   |\n| Batch Size (per device) | 64     |\n\n```bash\ntrain_esm2 \\\n  --create-tensorboard-logger \\\n  --resume-if-exists \\\n  --wandb-project=&lt;wandb-project-name&gt; \\\n  --save-top-k=10 \\\n  --train-cluster-path=/data/train_clusters.parquet \\  # (1)!\n  --train-database-path=/data/train.db \\\n  --valid-cluster-path=/data/valid_clusters.parquet \\\n  --valid-database-path=/data/validation.db \\\n  --num-steps=500_000 \\\n  --metric-to-monitor-for-checkpoints=val_loss \\\n  --micro-batch-size=64 \\\n  --num-nodes=4 \\\n  --num-gpus=8 \\\n  --val-check-interval=10000 \\\n  --limit-val-batches=1.0 \\\n  --result-dir=/results/esm2_pretrain_8m \\\n  --experiment-name=esm2_pretrain_8m \\\n  --num-layers=6 \\\n  --hidden-size=320 \\\n  --num-attention-heads=20 \\\n  --ffn-hidden-size=1280;\n```\n\n1. Paths here must be mounted into the `bionemo-framework` docker image.\n</code></pre> 650M <pre><code>```python\nesm2_650m_ckpt_path = load(\"esm2/nv_650m:2.1\")\n```\n\n### Training Script\n\n| Training Parameters     | Value  |\n| ----------------------- | ------ |\n| # of GPUs               | 64     |\n| GPU Type                | H100   |\n| Batch Size (per device) | 32     |\n\n```bash\ntrain_esm2 \\\n  --create-tensorboard-logger \\\n  --resume-if-exists \\\n  --wandb-project=&lt;wandb-project-name&gt; \\\n  --save-top-k=10 \\\n  --train-cluster-path=/data/train_clusters.parquet \\  # (1)!\n  --train-database-path=/data/train.db \\\n  --valid-cluster-path=/data/valid_clusters.parquet \\\n  --valid-database-path=/data/validation.db \\\n  --num-steps=500_000 \\\n  --metric-to-monitor-for-checkpoints=val_loss \\\n  --micro-batch-size=32 \\\n  --num-nodes=8 \\\n  --num-gpus=8 \\\n  --val-check-interval=10000 \\\n  --limit-val-batches=1.0 \\\n  --result-dir=/results/esm2_pretrain_650m \\\n  --experiment-name=esm2_pretrain_650m \\\n  --min-seq-length=1024 \\\n  --max-seq-length=1024 \\\n  --num-layers=33 \\\n  --hidden-size=1280 \\\n  --num-attention-heads=20 \\\n  --ffn-hidden-size=5120;\n```\n\n1. Paths here must be mounted into the `bionemo-framework` docker image.\n</code></pre> 3B <pre><code>```python\nesm2_3b_ckpt_path = load(\"esm2/nv_3b:2.1\")\n```\n\n### Training Script\n\n| Training Parameters     | Value  |\n| ----------------------- | ------ |\n| # of GPUs               | 128    |\n| GPU Type                | H100   |\n| Batch Size (per device) | 16     |\n| Warmup Steps            | 20,000 |\n\n```bash\ntrain_esm2 \\\n  --create-tensorboard-logger \\\n  --resume-if-exists \\\n  --wandb-project=&lt;wandb-project-name&gt; \\\n  --save-top-k=10 \\\n  --train-cluster-path=/data/train_clusters.parquet \\  # (2)!\n  --train-database-path=/data/train.db \\\n  --valid-cluster-path=/data/valid_clusters.parquet \\\n  --valid-database-path=/data/validation.db \\\n  --num-steps=500_000 \\\n  --warmup-steps=20_000 \\  # (1)!\n  --metric-to-monitor-for-checkpoints=val_loss \\\n  --micro-batch-size=16 \\\n  --num-nodes=16 \\\n  --num-gpus=8 \\\n  --val-check-interval=2500 \\\n  --limit-val-batches=1.0 \\\n  --result-dir=/results/esm2_pretrain_3b \\\n  --experiment-name=esm2_pretrain_3b \\\n  --min-seq-length=1024 \\\n  --max-seq-length=1024 \\\n  --num-layers=36 \\\n  --hidden-size=2560 \\\n  --num-attention-heads=40 \\\n  --ffn-hidden-size=10240;\n```\n\n1. We had to increase the number of warmup steps 10x over the published training recipe for ESM-2 3B, which was\n   likely trained with fp16 precision. This gave us an overall similar initial curve, but avoided convergence issues\n   at around 2,000 steps.\n\n2. Paths here must be mounted into the `bionemo-framework` docker image.\n</code></pre>"}]}