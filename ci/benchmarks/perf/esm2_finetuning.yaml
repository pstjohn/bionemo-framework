scope: perf
time_limit: 3600
key_segments:
  # Modify keys to be renamed (str) or excluded (False) from run identifier. By default, all args under script_args are included.
  train_data_path: false
  valid_data_path: false
  data_base_path: false
  limit_val_batches: false
  limit_test_batches: false
  val_check_interval: false
  dataset_class: false
  task_type: false
  config_class: false
  num_workers: false
  experiment_name: false
  workspace: false
  restore_from_checkpoint_path: false
script_args:
  # All arguments referenced in the script string must be specified here.
  # Arguments not referenced in the script string must have the 'arg' field specified.
  # See jet/core/configs.py for the specification of the configuration class
  workspace: /workspace/bionemo2
  data_base_path: /data/FLIP
  restore_from_checkpoint_path: /data/esm2_650M_nemo2
  gpus: 8
  model: esm2
  variant: finetune
  config_name: 650M
  precision: [bf16-mixed]
  num_workers: 8
  limit_val_batches: 1
  limit_test_batches: 1
  task: seq_classification
  train_data_path: scl/train/x000.csv
  valid_data_path: scl/val/x000.csv
  task_type: classification
  config_class: ESM2FineTuneSeqConfig
  dataset_class: InMemorySingleValueDataset
  max_steps: 30000
  stop_steps: 300
  experiment_name: seq-level-classification
  val_check_interval: 100
  products:
    - nodes: 1
      batch_size: 16
      pp: 1
      tp: 1
    - nodes: 1
      batch_size: 64
      pp: 1
      tp: 1
    - nodes: 2
      batch_size: 16
      pp: 1
      tp: 1
    - nodes: 2
      batch_size: 64
      pp: 1
      tp: 1
script: |-
  WANDB_API_KEY=$BIONEMO_WANDB_API_KEY ${variant}_${model} \
    --train-data-path=${data_base_path}/${train_data_path} \
    --valid-data-path=${data_base_path}/${valid_data_path} \
    --restore-from-checkpoint-path=${restore_from_checkpoint_path} \
    --task-type=${task_type} \
    --config-class=${config_class} \
    --dataset-class=${dataset_class} \
    --num-steps=${max_steps} \
    --experiment-name=${experiment_name}_${batch_size}bs_${nodes}node_${gpus}gpu_${max_steps}s_${precision}prec_tp${tp}_pp_${pp} \
    --lr=0.0005 \
    --result-dir=${tensorboard_dir} \
    --micro-batch-size=${batch_size} \
    --limit-val-batches=${limit_val_batches} \
    --limit-test-batches=${limit_test_batches} \
    --precision=${precision} \
    --label-column=scl_label \
    --num-gpus=${gpus} \
    --num-nodes=${nodes} \
    --accumulate-grad-batches=1 \
    --val-check-interval=${val_check_interval} \
    --num-dataset-workers=${num_workers} \
    --wandb-project=${wandb_project_name} \
    --wandb-group=${model}_${variant}_${config_name}_${task}_${target} \
    --create-tensorboard-logger \
    --encoder-frozen \
    --mlp-ft-dropout=0.25 \
    --mlp-hidden-size=256 \
    --mlp-target-size=10 \
    --disable-checkpointing \
    --pipeline-model-parallel-size=${pp} \
    --tensor-model-parallel-size=${tp} \
    --early-stop-on-step=${stop_steps} \
    --create-tflops-callback;
