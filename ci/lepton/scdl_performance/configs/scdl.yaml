############################################################
# Template Type
# Defines the template type for the job.
# - convergence_tests: for convergence tests
# - scdl_performance: for SCDL performance tests
############################################################
job_name: scdl-performance-test
template_type: scdl_performance

############################################################
# Container Runtime
# Defines the base Docker image and registry auth needed
############################################################
container:
  image: nvcr.io/nvidia/pytorch:25.06-py3
  registry_auth: lepton-nvidia

############################################################
# Environment Variables
# These keys must be present for the job to authenticate with
# external services (W&B, Kratos, Lepton) and control runtime caching.
# HF_HOME is optional but recommended to speed up Hugging Face model loading.
############################################################
environment_variables:
  - name: WANDB_API_KEY
    value_from: JWILBER_WANDB_API_KEY
  - name: KRATOS_SSA_URL
    value_from: KRATOS_SSA_URL
  - name: KRATOS_SSA_CLIENT_ID
    value_from: KRATOS_SSA_CLIENT_ID
  - name: KRATOS_SSA_SECRET
    value_from: KRATOS_SSA_SECRET
  - name: LEP_LOGIN_CREDENTIALS
    value_from: LEP_LOGIN_CREDENTIALS

############################################################
# Lepton Cluster Selection & Node Group
# Select the GPU cluster where the job will run.
# - h100: yo-bom-lepton-001
# - h200: nv-int-multiteam-nebius-h200-01
# - a100: az-sat-lepton-001
# all clusters have cpu.small, cpu.medium, and cpu.large
############################################################
node_group: yo-bom-lepton-001

############################################################
# Shared Mounts
# Mount paths for accessing shared datasets, model checkpoints,
# or intermediate artifacts. The NFS source should match the cluster.
# - yo-bom-lepton-001 uses node-nfs:fs1
# - nv-int-multiteam-nebius-h200-01 uses node-nfs:lepton-shared-fs
############################################################
mount_from: node-nfs:fs1

mounts:
  - path: /BioNeMo
    mount_path: /data
    from_: ${mount_from}

num_nodes: 1
resource_shape: "my.cpu.large-40gb-mem"

############################################################
# Git Checkout Options
# Configure which version of the recipe to pull from GitHub.
# - `branch`: defaults to main
# - `commit_sha`: overrides branch if provided
############################################################
branch: main
commit_sha: ""

############################################################
# Checkout Script
# Standardized script to clone the BioNeMo repository and install
# dependencies before the training run starts. Child configs can
# inherit and reuse this logic without modification.
############################################################
checkout_script: |
  echo "pwd: $(pwd)"
  echo "ls ..: $(ls -la ..)"
  echo "ls ../data: $(ls -la ../data)"
  git clone https://github.com/NVIDIA/bionemo-framework.git
  cd bionemo-framework
  if [ -n "${commit_sha}" ]; then
    echo "Checking out commit: ${commit_sha}"
    git checkout "${commit_sha}"
  elif [ "${branch}" != "main" ]; then
    echo "Checking out branch: ${branch}"
    git checkout "${branch}"
  fi
  cd sub-packages/bionemo-scdl
  pip install -e .

############################################################
# Runtime Commands
# `run_script` runs after the checkout step.
# The `script` section combines the checkout step with the run step.
############################################################
run_script: |
  python simple-benchmark/scdl_speedtest.py \
    --json /workspace/benchmark.json \
    --max-time 120 \
    --warmup-time 30 \
    --generate-baseline \
    -i ../../../../data/scdl_performance/tahoe_data \
    --scdl-path ../../../../data/scdl_performance/all_tahoe_memmap

script: |
  ${checkout_script}
  ${run_script}
