[build-system]
# Learning: Pin setuptools < 80.0.0. Newer versions break some CUDA extension builds
# by removing legacy distutils functionality.
requires = ["setuptools>=64,<80.0.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "bionemo-evo2"
readme = "README.md"
description = "Library containing data preprocessing, training, and inference tooling for Evo2."
authors = [{ name = "BioNeMo Team", email = "bionemofeedback@nvidia.com" }]
requires-python = ">=3.10"
license = { file = "LICENSE" }
dynamic = ["version"]
dependencies = [
    # internal / standard
    "bionemo-noodles",
    "nvidia-modelopt[torch]>=0.37.0",

    # These sources are defined in [tool.uv.sources] below
    "bionemo-core",
    "megatron-bridge",
    "causal_conv1d",
    "nv-grouped-gemm",
    "megatron-core",
    "nvidia-resiliency-ext",
    "emerging_optimizers",
    "subquadratic-ops-torch-cu12",
    # External
    "polars", # needed by sharded_eden_dataset_provider
]

[project.optional-dependencies]
test = []

[project.scripts]
torchrun = "torch.distributed.run:main"
#infer_evo2 = "bionemo.evo2.run.infer:main"
train_evo2 = "bionemo.evo2.run.train:main"
#predict_evo2 = "bionemo.evo2.run.predict:main"
preprocess_evo2 = "bionemo.evo2.data.preprocess:main"
splice_evo2 = "bionemo.evo2.data.transcript_extraction:main"
evo2_convert_nemo2_to_mbridge = "bionemo.evo2.utils.checkpoint.nemo2_to_mbridge:main"
#evo2_convert_to_nemo2 = "bionemo.evo2.utils.checkpoint.convert_to_nemo:main"
#evo2_nemo2_to_hf = "bionemo.evo2.utils.checkpoint.nemo2_to_hf:main"
#evo2_remove_optimizer = "bionemo.evo2.utils.checkpoint.evo2_remove_optimizer:main"

[tool.setuptools.packages.find]
where = ["src"]
include = ["bionemo.*"]
namespaces = true
exclude = ["test*."]

[tool.setuptools.dynamic]
version = { file = "VERSION" }

# ------------------------------------------------------------------
# UV Configuration
# ------------------------------------------------------------------

[tool.uv]
cache-keys = [{ git = true }]
prerelease = "allow"

# Learning: This list matches the official Megatron-Bridge config.
# These packages must be built against the SYSTEM PyTorch (no isolation).
no-build-isolation-package = [
    "flash_mla",
    "flash-linear-attention",
    "causal-conv1d",
    "nv-grouped-gemm",
    "mamba-ssm",
    "transformer-engine",
    "transformer-engine-torch",
]

# Learning: The 'sys_platform == "never"' hack is the correct way
# to force uv to accept the container's pre-installed versions.
override-dependencies = [
    "nvidia-modelopt[torch]>=0.37.0",
    "torch; sys_platform == 'never'",
    "torchvision; sys_platform == 'never'",
    "triton; sys_platform == 'never'",
    "transformer-engine; sys_platform == 'never'",
    "transformer-engine[pytorch]; sys_platform == 'never'",
]

[tool.uv.sources]
# External dependencies with specific git tags/commits
causal_conv1d = { git = "https://github.com/Dao-AILab/causal-conv1d.git", tag = "v1.5.4" }
nv-grouped-gemm = { git = "https://github.com/fanshiqing/grouped_gemm", tag = "v1.1.4.post6" }

# Internal dependencies
bionemo-core = { git = "https://github.com/NVIDIA/bionemo-framework.git", branch = "main", subdirectory = "sub-packages/bionemo-core" }
nvidia-resiliency-ext = { git = "https://github.com/NVIDIA/nvidia-resiliency-ext.git", rev = "54f85fe422d296cf04ea524130014bd3a2c3add1" }  # pragma: allowlist secret

# Megatron Bundle. This points to a version that still supports the deprecated no_weight_decay_cond field until the API for an alternative has been finalized.
megatron-bridge = { git = "https://github.com/NVIDIA-NeMo/Megatron-Bridge.git", rev = "18ef1b61309dd45bc0535fb7c60064b9d8829a35" }  # pragma: allowlist secret
megatron-core = { git = "https://github.com/NVIDIA-NeMo/Megatron-Bridge.git", rev = "18ef1b61309dd45bc0535fb7c60064b9d8829a35", subdirectory = "3rdparty/Megatron-LM" }  # pragma: allowlist secret

[tool.uv.extra-build-dependencies]
warp-lang = ["wheel_stub"]
