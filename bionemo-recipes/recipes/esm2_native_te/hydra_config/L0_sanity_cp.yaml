defaults:
  - defaults
  - _self_

# Training config
model_tag: ./example_8m_checkpoint  # E.g., nvidia/esm2_t6_8M_UR50D or facebook/esm2_t6_8M_UR50D
num_train_steps: 250

# We want this on in CI/CD to validate that the script runs successfully with torch.compile.
# TODO(@jomitchell): Not sure if this works with CP yet.
use_torch_compile: false

# Whether to use context parallelism or not.
cp_size: 2

use_sequence_packing: true
dataset:
  tokenizer_name: ${model_tag}
  micro_batch_size: null
  num_workers: 0
  max_seq_length: 1024
  load_dataset_kwargs:
    path: "parquet"
    split: "train"
    data_files: "train.parquet"
  token_micro_batch_size: 1600
  pad_sequences_to_be_divisible_by: 16



# WandB config
wandb_init_args:
  name: "esm2_t6_8M_UR50D_mfsdp_sanity"
  mode: "offline"

# Learning rate scheduler config
lr_scheduler_kwargs:
  num_warmup_steps: 100

checkpoint:
  ckpt_dir: null
  resume_from_checkpoint: true
  save_every_n_steps: 50
  save_final_model: false

logger:
  frequency: 1
