# Training config
model_tag: ??? # E.g., meta-llama/Llama-3.2-1B or a local path
config_kwargs: # Arguments to pass to the AutoConfig.from_pretrained method
  trust_remote_code: true
  vocab_size: 256  # Overrides to the default config that comes from meta-llama/Llama-3.2-1B
  tie_word_embeddings: false
  eos_token_id: 0
  pad_token_id: 1
  bos_token_id: 2
  attn_input_format: "bshd"

num_train_steps: ???

# TODO: Once BIONEMO-2583 and BIONEMO-2719 are fixed, enable this by default and simplify training scripts to remove the
# meta-device conditional.
use_meta_device: false

# Whether to wrap the model in torch.compile. Note, this is currently not supported with mfsdp (BIONEMO-2977).
# We leave this off by default since we don't see much of a performance improvement with TE layers.
use_torch_compile: false

# Whether to use gradient checkpointing to trade compute for memory
use_gradient_checkpointing: false

use_sequence_packing: false

dataset:
  tokenizer_path: ${model_tag} # Set to the path of your tokenizer (e.g., ./example_checkpoint)
  micro_batch_size: 8
  num_workers: 1
  max_seq_length: 8192 # Window size for genomic sequences
  stride: 200 # Overlap for windowing
  buffer_size: 500_000 # Shuffle buffer size
  use_lazy_tokenization: true
  use_stateful_dataloader: false # Until https://github.com/pytorch/pytorch/pull/163102 is resolved with torchdata.
  load_dataset_kwargs:
    path: ???
    split: "train"
    streaming: True

# WandB config
wandb_init_args:
  name: ???
  project: null # Optional: set to your wandb project name

# mFSDP config
fully_shard_kwargs:
  zero_dp_strategy: "optim_grads_params"
  calculate_per_token_loss: false
  init_model_with_meta_device: ${use_meta_device}
  check_for_nan_in_grad: true
  grad_reduce_in_fp32: false
  preserve_fp32_weights: true
  overlap_grad_reduce: true
  overlap_param_gather: true
  sync_model_each_microbatch: true
  average_in_collective: false

# TransformerEngine FP8 config. See
# https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html for more information on
# supported formats.
fp8_config:
  enabled: false
  fp8_recipe: transformer_engine.common.recipe.DelayedScaling
  fp8_format: "HYBRID"
  fp8_recipe_kwargs: {}
  fp8_model_init_kwargs:
    enabled: false # If this is set to true, fp8_config.enabled must also be set to true.

# Optimizer config
adamw_kwargs:
  lr: 4e-4
  fused: true
  betas: [0.9, 0.98]
  eps: 1e-8
  weight_decay: 0.01

# Learning rate scheduler config
lr_scheduler_kwargs:
  num_warmup_steps: 2_000
  num_training_steps: 500_000

# Checkpoint config
checkpoint:
  ckpt_dir: ???
  save_final_model: true
  resume_from_checkpoint: true
  save_every_n_steps: 50

logger:
  frequency: 100
