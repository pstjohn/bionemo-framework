# Config to match the Llama-3.2-1B model pre-training experiments from https://github.com/facebookresearch/lingua.

defaults:
  - defaults
  - _self_

config_name_or_path: ./model_configs/meta-llama/Llama-3.2-1B

config_kwargs:
  attn_input_format: thd

use_sequence_packing: true

wandb:
  name: lingua-1b-te
  project: null # Optional: set to your wandb project name

num_train_steps: 60_000

dataset:
  tokenizer_name_or_path: nvidia/Llama-3.1-8B-Instruct-FP8
  micro_batch_size: 4
  num_workers: 8
  max_seq_length: 4096
  stride: 512
  buffer_size: 5_000
  use_lazy_tokenization: true
  use_stateful_dataloader: false
  mask_degenerate_bases: false
  uppercase_labels: false
  load_dataset_kwargs:
    path: "mlfoundations/dclm-baseline-1.0"
    data_dir: "global-shard_01_of_10"
    split: "train"
    streaming: True

adamw_kwargs:
  lr: .003
  fused: true
  betas: [0.9, 0.95]
  eps: 0.00000001
  weight_decay: 0.033

lr_scheduler_kwargs:
  num_warmup_steps: 5_000
  num_decay_steps: 55_000 # total_steps - num_warmup_steps = 60_000 - 5_000
  min_lr_ratio: 0.000001

# Checkpoint config
checkpoint:
  ckpt_dir: null
  save_final_model: true
  resume_from_checkpoint: true
  save_every_n_steps: 10_000

profiler:
  enabled: false
  schedule:
    wait: 125
    warmup: 125
    active: 10
    repeat: 1
