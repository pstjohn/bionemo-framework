# @package _global_

# Convergence test configuration with tiny Llama model (~1M params)
# Tests that the model can overfit on a small dataset
# Works with both DDP and FSDP2, single GPU or multi-GPU

defaults:
  - defaults
  - _self_

# Use tiny Llama config for fast convergence testing
model_tag: ./example_checkpoint

num_train_steps: 270_000

dataset:
  micro_batch_size: 1 # Conservative for single GPU
  load_dataset_kwargs:
    path: "arcinstitute/opengenome2"
    data_dir: "json/pretraining_or_both_phases"
    split: "train"
    streaming: true # Use streaming to avoid loading entire dataset into memory

adamw_kwargs:
  lr: 5e-4
  fused: true
  betas: [0.9, 0.98]
  eps: 1e-8
  weight_decay: 0.01

lr_scheduler_kwargs:
  num_warmup_steps: 20_000
  num_training_steps: 500_000

checkpoint:
  ckpt_dir: null # No checkpoints
  save_final_model: false # Don't save final model
  resume_from_checkpoint: false # Start fresh for convergence test
  save_every_n_steps: null # No intermediate checkpoints

logger:
  frequency: 100

# WandB configuration
wandb_init_args:
  project: "llama3-genomic-convergence"
  name: "tiny-llama-convergence-test"
  mode: "online"
  tags:
    - convergence-test
    - tiny-model
    - 1M-params
    - 8192-context

use_meta_device: false
use_torch_compile: false

fp8_config:
  enabled: false
  fp8_recipe: transformer_engine.common.recipe.DelayedScaling
  fp8_format: "HYBRID"
  fp8_recipe_kwargs: {}
  fp8_model_init_kwargs:
    enabled: false
