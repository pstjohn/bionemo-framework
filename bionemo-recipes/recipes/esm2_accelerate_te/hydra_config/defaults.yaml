model_tag: "nvidia/esm2_t6_8M_UR50D"
stop_after_n_steps: 500_000

dataset:
  tokenizer_name: ${model_tag}
  max_seq_length: 1024
  # TODO(BIONEMO-2783): Replace this with our ESM-2 parquet dataset when it's ready.
  train_load_dataset_kwargs:
    path: "chandar-lab/UR100P"
    split: "train"
    revision: "refs/convert/parquet"
    streaming: True
  eval_load_dataset_kwargs:
    path: "chandar-lab/UR100P"
    split: "test"
    revision: "refs/convert/parquet"
    streaming: True
  # Whether to truncate the eval dataset; HF Trainer will run the full eval dataset each eval step.
  # If set to an integer, the eval dataset will be truncated to that number of examples.
  truncate_eval_dataset: null

trainer:
  output_dir: "results"
  run_name: ???
  save_safetensors: True
  do_train: True
  do_eval: False
  per_device_train_batch_size: ???
  per_device_eval_batch_size: ???
  optim: "adamw_torch_fused"
  learning_rate: 4e-4
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.98
  max_grad_norm: 1.0
  adam_epsilon: 1e-8
  num_train_epochs: 1
  lr_scheduler_type: "linear"
  warmup_steps: 2_000
  max_steps: 500_000
  save_strategy: "steps"
  save_steps: 10_000
  eval_strategy: "steps"
  eval_steps: 10_000
  batch_eval_metrics: True
  save_total_limit: 5
  logging_steps: 10
  report_to: "none"
  bf16: True
  remove_unused_columns: False
  include_num_input_tokens_seen: True
  dataloader_num_workers: 8
  torch_compile: False
